{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "083d2f08-47fb-4061-ba48-74213a3d0be4",
   "metadata": {},
   "source": [
    "<center><br><br>\n",
    "<font size=6>üéì <b>Advanced Deep Learning - NLP Final Project</b></font><br>\n",
    "<font size=6>üìä <b>mDeBERTa-v3-base Training (EDA)</b></font><br>\n",
    "<font size=5>üë• <b>Group W</b></font><br><br>\n",
    "<b>Adi Shalit</b>, ID: <code>206628885</code><br>\n",
    "<b>Gal Gussarsky</b>, ID: <code>206453540</code><br><br>\n",
    "<font size=4>üìò Course ID: <code>05714184</code></font><br>\n",
    "<font size=4>üìÖ Spring 2025</font>\n",
    "<br><br>\n",
    "<hr style=\"width:60%; border:1px solid gray;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097d3b14-7869-4dfe-bea7-704e3e7071fc",
   "metadata": {},
   "source": [
    "# üìë Table of Contents\n",
    "\n",
    "- [Training EX4](#Training-EX4)\n",
    "- [Load best Model & Test EX4](#Load-Best-Model-EX4)\n",
    "- [Training EX5](#Training-EX5)\n",
    "- [Load best Model & Test EX5](#Load-Best-Model-EX5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a899a0-3c42-422a-85fe-94ef2056fdf7",
   "metadata": {},
   "source": [
    "# üöÄ Training **mDeBERTa-v3-base** (Exercise 4 Style)\n",
    "\n",
    "In this stage, we fine-tune **mDeBERTa-v3-base** for **5-class sentiment classification**.  \n",
    "DeBERTa improves upon BERT/RoBERTa by introducing **disentangled attention** (separating content & position) and **relative position embeddings**, which allow it to capture context and word order more effectively.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ **What we do here**\n",
    "- ‚ö° **Pretrained Backbone**: Start from `microsoft/mdeberta-v3-base`.\n",
    "- üîí **Layer Freezing**: Freeze most layers, unfreezing only the **last *k*** (with *k* optimized by Optuna).\n",
    "- üè∑ **Classification Head**: Add a linear classifier for **5 sentiment classes**.\n",
    "- üéõ **Hyperparameter Tuning**: Use **Optuna** to search learning rate, weight decay, batch size, etc.\n",
    "- ‚è≥ **Early Stopping**: Stop training when validation **F1** stops improving.\n",
    "- üìä **Experiment Tracking**: Log all runs with **Weights & Biases (W&B)**.\n",
    "- ‚öôÔ∏è **Training Tricks**: Mixed precision + gradient clipping for faster and more stable convergence.  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82d568f9d8b4a411",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T01:02:45.521322Z",
     "start_time": "2025-08-15T01:02:45.518356Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"   # force-disable TensorFlow backend\n",
    "# os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\" # optional: also disable Flax/JAX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbbc6f-e360-4c5f-b6ed-5c674336eb35",
   "metadata": {},
   "source": [
    "# Training EX4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T12:28:06.736419Z",
     "start_time": "2025-08-16T12:28:01.926289Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: setup & speed knobs ---\n",
    "# !pip  optuna==3.6.1 wandb==0.17.5\n",
    "\n",
    "import os, math, random, time\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "\n",
    "# -------- RTX 4090 performance switches --------\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = False   # allow fast kernels\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- W&B non-blocking defaults ---\n",
    "os.environ.setdefault(\"WANDB_MODE\", \"online\")  # set to \"offline\" if you prefer\n",
    "WANDB_PROJECT = \"adv-dl-deberta-v3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab90839-6811-4a0c-8d11-3129eefecef1",
   "metadata": {},
   "source": [
    "# üåç mDeBERTa-v3-base Description\n",
    "\n",
    "After first experimenting with **RoBERTa-base**, we now move to a stronger multilingual model:  \n",
    "**mDeBERTa-v3-base** (multilingual DeBERTa v3).  \n",
    "\n",
    "---\n",
    "\n",
    "## üîé Why mDeBERTa?\n",
    "- üß† **Better architecture** ‚Üí DeBERTa introduces **disentangled attention** (separating content vs. position info) and **relative position embeddings**, which help the model better capture context and word order.  \n",
    "- ‚ö° **DeBERTa v3** ‚Üí improves pretraining with **ELECTRA-style gradient-disentangled embedding sharing**, making training more efficient and boosting accuracy.  \n",
    "- üåê **Multilingual support** ‚Üí pretrained on **CC100 dataset** (~2.5T tokens, 250k vocab), enabling cross-lingual transfer.  \n",
    "- üìè **Model size** ‚Üí 12 Transformer layers, hidden size 768, ~279M parameters (all trainable in our setup).  \n",
    "\n",
    "---\n",
    "\n",
    "## üèãÔ∏è Training Plan\n",
    "Initially, we considered training also on the **original untranslated dataset**, but due to time and complexity we decided to **focus only on the clean translated version**.  \n",
    "Thus, the final plan is:  \n",
    "\n",
    "- ‚úÖ **Train** on the **clean translated training data**.  \n",
    "- ‚úÖ **Evaluate** on the **clean translated test set** (same conditions as training).  \n",
    "\n",
    "‚ú® This ensures a controlled, consistent training pipeline, while letting us directly compare mDeBERTa‚Äôs performance with RoBERTa under the same clean setup.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011abe5f-08fc-4542-a7eb-c84fcc54994c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 278,813,189 / 278,813,189 (100.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/mdeberta-v3-base\", num_labels=5\n",
    ").to(DEVICE)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {trainable_params:,} / {total_params:,} \"\n",
    "      f\"({100.0*trainable_params/total_params:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c300ee-7387-4bd1-b5b2-3791eed96797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(251000, 768, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (pos_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f60b56-a550-4651-a85f-3f188e68c3e4",
   "metadata": {},
   "source": [
    "# üìä First Training Results ‚Äì Hyperparameter Insights\n",
    "\n",
    "https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2?nw=nwuseradishalit1\n",
    "\n",
    "In the **first training phase** with **mDeBERTa-v3-base**, we used **Optuna** to explore a wide hyperparameter space.  \n",
    "Although prior research suggests that DeBERTa works best with learning rates around **1e-5 ‚Äì 6e-5**,  \n",
    "we intentionally started with **much wider bounds (1e-6 ‚Äì 5e-5)** to test stability and robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Key Observations\n",
    "- **‚≠ê Best Trial (Trial 2):**  \n",
    "  - Validation F1 = **0.88022**  \n",
    "  - Learning rate = **3.5e-5**  \n",
    "  - Batch size = **8**  \n",
    "  - Weight decay = **9.4e-5**  \n",
    "  - **All 12 encoder layers unfrozen**  \n",
    "\n",
    "  üîπ This score is almost identical to the **real-world English benchmark performance (~88.2 F1)**, confirming our setup reproduces strong results.\n",
    "\n",
    "---\n",
    "\n",
    "### üîë Hyperparameter Insights\n",
    "- **Learning rate:**  \n",
    "  - Best range: **3e-5 ‚Äì 9e-5**  \n",
    "  - Too high ‚Üí unstable training  \n",
    "  - Too low (<1e-5) ‚Üí stable but plateaued\n",
    "\n",
    "- **Unfrozen layers:**  \n",
    "  - **9‚Äì12 layers unfrozen** performed best  \n",
    "  - Full fine-tuning > partial freezing\n",
    "\n",
    "- **Weight decay:**  \n",
    "  - Small values (<1e-5) gave stability  \n",
    "  - Very large ‚Üí degraded performance\n",
    "\n",
    "- **Batch size:**  \n",
    "  - **8‚Äì16** = stable  \n",
    "  - Larger sizes didn‚Äôt improve performance\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "- üîé Narrow LR search to **1e-5 ‚Äì 5e-5** (sweet spot).  \n",
    "- ‚úÖ Stick with **full fine-tuning (12 layers)**.  \n",
    "- üìâ Use **low weight decay (<1e-5)**.  \n",
    "- üîÑ Retrain with **refined bounds** for a cleaner checkpoint and more reliable results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11df79577da5e5c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T23:23:24.278932Z",
     "start_time": "2025-08-15T23:23:24.271361Z"
    }
   },
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # ADV DL ‚Äì Part B: Monolingual baseline (RoBERTa) ‚Äì Exercise-4 style\n",
    "# # Custom loop + early stopping + W&B + Optuna ONLY; freeze base, unfreeze last k layers\n",
    "# # Uses df_train / df_test with columns: OriginalTweet (str), Sentiment (str)\n",
    "# # =========================\n",
    "\n",
    "# import os, math, random, time, json\n",
    "# from typing import Dict, List, Tuple\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# # ---- deps ----\n",
    "# # !pip -q install transformers==4.43.3 optuna==3.6.1 wandb==0.17.5 >/dev/null\n",
    "\n",
    "# import transformers\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModelForSequenceClassification,\n",
    "#     DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "# )\n",
    "\n",
    "# import optuna\n",
    "# import wandb\n",
    "\n",
    "# # # from google.colab import drive\n",
    "# # drive.mount(\"/content/drive\")\n",
    "# # DRIVE_OUT_DIR = \"/content/drive/MyDrive/adv_dl_models\"\n",
    "# # os.makedirs(DRIVE_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# # -------------------------\n",
    "# # Constants (no CFG, Optuna-only workflow)\n",
    "# # -------------------------\n",
    "# MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "# MAX_LEN = 512\n",
    "# BATCH_SIZE = 16\n",
    "# WARMUP_RATIO = 0.06\n",
    "# GRAD_CLIP = 1.0\n",
    "# USE_AMP = True\n",
    "# PROJECT = \"adv-dl-p2\"\n",
    "# BASE_RUN_NAME = \"microsoft/mdeberta-v3-base_full_ex_4\"\n",
    "# TRIALS = 12\n",
    "# SEED = 42\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed); np.random.seed(seed)\n",
    "#     torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set_seed(SEED)\n",
    "\n",
    "# # -------------------------\n",
    "# # Label mapping (5-way sentiment)\n",
    "# # -------------------------\n",
    "# CANON = {\n",
    "#     \"extremely negative\": \"extremely negative\",\n",
    "#     \"negative\": \"negative\",\n",
    "#     \"neutral\": \"neutral\",\n",
    "#     \"positive\": \"positive\",\n",
    "#     \"extremely positive\": \"extremely positive\",\n",
    "# }\n",
    "# ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "# LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "# ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "# def normalize_label(s: str) -> str:\n",
    "#     s = str(s).strip().lower()\n",
    "#     s = s.replace(\"very negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"very positive\", \"extremely positive\")\n",
    "#     s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "#     return CANON.get(s, s)\n",
    "\n",
    "# # -------------------------\n",
    "# # Expect df_train, df_test in memory\n",
    "# # -------------------------\n",
    "# assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns, \"df_train missing required columns\"\n",
    "# assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns, \"df_test missing required columns\"\n",
    "\n",
    "# def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df = df.copy()\n",
    "#     df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "#     df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "#     df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "#     df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "#     df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "#     return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "# dftrain_ = prep_df(df_train)\n",
    "# dftest_  = prep_df(df_test)\n",
    "\n",
    "# train_df, val_df = train_test_split(\n",
    "#     dftrain_, test_size=0.1, stratify=dftrain_[\"label\"], random_state=SEED\n",
    "# )\n",
    "# print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n",
    "\n",
    "# # -------------------------\n",
    "# # Dataset & Collator\n",
    "# # -------------------------\n",
    "# class TweetDataset(Dataset):\n",
    "#     def __init__(self, df: pd.DataFrame, tokenizer: transformers.PreTrainedTokenizerBase, max_len: int):\n",
    "#         self.texts = df[\"text\"].tolist()\n",
    "#         self.labels = df[\"label\"].tolist()\n",
    "#         self.tok = tokenizer\n",
    "#         self.max_len = max_len\n",
    "#     def __len__(self): return len(self.texts)\n",
    "#     def __getitem__(self, idx):\n",
    "#         enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "#         enc[\"labels\"] = self.labels[idx]\n",
    "#         return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# train_ds = TweetDataset(train_df, tokenizer, MAX_LEN)\n",
    "# val_ds   = TweetDataset(val_df, tokenizer, MAX_LEN)\n",
    "# test_ds  = TweetDataset(dftest_, tokenizer, MAX_LEN)\n",
    "\n",
    "# collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "# # -------------------------\n",
    "# # Model & Freeze/Unfreeze strategy\n",
    "# # -------------------------\n",
    "# def build_model(num_unfreeze_last_layers: int = 4):\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         MODEL_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "#     )\n",
    "#     base = getattr(model, \"roberta\", None) or getattr(model, \"bert\", None) or getattr(model, \"deberta\", None)\n",
    "#     if base is not None:\n",
    "#         for p in base.parameters(): p.requires_grad = False\n",
    "#         if hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "#             k = num_unfreeze_last_layers\n",
    "#             if k > 0:\n",
    "#                 for layer in base.encoder.layer[-k:]:\n",
    "#                     for p in layer.parameters(): p.requires_grad = True\n",
    "#     for p in model.classifier.parameters(): p.requires_grad = True\n",
    "#     return model.to(DEVICE)\n",
    "\n",
    "# # -------------------------\n",
    "# # Train / Eval utilities\n",
    "# # -------------------------\n",
    "# def get_optimizer_scheduler(model, num_training_steps: int, lr: float, weight_decay: float):\n",
    "#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "#     ]\n",
    "#     optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "#     num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps)\n",
    "#     return optimizer, scheduler\n",
    "\n",
    "# def evaluate(model, loader) -> Dict[str, float]:\n",
    "#     model.eval()\n",
    "#     preds, labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "#             logits = model(**batch).logits\n",
    "#             preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "#             labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "#     return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "# def train_one_run(hp: Dict) -> Tuple[str, Dict[str, float]]:\n",
    "#     \"\"\"\n",
    "#     hp keys: run_name, num_unfreeze_last_layers, lr, weight_decay, epochs, patience, trial_number\n",
    "#     \"\"\"\n",
    "#     run_name = hp[\"run_name\"]\n",
    "#     num_unfreeze = int(hp[\"num_unfreeze_last_layers\"])\n",
    "#     lr = float(hp[\"lr\"])\n",
    "#     wd = float(hp[\"weight_decay\"])\n",
    "#     epochs = int(hp[\"epochs\"])\n",
    "#     patience = int(hp[\"patience\"])\n",
    "\n",
    "#     model = build_model(num_unfreeze)\n",
    "#     total_steps = int(math.ceil(len(train_loader) * epochs))\n",
    "#     optimizer, scheduler = get_optimizer_scheduler(model, total_steps, lr, wd)\n",
    "\n",
    "#     scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
    "#     best_metric = -1.0\n",
    "#     best_path = f\"best_{run_name}.pt\"\n",
    "#     no_improve = 0\n",
    "#     wandb_run = wandb.init(\n",
    "#         project=PROJECT,\n",
    "#         name=run_name,\n",
    "#         config={\n",
    "#             \"model\": MODEL_NAME,\n",
    "#             \"max_len\": MAX_LEN,\n",
    "#             \"batch_size\": BATCH_SIZE,\n",
    "#             \"epochs\": epochs,\n",
    "#             \"lr\": lr,\n",
    "#             \"weight_decay\": wd,\n",
    "#             \"warmup_ratio\": WARMUP_RATIO,\n",
    "#             \"grad_clip\": GRAD_CLIP,\n",
    "#             \"num_unfreeze_last_layers\": num_unfreeze,\n",
    "#             # >>> ADDED ‚Äî purely for tracking the Optuna suggestion (even if not used by loaders)\n",
    "#             \"suggested_batch_size\": hp.get(\"batch_size\", BATCH_SIZE),\n",
    "#             \"trial_number\": hp.get(\"trial_number\", -1),\n",
    "#         },\n",
    "#         reinit=True,\n",
    "#     )\n",
    "\n",
    "#     # >>> ADDED ‚Äî define step metrics so W&B charts look great\n",
    "#     wandb.define_metric(\"epoch\")\n",
    "#     wandb.define_metric(\"step\")\n",
    "#     wandb.define_metric(\"train/*\", step_metric=\"step\")\n",
    "#     wandb.define_metric(\"val/*\",   step_metric=\"epoch\")\n",
    "\n",
    "#     # >>> ADDED ‚Äî visibility of trainable params\n",
    "#     total_params     = sum(p.numel() for p in model.parameters())\n",
    "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     print(f\"Trainable params: {trainable_params:,} / {total_params:,} \"\n",
    "#           f\"({100.0*trainable_params/total_params:.2f}%) ; unfreeze_last_k={num_unfreeze}\")\n",
    "#     wandb.log({\n",
    "#         \"params/total\": total_params,\n",
    "#         \"params/trainable\": trainable_params,\n",
    "#         \"params/ratio\": trainable_params / max(1, total_params),\n",
    "#     }, step=0)\n",
    "\n",
    "\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         t0 = time.time()\n",
    "#         running_loss = 0.0\n",
    "#         for step, batch in enumerate(train_loader):\n",
    "#             batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "#                 outputs = model(**batch)\n",
    "#                 loss = outputs.loss\n",
    "#             scaler.scale(loss).backward()\n",
    "#             if GRAD_CLIP is not None:\n",
    "#                 scaler.unscale_(optimizer)\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "#             scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             if step % 20 == 0:\n",
    "#                 wandb.log({\"train/loss\": loss.item(), \"step\": step + 1, \"epoch\": epoch + 1})\n",
    "\n",
    "#         # epoch-end validation\n",
    "#         val_metrics = evaluate(model, val_loader)\n",
    "#         elapsed = time.time() - t0\n",
    "\n",
    "#         epoch_loss = running_loss / max(1, len(train_loader))\n",
    "#         current_lr = scheduler.get_last_lr()[0]\n",
    "#         wandb.log({\n",
    "#             \"train/epoch_loss\": epoch_loss,\n",
    "#             \"val/acc\": val_metrics[\"acc\"],\n",
    "#             \"val/precision\": val_metrics[\"precision\"],\n",
    "#             \"val/recall\": val_metrics[\"recall\"],\n",
    "#             \"val/f1\": val_metrics[\"f1\"],\n",
    "#             \"lr\": current_lr,\n",
    "#             \"time/epoch_sec\": elapsed,\n",
    "#             \"epoch\": epoch + 1,\n",
    "#         })\n",
    "\n",
    "#         # Early stopping on val f1\n",
    "#         if val_metrics[\"f1\"] > best_metric:\n",
    "#             best_metric = val_metrics[\"f1\"]\n",
    "#             torch.save(model.state_dict(), best_path)\n",
    "#             no_improve = 0\n",
    "#             wandb_run.summary[\"best_val_f1\"] = best_metric\n",
    "#             wandb_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "#         else:\n",
    "#             no_improve += 1\n",
    "#             if no_improve >= patience:\n",
    "#                 print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#                 break\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "#               f\"loss={epoch_loss:.4f} | \"\n",
    "#               f\"val_acc={val_metrics['acc']:.4f} | val_f1={val_metrics['f1']:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "#     wandb.finish()\n",
    "\n",
    "#     # Load best and return path + metrics on val for reference\n",
    "#     model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "#     final_val = evaluate(model, val_loader)\n",
    "#     return best_path, final_val\n",
    "\n",
    "# # -------------------------\n",
    "# # Optuna hyperparameter tuning (ALWAYS ON)\n",
    "# # -------------------------\n",
    "\n",
    "# # Constants\n",
    "# FIXED_EPOCHS = 12\n",
    "# FIXED_PATIENCE = 4\n",
    "\n",
    "\n",
    "# def objective(trial: optuna.trial.Trial):\n",
    "#     params = {\n",
    "#         \"run_name\": f\"{BASE_RUN_NAME}_optuna_trial_{trial.number}\",\n",
    "#         \"num_unfreeze_last_layers\": trial.suggest_int(\"num_unfreeze_last_layers\", 4, 12),\n",
    "#         \"lr\": trial.suggest_float(\"lr\", 1e-6, 5e-5, log=True),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-1, log=True),\n",
    "#         \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64]),\n",
    "#         \"epochs\": FIXED_EPOCHS,\n",
    "#         \"patience\": FIXED_PATIENCE,\n",
    "#         \"trial_number\": trial.number,\n",
    "#     }\n",
    "#     path, val_metrics = train_one_run(params)\n",
    "#     # report intermediate value for pruning if enabled\n",
    "#     trial.report(val_metrics[\"f1\"], step=1)\n",
    "#     return val_metrics[\"f1\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)\n",
    "# print(\"Best trial:\", study.best_trial.number, \"F1:\", study.best_value)\n",
    "# best_params = {\"run_name\": f\"{BASE_RUN_NAME}_best_optuna\", **study.best_trial.params}\n",
    "\n",
    "# # Retrain best config to get a clean checkpoint\n",
    "# best_ckpt, _ = train_one_run(best_params)\n",
    "# best_path = best_ckpt\n",
    "\n",
    "# # -------------------------\n",
    "# # Final evaluation on TEST (+ W&B logging)\n",
    "# # -------------------------\n",
    "# model = build_model(best_params[\"num_unfreeze_last_layers\"])\n",
    "# model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "# model.eval()\n",
    "\n",
    "# all_preds, all_labels = [], []\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "#         logits = model(**batch).logits\n",
    "#         all_preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "#         all_labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "\n",
    "# acc = accuracy_score(all_labels, all_preds)\n",
    "# p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "# print(f\"\\nTEST | acc={acc:.4f} | f1_macro={f1:.4f} | precision_macro={p:.4f} | recall_macro={r:.4f}\\n\")\n",
    "\n",
    "# print(\"Per-class report (ids map to labels):\")\n",
    "# print(ID2LABEL)\n",
    "# report = classification_report(\n",
    "#     all_labels, all_preds,\n",
    "#     target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "#     zero_division=0, output_dict=True\n",
    "# )\n",
    "# print(classification_report(\n",
    "#     all_labels, all_preds,\n",
    "#     target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "#     zero_division=0\n",
    "# ))\n",
    "\n",
    "# # ---- W&B: log test metrics, per-class scores, and confusion matrix ----\n",
    "# test_run = wandb.init(project=PROJECT, name=f\"{BASE_RUN_NAME}_test\", resume=\"allow\", reinit=True)\n",
    "# log_payload = {\n",
    "#     \"test/acc\": acc,\n",
    "#     \"test/precision_macro\": p,\n",
    "#     \"test/recall_macro\": r,\n",
    "#     \"test/f1_macro\": f1,\n",
    "# }\n",
    "# for cls_name in ORDER:\n",
    "#     if cls_name in report:\n",
    "#         log_payload[f\"test/{cls_name}/precision\"] = report[cls_name][\"precision\"]\n",
    "#         log_payload[f\"test/{cls_name}/recall\"]    = report[cls_name][\"recall\"]\n",
    "#         log_payload[f\"test/{cls_name}/f1\"]        = report[cls_name][\"f1-score\"]\n",
    "\n",
    "# wandb.log(log_payload)\n",
    "\n",
    "# cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(ORDER))))\n",
    "# wandb.log({\n",
    "#     \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "#         y_true=all_labels,\n",
    "#         preds=all_preds,\n",
    "#         class_names=[ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "#     )\n",
    "# })\n",
    "# test_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "# test_run.summary[\"test_f1_macro\"] = f1\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210cc7635a0a6d3b",
   "metadata": {},
   "source": [
    "# üîß Second Hyperparameter Tuning ‚Äì Testing Bigger Batches & Higher LRs\n",
    "\n",
    "From the **first tuning round**, we figured out that:  \n",
    "- ‚úÖ Unfreezing **all 12 layers** works best  \n",
    "- ‚úÖ LR around **3e-5** was the sweet spot  \n",
    "\n",
    "But we weren‚Äôt fully sure how far we can push things, so in this round we tried a **wider search** to see what happens.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Changes we made\n",
    "- **Batch size:** tried `[4, 8, 16, 32, 64]` ‚Üí to check if bigger batches give more stable training, or if small ones still generalize better.  \n",
    "- **Learning rate:** expanded up to `1e-2` ü§Ø ‚Üí just to test if more aggressive fine-tuning can still converge.  \n",
    "- **Unfreezing:** kept the range at **8‚Äì12 layers**, since deeper unfreezing looked clearly better last time.  \n",
    "- **Weight decay:** limited to `1e-6 ‚Äì 1e-4`, because we saw high values ruin performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why we did this\n",
    "- To see if **large batches** speed things up without hurting F1.  \n",
    "- To test whether **big LRs** can still work with proper scheduling.  \n",
    "- To confirm if **full unfreezing** keeps outperforming partial freezing.  \n",
    "\n",
    "---\n",
    "\n",
    "## üìù What we expect\n",
    "Honestly, we think the **lower LR region (1e-5 ‚Äì 1e-4)** will still win üèÜ,  \n",
    "but this round should give us a better sense of the **stability limits** when scaling both **batch size** and **learning rate**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d47aae8893d3f92a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T10:03:12.981400Z",
     "start_time": "2025-08-17T10:03:12.842600Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load CSVs (columns: ['UserName','ScreenName','Location','TweetAt','OriginalTweet','Sentiment'])\n",
    "TRAIN_CSV = \"Corona_NLP_train_cleaned_translated.csv\"   # or \"Corona_NLP_train.csv\"\n",
    "TEST_CSV  = \"Corona_NLP_test_cleaned_translated.csv\"    # or \"Corona_NLP_test.csv\"\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV, encoding=\"utf-8\", engine=\"python\")\n",
    "df_test  = pd.read_csv(TEST_CSV,  encoding=\"utf-8\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268f391b85f500e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T10:03:21.087253Z",
     "start_time": "2025-08-17T10:03:19.595932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 37039/4116/3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ADV DL ‚Äì Part B: Monolingual baseline (RoBERTa) ‚Äì Exercise-4 style\n",
    "# Custom loop + early stopping + W&B + Optuna ONLY; freeze base, unfreeze last k layers\n",
    "# Uses df_train / df_test with columns: OriginalTweet (str), Sentiment (str)\n",
    "# =========================\n",
    "\n",
    "import os, math, random, time, json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ---- deps ----\n",
    "# !pip -q install transformers==4.43.3 optuna==3.6.1 wandb==0.17.5 >/dev/null\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    ")\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# -------------------------\n",
    "# Constants (no CFG, Optuna-only workflow)\n",
    "# -------------------------\n",
    "MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "WARMUP_RATIO = 0.06\n",
    "GRAD_CLIP = 1.0\n",
    "USE_AMP = True\n",
    "# PROJECT = \"adv-dl-p2\"\n",
    "PROJECT = \"adv-dl-p2-deberta-full\"\n",
    "\n",
    "BASE_RUN_NAME = \"microsoft/mdeberta-v3-base_full_ex_4\"\n",
    "TRIALS = 20\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ---- GPU perf toggles (Windows-safe) ----\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Label mapping (5-way sentiment)\n",
    "# -------------------------\n",
    "CANON = {\n",
    "    \"extremely negative\": \"extremely negative\",\n",
    "    \"negative\": \"negative\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"positive\": \"positive\",\n",
    "    \"extremely positive\": \"extremely positive\",\n",
    "}\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return CANON.get(s, s)\n",
    "\n",
    "# -------------------------\n",
    "# Expect df_train, df_test in memory\n",
    "# -------------------------\n",
    "assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns, \"df_train missing required columns\"\n",
    "assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns, \"df_test missing required columns\"\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "dftrain_ = prep_df(df_train)\n",
    "dftest_  = prep_df(df_test)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    dftrain_, test_size=0.1, stratify=dftrain_[\"label\"], random_state=SEED\n",
    ")\n",
    "print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset & Collator\n",
    "# -------------------------\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: transformers.PreTrainedTokenizerBase, max_len: int):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "        enc[\"labels\"] = self.labels[idx]\n",
    "        return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "train_ds = TweetDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_ds   = TweetDataset(val_df, tokenizer, MAX_LEN)\n",
    "test_ds  = TweetDataset(dftest_, tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "BATCH_SIZE=16\n",
    "# ---- pad_to_multiple_of=8 for Tensor Cores; Windows: workers=0 is often faster ----\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "# -------------------------\n",
    "# Model & Freeze/Unfreeze strategy\n",
    "# -------------------------\n",
    "def build_model(num_unfreeze_last_layers: int = 4):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    )\n",
    "    base = getattr(model, \"roberta\", None) or getattr(model, \"bert\", None) or getattr(model, \"deberta\", None)\n",
    "    if base is not None:\n",
    "        for p in base.parameters(): p.requires_grad = False\n",
    "        if hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "            k = num_unfreeze_last_layers\n",
    "            if k > 0:\n",
    "                for layer in base.encoder.layer[-k:]:\n",
    "                    for p in layer.parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Eval utilities\n",
    "# -------------------------\n",
    "def get_optimizer_scheduler(model, num_training_steps: int, lr: float, weight_decay: float):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    # try fused AdamW on CUDA (faster step) ‚Äî falls back if unavailable\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay, fused=(DEVICE==\"cuda\"))\n",
    "    except TypeError:\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay)\n",
    "    num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def evaluate(model, loader) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "            # AMP autocast for faster eval math\n",
    "            with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "                          enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "                logits = model(**batch).logits\n",
    "            preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "            labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "def train_one_run(hp: Dict) -> Tuple[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    hp keys: run_name, num_unfreeze_last_layers, lr, weight_decay, epochs, patience, trial_number\n",
    "    \"\"\"\n",
    "    run_name = hp[\"run_name\"]\n",
    "    num_unfreeze = int(hp[\"num_unfreeze_last_layers\"])\n",
    "    lr = float(hp[\"lr\"])\n",
    "    wd = float(hp[\"weight_decay\"])\n",
    "    # epochs = int(hp[\"epochs\"])\n",
    "    # patience = int(hp[\"patience\"])\n",
    "    epochs   = int(hp.get(\"epochs\",   FIXED_EPOCHS))\n",
    "    patience = int(hp.get(\"patience\", FIXED_PATIENCE))\n",
    "    model = build_model(num_unfreeze)\n",
    "    total_steps = int(math.ceil(len(train_loader) * epochs))\n",
    "    optimizer, scheduler = get_optimizer_scheduler(model, total_steps, lr, wd)\n",
    "\n",
    "    scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
    "    best_metric = -1.0\n",
    "    # best_path = f\"best_{run_name}.pt\"\n",
    "    no_improve = 0\n",
    "    # make run_name safe for filesystem paths and ensure a folder exists\n",
    "    safe_run_name = run_name.replace(\"/\", \"__\").replace(\"\\\\\", \"__\")\n",
    "    ckpt_dir = \"checkpoints\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    best_path = os.path.join(ckpt_dir, f\"best_{safe_run_name}.pt\")\n",
    "\n",
    "    wandb_run = wandb.init(\n",
    "        project=PROJECT,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": epochs,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": wd,\n",
    "            \"warmup_ratio\": WARMUP_RATIO,\n",
    "            \"grad_clip\": GRAD_CLIP,\n",
    "            \"num_unfreeze_last_layers\": num_unfreeze,\n",
    "            \"trial_number\": hp.get(\"trial_number\", -1),\n",
    "            \"suggested_batch_size\": hp.get(\"batch_size\", BATCH_SIZE),\n",
    "        },\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    # nicer W&B charts\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric(\"step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"step\")\n",
    "    wandb.define_metric(\"val/*\",   step_metric=\"epoch\")\n",
    "\n",
    "    # print + log trainable params\n",
    "    total_params     = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable_params:,} / {total_params:,} \"\n",
    "          f\"({100.0*trainable_params/total_params:.2f}%) ; unfreeze_last_k={num_unfreeze}\")\n",
    "    wandb.log({\"params/total\": total_params,\n",
    "               \"params/trainable\": trainable_params,\n",
    "               \"params/ratio\": trainable_params/max(1,total_params)}, step=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            # use BF16 if supported; else FP16\n",
    "            with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "                          enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            if GRAD_CLIP is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % 20 == 0:\n",
    "                wandb.log({\"train/loss\": loss.item(), \"step\": step + 1, \"epoch\": epoch + 1})\n",
    "\n",
    "            # periodic console + throughput log (about 10x per epoch)\n",
    "            if step % max(1, len(train_loader)//10) == 0 or step == 1:\n",
    "                avg_loss = running_loss / max(1, (step + 1))\n",
    "                elapsed  = time.time() - t0\n",
    "                items    = (step + 1) * BATCH_SIZE\n",
    "                itps     = items / max(elapsed, 1e-6)\n",
    "                print(f\"[e{epoch+1} b{step+1}/{len(train_loader)}] loss={loss.item():.4f} avg={avg_loss:.4f} it/s={itps:.1f}\")\n",
    "                wandb.log({\"train/avg_loss_so_far\": avg_loss,\n",
    "                           \"train/items_per_sec\": itps,\n",
    "                           \"step\": (epoch * len(train_loader)) + (step + 1),\n",
    "                           \"epoch\": epoch + 1})\n",
    "\n",
    "        # epoch-end validation\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        epoch_loss = running_loss / max(1, len(train_loader))\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        wandb.log({\n",
    "            \"train/epoch_loss\": epoch_loss,\n",
    "            \"val/acc\": val_metrics[\"acc\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "            \"lr\": current_lr,\n",
    "            \"time/epoch_sec\": elapsed,\n",
    "            \"epoch\": epoch + 1,\n",
    "        })\n",
    "\n",
    "        # Early stopping on val f1\n",
    "        if val_metrics[\"f1\"] > best_metric:\n",
    "            best_metric = val_metrics[\"f1\"]\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            no_improve = 0\n",
    "            wandb_run.summary[\"best_val_f1\"] = best_metric\n",
    "            wandb_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "            wandb.log({\"val/best_f1_so_far\": best_metric, \"val/best_epoch\": epoch + 1})\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"loss={epoch_loss:.4f} | \"\n",
    "              f\"val_acc={val_metrics['acc']:.4f} | val_f1={val_metrics['f1']:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Load best and return path + metrics on val for reference\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    final_val = evaluate(model, val_loader)\n",
    "\n",
    "    # store final val in W&B summary for quick sorting\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.summary[\"final_val_acc\"] = final_val[\"acc\"]\n",
    "        wandb.run.summary[\"final_val_precision\"] = final_val[\"precision\"]\n",
    "        wandb.run.summary[\"final_val_recall\"] = final_val[\"recall\"]\n",
    "        wandb.run.summary[\"final_val_f1\"] = final_val[\"f1\"]\n",
    "\n",
    "    return best_path, final_val\n",
    "\n",
    "# -------------------------\n",
    "# Optuna hyperparameter tuning (ALWAYS ON)\n",
    "# -------------------------\n",
    "\n",
    "# Constants\n",
    "FIXED_EPOCHS = 12\n",
    "FIXED_PATIENCE = 4\n",
    "\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    params = {\n",
    "        \"run_name\": f\"{BASE_RUN_NAME}_optuna_trial_{trial.number}\",\n",
    "        \"num_unfreeze_last_layers\": trial.suggest_int(\"num_unfreeze_last_layers\", 8, 12),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64]),\n",
    "        \"epochs\": FIXED_EPOCHS,\n",
    "        \"patience\": FIXED_PATIENCE,\n",
    "        \"trial_number\": trial.number,\n",
    "    }\n",
    "    path, val_metrics = train_one_run(params)\n",
    "    # console visibility per trial\n",
    "    print(f\"[Trial {trial.number}] f1={val_metrics['f1']:.4f} | \"\n",
    "          f\"unfreeze_k={params['num_unfreeze_last_layers']} lr={params['lr']:.2e} \"\n",
    "          f\"wd={params['weight_decay']:.1e} suggested_bs={params['batch_size']}\")\n",
    "    # report intermediate value for pruning if enabled\n",
    "    trial.report(val_metrics[\"f1\"], step=1)\n",
    "    return val_metrics[\"f1\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64b0fe02b14c261",
   "metadata": {},
   "source": [
    "### Run Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd217d4c136bc30d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T03:10:37.431168Z",
     "start_time": "2025-08-15T23:23:31.027622Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 02:23:31,029] A new study created in memory with name: no-name-b0cc087a-195e-454e-8e52-0f0bae1b98f4\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_022333-t9z49etj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/t9z49etj' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_0</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/t9z49etj' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/t9z49etj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b1/2315] loss=1.6808 avg=1.6808 it/s=38.9\n",
      "[e1 b2/2315] loss=1.6321 avg=1.6564 it/s=60.4\n",
      "[e1 b232/2315] loss=1.5153 avg=1.5628 it/s=329.1\n",
      "[e1 b463/2315] loss=1.1529 avg=1.4154 it/s=340.2\n",
      "[e1 b694/2315] loss=1.0651 avg=1.3113 it/s=349.1\n",
      "[e1 b925/2315] loss=1.4492 avg=1.2348 it/s=352.0\n",
      "[e1 b1156/2315] loss=0.6699 avg=1.1797 it/s=352.3\n",
      "[e1 b1387/2315] loss=1.5224 avg=1.2216 it/s=338.3\n",
      "[e1 b1618/2315] loss=1.5436 avg=1.2721 it/s=326.3\n",
      "[e1 b1849/2315] loss=1.6322 avg=1.3118 it/s=329.8\n",
      "[e1 b2080/2315] loss=1.6153 avg=1.3416 it/s=332.6\n",
      "[e1 b2311/2315] loss=1.6319 avg=1.3653 it/s=333.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.3656 | val_acc=0.2775 | val_f1=0.0869 | time=115.5s\n",
      "[e2 b1/2315] loss=1.4302 avg=1.4302 it/s=333.8\n",
      "[e2 b2/2315] loss=1.5803 avg=1.5053 it/s=366.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.5652 avg=1.5686 it/s=349.4\n",
      "[e2 b463/2315] loss=1.6740 avg=1.5773 it/s=338.0\n",
      "[e2 b694/2315] loss=1.5795 avg=1.5784 it/s=344.3\n",
      "[e2 b925/2315] loss=1.5484 avg=1.5786 it/s=348.7\n",
      "[e2 b1156/2315] loss=1.5892 avg=1.5781 it/s=352.7\n",
      "[e2 b1387/2315] loss=1.6029 avg=1.5772 it/s=351.6\n",
      "[e2 b1618/2315] loss=1.5469 avg=1.5774 it/s=349.4\n",
      "[e2 b1849/2315] loss=1.5714 avg=1.5775 it/s=346.4\n",
      "[e2 b2080/2315] loss=1.5982 avg=1.5784 it/s=345.2\n",
      "[e2 b2311/2315] loss=1.6026 avg=1.5784 it/s=344.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5784 | val_acc=0.2775 | val_f1=0.0869 | time=112.4s\n",
      "[e3 b1/2315] loss=1.5432 avg=1.5432 it/s=358.2\n",
      "[e3 b2/2315] loss=1.4834 avg=1.5133 it/s=372.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5902 avg=1.5833 it/s=335.7\n",
      "[e3 b463/2315] loss=1.5635 avg=1.5781 it/s=342.5\n",
      "[e3 b694/2315] loss=1.5055 avg=1.5775 it/s=344.7\n",
      "[e3 b925/2315] loss=1.5748 avg=1.5787 it/s=349.3\n",
      "[e3 b1156/2315] loss=1.6232 avg=1.5773 it/s=351.9\n",
      "[e3 b1387/2315] loss=1.4228 avg=1.5772 it/s=351.6\n",
      "[e3 b1618/2315] loss=1.5484 avg=1.5768 it/s=346.4\n",
      "[e3 b1849/2315] loss=1.5915 avg=1.5767 it/s=343.4\n",
      "[e3 b2080/2315] loss=1.6284 avg=1.5774 it/s=341.9\n",
      "[e3 b2311/2315] loss=1.6951 avg=1.5776 it/s=342.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5776 | val_acc=0.2775 | val_f1=0.0869 | time=112.8s\n",
      "[e4 b1/2315] loss=1.5595 avg=1.5595 it/s=341.1\n",
      "[e4 b2/2315] loss=1.6092 avg=1.5844 it/s=300.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.4839 avg=1.5767 it/s=351.4\n",
      "[e4 b463/2315] loss=1.4571 avg=1.5799 it/s=348.7\n",
      "[e4 b694/2315] loss=1.6158 avg=1.5780 it/s=349.6\n",
      "[e4 b925/2315] loss=1.6341 avg=1.5772 it/s=350.9\n",
      "[e4 b1156/2315] loss=1.4867 avg=1.5782 it/s=354.2\n",
      "[e4 b1387/2315] loss=1.5687 avg=1.5774 it/s=355.6\n",
      "[e4 b1618/2315] loss=1.4918 avg=1.5774 it/s=353.3\n",
      "[e4 b1849/2315] loss=1.3948 avg=1.5768 it/s=349.0\n",
      "[e4 b2080/2315] loss=1.5559 avg=1.5766 it/s=347.6\n",
      "[e4 b2311/2315] loss=1.6125 avg=1.5772 it/s=346.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5772 | val_acc=0.2410 | val_f1=0.0777 | time=111.6s\n",
      "[e5 b1/2315] loss=1.5629 avg=1.5629 it/s=371.3\n",
      "[e5 b2/2315] loss=1.5717 avg=1.5673 it/s=320.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5188 avg=1.5754 it/s=339.7\n",
      "[e5 b463/2315] loss=1.7075 avg=1.5738 it/s=338.1\n",
      "[e5 b694/2315] loss=1.6032 avg=1.5753 it/s=329.9\n",
      "[e5 b925/2315] loss=1.5946 avg=1.5761 it/s=329.3\n",
      "[e5 b1156/2315] loss=1.6991 avg=1.5766 it/s=334.4\n",
      "[e5 b1387/2315] loss=1.6866 avg=1.5752 it/s=339.0\n",
      "[e5 b1618/2315] loss=1.5170 avg=1.5754 it/s=339.6\n",
      "[e5 b1849/2315] loss=1.6717 avg=1.5767 it/s=337.6\n",
      "[e5 b2080/2315] loss=1.5221 avg=1.5767 it/s=334.9\n",
      "[e5 b2311/2315] loss=1.6045 avg=1.5769 it/s=334.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñÇ‚ñÉ‚ñÅ‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñà‚ñÜ</td></tr><tr><td>val/acc</td><td>‚ñà‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñà‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñà‚ñà‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>9e-05</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>115.52365</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57691</td></tr><tr><td>train/epoch_loss</td><td>1.57693</td></tr><tr><td>train/items_per_sec</td><td>334.52807</td></tr><tr><td>train/loss</td><td>1.55505</td></tr><tr><td>val/acc</td><td>0.24101</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.07768</td></tr><tr><td>val/precision</td><td>0.0482</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_0</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/t9z49etj' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/t9z49etj</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_022333-t9z49etj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.0868771:   5%|‚ñå         | 1/20 [09:38<3:03:17, 578.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 0] f1=0.0869 | unfreeze_k=12 lr=1.45e-04 wd=4.9e-06 suggested_bs=16\n",
      "[I 2025-08-16 02:33:09,835] Trial 0 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 0.00014543889726670146, 'weight_decay': 4.883984881682272e-06, 'batch_size': 16}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_023311-naf0iehs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/naf0iehs' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_1</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/naf0iehs' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/naf0iehs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[e1 b1/2315] loss=1.6378 avg=1.6378 it/s=116.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b2/2315] loss=1.6414 avg=1.6396 it/s=153.0\n",
      "[e1 b232/2315] loss=1.6606 avg=1.6056 it/s=328.4\n",
      "[e1 b463/2315] loss=1.5806 avg=1.5987 it/s=327.3\n",
      "[e1 b694/2315] loss=1.5971 avg=1.5942 it/s=333.3\n",
      "[e1 b925/2315] loss=1.4688 avg=1.5925 it/s=334.0\n",
      "[e1 b1156/2315] loss=1.5293 avg=1.5917 it/s=336.1\n",
      "[e1 b1387/2315] loss=1.6028 avg=1.5904 it/s=339.0\n",
      "[e1 b1618/2315] loss=1.6762 avg=1.5895 it/s=337.7\n",
      "[e1 b1849/2315] loss=1.5924 avg=1.5880 it/s=337.1\n",
      "[e1 b2080/2315] loss=1.5871 avg=1.5869 it/s=335.6\n",
      "[e1 b2311/2315] loss=1.6652 avg=1.5862 it/s=335.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5861 | val_acc=0.2775 | val_f1=0.0869 | time=114.9s\n",
      "[e2 b1/2315] loss=1.6434 avg=1.6434 it/s=337.1\n",
      "[e2 b2/2315] loss=1.6566 avg=1.6500 it/s=365.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6233 avg=1.5693 it/s=350.1\n",
      "[e2 b463/2315] loss=1.5549 avg=1.5736 it/s=345.0\n",
      "[e2 b694/2315] loss=1.5481 avg=1.5731 it/s=340.6\n",
      "[e2 b925/2315] loss=1.5167 avg=1.5750 it/s=338.4\n",
      "[e2 b1156/2315] loss=1.5878 avg=1.5763 it/s=339.8\n",
      "[e2 b1387/2315] loss=1.5893 avg=1.5756 it/s=343.3\n",
      "[e2 b1618/2315] loss=1.5569 avg=1.5747 it/s=345.5\n",
      "[e2 b1849/2315] loss=1.6099 avg=1.5751 it/s=342.7\n",
      "[e2 b2080/2315] loss=1.5239 avg=1.5750 it/s=341.4\n",
      "[e2 b2311/2315] loss=1.6262 avg=1.5756 it/s=339.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5756 | val_acc=0.2775 | val_f1=0.0869 | time=113.9s\n",
      "[e3 b1/2315] loss=1.5289 avg=1.5289 it/s=312.7\n",
      "[e3 b2/2315] loss=1.5566 avg=1.5428 it/s=328.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.6162 avg=1.5681 it/s=343.6\n",
      "[e3 b463/2315] loss=1.6500 avg=1.5717 it/s=345.8\n",
      "[e3 b694/2315] loss=1.6132 avg=1.5758 it/s=342.2\n",
      "[e3 b925/2315] loss=1.5029 avg=1.5753 it/s=341.7\n",
      "[e3 b1156/2315] loss=1.5983 avg=1.5734 it/s=342.7\n",
      "[e3 b1387/2315] loss=1.6592 avg=1.5744 it/s=345.5\n",
      "[e3 b1618/2315] loss=1.5318 avg=1.5753 it/s=347.4\n",
      "[e3 b1849/2315] loss=1.5763 avg=1.5759 it/s=347.5\n",
      "[e3 b2080/2315] loss=1.5539 avg=1.5755 it/s=344.0\n",
      "[e3 b2311/2315] loss=1.5121 avg=1.5754 it/s=341.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5753 | val_acc=0.2775 | val_f1=0.0869 | time=113.1s\n",
      "[e4 b1/2315] loss=1.6503 avg=1.6503 it/s=331.5\n",
      "[e4 b2/2315] loss=1.5532 avg=1.6017 it/s=320.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.6231 avg=1.5665 it/s=335.4\n",
      "[e4 b463/2315] loss=1.5951 avg=1.5695 it/s=330.6\n",
      "[e4 b694/2315] loss=1.5835 avg=1.5723 it/s=331.9\n",
      "[e4 b925/2315] loss=1.6245 avg=1.5746 it/s=330.4\n",
      "[e4 b1156/2315] loss=1.5473 avg=1.5751 it/s=330.1\n",
      "[e4 b1387/2315] loss=1.5452 avg=1.5755 it/s=333.2\n",
      "[e4 b1618/2315] loss=1.6270 avg=1.5753 it/s=335.7\n",
      "[e4 b1849/2315] loss=1.6238 avg=1.5759 it/s=338.6\n",
      "[e4 b2080/2315] loss=1.7159 avg=1.5753 it/s=338.3\n",
      "[e4 b2311/2315] loss=1.6571 avg=1.5752 it/s=337.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5752 | val_acc=0.2775 | val_f1=0.0869 | time=114.7s\n",
      "[e5 b1/2315] loss=1.6238 avg=1.6238 it/s=362.2\n",
      "[e5 b2/2315] loss=1.4971 avg=1.5604 it/s=339.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5562 avg=1.5730 it/s=338.9\n",
      "[e5 b463/2315] loss=1.6033 avg=1.5711 it/s=341.6\n",
      "[e5 b694/2315] loss=1.5936 avg=1.5730 it/s=341.0\n",
      "[e5 b925/2315] loss=1.5695 avg=1.5741 it/s=341.7\n",
      "[e5 b1156/2315] loss=1.6179 avg=1.5745 it/s=342.0\n",
      "[e5 b1387/2315] loss=1.7078 avg=1.5744 it/s=341.5\n",
      "[e5 b1618/2315] loss=1.7004 avg=1.5757 it/s=343.7\n",
      "[e5 b1849/2315] loss=1.5581 avg=1.5755 it/s=344.9\n",
      "[e5 b2080/2315] loss=1.5764 avg=1.5753 it/s=345.5\n",
      "[e5 b2311/2315] loss=1.6550 avg=1.5753 it/s=345.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÅ‚ñÇ‚ñà‚ñÉ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñÖ‚ñÉ‚ñá‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñá‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñà‚ñÉ‚ñà‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÇ‚ñÜ‚ñÇ‚ñÜ‚ñÉ‚ñÅ‚ñá‚ñÜ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00213</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>112.23571</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57527</td></tr><tr><td>train/epoch_loss</td><td>1.57526</td></tr><tr><td>train/items_per_sec</td><td>344.96003</td></tr><tr><td>train/loss</td><td>1.54548</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_1</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/naf0iehs' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/naf0iehs</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_023311-naf0iehs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.0868771:  10%|‚ñà         | 2/20 [19:17<2:53:33, 578.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 1] f1=0.0869 | unfreeze_k=12 lr=3.43e-03 wd=7.8e-05 suggested_bs=4\n",
      "[I 2025-08-16 02:42:48,167] Trial 1 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 0.003433856232028575, 'weight_decay': 7.805045784095608e-05, 'batch_size': 4}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_024249-4m1kcnn9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4m1kcnn9' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_2</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4m1kcnn9' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4m1kcnn9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[e1 b1/2315] loss=1.5846 avg=1.5846 it/s=238.5\n",
      "[e1 b2/2315] loss=1.5538 avg=1.5692 it/s=246.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5739 avg=1.5397 it/s=361.7\n",
      "[e1 b463/2315] loss=1.5558 avg=1.5122 it/s=365.0\n",
      "[e1 b694/2315] loss=1.5310 avg=1.5372 it/s=365.7\n",
      "[e1 b925/2315] loss=1.5938 avg=1.5501 it/s=362.5\n",
      "[e1 b1156/2315] loss=1.5692 avg=1.5569 it/s=358.9\n",
      "[e1 b1387/2315] loss=1.4163 avg=1.5618 it/s=359.1\n",
      "[e1 b1618/2315] loss=1.6138 avg=1.5650 it/s=361.7\n",
      "[e1 b1849/2315] loss=1.6691 avg=1.5678 it/s=364.1\n",
      "[e1 b2080/2315] loss=1.5256 avg=1.5690 it/s=366.1\n",
      "[e1 b2311/2315] loss=1.6005 avg=1.5705 it/s=365.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5705 | val_acc=0.2775 | val_f1=0.0869 | time=106.1s\n",
      "[e2 b1/2315] loss=1.4957 avg=1.4957 it/s=310.3\n",
      "[e2 b2/2315] loss=1.6501 avg=1.5729 it/s=357.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.5008 avg=1.5751 it/s=335.0\n",
      "[e2 b463/2315] loss=1.6344 avg=1.5756 it/s=346.9\n",
      "[e2 b694/2315] loss=1.5768 avg=1.5769 it/s=338.7\n",
      "[e2 b925/2315] loss=1.5013 avg=1.5742 it/s=335.6\n",
      "[e2 b1156/2315] loss=1.5954 avg=1.5767 it/s=338.8\n",
      "[e2 b1387/2315] loss=1.5392 avg=1.5766 it/s=341.5\n",
      "[e2 b1618/2315] loss=1.6447 avg=1.5761 it/s=341.8\n",
      "[e2 b1849/2315] loss=1.6838 avg=1.5766 it/s=345.7\n",
      "[e2 b2080/2315] loss=1.5803 avg=1.5781 it/s=348.8\n",
      "[e2 b2311/2315] loss=1.6587 avg=1.5779 it/s=350.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5780 | val_acc=0.2775 | val_f1=0.0869 | time=110.6s\n",
      "[e3 b1/2315] loss=1.5475 avg=1.5475 it/s=392.1\n",
      "[e3 b2/2315] loss=1.5026 avg=1.5251 it/s=305.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5488 avg=1.5730 it/s=335.9\n",
      "[e3 b463/2315] loss=1.5503 avg=1.5728 it/s=331.2\n",
      "[e3 b694/2315] loss=1.5780 avg=1.5778 it/s=332.4\n",
      "[e3 b925/2315] loss=1.6537 avg=1.5781 it/s=339.5\n",
      "[e3 b1156/2315] loss=1.6501 avg=1.5789 it/s=342.2\n",
      "[e3 b1387/2315] loss=1.4196 avg=1.5777 it/s=344.4\n",
      "[e3 b1618/2315] loss=1.5088 avg=1.5774 it/s=346.7\n",
      "[e3 b1849/2315] loss=1.5759 avg=1.5769 it/s=348.9\n",
      "[e3 b2080/2315] loss=1.6033 avg=1.5769 it/s=352.0\n",
      "[e3 b2311/2315] loss=1.5270 avg=1.5764 it/s=354.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5765 | val_acc=0.2775 | val_f1=0.0869 | time=109.1s\n",
      "[e4 b1/2315] loss=1.5310 avg=1.5310 it/s=316.3\n",
      "[e4 b2/2315] loss=1.6388 avg=1.5849 it/s=323.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.5686 avg=1.5771 it/s=364.0\n",
      "[e4 b463/2315] loss=1.5602 avg=1.5797 it/s=350.9\n",
      "[e4 b694/2315] loss=1.5290 avg=1.5782 it/s=350.6\n",
      "[e4 b925/2315] loss=1.6474 avg=1.5758 it/s=351.0\n",
      "[e4 b1156/2315] loss=1.4673 avg=1.5741 it/s=353.3\n",
      "[e4 b1387/2315] loss=1.5972 avg=1.5753 it/s=357.0\n",
      "[e4 b1618/2315] loss=1.4762 avg=1.5749 it/s=358.9\n",
      "[e4 b1849/2315] loss=1.5863 avg=1.5752 it/s=357.6\n",
      "[e4 b2080/2315] loss=1.6037 avg=1.5754 it/s=358.8\n",
      "[e4 b2311/2315] loss=1.6893 avg=1.5759 it/s=359.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5760 | val_acc=0.2775 | val_f1=0.0869 | time=107.6s\n",
      "[e5 b1/2315] loss=1.5547 avg=1.5547 it/s=382.6\n",
      "[e5 b2/2315] loss=1.6833 avg=1.6190 it/s=354.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.6363 avg=1.5762 it/s=333.3\n",
      "[e5 b463/2315] loss=1.5842 avg=1.5744 it/s=344.5\n",
      "[e5 b694/2315] loss=1.5812 avg=1.5752 it/s=344.7\n",
      "[e5 b925/2315] loss=1.6426 avg=1.5753 it/s=342.3\n",
      "[e5 b1156/2315] loss=1.5018 avg=1.5750 it/s=344.3\n",
      "[e5 b1387/2315] loss=1.6505 avg=1.5751 it/s=346.8\n",
      "[e5 b1618/2315] loss=1.5657 avg=1.5752 it/s=347.9\n",
      "[e5 b1849/2315] loss=1.5366 avg=1.5752 it/s=347.8\n",
      "[e5 b2080/2315] loss=1.5946 avg=1.5754 it/s=348.2\n",
      "[e5 b2311/2315] loss=1.5556 avg=1.5757 it/s=348.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñÅ‚ñá‚ñÖ‚ñÉ‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñÑ‚ñÖ‚ñÇ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÅ‚ñÑ‚ñá‚ñÑ‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÉ‚ñá‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00068</td></tr><tr><td>params/ratio</td><td>0.28177</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>78561029</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>111.21472</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57574</td></tr><tr><td>train/epoch_loss</td><td>1.57576</td></tr><tr><td>train/items_per_sec</td><td>348.15371</td></tr><tr><td>train/loss</td><td>1.50803</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_2</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4m1kcnn9' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4m1kcnn9</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_024249-4m1kcnn9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.0868771:  15%|‚ñà‚ñå        | 3/20 [28:31<2:40:44, 567.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 2] f1=0.0869 | unfreeze_k=11 lr=1.10e-03 wd=1.2e-05 suggested_bs=16\n",
      "[I 2025-08-16 02:52:02,203] Trial 2 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 11, 'lr': 0.0010978882671672725, 'weight_decay': 1.2213686939219737e-05, 'batch_size': 16}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_025203-g3mqvv6u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/g3mqvv6u' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_3</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/g3mqvv6u' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/g3mqvv6u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[e1 b1/2315] loss=1.7347 avg=1.7347 it/s=248.3\n",
      "[e1 b2/2315] loss=1.6141 avg=1.6744 it/s=283.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.2967 avg=1.5205 it/s=370.9\n",
      "[e1 b463/2315] loss=1.6420 avg=1.5107 it/s=369.3\n",
      "[e1 b694/2315] loss=1.5959 avg=1.5375 it/s=350.9\n",
      "[e1 b925/2315] loss=1.6525 avg=1.5479 it/s=345.5\n",
      "[e1 b1156/2315] loss=1.5897 avg=1.5551 it/s=345.5\n",
      "[e1 b1387/2315] loss=1.6541 avg=1.5604 it/s=348.3\n",
      "[e1 b1618/2315] loss=1.5219 avg=1.5645 it/s=348.8\n",
      "[e1 b1849/2315] loss=1.4165 avg=1.5665 it/s=351.4\n",
      "[e1 b2080/2315] loss=1.5755 avg=1.5680 it/s=352.3\n",
      "[e1 b2311/2315] loss=1.6179 avg=1.5701 it/s=353.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5700 | val_acc=0.2775 | val_f1=0.0869 | time=109.7s\n",
      "[e2 b1/2315] loss=1.6608 avg=1.6608 it/s=349.3\n",
      "[e2 b2/2315] loss=1.6035 avg=1.6321 it/s=349.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6730 avg=1.5753 it/s=377.3\n",
      "[e2 b463/2315] loss=1.6403 avg=1.5805 it/s=374.7\n",
      "[e2 b694/2315] loss=1.6995 avg=1.5790 it/s=374.0\n",
      "[e2 b925/2315] loss=1.7749 avg=1.5788 it/s=370.8\n",
      "[e2 b1156/2315] loss=1.5543 avg=1.5788 it/s=364.7\n",
      "[e2 b1387/2315] loss=1.4817 avg=1.5792 it/s=363.3\n",
      "[e2 b1618/2315] loss=1.5111 avg=1.5785 it/s=363.5\n",
      "[e2 b1849/2315] loss=1.5856 avg=1.5783 it/s=363.3\n",
      "[e2 b2080/2315] loss=1.5264 avg=1.5781 it/s=363.7\n",
      "[e2 b2311/2315] loss=1.6283 avg=1.5783 it/s=364.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5783 | val_acc=0.2775 | val_f1=0.0869 | time=106.7s\n",
      "[e3 b1/2315] loss=1.6973 avg=1.6973 it/s=395.2\n",
      "[e3 b2/2315] loss=1.6605 avg=1.6789 it/s=354.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5737 avg=1.5774 it/s=350.2\n",
      "[e3 b463/2315] loss=1.5078 avg=1.5793 it/s=346.7\n",
      "[e3 b694/2315] loss=1.5811 avg=1.5780 it/s=348.9\n",
      "[e3 b925/2315] loss=1.5757 avg=1.5785 it/s=346.9\n",
      "[e3 b1156/2315] loss=1.6612 avg=1.5785 it/s=346.0\n",
      "[e3 b1387/2315] loss=1.5547 avg=1.5779 it/s=347.4\n",
      "[e3 b1618/2315] loss=1.5660 avg=1.5780 it/s=346.0\n",
      "[e3 b1849/2315] loss=1.5709 avg=1.5779 it/s=347.3\n",
      "[e3 b2080/2315] loss=1.6802 avg=1.5774 it/s=349.1\n",
      "[e3 b2311/2315] loss=1.5554 avg=1.5767 it/s=349.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5768 | val_acc=0.2775 | val_f1=0.0869 | time=110.7s\n",
      "[e4 b1/2315] loss=1.5357 avg=1.5357 it/s=378.0\n",
      "[e4 b2/2315] loss=1.5260 avg=1.5308 it/s=311.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.5854 avg=1.5756 it/s=348.2\n",
      "[e4 b463/2315] loss=1.5633 avg=1.5788 it/s=349.6\n",
      "[e4 b694/2315] loss=1.5377 avg=1.5756 it/s=357.4\n",
      "[e4 b925/2315] loss=1.5177 avg=1.5755 it/s=361.5\n",
      "[e4 b1156/2315] loss=1.5707 avg=1.5764 it/s=362.9\n",
      "[e4 b1387/2315] loss=1.6020 avg=1.5764 it/s=358.6\n",
      "[e4 b1618/2315] loss=1.5705 avg=1.5766 it/s=355.1\n",
      "[e4 b1849/2315] loss=1.6270 avg=1.5756 it/s=354.4\n",
      "[e4 b2080/2315] loss=1.5864 avg=1.5761 it/s=354.6\n",
      "[e4 b2311/2315] loss=1.6322 avg=1.5760 it/s=354.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5762 | val_acc=0.2775 | val_f1=0.0869 | time=109.3s\n",
      "[e5 b1/2315] loss=1.6017 avg=1.6017 it/s=305.9\n",
      "[e5 b2/2315] loss=1.5445 avg=1.5731 it/s=308.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5794 avg=1.5775 it/s=352.7\n",
      "[e5 b463/2315] loss=1.5576 avg=1.5780 it/s=357.9\n",
      "[e5 b694/2315] loss=1.5687 avg=1.5775 it/s=352.7\n",
      "[e5 b925/2315] loss=1.5131 avg=1.5785 it/s=356.1\n",
      "[e5 b1156/2315] loss=1.6051 avg=1.5787 it/s=358.3\n",
      "[e5 b1387/2315] loss=1.5211 avg=1.5784 it/s=360.4\n",
      "[e5 b1618/2315] loss=1.6128 avg=1.5774 it/s=356.4\n",
      "[e5 b1849/2315] loss=1.5619 avg=1.5760 it/s=353.0\n",
      "[e5 b2080/2315] loss=1.5520 avg=1.5760 it/s=352.1\n",
      "[e5 b2311/2315] loss=1.5141 avg=1.5758 it/s=353.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÜ‚ñÅ‚ñà‚ñÜ‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñá‚ñÜ‚ñÜ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÉ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÉ‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÑ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.0005</td></tr><tr><td>params/ratio</td><td>0.28177</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>78561029</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>109.5185</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57581</td></tr><tr><td>train/epoch_loss</td><td>1.5758</td></tr><tr><td>train/items_per_sec</td><td>353.29475</td></tr><tr><td>train/loss</td><td>1.64576</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_3</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/g3mqvv6u' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/g3mqvv6u</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_025203-g3mqvv6u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.0868771:  20%|‚ñà‚ñà        | 4/20 [37:46<2:30:01, 562.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 3] f1=0.0869 | unfreeze_k=11 lr=8.01e-04 wd=1.2e-06 suggested_bs=32\n",
      "[I 2025-08-16 03:01:17,524] Trial 3 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 11, 'lr': 0.0008013317147079586, 'weight_decay': 1.1595878734573544e-06, 'batch_size': 32}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_030118-q33hqf9w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/q33hqf9w' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_4</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/q33hqf9w' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/q33hqf9w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[e1 b1/2315] loss=1.6446 avg=1.6446 it/s=223.8\n",
      "[e1 b2/2315] loss=1.6263 avg=1.6354 it/s=247.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5899 avg=1.5610 it/s=346.3\n",
      "[e1 b463/2315] loss=1.5345 avg=1.5703 it/s=346.8\n",
      "[e1 b694/2315] loss=1.5665 avg=1.5770 it/s=349.5\n",
      "[e1 b925/2315] loss=1.6103 avg=1.5814 it/s=355.8\n",
      "[e1 b1156/2315] loss=1.5047 avg=1.5823 it/s=360.8\n",
      "[e1 b1387/2315] loss=1.5970 avg=1.5831 it/s=364.7\n",
      "[e1 b1618/2315] loss=1.5200 avg=1.5843 it/s=363.1\n",
      "[e1 b1849/2315] loss=1.5443 avg=1.5855 it/s=359.2\n",
      "[e1 b2080/2315] loss=1.6031 avg=1.5847 it/s=357.1\n",
      "[e1 b2311/2315] loss=1.5934 avg=1.5843 it/s=354.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5843 | val_acc=0.2775 | val_f1=0.0869 | time=109.6s\n",
      "[e2 b1/2315] loss=1.5098 avg=1.5098 it/s=409.9\n",
      "[e2 b2/2315] loss=1.6034 avg=1.5566 it/s=369.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6358 avg=1.5799 it/s=332.4\n",
      "[e2 b463/2315] loss=1.6434 avg=1.5815 it/s=341.6\n",
      "[e2 b694/2315] loss=1.5707 avg=1.5810 it/s=346.6\n",
      "[e2 b925/2315] loss=1.6916 avg=1.5781 it/s=348.7\n",
      "[e2 b1156/2315] loss=1.6549 avg=1.5781 it/s=351.7\n",
      "[e2 b1387/2315] loss=1.5724 avg=1.5796 it/s=354.8\n",
      "[e2 b1618/2315] loss=1.6330 avg=1.5785 it/s=356.8\n",
      "[e2 b1849/2315] loss=1.5563 avg=1.5782 it/s=355.0\n",
      "[e2 b2080/2315] loss=1.5173 avg=1.5780 it/s=350.8\n",
      "[e2 b2311/2315] loss=1.4785 avg=1.5783 it/s=348.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5783 | val_acc=0.2775 | val_f1=0.0869 | time=111.2s\n",
      "[e3 b1/2315] loss=1.6988 avg=1.6988 it/s=383.1\n",
      "[e3 b2/2315] loss=1.5779 avg=1.6384 it/s=391.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5573 avg=1.5843 it/s=354.8\n",
      "[e3 b463/2315] loss=1.5343 avg=1.5770 it/s=350.8\n",
      "[e3 b694/2315] loss=1.6748 avg=1.5779 it/s=353.4\n",
      "[e3 b925/2315] loss=1.6094 avg=1.5775 it/s=356.6\n",
      "[e3 b1156/2315] loss=1.6212 avg=1.5766 it/s=354.7\n",
      "[e3 b1387/2315] loss=1.6550 avg=1.5770 it/s=358.5\n",
      "[e3 b1618/2315] loss=1.5789 avg=1.5757 it/s=362.1\n",
      "[e3 b1849/2315] loss=1.5823 avg=1.5761 it/s=364.9\n",
      "[e3 b2080/2315] loss=1.7741 avg=1.5760 it/s=362.4\n",
      "[e3 b2311/2315] loss=1.6372 avg=1.5765 it/s=361.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5765 | val_acc=0.2775 | val_f1=0.0869 | time=107.8s\n",
      "[e4 b1/2315] loss=1.6831 avg=1.6831 it/s=301.0\n",
      "[e4 b2/2315] loss=1.6141 avg=1.6486 it/s=324.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.5656 avg=1.5756 it/s=345.9\n",
      "[e4 b463/2315] loss=1.6067 avg=1.5771 it/s=350.0\n",
      "[e4 b694/2315] loss=1.5584 avg=1.5774 it/s=353.8\n",
      "[e4 b925/2315] loss=1.5874 avg=1.5765 it/s=355.2\n",
      "[e4 b1156/2315] loss=1.5999 avg=1.5763 it/s=355.4\n",
      "[e4 b1387/2315] loss=1.6144 avg=1.5762 it/s=358.4\n",
      "[e4 b1618/2315] loss=1.6492 avg=1.5766 it/s=361.2\n",
      "[e4 b1849/2315] loss=1.5981 avg=1.5765 it/s=364.3\n",
      "[e4 b2080/2315] loss=1.6073 avg=1.5759 it/s=366.7\n",
      "[e4 b2311/2315] loss=1.6157 avg=1.5760 it/s=365.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5760 | val_acc=0.2775 | val_f1=0.0869 | time=106.4s\n",
      "[e5 b1/2315] loss=1.5403 avg=1.5403 it/s=292.1\n",
      "[e5 b2/2315] loss=1.6032 avg=1.5717 it/s=316.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5298 avg=1.5701 it/s=318.1\n",
      "[e5 b463/2315] loss=1.6164 avg=1.5739 it/s=327.6\n",
      "[e5 b694/2315] loss=1.5285 avg=1.5744 it/s=335.9\n",
      "[e5 b925/2315] loss=1.5815 avg=1.5747 it/s=337.3\n",
      "[e5 b1156/2315] loss=1.5149 avg=1.5757 it/s=342.3\n",
      "[e5 b1387/2315] loss=1.5883 avg=1.5762 it/s=345.1\n",
      "[e5 b1618/2315] loss=1.5145 avg=1.5757 it/s=345.1\n",
      "[e5 b1849/2315] loss=1.5051 avg=1.5761 it/s=348.1\n",
      "[e5 b2080/2315] loss=1.5570 avg=1.5759 it/s=350.8\n",
      "[e5 b2311/2315] loss=1.4400 avg=1.5758 it/s=352.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÜ‚ñà‚ñÉ‚ñÅ‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÅ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñà‚ñà‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÖ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.0006</td></tr><tr><td>params/ratio</td><td>0.28177</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>78561029</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>109.92836</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57576</td></tr><tr><td>train/epoch_loss</td><td>1.57575</td></tr><tr><td>train/items_per_sec</td><td>352.34995</td></tr><tr><td>train/loss</td><td>1.56301</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_4</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/q33hqf9w' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/q33hqf9w</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_030118-q33hqf9w\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.0868771:  25%|‚ñà‚ñà‚ñå       | 5/20 [47:00<2:19:54, 559.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 4] f1=0.0869 | unfreeze_k=11 lr=9.72e-04 wd=2.4e-06 suggested_bs=32\n",
      "[I 2025-08-16 03:10:31,982] Trial 4 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 11, 'lr': 0.000972457938282282, 'weight_decay': 2.44725304634994e-06, 'batch_size': 32}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_031033-dbaj5myw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/dbaj5myw' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_5</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/dbaj5myw' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/dbaj5myw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n",
      "[e1 b1/2315] loss=1.5872 avg=1.5872 it/s=134.9\n",
      "[e1 b2/2315] loss=1.6386 avg=1.6129 it/s=183.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.7015 avg=1.6000 it/s=348.6\n",
      "[e1 b463/2315] loss=1.7169 avg=1.5976 it/s=349.9\n",
      "[e1 b694/2315] loss=1.5347 avg=1.5953 it/s=353.7\n",
      "[e1 b925/2315] loss=1.5900 avg=1.5954 it/s=356.9\n",
      "[e1 b1156/2315] loss=1.6938 avg=1.5934 it/s=360.5\n",
      "[e1 b1387/2315] loss=1.5294 avg=1.5907 it/s=360.4\n",
      "[e1 b1618/2315] loss=1.5021 avg=1.5902 it/s=362.0\n",
      "[e1 b1849/2315] loss=1.5310 avg=1.5890 it/s=363.8\n",
      "[e1 b2080/2315] loss=1.4835 avg=1.5868 it/s=367.0\n",
      "[e1 b2311/2315] loss=1.6154 avg=1.5853 it/s=370.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5853 | val_acc=0.2775 | val_f1=0.0869 | time=104.8s\n",
      "[e2 b1/2315] loss=1.5165 avg=1.5165 it/s=366.9\n",
      "[e2 b2/2315] loss=1.6145 avg=1.5655 it/s=335.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6088 avg=1.5816 it/s=365.0\n",
      "[e2 b463/2315] loss=1.5169 avg=1.5802 it/s=358.7\n",
      "[e2 b694/2315] loss=1.5104 avg=1.5770 it/s=354.1\n",
      "[e2 b925/2315] loss=1.6194 avg=1.5770 it/s=358.4\n",
      "[e2 b1156/2315] loss=1.5474 avg=1.5754 it/s=362.1\n",
      "[e2 b1387/2315] loss=1.6633 avg=1.5752 it/s=366.0\n",
      "[e2 b1618/2315] loss=1.6634 avg=1.5741 it/s=365.0\n",
      "[e2 b1849/2315] loss=1.6291 avg=1.5750 it/s=365.4\n",
      "[e2 b2080/2315] loss=1.5559 avg=1.5757 it/s=363.9\n",
      "[e2 b2311/2315] loss=1.4976 avg=1.5761 it/s=365.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5761 | val_acc=0.2775 | val_f1=0.0869 | time=105.8s\n",
      "[e3 b1/2315] loss=1.5508 avg=1.5508 it/s=408.3\n",
      "[e3 b2/2315] loss=1.5859 avg=1.5684 it/s=455.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5744 avg=1.5830 it/s=380.7\n",
      "[e3 b463/2315] loss=1.7045 avg=1.5832 it/s=382.2\n",
      "[e3 b694/2315] loss=1.5800 avg=1.5839 it/s=369.2\n",
      "[e3 b925/2315] loss=1.4985 avg=1.5807 it/s=369.5\n",
      "[e3 b1156/2315] loss=1.5716 avg=1.5801 it/s=366.6\n",
      "[e3 b1387/2315] loss=1.6324 avg=1.5782 it/s=367.0\n",
      "[e3 b1618/2315] loss=1.6817 avg=1.5782 it/s=366.3\n",
      "[e3 b1849/2315] loss=1.6026 avg=1.5775 it/s=366.4\n",
      "[e3 b2080/2315] loss=1.4609 avg=1.5773 it/s=367.4\n",
      "[e3 b2311/2315] loss=1.6115 avg=1.5777 it/s=367.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5778 | val_acc=0.2775 | val_f1=0.0869 | time=105.5s\n",
      "[e4 b1/2315] loss=1.6581 avg=1.6581 it/s=356.8\n",
      "[e4 b2/2315] loss=1.5281 avg=1.5931 it/s=419.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.6411 avg=1.5722 it/s=385.1\n",
      "[e4 b463/2315] loss=1.5977 avg=1.5749 it/s=389.9\n",
      "[e4 b694/2315] loss=1.6205 avg=1.5766 it/s=390.6\n",
      "[e4 b925/2315] loss=1.6608 avg=1.5773 it/s=385.8\n",
      "[e4 b1156/2315] loss=1.6518 avg=1.5765 it/s=375.4\n",
      "[e4 b1387/2315] loss=1.5106 avg=1.5760 it/s=368.0\n",
      "[e4 b1618/2315] loss=1.6137 avg=1.5758 it/s=365.9\n",
      "[e4 b1849/2315] loss=1.6417 avg=1.5756 it/s=367.0\n",
      "[e4 b2080/2315] loss=1.6311 avg=1.5760 it/s=366.5\n",
      "[e4 b2311/2315] loss=1.5174 avg=1.5756 it/s=367.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5756 | val_acc=0.2775 | val_f1=0.0869 | time=105.8s\n",
      "[e5 b1/2315] loss=1.5357 avg=1.5357 it/s=355.3\n",
      "[e5 b2/2315] loss=1.5856 avg=1.5606 it/s=338.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5079 avg=1.5795 it/s=379.4\n",
      "[e5 b463/2315] loss=1.5891 avg=1.5811 it/s=376.7\n",
      "[e5 b694/2315] loss=1.5522 avg=1.5803 it/s=375.1\n",
      "[e5 b925/2315] loss=1.6026 avg=1.5774 it/s=375.5\n",
      "[e5 b1156/2315] loss=1.6286 avg=1.5764 it/s=373.8\n",
      "[e5 b1387/2315] loss=1.5966 avg=1.5773 it/s=369.2\n",
      "[e5 b1618/2315] loss=1.6172 avg=1.5770 it/s=367.6\n",
      "[e5 b1849/2315] loss=1.5693 avg=1.5765 it/s=367.7\n",
      "[e5 b2080/2315] loss=1.4663 avg=1.5763 it/s=369.7\n",
      "[e5 b2311/2315] loss=1.5638 avg=1.5766 it/s=371.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÉ‚ñà‚ñá‚ñà‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÅ‚ñÉ‚ñÅ‚ñÇ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÉ‚ñÇ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÉ‚ñÑ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00346</td></tr><tr><td>params/ratio</td><td>0.25635</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>71473157</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>104.25235</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57661</td></tr><tr><td>train/epoch_loss</td><td>1.57662</td></tr><tr><td>train/items_per_sec</td><td>371.69358</td></tr><tr><td>train/loss</td><td>1.6149</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_5</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/dbaj5myw' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/dbaj5myw</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_031033-dbaj5myw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.0868771:  30%|‚ñà‚ñà‚ñà       | 6/20 [55:56<2:08:40, 551.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 5] f1=0.0869 | unfreeze_k=10 lr=5.58e-03 wd=3.6e-05 suggested_bs=4\n",
      "[I 2025-08-16 03:19:27,593] Trial 5 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 10, 'lr': 0.005577924778536141, 'weight_decay': 3.587344647763334e-05, 'batch_size': 4}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_031928-wm6k3ipf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/wm6k3ipf' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_6</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/wm6k3ipf' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/wm6k3ipf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n",
      "[e1 b1/2315] loss=1.5964 avg=1.5964 it/s=219.1\n",
      "[e1 b2/2315] loss=1.5804 avg=1.5884 it/s=250.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4293 avg=1.5170 it/s=369.7\n",
      "[e1 b463/2315] loss=1.1116 avg=1.3605 it/s=367.1\n",
      "[e1 b694/2315] loss=1.6282 avg=1.3897 it/s=363.9\n",
      "[e1 b925/2315] loss=1.5849 avg=1.4409 it/s=370.1\n",
      "[e1 b1156/2315] loss=1.6414 avg=1.4709 it/s=375.2\n",
      "[e1 b1387/2315] loss=1.6236 avg=1.4901 it/s=374.9\n",
      "[e1 b1618/2315] loss=1.6635 avg=1.5032 it/s=372.9\n",
      "[e1 b1849/2315] loss=1.5444 avg=1.5135 it/s=371.4\n",
      "[e1 b2080/2315] loss=1.5485 avg=1.5208 it/s=368.9\n",
      "[e1 b2311/2315] loss=1.5876 avg=1.5269 it/s=368.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5270 | val_acc=0.2775 | val_f1=0.0869 | time=105.2s\n",
      "[e2 b1/2315] loss=1.4975 avg=1.4975 it/s=309.3\n",
      "[e2 b2/2315] loss=1.5963 avg=1.5469 it/s=358.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.5886 avg=1.5846 it/s=353.5\n",
      "[e2 b463/2315] loss=1.4518 avg=1.5812 it/s=361.6\n",
      "[e2 b694/2315] loss=1.6056 avg=1.5817 it/s=363.6\n",
      "[e2 b925/2315] loss=1.6155 avg=1.5812 it/s=366.9\n",
      "[e2 b1156/2315] loss=1.6304 avg=1.5814 it/s=371.9\n",
      "[e2 b1387/2315] loss=1.4889 avg=1.5803 it/s=373.1\n",
      "[e2 b1618/2315] loss=1.5493 avg=1.5804 it/s=374.9\n",
      "[e2 b1849/2315] loss=1.6541 avg=1.5809 it/s=370.6\n",
      "[e2 b2080/2315] loss=1.6080 avg=1.5807 it/s=367.9\n",
      "[e2 b2311/2315] loss=1.5669 avg=1.5804 it/s=366.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5804 | val_acc=0.2775 | val_f1=0.0869 | time=106.0s\n",
      "[e3 b1/2315] loss=1.5524 avg=1.5524 it/s=352.3\n",
      "[e3 b2/2315] loss=1.6559 avg=1.6041 it/s=346.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5415 avg=1.5749 it/s=377.2\n",
      "[e3 b463/2315] loss=1.5897 avg=1.5753 it/s=375.1\n",
      "[e3 b694/2315] loss=1.6474 avg=1.5754 it/s=373.7\n",
      "[e3 b925/2315] loss=1.4304 avg=1.5759 it/s=371.3\n",
      "[e3 b1156/2315] loss=1.5343 avg=1.5773 it/s=368.1\n",
      "[e3 b1387/2315] loss=1.6368 avg=1.5778 it/s=371.5\n",
      "[e3 b1618/2315] loss=1.7438 avg=1.5783 it/s=374.4\n",
      "[e3 b1849/2315] loss=1.4793 avg=1.5786 it/s=377.1\n",
      "[e3 b2080/2315] loss=1.6489 avg=1.5786 it/s=375.2\n",
      "[e3 b2311/2315] loss=1.5204 avg=1.5787 it/s=371.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5787 | val_acc=0.2775 | val_f1=0.0869 | time=104.8s\n",
      "[e4 b1/2315] loss=1.6637 avg=1.6637 it/s=298.7\n",
      "[e4 b2/2315] loss=1.6908 avg=1.6772 it/s=299.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.6486 avg=1.5825 it/s=349.6\n",
      "[e4 b463/2315] loss=1.5055 avg=1.5767 it/s=350.3\n",
      "[e4 b694/2315] loss=1.6754 avg=1.5768 it/s=354.5\n",
      "[e4 b925/2315] loss=1.6348 avg=1.5767 it/s=357.6\n",
      "[e4 b1156/2315] loss=1.6478 avg=1.5768 it/s=359.0\n",
      "[e4 b1387/2315] loss=1.5606 avg=1.5776 it/s=359.3\n",
      "[e4 b1618/2315] loss=1.5329 avg=1.5779 it/s=363.9\n",
      "[e4 b1849/2315] loss=1.6214 avg=1.5778 it/s=367.9\n",
      "[e4 b2080/2315] loss=1.5702 avg=1.5779 it/s=371.5\n",
      "[e4 b2311/2315] loss=1.5989 avg=1.5780 it/s=373.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5780 | val_acc=0.2775 | val_f1=0.0869 | time=104.0s\n",
      "[e5 b1/2315] loss=1.5773 avg=1.5773 it/s=356.1\n",
      "[e5 b2/2315] loss=1.5464 avg=1.5619 it/s=392.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5407 avg=1.5784 it/s=358.3\n",
      "[e5 b463/2315] loss=1.4804 avg=1.5783 it/s=348.8\n",
      "[e5 b694/2315] loss=1.7122 avg=1.5790 it/s=346.3\n",
      "[e5 b925/2315] loss=1.5106 avg=1.5772 it/s=346.6\n",
      "[e5 b1156/2315] loss=1.5701 avg=1.5767 it/s=345.7\n",
      "[e5 b1387/2315] loss=1.6431 avg=1.5775 it/s=350.0\n",
      "[e5 b1618/2315] loss=1.5932 avg=1.5776 it/s=353.5\n",
      "[e5 b1849/2315] loss=1.4727 avg=1.5762 it/s=354.0\n",
      "[e5 b2080/2315] loss=1.5904 avg=1.5764 it/s=357.7\n",
      "[e5 b2311/2315] loss=1.5821 avg=1.5771 it/s=360.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñà‚ñà‚ñÑ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÜ‚ñÜ‚ñÑ‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00027</td></tr><tr><td>params/ratio</td><td>0.25635</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>71473157</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>107.5273</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57708</td></tr><tr><td>train/epoch_loss</td><td>1.57701</td></tr><tr><td>train/items_per_sec</td><td>360.18659</td></tr><tr><td>train/loss</td><td>1.64939</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_6</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/wm6k3ipf' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/wm6k3ipf</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_031928-wm6k3ipf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.0868771:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:04:53<1:58:27, 546.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 6] f1=0.0869 | unfreeze_k=10 lr=4.27e-04 wd=1.1e-05 suggested_bs=16\n",
      "[I 2025-08-16 03:28:24,646] Trial 6 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 10, 'lr': 0.00042707494144126896, 'weight_decay': 1.1005216961578654e-05, 'batch_size': 16}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_032825-2q2brd6j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2q2brd6j' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_7</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2q2brd6j' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2q2brd6j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 57,297,413 / 278,813,189 (20.55%) ; unfreeze_last_k=8\n",
      "[e1 b1/2315] loss=1.6088 avg=1.6088 it/s=267.3\n",
      "[e1 b2/2315] loss=1.6936 avg=1.6512 it/s=296.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5855 avg=1.5615 it/s=380.1\n",
      "[e1 b463/2315] loss=1.2108 avg=1.4585 it/s=380.3\n",
      "[e1 b694/2315] loss=1.0253 avg=1.3466 it/s=385.1\n",
      "[e1 b925/2315] loss=1.3797 avg=1.2653 it/s=391.4\n",
      "[e1 b1156/2315] loss=0.5378 avg=1.1974 it/s=399.3\n",
      "[e1 b1387/2315] loss=0.6789 avg=1.1460 it/s=401.4\n",
      "[e1 b1618/2315] loss=1.1922 avg=1.1068 it/s=404.2\n",
      "[e1 b1849/2315] loss=0.8633 avg=1.0748 it/s=407.1\n",
      "[e1 b2080/2315] loss=0.7380 avg=1.0510 it/s=410.7\n",
      "[e1 b2311/2315] loss=0.8870 avg=1.0226 it/s=412.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.0222 | val_acc=0.7259 | val_f1=0.7356 | time=94.6s\n",
      "[e2 b1/2315] loss=0.5331 avg=0.5331 it/s=389.1\n",
      "[e2 b2/2315] loss=0.6935 avg=0.6133 it/s=384.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.7953 avg=0.8235 it/s=450.9\n",
      "[e2 b463/2315] loss=1.1513 avg=0.8316 it/s=451.0\n",
      "[e2 b694/2315] loss=0.5906 avg=0.8264 it/s=443.8\n",
      "[e2 b925/2315] loss=1.2958 avg=0.8114 it/s=422.0\n",
      "[e2 b1156/2315] loss=0.9201 avg=0.8019 it/s=409.1\n",
      "[e2 b1387/2315] loss=1.0292 avg=0.7990 it/s=407.2\n",
      "[e2 b1618/2315] loss=0.9451 avg=0.7901 it/s=408.5\n",
      "[e2 b1849/2315] loss=0.8406 avg=0.7839 it/s=408.3\n",
      "[e2 b2080/2315] loss=0.7381 avg=0.7748 it/s=408.9\n",
      "[e2 b2311/2315] loss=0.7473 avg=0.7671 it/s=408.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=0.7675 | val_acc=0.7502 | val_f1=0.7579 | time=95.3s\n",
      "[e3 b1/2315] loss=0.7626 avg=0.7626 it/s=321.2\n",
      "[e3 b2/2315] loss=0.3377 avg=0.5502 it/s=348.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.8601 avg=0.6551 it/s=407.9\n",
      "[e3 b463/2315] loss=0.3726 avg=0.6491 it/s=411.3\n",
      "[e3 b694/2315] loss=0.3938 avg=0.6431 it/s=424.3\n",
      "[e3 b925/2315] loss=0.2758 avg=0.6454 it/s=430.6\n",
      "[e3 b1156/2315] loss=0.6477 avg=0.6361 it/s=431.6\n",
      "[e3 b1387/2315] loss=0.2946 avg=0.6300 it/s=428.9\n",
      "[e3 b1618/2315] loss=0.7511 avg=0.6264 it/s=420.5\n",
      "[e3 b1849/2315] loss=0.4650 avg=0.6218 it/s=418.3\n",
      "[e3 b2080/2315] loss=0.6790 avg=0.6155 it/s=417.6\n",
      "[e3 b2311/2315] loss=0.4684 avg=0.6152 it/s=417.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=0.6149 | val_acc=0.7677 | val_f1=0.7737 | time=93.6s\n",
      "[e4 b1/2315] loss=0.7162 avg=0.7162 it/s=318.7\n",
      "[e4 b2/2315] loss=0.7587 avg=0.7375 it/s=407.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.1359 avg=0.5629 it/s=397.1\n",
      "[e4 b463/2315] loss=0.5230 avg=0.5554 it/s=407.9\n",
      "[e4 b694/2315] loss=0.2238 avg=0.5467 it/s=407.3\n",
      "[e4 b925/2315] loss=0.6031 avg=0.5337 it/s=407.9\n",
      "[e4 b1156/2315] loss=0.6168 avg=0.5272 it/s=412.3\n",
      "[e4 b1387/2315] loss=0.3383 avg=0.5297 it/s=415.9\n",
      "[e4 b1618/2315] loss=0.5899 avg=0.5290 it/s=420.2\n",
      "[e4 b1849/2315] loss=0.5697 avg=0.5286 it/s=422.7\n",
      "[e4 b2080/2315] loss=0.4777 avg=0.5256 it/s=419.5\n",
      "[e4 b2311/2315] loss=0.3851 avg=0.5215 it/s=415.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=0.5212 | val_acc=0.8260 | val_f1=0.8315 | time=94.0s\n",
      "[e5 b1/2315] loss=0.2250 avg=0.2250 it/s=345.4\n",
      "[e5 b2/2315] loss=0.4246 avg=0.3248 it/s=369.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.7085 avg=0.4426 it/s=410.4\n",
      "[e5 b463/2315] loss=0.2381 avg=0.4431 it/s=421.1\n",
      "[e5 b694/2315] loss=0.3406 avg=0.4430 it/s=427.8\n",
      "[e5 b925/2315] loss=0.1217 avg=0.4459 it/s=421.6\n",
      "[e5 b1156/2315] loss=0.5031 avg=0.4489 it/s=425.1\n",
      "[e5 b1387/2315] loss=0.3544 avg=0.4476 it/s=422.6\n",
      "[e5 b1618/2315] loss=0.3058 avg=0.4438 it/s=422.1\n",
      "[e5 b1849/2315] loss=0.2810 avg=0.4423 it/s=423.6\n",
      "[e5 b2080/2315] loss=0.4732 avg=0.4448 it/s=424.6\n",
      "[e5 b2311/2315] loss=1.2225 avg=0.4453 it/s=426.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 | loss=0.4453 | val_acc=0.8355 | val_f1=0.8406 | time=91.6s\n",
      "[e6 b1/2315] loss=0.2308 avg=0.2308 it/s=400.5\n",
      "[e6 b2/2315] loss=0.2252 avg=0.2280 it/s=425.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.5146 avg=0.3830 it/s=388.0\n",
      "[e6 b463/2315] loss=0.0987 avg=0.3843 it/s=395.7\n",
      "[e6 b694/2315] loss=0.2279 avg=0.3958 it/s=400.2\n",
      "[e6 b925/2315] loss=0.7978 avg=0.3942 it/s=407.5\n",
      "[e6 b1156/2315] loss=0.8454 avg=0.3938 it/s=409.6\n",
      "[e6 b1387/2315] loss=0.1906 avg=0.3974 it/s=413.7\n",
      "[e6 b1618/2315] loss=0.9221 avg=0.3955 it/s=414.7\n",
      "[e6 b1849/2315] loss=0.5118 avg=0.3923 it/s=415.9\n",
      "[e6 b2080/2315] loss=0.5120 avg=0.3931 it/s=414.2\n",
      "[e6 b2311/2315] loss=0.1925 avg=0.3923 it/s=412.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 | loss=0.3925 | val_acc=0.8397 | val_f1=0.8448 | time=94.5s\n",
      "[e7 b1/2315] loss=0.1280 avg=0.1280 it/s=426.2\n",
      "[e7 b2/2315] loss=0.4097 avg=0.2688 it/s=420.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.0646 avg=0.3412 it/s=443.8\n",
      "[e7 b463/2315] loss=0.2577 avg=0.3384 it/s=448.1\n",
      "[e7 b694/2315] loss=0.7532 avg=0.3455 it/s=447.6\n",
      "[e7 b925/2315] loss=0.1608 avg=0.3520 it/s=435.4\n",
      "[e7 b1156/2315] loss=0.2818 avg=0.3521 it/s=426.6\n",
      "[e7 b1387/2315] loss=0.2868 avg=0.3501 it/s=421.6\n",
      "[e7 b1618/2315] loss=0.3685 avg=0.3473 it/s=418.4\n",
      "[e7 b1849/2315] loss=0.1103 avg=0.3460 it/s=415.0\n",
      "[e7 b2080/2315] loss=0.0447 avg=0.3433 it/s=415.1\n",
      "[e7 b2311/2315] loss=0.1556 avg=0.3446 it/s=415.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 | loss=0.3445 | val_acc=0.8431 | val_f1=0.8489 | time=94.1s\n",
      "[e8 b1/2315] loss=0.5413 avg=0.5413 it/s=386.3\n",
      "[e8 b2/2315] loss=0.0901 avg=0.3157 it/s=414.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e8 b232/2315] loss=0.0318 avg=0.2792 it/s=417.8\n",
      "[e8 b463/2315] loss=0.1715 avg=0.2936 it/s=416.6\n",
      "[e8 b694/2315] loss=0.5437 avg=0.2947 it/s=421.7\n",
      "[e8 b925/2315] loss=0.1087 avg=0.2943 it/s=425.7\n",
      "[e8 b1156/2315] loss=0.4963 avg=0.2950 it/s=427.6\n",
      "[e8 b1387/2315] loss=0.3400 avg=0.2916 it/s=429.3\n",
      "[e8 b1618/2315] loss=0.6454 avg=0.2909 it/s=422.1\n",
      "[e8 b1849/2315] loss=0.0312 avg=0.2912 it/s=417.0\n",
      "[e8 b2080/2315] loss=0.1026 avg=0.2923 it/s=413.9\n",
      "[e8 b2311/2315] loss=0.2240 avg=0.2919 it/s=414.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 | loss=0.2918 | val_acc=0.8416 | val_f1=0.8457 | time=94.2s\n",
      "[e9 b1/2315] loss=0.3393 avg=0.3393 it/s=436.4\n",
      "[e9 b2/2315] loss=0.2149 avg=0.2771 it/s=425.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e9 b232/2315] loss=0.0461 avg=0.2330 it/s=424.0\n",
      "[e9 b463/2315] loss=0.2691 avg=0.2407 it/s=422.7\n",
      "[e9 b694/2315] loss=0.6171 avg=0.2426 it/s=419.0\n",
      "[e9 b925/2315] loss=0.0374 avg=0.2434 it/s=419.3\n",
      "[e9 b1156/2315] loss=0.2369 avg=0.2432 it/s=421.3\n",
      "[e9 b1387/2315] loss=0.3221 avg=0.2448 it/s=424.4\n",
      "[e9 b1618/2315] loss=0.3104 avg=0.2483 it/s=426.2\n",
      "[e9 b1849/2315] loss=1.1139 avg=0.2494 it/s=429.2\n",
      "[e9 b2080/2315] loss=0.0212 avg=0.2487 it/s=431.4\n",
      "[e9 b2311/2315] loss=0.0080 avg=0.2480 it/s=428.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 | loss=0.2478 | val_acc=0.8571 | val_f1=0.8624 | time=91.7s\n",
      "[e10 b1/2315] loss=0.0115 avg=0.0115 it/s=358.4\n",
      "[e10 b2/2315] loss=0.3032 avg=0.1573 it/s=372.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e10 b232/2315] loss=0.1145 avg=0.1873 it/s=382.8\n",
      "[e10 b463/2315] loss=0.0120 avg=0.1999 it/s=388.7\n",
      "[e10 b694/2315] loss=0.6186 avg=0.2106 it/s=401.3\n",
      "[e10 b925/2315] loss=0.2373 avg=0.2122 it/s=407.6\n",
      "[e10 b1156/2315] loss=0.0178 avg=0.2117 it/s=410.6\n",
      "[e10 b1387/2315] loss=0.1070 avg=0.2125 it/s=408.6\n",
      "[e10 b1618/2315] loss=0.1147 avg=0.2107 it/s=402.1\n",
      "[e10 b1849/2315] loss=0.2186 avg=0.2078 it/s=402.5\n",
      "[e10 b2080/2315] loss=0.2331 avg=0.2087 it/s=407.2\n",
      "[e10 b2311/2315] loss=1.0675 avg=0.2080 it/s=410.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 | loss=0.2082 | val_acc=0.8576 | val_f1=0.8625 | time=95.1s\n",
      "[e11 b1/2315] loss=0.1540 avg=0.1540 it/s=353.3\n",
      "[e11 b2/2315] loss=0.6777 avg=0.4158 it/s=425.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e11 b232/2315] loss=0.5734 avg=0.1887 it/s=446.2\n",
      "[e11 b463/2315] loss=0.3417 avg=0.1795 it/s=423.1\n",
      "[e11 b694/2315] loss=0.0059 avg=0.1790 it/s=413.6\n",
      "[e11 b925/2315] loss=0.7780 avg=0.1814 it/s=409.3\n",
      "[e11 b1156/2315] loss=0.0257 avg=0.1839 it/s=407.2\n",
      "[e11 b1387/2315] loss=0.0538 avg=0.1781 it/s=410.0\n",
      "[e11 b1618/2315] loss=0.3056 avg=0.1774 it/s=410.6\n",
      "[e11 b1849/2315] loss=0.0070 avg=0.1781 it/s=410.9\n",
      "[e11 b2080/2315] loss=0.3084 avg=0.1762 it/s=410.8\n",
      "[e11 b2311/2315] loss=0.0138 avg=0.1751 it/s=412.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 | loss=0.1750 | val_acc=0.8613 | val_f1=0.8661 | time=94.4s\n",
      "[e12 b1/2315] loss=0.2205 avg=0.2205 it/s=412.5\n",
      "[e12 b2/2315] loss=0.2044 avg=0.2125 it/s=371.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e12 b232/2315] loss=0.0325 avg=0.1424 it/s=440.1\n",
      "[e12 b463/2315] loss=0.9202 avg=0.1486 it/s=446.9\n",
      "[e12 b694/2315] loss=0.0096 avg=0.1506 it/s=448.0\n",
      "[e12 b925/2315] loss=0.0058 avg=0.1503 it/s=447.2\n",
      "[e12 b1156/2315] loss=0.0104 avg=0.1456 it/s=439.3\n",
      "[e12 b1387/2315] loss=0.0054 avg=0.1479 it/s=433.8\n",
      "[e12 b1618/2315] loss=0.4413 avg=0.1509 it/s=427.2\n",
      "[e12 b1849/2315] loss=0.0096 avg=0.1505 it/s=425.1\n",
      "[e12 b2080/2315] loss=0.4762 avg=0.1471 it/s=422.9\n",
      "[e12 b2311/2315] loss=0.2654 avg=0.1478 it/s=422.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 | loss=0.1478 | val_acc=0.8593 | val_f1=0.8639 | time=92.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñá‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñá‚ñÜ‚ñÜ‚ñÅ‚ñà‚ñÜ‚ñÉ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÉ‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.86611</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.2055</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>57297413</td></tr><tr><td>step</td><td>27776</td></tr><tr><td>time/epoch_sec</td><td>92.42218</td></tr><tr><td>train/avg_loss_so_far</td><td>0.14782</td></tr><tr><td>train/epoch_loss</td><td>0.14783</td></tr><tr><td>train/items_per_sec</td><td>422.90075</td></tr><tr><td>train/loss</td><td>0.0163</td></tr><tr><td>val/acc</td><td>0.85933</td></tr><tr><td>val/best_epoch</td><td>11</td></tr><tr><td>val/best_f1_so_far</td><td>0.86611</td></tr><tr><td>val/f1</td><td>0.86393</td></tr><tr><td>val/precision</td><td>0.86093</td></tr><tr><td>val/recall</td><td>0.86759</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_7</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2q2brd6j' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2q2brd6j</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_032825-2q2brd6j\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 7. Best value: 0.866111:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:23:57<2:27:21, 736.83s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 7] f1=0.8661 | unfreeze_k=8 lr=1.18e-04 wd=3.0e-05 suggested_bs=64\n",
      "[I 2025-08-16 03:47:28,442] Trial 7 finished with value: 0.8661112815752661 and parameters: {'num_unfreeze_last_layers': 8, 'lr': 0.00011796756021837212, 'weight_decay': 2.981045031286568e-05, 'batch_size': 64}. Best is trial 7 with value: 0.8661112815752661.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_034729-aejlc76a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/aejlc76a' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_8</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/aejlc76a' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/aejlc76a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[e1 b1/2315] loss=1.5906 avg=1.5906 it/s=116.7\n",
      "[e1 b2/2315] loss=1.6004 avg=1.5955 it/s=162.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5227 avg=1.5610 it/s=335.7\n",
      "[e1 b463/2315] loss=1.1630 avg=1.4573 it/s=339.9\n",
      "[e1 b694/2315] loss=1.0807 avg=1.3281 it/s=343.5\n",
      "[e1 b925/2315] loss=0.8026 avg=1.2396 it/s=344.3\n",
      "[e1 b1156/2315] loss=0.7963 avg=1.1762 it/s=347.0\n",
      "[e1 b1387/2315] loss=1.0412 avg=1.1352 it/s=340.7\n",
      "[e1 b1618/2315] loss=0.6206 avg=1.1053 it/s=336.3\n",
      "[e1 b1849/2315] loss=0.6923 avg=1.0804 it/s=336.1\n",
      "[e1 b2080/2315] loss=0.9028 avg=1.0614 it/s=337.2\n",
      "[e1 b2311/2315] loss=0.7917 avg=1.0444 it/s=338.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.0440 | val_acc=0.7058 | val_f1=0.7143 | time=114.1s\n",
      "[e2 b1/2315] loss=0.6460 avg=0.6460 it/s=227.4\n",
      "[e2 b2/2315] loss=0.6993 avg=0.6726 it/s=268.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.0409 avg=0.8494 it/s=328.4\n",
      "[e2 b463/2315] loss=0.7749 avg=0.8555 it/s=325.4\n",
      "[e2 b694/2315] loss=0.7434 avg=0.8376 it/s=330.2\n",
      "[e2 b925/2315] loss=0.6527 avg=0.8329 it/s=334.3\n",
      "[e2 b1156/2315] loss=0.6264 avg=0.8318 it/s=338.9\n",
      "[e2 b1387/2315] loss=0.6620 avg=0.8192 it/s=337.8\n",
      "[e2 b1618/2315] loss=0.4748 avg=0.8078 it/s=333.0\n",
      "[e2 b1849/2315] loss=0.9291 avg=0.7996 it/s=331.2\n",
      "[e2 b2080/2315] loss=0.5288 avg=0.7921 it/s=330.1\n",
      "[e2 b2311/2315] loss=0.8770 avg=0.7865 it/s=330.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=0.7863 | val_acc=0.7247 | val_f1=0.7314 | time=117.0s\n",
      "[e3 b1/2315] loss=0.7788 avg=0.7788 it/s=307.3\n",
      "[e3 b2/2315] loss=0.8510 avg=0.8149 it/s=301.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.9150 avg=0.7552 it/s=327.7\n",
      "[e3 b463/2315] loss=0.4817 avg=0.7627 it/s=325.5\n",
      "[e3 b694/2315] loss=0.7236 avg=0.7532 it/s=327.7\n",
      "[e3 b925/2315] loss=0.1977 avg=0.7425 it/s=333.7\n",
      "[e3 b1156/2315] loss=0.7167 avg=0.7328 it/s=337.0\n",
      "[e3 b1387/2315] loss=0.6389 avg=0.7221 it/s=335.1\n",
      "[e3 b1618/2315] loss=0.5884 avg=0.7144 it/s=332.2\n",
      "[e3 b1849/2315] loss=0.3464 avg=0.7116 it/s=328.7\n",
      "[e3 b2080/2315] loss=0.8675 avg=0.7153 it/s=328.1\n",
      "[e3 b2311/2315] loss=0.7048 avg=0.7129 it/s=329.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=0.7128 | val_acc=0.7515 | val_f1=0.7608 | time=117.4s\n",
      "[e4 b1/2315] loss=0.4075 avg=0.4075 it/s=238.6\n",
      "[e4 b2/2315] loss=0.5164 avg=0.4619 it/s=263.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.8834 avg=0.6276 it/s=335.4\n",
      "[e4 b463/2315] loss=0.6995 avg=0.6386 it/s=339.8\n",
      "[e4 b694/2315] loss=0.4644 avg=0.6335 it/s=340.7\n",
      "[e4 b925/2315] loss=0.4245 avg=0.6311 it/s=344.1\n",
      "[e4 b1156/2315] loss=0.6617 avg=0.6371 it/s=346.3\n",
      "[e4 b1387/2315] loss=0.6949 avg=0.6343 it/s=346.6\n",
      "[e4 b1618/2315] loss=0.7790 avg=0.6353 it/s=340.6\n",
      "[e4 b1849/2315] loss=0.4211 avg=0.6356 it/s=338.4\n",
      "[e4 b2080/2315] loss=0.4639 avg=0.6324 it/s=337.6\n",
      "[e4 b2311/2315] loss=0.3761 avg=0.6276 it/s=338.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=0.6277 | val_acc=0.7949 | val_f1=0.8012 | time=114.3s\n",
      "[e5 b1/2315] loss=0.7334 avg=0.7334 it/s=297.8\n",
      "[e5 b2/2315] loss=0.3572 avg=0.5453 it/s=295.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.4321 avg=0.5281 it/s=341.2\n",
      "[e5 b463/2315] loss=0.7806 avg=0.5578 it/s=327.6\n",
      "[e5 b694/2315] loss=0.9168 avg=0.5563 it/s=327.2\n",
      "[e5 b925/2315] loss=0.8039 avg=0.5601 it/s=333.6\n",
      "[e5 b1156/2315] loss=0.6044 avg=0.5567 it/s=338.5\n",
      "[e5 b1387/2315] loss=0.3032 avg=0.5570 it/s=341.6\n",
      "[e5 b1618/2315] loss=0.6337 avg=0.5533 it/s=339.6\n",
      "[e5 b1849/2315] loss=0.4399 avg=0.5504 it/s=336.3\n",
      "[e5 b2080/2315] loss=0.6536 avg=0.5475 it/s=334.5\n",
      "[e5 b2311/2315] loss=0.7084 avg=0.5453 it/s=331.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 | loss=0.5459 | val_acc=0.7940 | val_f1=0.8023 | time=116.7s\n",
      "[e6 b1/2315] loss=0.7295 avg=0.7295 it/s=288.1\n",
      "[e6 b2/2315] loss=0.5249 avg=0.6272 it/s=311.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2257 avg=0.4799 it/s=331.1\n",
      "[e6 b463/2315] loss=0.6032 avg=0.4824 it/s=336.2\n",
      "[e6 b694/2315] loss=0.3541 avg=0.4916 it/s=338.0\n",
      "[e6 b925/2315] loss=1.0902 avg=0.4956 it/s=340.0\n",
      "[e6 b1156/2315] loss=0.3937 avg=0.4984 it/s=343.1\n",
      "[e6 b1387/2315] loss=0.7798 avg=0.4975 it/s=344.9\n",
      "[e6 b1618/2315] loss=0.2966 avg=0.4913 it/s=341.6\n",
      "[e6 b1849/2315] loss=0.2350 avg=0.4903 it/s=336.3\n",
      "[e6 b2080/2315] loss=0.5908 avg=0.4880 it/s=333.1\n",
      "[e6 b2311/2315] loss=0.5248 avg=0.4827 it/s=334.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 | loss=0.4830 | val_acc=0.8039 | val_f1=0.8054 | time=115.4s\n",
      "[e7 b1/2315] loss=0.3346 avg=0.3346 it/s=313.4\n",
      "[e7 b2/2315] loss=0.3684 avg=0.3515 it/s=306.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.1581 avg=0.4367 it/s=343.6\n",
      "[e7 b463/2315] loss=0.6417 avg=0.4272 it/s=341.5\n",
      "[e7 b694/2315] loss=0.1136 avg=0.4262 it/s=337.0\n",
      "[e7 b925/2315] loss=0.5892 avg=0.4200 it/s=338.8\n",
      "[e7 b1156/2315] loss=0.6244 avg=0.4186 it/s=341.6\n",
      "[e7 b1387/2315] loss=0.4607 avg=0.4157 it/s=343.1\n",
      "[e7 b1618/2315] loss=0.3270 avg=0.4119 it/s=343.1\n",
      "[e7 b1849/2315] loss=0.2044 avg=0.4085 it/s=341.1\n",
      "[e7 b2080/2315] loss=0.2486 avg=0.4067 it/s=339.6\n",
      "[e7 b2311/2315] loss=0.2738 avg=0.4062 it/s=337.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 | loss=0.4064 | val_acc=0.8226 | val_f1=0.8283 | time=114.6s\n",
      "[e8 b1/2315] loss=0.2750 avg=0.2750 it/s=276.5\n",
      "[e8 b2/2315] loss=0.0707 avg=0.1729 it/s=341.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e8 b232/2315] loss=0.4178 avg=0.3702 it/s=346.6\n",
      "[e8 b463/2315] loss=0.5429 avg=0.3445 it/s=335.6\n",
      "[e8 b694/2315] loss=0.1056 avg=0.3459 it/s=331.3\n",
      "[e8 b925/2315] loss=0.4455 avg=0.3490 it/s=329.1\n",
      "[e8 b1156/2315] loss=0.2290 avg=0.3476 it/s=334.3\n",
      "[e8 b1387/2315] loss=0.1374 avg=0.3479 it/s=337.9\n",
      "[e8 b1618/2315] loss=0.0688 avg=0.3488 it/s=341.2\n",
      "[e8 b1849/2315] loss=0.0827 avg=0.3469 it/s=339.8\n",
      "[e8 b2080/2315] loss=0.2884 avg=0.3462 it/s=338.3\n",
      "[e8 b2311/2315] loss=0.8219 avg=0.3460 it/s=338.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 | loss=0.3460 | val_acc=0.8414 | val_f1=0.8461 | time=114.2s\n",
      "[e9 b1/2315] loss=0.0603 avg=0.0603 it/s=267.9\n",
      "[e9 b2/2315] loss=0.2789 avg=0.1696 it/s=295.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e9 b232/2315] loss=0.4551 avg=0.2963 it/s=337.5\n",
      "[e9 b463/2315] loss=0.0233 avg=0.2996 it/s=341.0\n",
      "[e9 b694/2315] loss=0.0943 avg=0.2913 it/s=338.8\n",
      "[e9 b925/2315] loss=0.3323 avg=0.2971 it/s=335.3\n",
      "[e9 b1156/2315] loss=0.0689 avg=0.3004 it/s=336.5\n",
      "[e9 b1387/2315] loss=0.2972 avg=0.3005 it/s=339.0\n",
      "[e9 b1618/2315] loss=0.1462 avg=0.2991 it/s=341.9\n",
      "[e9 b1849/2315] loss=0.0763 avg=0.2961 it/s=339.8\n",
      "[e9 b2080/2315] loss=0.5195 avg=0.3001 it/s=334.9\n",
      "[e9 b2311/2315] loss=0.1937 avg=0.3005 it/s=332.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 | loss=0.3004 | val_acc=0.8372 | val_f1=0.8420 | time=116.4s\n",
      "[e10 b1/2315] loss=0.3307 avg=0.3307 it/s=324.2\n",
      "[e10 b2/2315] loss=0.4196 avg=0.3752 it/s=353.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e10 b232/2315] loss=0.7900 avg=0.2830 it/s=324.1\n",
      "[e10 b463/2315] loss=0.0722 avg=0.2659 it/s=325.3\n",
      "[e10 b694/2315] loss=0.3437 avg=0.2638 it/s=330.6\n",
      "[e10 b925/2315] loss=0.7438 avg=0.2647 it/s=334.2\n",
      "[e10 b1156/2315] loss=0.2349 avg=0.2585 it/s=336.0\n",
      "[e10 b1387/2315] loss=0.6575 avg=0.2579 it/s=339.1\n",
      "[e10 b1618/2315] loss=0.1110 avg=0.2520 it/s=342.0\n",
      "[e10 b1849/2315] loss=0.0653 avg=0.2518 it/s=343.2\n",
      "[e10 b2080/2315] loss=0.1145 avg=0.2511 it/s=343.0\n",
      "[e10 b2311/2315] loss=0.2309 avg=0.2502 it/s=340.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 | loss=0.2501 | val_acc=0.8438 | val_f1=0.8479 | time=113.7s\n",
      "[e11 b1/2315] loss=0.4378 avg=0.4378 it/s=314.6\n",
      "[e11 b2/2315] loss=0.4188 avg=0.4283 it/s=317.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e11 b232/2315] loss=0.3313 avg=0.2077 it/s=337.3\n",
      "[e11 b463/2315] loss=0.0290 avg=0.2085 it/s=341.2\n",
      "[e11 b694/2315] loss=0.6323 avg=0.2096 it/s=337.0\n",
      "[e11 b925/2315] loss=0.0332 avg=0.2090 it/s=333.9\n",
      "[e11 b1156/2315] loss=0.7619 avg=0.2083 it/s=330.8\n",
      "[e11 b1387/2315] loss=0.0086 avg=0.2088 it/s=328.0\n",
      "[e11 b1618/2315] loss=0.3504 avg=0.2083 it/s=327.5\n",
      "[e11 b1849/2315] loss=0.2199 avg=0.2092 it/s=324.2\n",
      "[e11 b2080/2315] loss=0.3522 avg=0.2086 it/s=324.3\n",
      "[e11 b2311/2315] loss=0.2729 avg=0.2046 it/s=324.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 | loss=0.2046 | val_acc=0.8593 | val_f1=0.8627 | time=119.3s\n",
      "[e12 b1/2315] loss=0.0186 avg=0.0186 it/s=310.8\n",
      "[e12 b2/2315] loss=0.0057 avg=0.0122 it/s=332.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e12 b232/2315] loss=0.0102 avg=0.1612 it/s=307.7\n",
      "[e12 b463/2315] loss=0.3372 avg=0.1655 it/s=313.2\n",
      "[e12 b694/2315] loss=0.0847 avg=0.1673 it/s=317.3\n",
      "[e12 b925/2315] loss=0.0064 avg=0.1677 it/s=322.4\n",
      "[e12 b1156/2315] loss=0.2920 avg=0.1673 it/s=324.9\n",
      "[e12 b1387/2315] loss=0.0274 avg=0.1682 it/s=328.1\n",
      "[e12 b1618/2315] loss=0.0739 avg=0.1704 it/s=331.2\n",
      "[e12 b1849/2315] loss=0.0179 avg=0.1688 it/s=333.6\n",
      "[e12 b2080/2315] loss=0.0215 avg=0.1696 it/s=330.2\n",
      "[e12 b2311/2315] loss=0.3403 avg=0.1676 it/s=329.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 | loss=0.1680 | val_acc=0.8557 | val_f1=0.8593 | time=117.3s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñà‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ‚ñÖ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.86273</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>27776</td></tr><tr><td>time/epoch_sec</td><td>117.31185</td></tr><tr><td>train/avg_loss_so_far</td><td>0.16763</td></tr><tr><td>train/epoch_loss</td><td>0.16803</td></tr><tr><td>train/items_per_sec</td><td>329.53415</td></tr><tr><td>train/loss</td><td>0.0899</td></tr><tr><td>val/acc</td><td>0.85569</td></tr><tr><td>val/best_epoch</td><td>11</td></tr><tr><td>val/best_f1_so_far</td><td>0.86273</td></tr><tr><td>val/f1</td><td>0.85932</td></tr><tr><td>val/precision</td><td>0.85442</td></tr><tr><td>val/recall</td><td>0.86578</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_8</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/aejlc76a' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/aejlc76a</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_034729-aejlc76a\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 7. Best value: 0.866111:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:47:27<2:53:40, 947.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 8] f1=0.8627 | unfreeze_k=12 lr=1.12e-04 wd=1.9e-06 suggested_bs=64\n",
      "[I 2025-08-16 04:10:58,654] Trial 8 finished with value: 0.8627319723752823 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 0.00011188289822099445, 'weight_decay': 1.907820133444184e-06, 'batch_size': 64}. Best is trial 7 with value: 0.8661112815752661.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_041100-9v8ol68o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/9v8ol68o' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_9</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/9v8ol68o' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/9v8ol68o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[e1 b1/2315] loss=1.6074 avg=1.6074 it/s=107.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b2/2315] loss=1.6571 avg=1.6323 it/s=149.0\n",
      "[e1 b232/2315] loss=1.6314 avg=1.5917 it/s=336.5\n",
      "[e1 b463/2315] loss=1.5659 avg=1.5916 it/s=341.1\n",
      "[e1 b694/2315] loss=1.8088 avg=1.5984 it/s=344.4\n",
      "[e1 b925/2315] loss=1.4641 avg=1.5955 it/s=345.8\n",
      "[e1 b1156/2315] loss=1.5447 avg=1.5938 it/s=347.3\n",
      "[e1 b1387/2315] loss=1.6746 avg=1.5910 it/s=349.5\n",
      "[e1 b1618/2315] loss=1.6496 avg=1.5922 it/s=350.4\n",
      "[e1 b1849/2315] loss=1.6432 avg=1.5912 it/s=349.2\n",
      "[e1 b2080/2315] loss=1.5245 avg=1.5895 it/s=345.0\n",
      "[e1 b2311/2315] loss=1.5857 avg=1.5882 it/s=341.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5880 | val_acc=0.2775 | val_f1=0.0869 | time=113.5s\n",
      "[e2 b1/2315] loss=1.5201 avg=1.5201 it/s=311.1\n",
      "[e2 b2/2315] loss=1.6033 avg=1.5617 it/s=336.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6502 avg=1.5756 it/s=340.6\n",
      "[e2 b463/2315] loss=1.5309 avg=1.5778 it/s=337.2\n",
      "[e2 b694/2315] loss=1.7190 avg=1.5782 it/s=328.7\n",
      "[e2 b925/2315] loss=1.5597 avg=1.5767 it/s=329.7\n",
      "[e2 b1156/2315] loss=1.6124 avg=1.5768 it/s=324.1\n",
      "[e2 b1387/2315] loss=1.5072 avg=1.5772 it/s=328.1\n",
      "[e2 b1618/2315] loss=1.4943 avg=1.5773 it/s=332.0\n",
      "[e2 b1849/2315] loss=1.5887 avg=1.5768 it/s=335.3\n",
      "[e2 b2080/2315] loss=1.6874 avg=1.5760 it/s=334.1\n",
      "[e2 b2311/2315] loss=1.5985 avg=1.5753 it/s=331.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5752 | val_acc=0.2775 | val_f1=0.0869 | time=116.6s\n",
      "[e3 b1/2315] loss=1.5978 avg=1.5978 it/s=273.8\n",
      "[e3 b2/2315] loss=1.6064 avg=1.6021 it/s=302.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5817 avg=1.5805 it/s=298.1\n",
      "[e3 b463/2315] loss=1.4832 avg=1.5782 it/s=310.7\n",
      "[e3 b694/2315] loss=1.5686 avg=1.5771 it/s=312.8\n",
      "[e3 b925/2315] loss=1.7594 avg=1.5756 it/s=317.3\n",
      "[e3 b1156/2315] loss=1.5554 avg=1.5750 it/s=319.6\n",
      "[e3 b1387/2315] loss=1.6730 avg=1.5743 it/s=324.2\n",
      "[e3 b1618/2315] loss=1.3810 avg=1.5749 it/s=327.7\n",
      "[e3 b1849/2315] loss=1.6002 avg=1.5757 it/s=330.1\n",
      "[e3 b2080/2315] loss=1.4804 avg=1.5749 it/s=329.3\n",
      "[e3 b2311/2315] loss=1.6330 avg=1.5754 it/s=329.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5755 | val_acc=0.2775 | val_f1=0.0869 | time=117.4s\n",
      "[e4 b1/2315] loss=1.4830 avg=1.4830 it/s=256.2\n",
      "[e4 b2/2315] loss=1.7079 avg=1.5955 it/s=282.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.6338 avg=1.5798 it/s=331.5\n",
      "[e4 b463/2315] loss=1.5332 avg=1.5732 it/s=338.5\n",
      "[e4 b694/2315] loss=1.6436 avg=1.5747 it/s=343.1\n",
      "[e4 b925/2315] loss=1.6740 avg=1.5757 it/s=344.5\n",
      "[e4 b1156/2315] loss=1.6276 avg=1.5759 it/s=346.3\n",
      "[e4 b1387/2315] loss=1.5473 avg=1.5751 it/s=345.7\n",
      "[e4 b1618/2315] loss=1.5941 avg=1.5754 it/s=347.7\n",
      "[e4 b1849/2315] loss=1.5875 avg=1.5751 it/s=349.0\n",
      "[e4 b2080/2315] loss=1.4742 avg=1.5753 it/s=350.2\n",
      "[e4 b2311/2315] loss=1.5502 avg=1.5753 it/s=346.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5754 | val_acc=0.2775 | val_f1=0.0869 | time=111.9s\n",
      "[e5 b1/2315] loss=1.6488 avg=1.6488 it/s=343.8\n",
      "[e5 b2/2315] loss=1.5738 avg=1.6113 it/s=336.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5515 avg=1.5772 it/s=323.2\n",
      "[e5 b463/2315] loss=1.5757 avg=1.5799 it/s=328.0\n",
      "[e5 b694/2315] loss=1.6036 avg=1.5765 it/s=331.6\n",
      "[e5 b925/2315] loss=1.5834 avg=1.5771 it/s=335.2\n",
      "[e5 b1156/2315] loss=1.5903 avg=1.5754 it/s=334.8\n",
      "[e5 b1387/2315] loss=1.5117 avg=1.5760 it/s=332.6\n",
      "[e5 b1618/2315] loss=1.6593 avg=1.5759 it/s=334.2\n",
      "[e5 b1849/2315] loss=1.5279 avg=1.5755 it/s=335.4\n",
      "[e5 b2080/2315] loss=1.4750 avg=1.5754 it/s=335.6\n",
      "[e5 b2311/2315] loss=1.5521 avg=1.5753 it/s=335.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÉ‚ñá‚ñà‚ñÅ‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñÉ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÖ‚ñÑ‚ñá‚ñÇ‚ñá‚ñá‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÅ‚ñá‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00198</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>115.44319</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57533</td></tr><tr><td>train/epoch_loss</td><td>1.57531</td></tr><tr><td>train/items_per_sec</td><td>335.53142</td></tr><tr><td>train/loss</td><td>1.67367</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_9</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/9v8ol68o' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/9v8ol68o</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_041100-9v8ol68o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 7. Best value: 0.866111:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:57:12<2:19:14, 835.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 9] f1=0.0869 | unfreeze_k=12 lr=3.20e-03 wd=2.1e-05 suggested_bs=16\n",
      "[I 2025-08-16 04:20:43,670] Trial 9 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 0.00319802754676831, 'weight_decay': 2.0716735238547033e-05, 'batch_size': 16}. Best is trial 7 with value: 0.8661112815752661.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_042045-pq2v1gjt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/pq2v1gjt' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_10</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/pq2v1gjt' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/pq2v1gjt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 57,297,413 / 278,813,189 (20.55%) ; unfreeze_last_k=8\n",
      "[e1 b1/2315] loss=1.6636 avg=1.6636 it/s=110.5\n",
      "[e1 b2/2315] loss=1.6748 avg=1.6692 it/s=165.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.3313 avg=1.5650 it/s=388.6\n",
      "[e1 b463/2315] loss=1.4368 avg=1.4286 it/s=400.6\n",
      "[e1 b694/2315] loss=0.9870 avg=1.3233 it/s=408.8\n",
      "[e1 b925/2315] loss=1.1821 avg=1.2514 it/s=410.9\n",
      "[e1 b1156/2315] loss=1.2061 avg=1.2186 it/s=413.0\n",
      "[e1 b1387/2315] loss=1.2778 avg=1.2372 it/s=413.5\n",
      "[e1 b1618/2315] loss=1.6307 avg=1.2833 it/s=414.0\n",
      "[e1 b1849/2315] loss=1.6043 avg=1.3209 it/s=415.9\n",
      "[e1 b2080/2315] loss=1.7989 avg=1.3499 it/s=417.4\n",
      "[e1 b2311/2315] loss=1.6437 avg=1.3731 it/s=418.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.3733 | val_acc=0.2410 | val_f1=0.0777 | time=93.2s\n",
      "[e2 b1/2315] loss=1.5059 avg=1.5059 it/s=388.7\n",
      "[e2 b2/2315] loss=1.6141 avg=1.5600 it/s=411.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6005 avg=1.5830 it/s=402.9\n",
      "[e2 b463/2315] loss=1.5913 avg=1.5815 it/s=397.6\n",
      "[e2 b694/2315] loss=1.6431 avg=1.5817 it/s=388.4\n",
      "[e2 b925/2315] loss=1.5699 avg=1.5801 it/s=387.3\n",
      "[e2 b1156/2315] loss=1.5077 avg=1.5797 it/s=388.5\n",
      "[e2 b1387/2315] loss=1.6374 avg=1.5790 it/s=391.2\n",
      "[e2 b1618/2315] loss=1.4535 avg=1.5786 it/s=391.7\n",
      "[e2 b1849/2315] loss=1.5939 avg=1.5789 it/s=393.2\n",
      "[e2 b2080/2315] loss=1.6545 avg=1.5779 it/s=394.0\n",
      "[e2 b2311/2315] loss=1.4607 avg=1.5776 it/s=396.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5777 | val_acc=0.2775 | val_f1=0.0869 | time=98.2s\n",
      "[e3 b1/2315] loss=1.5554 avg=1.5554 it/s=444.6\n",
      "[e3 b2/2315] loss=1.5894 avg=1.5724 it/s=456.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.6224 avg=1.5784 it/s=435.6\n",
      "[e3 b463/2315] loss=1.5086 avg=1.5763 it/s=435.9\n",
      "[e3 b694/2315] loss=1.5520 avg=1.5742 it/s=434.5\n",
      "[e3 b925/2315] loss=1.5960 avg=1.5748 it/s=422.0\n",
      "[e3 b1156/2315] loss=1.6052 avg=1.5765 it/s=410.0\n",
      "[e3 b1387/2315] loss=1.6347 avg=1.5765 it/s=403.1\n",
      "[e3 b1618/2315] loss=1.7059 avg=1.5767 it/s=404.1\n",
      "[e3 b1849/2315] loss=1.4278 avg=1.5759 it/s=407.8\n",
      "[e3 b2080/2315] loss=1.6579 avg=1.5762 it/s=409.1\n",
      "[e3 b2311/2315] loss=1.5522 avg=1.5765 it/s=408.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5765 | val_acc=0.2775 | val_f1=0.0869 | time=95.4s\n",
      "[e4 b1/2315] loss=1.5380 avg=1.5380 it/s=424.7\n",
      "[e4 b2/2315] loss=1.5323 avg=1.5351 it/s=398.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.7128 avg=1.5796 it/s=407.7\n",
      "[e4 b463/2315] loss=1.5160 avg=1.5779 it/s=410.7\n",
      "[e4 b694/2315] loss=1.5462 avg=1.5777 it/s=421.8\n",
      "[e4 b925/2315] loss=1.5704 avg=1.5775 it/s=427.8\n",
      "[e4 b1156/2315] loss=1.5787 avg=1.5770 it/s=428.4\n",
      "[e4 b1387/2315] loss=1.6296 avg=1.5775 it/s=429.5\n",
      "[e4 b1618/2315] loss=1.5998 avg=1.5761 it/s=422.5\n",
      "[e4 b1849/2315] loss=1.5650 avg=1.5754 it/s=417.1\n",
      "[e4 b2080/2315] loss=1.5767 avg=1.5757 it/s=415.5\n",
      "[e4 b2311/2315] loss=1.5435 avg=1.5761 it/s=416.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5760 | val_acc=0.2775 | val_f1=0.0869 | time=93.6s\n",
      "[e5 b1/2315] loss=1.6385 avg=1.6385 it/s=587.2\n",
      "[e5 b2/2315] loss=1.5489 avg=1.5937 it/s=462.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5125 avg=1.5832 it/s=413.6\n",
      "[e5 b463/2315] loss=1.6174 avg=1.5794 it/s=405.0\n",
      "[e5 b694/2315] loss=1.6764 avg=1.5779 it/s=407.4\n",
      "[e5 b925/2315] loss=1.5786 avg=1.5761 it/s=412.3\n",
      "[e5 b1156/2315] loss=1.6209 avg=1.5764 it/s=412.9\n",
      "[e5 b1387/2315] loss=1.6021 avg=1.5752 it/s=417.3\n",
      "[e5 b1618/2315] loss=1.5537 avg=1.5755 it/s=419.3\n",
      "[e5 b1849/2315] loss=1.4961 avg=1.5761 it/s=421.0\n",
      "[e5 b2080/2315] loss=1.5363 avg=1.5759 it/s=417.8\n",
      "[e5 b2311/2315] loss=1.5471 avg=1.5759 it/s=413.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 | loss=1.5759 | val_acc=0.2775 | val_f1=0.0869 | time=94.6s\n",
      "[e6 b1/2315] loss=1.5497 avg=1.5497 it/s=371.5\n",
      "[e6 b2/2315] loss=1.6143 avg=1.5820 it/s=498.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=1.6004 avg=1.5775 it/s=379.4\n",
      "[e6 b463/2315] loss=1.5904 avg=1.5755 it/s=396.4\n",
      "[e6 b694/2315] loss=1.4713 avg=1.5736 it/s=404.1\n",
      "[e6 b925/2315] loss=1.5444 avg=1.5754 it/s=408.1\n",
      "[e6 b1156/2315] loss=1.5416 avg=1.5745 it/s=412.3\n",
      "[e6 b1387/2315] loss=1.4810 avg=1.5748 it/s=414.1\n",
      "[e6 b1618/2315] loss=1.5129 avg=1.5751 it/s=414.1\n",
      "[e6 b1849/2315] loss=1.5295 avg=1.5754 it/s=412.5\n",
      "[e6 b2080/2315] loss=1.5778 avg=1.5754 it/s=416.3\n",
      "[e6 b2311/2315] loss=1.5354 avg=1.5757 it/s=419.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÅ‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÅ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÖ‚ñà‚ñà‚ñá‚ñÖ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>lr</td><td>0.00017</td></tr><tr><td>params/ratio</td><td>0.2055</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>57297413</td></tr><tr><td>step</td><td>13886</td></tr><tr><td>time/epoch_sec</td><td>93.10518</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57571</td></tr><tr><td>train/epoch_loss</td><td>1.57573</td></tr><tr><td>train/items_per_sec</td><td>419.13267</td></tr><tr><td>train/loss</td><td>1.52496</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>2</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_10</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/pq2v1gjt' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/pq2v1gjt</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_042045-pq2v1gjt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 7. Best value: 0.866111:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [2:06:51<1:53:33, 757.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 10] f1=0.0869 | unfreeze_k=8 lr=3.12e-04 wd=8.9e-05 suggested_bs=64\n",
      "[I 2025-08-16 04:30:22,761] Trial 10 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 8, 'lr': 0.00031227487155926006, 'weight_decay': 8.926712608942563e-05, 'batch_size': 64}. Best is trial 7 with value: 0.8661112815752661.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_043024-38oultc2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/38oultc2' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_11</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/38oultc2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/38oultc2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 57,297,413 / 278,813,189 (20.55%) ; unfreeze_last_k=8\n",
      "[e1 b1/2315] loss=1.5741 avg=1.5741 it/s=260.9\n",
      "[e1 b2/2315] loss=1.6420 avg=1.6081 it/s=282.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.3953 avg=1.5713 it/s=395.1\n",
      "[e1 b463/2315] loss=1.1916 avg=1.4826 it/s=388.3\n",
      "[e1 b694/2315] loss=0.9879 avg=1.3671 it/s=388.5\n",
      "[e1 b925/2315] loss=0.7412 avg=1.2712 it/s=396.2\n",
      "[e1 b1156/2315] loss=0.8679 avg=1.2049 it/s=400.4\n",
      "[e1 b1387/2315] loss=1.0370 avg=1.1525 it/s=405.0\n",
      "[e1 b1618/2315] loss=1.0214 avg=1.1094 it/s=408.0\n",
      "[e1 b1849/2315] loss=0.9011 avg=1.0775 it/s=409.5\n",
      "[e1 b2080/2315] loss=0.5252 avg=1.0449 it/s=409.6\n",
      "[e1 b2311/2315] loss=0.3931 avg=1.0165 it/s=409.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.0161 | val_acc=0.7128 | val_f1=0.7229 | time=95.1s\n",
      "[e2 b1/2315] loss=0.3638 avg=0.3638 it/s=399.2\n",
      "[e2 b2/2315] loss=0.7488 avg=0.5563 it/s=411.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.4332 avg=0.7298 it/s=410.1\n",
      "[e2 b463/2315] loss=1.1351 avg=0.7072 it/s=418.6\n",
      "[e2 b694/2315] loss=0.5595 avg=0.6999 it/s=420.2\n",
      "[e2 b925/2315] loss=0.5168 avg=0.6921 it/s=410.9\n",
      "[e2 b1156/2315] loss=0.7191 avg=0.6885 it/s=409.3\n",
      "[e2 b1387/2315] loss=0.4723 avg=0.6818 it/s=407.2\n",
      "[e2 b1618/2315] loss=0.6468 avg=0.6701 it/s=407.9\n",
      "[e2 b1849/2315] loss=0.2358 avg=0.6668 it/s=405.0\n",
      "[e2 b2080/2315] loss=0.8136 avg=0.6600 it/s=404.9\n",
      "[e2 b2311/2315] loss=0.2666 avg=0.6550 it/s=407.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=0.6551 | val_acc=0.7942 | val_f1=0.8000 | time=95.7s\n",
      "[e3 b1/2315] loss=0.5782 avg=0.5782 it/s=417.7\n",
      "[e3 b2/2315] loss=0.4999 avg=0.5390 it/s=374.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.4458 avg=0.5317 it/s=403.9\n",
      "[e3 b463/2315] loss=0.3514 avg=0.5489 it/s=409.4\n",
      "[e3 b694/2315] loss=0.5925 avg=0.5462 it/s=417.8\n",
      "[e3 b925/2315] loss=0.4076 avg=0.5447 it/s=423.0\n",
      "[e3 b1156/2315] loss=0.3250 avg=0.5461 it/s=426.2\n",
      "[e3 b1387/2315] loss=0.5199 avg=0.5432 it/s=422.3\n",
      "[e3 b1618/2315] loss=1.4349 avg=0.5427 it/s=415.8\n",
      "[e3 b1849/2315] loss=0.5430 avg=0.5354 it/s=411.0\n",
      "[e3 b2080/2315] loss=0.1724 avg=0.5356 it/s=408.5\n",
      "[e3 b2311/2315] loss=0.6266 avg=0.5324 it/s=406.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=0.5324 | val_acc=0.8190 | val_f1=0.8242 | time=96.1s\n",
      "[e4 b1/2315] loss=0.2065 avg=0.2065 it/s=461.3\n",
      "[e4 b2/2315] loss=0.4936 avg=0.3501 it/s=454.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.5218 avg=0.4569 it/s=402.4\n",
      "[e4 b463/2315] loss=0.6237 avg=0.4604 it/s=410.5\n",
      "[e4 b694/2315] loss=0.4120 avg=0.4652 it/s=411.7\n",
      "[e4 b925/2315] loss=0.3373 avg=0.4652 it/s=415.2\n",
      "[e4 b1156/2315] loss=0.0364 avg=0.4617 it/s=418.2\n",
      "[e4 b1387/2315] loss=0.2746 avg=0.4592 it/s=422.1\n",
      "[e4 b1618/2315] loss=0.1483 avg=0.4541 it/s=426.1\n",
      "[e4 b1849/2315] loss=0.8854 avg=0.4522 it/s=428.5\n",
      "[e4 b2080/2315] loss=0.5785 avg=0.4503 it/s=424.4\n",
      "[e4 b2311/2315] loss=0.2212 avg=0.4481 it/s=422.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=0.4478 | val_acc=0.8017 | val_f1=0.8080 | time=92.3s\n",
      "[e5 b1/2315] loss=0.8688 avg=0.8688 it/s=686.0\n",
      "[e5 b2/2315] loss=0.1497 avg=0.5092 it/s=421.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.2904 avg=0.3888 it/s=422.8\n",
      "[e5 b463/2315] loss=0.5322 avg=0.3783 it/s=428.4\n",
      "[e5 b694/2315] loss=0.3977 avg=0.3754 it/s=429.4\n",
      "[e5 b925/2315] loss=0.5104 avg=0.3847 it/s=431.0\n",
      "[e5 b1156/2315] loss=0.4124 avg=0.3881 it/s=430.3\n",
      "[e5 b1387/2315] loss=0.2136 avg=0.3900 it/s=425.4\n",
      "[e5 b1618/2315] loss=0.3838 avg=0.3909 it/s=421.1\n",
      "[e5 b1849/2315] loss=0.2131 avg=0.3899 it/s=421.3\n",
      "[e5 b2080/2315] loss=0.4790 avg=0.3881 it/s=424.0\n",
      "[e5 b2311/2315] loss=0.5981 avg=0.3885 it/s=426.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 | loss=0.3882 | val_acc=0.8440 | val_f1=0.8475 | time=91.6s\n",
      "[e6 b1/2315] loss=0.6455 avg=0.6455 it/s=381.5\n",
      "[e6 b2/2315] loss=0.2859 avg=0.4657 it/s=390.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.0860 avg=0.3308 it/s=414.8\n",
      "[e6 b463/2315] loss=0.1791 avg=0.3373 it/s=415.4\n",
      "[e6 b694/2315] loss=0.8090 avg=0.3267 it/s=416.5\n",
      "[e6 b925/2315] loss=0.4906 avg=0.3299 it/s=412.7\n",
      "[e6 b1156/2315] loss=0.2831 avg=0.3296 it/s=410.0\n",
      "[e6 b1387/2315] loss=0.2883 avg=0.3285 it/s=412.5\n",
      "[e6 b1618/2315] loss=0.4220 avg=0.3294 it/s=410.7\n",
      "[e6 b1849/2315] loss=0.0597 avg=0.3316 it/s=409.3\n",
      "[e6 b2080/2315] loss=0.0642 avg=0.3345 it/s=409.1\n",
      "[e6 b2311/2315] loss=0.0537 avg=0.3320 it/s=411.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 | loss=0.3320 | val_acc=0.8397 | val_f1=0.8445 | time=95.0s\n",
      "[e7 b1/2315] loss=0.0729 avg=0.0729 it/s=408.7\n",
      "[e7 b2/2315] loss=0.0744 avg=0.0736 it/s=479.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.4538 avg=0.3061 it/s=433.9\n",
      "[e7 b463/2315] loss=0.0808 avg=0.2960 it/s=434.0\n",
      "[e7 b694/2315] loss=0.0265 avg=0.2901 it/s=432.9\n",
      "[e7 b925/2315] loss=0.4768 avg=0.2944 it/s=425.0\n",
      "[e7 b1156/2315] loss=0.8689 avg=0.2926 it/s=414.4\n",
      "[e7 b1387/2315] loss=0.2043 avg=0.2923 it/s=406.4\n",
      "[e7 b1618/2315] loss=0.2696 avg=0.2909 it/s=405.6\n",
      "[e7 b1849/2315] loss=0.4585 avg=0.2918 it/s=407.0\n",
      "[e7 b2080/2315] loss=0.0391 avg=0.2902 it/s=409.1\n",
      "[e7 b2311/2315] loss=0.1793 avg=0.2902 it/s=408.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 | loss=0.2902 | val_acc=0.8550 | val_f1=0.8597 | time=95.5s\n",
      "[e8 b1/2315] loss=0.2687 avg=0.2687 it/s=397.1\n",
      "[e8 b2/2315] loss=0.1744 avg=0.2216 it/s=392.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e8 b232/2315] loss=0.1471 avg=0.2458 it/s=416.9\n",
      "[e8 b463/2315] loss=0.0191 avg=0.2562 it/s=418.8\n",
      "[e8 b694/2315] loss=0.0422 avg=0.2528 it/s=424.5\n",
      "[e8 b925/2315] loss=0.4137 avg=0.2464 it/s=431.4\n",
      "[e8 b1156/2315] loss=0.2493 avg=0.2445 it/s=433.9\n",
      "[e8 b1387/2315] loss=0.0352 avg=0.2437 it/s=436.3\n",
      "[e8 b1618/2315] loss=0.0326 avg=0.2440 it/s=431.9\n",
      "[e8 b1849/2315] loss=0.4169 avg=0.2418 it/s=425.3\n",
      "[e8 b2080/2315] loss=0.3074 avg=0.2430 it/s=419.4\n",
      "[e8 b2311/2315] loss=0.0512 avg=0.2438 it/s=419.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 | loss=0.2437 | val_acc=0.8499 | val_f1=0.8555 | time=93.1s\n",
      "[e9 b1/2315] loss=0.2563 avg=0.2563 it/s=729.0\n",
      "[e9 b2/2315] loss=0.8885 avg=0.5724 it/s=439.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e9 b232/2315] loss=0.2576 avg=0.2083 it/s=430.5\n",
      "[e9 b463/2315] loss=0.0231 avg=0.2012 it/s=422.5\n",
      "[e9 b694/2315] loss=0.0220 avg=0.2075 it/s=409.6\n",
      "[e9 b925/2315] loss=0.2449 avg=0.2106 it/s=407.3\n",
      "[e9 b1156/2315] loss=0.0132 avg=0.2138 it/s=408.5\n",
      "[e9 b1387/2315] loss=0.5304 avg=0.2102 it/s=410.8\n",
      "[e9 b1618/2315] loss=0.2831 avg=0.2099 it/s=414.0\n",
      "[e9 b1849/2315] loss=0.2791 avg=0.2119 it/s=417.7\n",
      "[e9 b2080/2315] loss=0.3622 avg=0.2116 it/s=420.9\n",
      "[e9 b2311/2315] loss=0.6624 avg=0.2124 it/s=418.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 | loss=0.2122 | val_acc=0.8681 | val_f1=0.8720 | time=93.5s\n",
      "[e10 b1/2315] loss=0.2775 avg=0.2775 it/s=267.2\n",
      "[e10 b2/2315] loss=0.0265 avg=0.1520 it/s=327.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e10 b232/2315] loss=0.0204 avg=0.1572 it/s=382.6\n",
      "[e10 b463/2315] loss=0.3251 avg=0.1789 it/s=389.6\n",
      "[e10 b694/2315] loss=0.4267 avg=0.1773 it/s=389.9\n",
      "[e10 b925/2315] loss=0.0211 avg=0.1761 it/s=385.1\n",
      "[e10 b1156/2315] loss=0.4688 avg=0.1735 it/s=386.7\n",
      "[e10 b1387/2315] loss=0.4329 avg=0.1698 it/s=391.9\n",
      "[e10 b1618/2315] loss=0.8178 avg=0.1714 it/s=395.4\n",
      "[e10 b1849/2315] loss=0.3732 avg=0.1713 it/s=398.1\n",
      "[e10 b2080/2315] loss=0.4821 avg=0.1709 it/s=403.1\n",
      "[e10 b2311/2315] loss=0.0099 avg=0.1702 it/s=407.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 | loss=0.1703 | val_acc=0.8664 | val_f1=0.8705 | time=95.7s\n",
      "[e11 b1/2315] loss=0.0039 avg=0.0039 it/s=439.6\n",
      "[e11 b2/2315] loss=0.0078 avg=0.0059 it/s=451.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e11 b232/2315] loss=0.0042 avg=0.1478 it/s=435.4\n",
      "[e11 b463/2315] loss=0.0126 avg=0.1369 it/s=405.8\n",
      "[e11 b694/2315] loss=0.0032 avg=0.1444 it/s=394.2\n",
      "[e11 b925/2315] loss=0.3590 avg=0.1409 it/s=389.6\n",
      "[e11 b1156/2315] loss=0.0074 avg=0.1416 it/s=398.0\n",
      "[e11 b1387/2315] loss=0.1257 avg=0.1442 it/s=402.3\n",
      "[e11 b1618/2315] loss=0.3514 avg=0.1447 it/s=403.2\n",
      "[e11 b1849/2315] loss=0.0131 avg=0.1476 it/s=404.9\n",
      "[e11 b2080/2315] loss=0.0051 avg=0.1499 it/s=407.8\n",
      "[e11 b2311/2315] loss=0.0075 avg=0.1483 it/s=408.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 | loss=0.1485 | val_acc=0.8635 | val_f1=0.8677 | time=95.5s\n",
      "[e12 b1/2315] loss=0.0049 avg=0.0049 it/s=429.2\n",
      "[e12 b2/2315] loss=0.0168 avg=0.0108 it/s=407.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e12 b232/2315] loss=0.0022 avg=0.1151 it/s=432.7\n",
      "[e12 b463/2315] loss=0.0072 avg=0.1215 it/s=439.8\n",
      "[e12 b694/2315] loss=0.0030 avg=0.1125 it/s=442.1\n",
      "[e12 b925/2315] loss=0.0030 avg=0.1185 it/s=444.1\n",
      "[e12 b1156/2315] loss=0.0032 avg=0.1247 it/s=432.6\n",
      "[e12 b1387/2315] loss=0.0955 avg=0.1224 it/s=424.3\n",
      "[e12 b1618/2315] loss=0.4672 avg=0.1232 it/s=421.1\n",
      "[e12 b1849/2315] loss=0.0052 avg=0.1218 it/s=422.5\n",
      "[e12 b2080/2315] loss=0.0149 avg=0.1216 it/s=422.3\n",
      "[e12 b2311/2315] loss=0.0031 avg=0.1207 it/s=419.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 | loss=0.1207 | val_acc=0.8688 | val_f1=0.8732 | time=93.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÜ‚ñá‚ñà‚ñÇ‚ñÅ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñá‚ñá‚ñÉ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÉ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.87317</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.2055</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>57297413</td></tr><tr><td>step</td><td>27776</td></tr><tr><td>time/epoch_sec</td><td>93.10025</td></tr><tr><td>train/avg_loss_so_far</td><td>0.12071</td></tr><tr><td>train/epoch_loss</td><td>0.12074</td></tr><tr><td>train/items_per_sec</td><td>419.78634</td></tr><tr><td>train/loss</td><td>0.00332</td></tr><tr><td>val/acc</td><td>0.8688</td></tr><tr><td>val/best_epoch</td><td>12</td></tr><tr><td>val/best_f1_so_far</td><td>0.87317</td></tr><tr><td>val/f1</td><td>0.87317</td></tr><tr><td>val/precision</td><td>0.86882</td></tr><tr><td>val/recall</td><td>0.87857</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_11</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/38oultc2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/38oultc2</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_043024-38oultc2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 11. Best value: 0.873175:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:26:00<1:56:48, 876.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 11] f1=0.8732 | unfreeze_k=8 lr=1.06e-04 wd=3.9e-06 suggested_bs=64\n",
      "[I 2025-08-16 04:49:31,033] Trial 11 finished with value: 0.873174802426709 and parameters: {'num_unfreeze_last_layers': 8, 'lr': 0.0001061735334192977, 'weight_decay': 3.9302980013836756e-06, 'batch_size': 64}. Best is trial 11 with value: 0.873174802426709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_044932-kunb7na1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/kunb7na1' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_12</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/kunb7na1' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/kunb7na1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 57,297,413 / 278,813,189 (20.55%) ; unfreeze_last_k=8\n",
      "[e1 b1/2315] loss=1.6031 avg=1.6031 it/s=195.4\n",
      "[e1 b2/2315] loss=1.6014 avg=1.6023 it/s=245.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4609 avg=1.5490 it/s=381.3\n",
      "[e1 b463/2315] loss=1.1682 avg=1.3954 it/s=391.7\n",
      "[e1 b694/2315] loss=0.3986 avg=1.2816 it/s=400.0\n",
      "[e1 b925/2315] loss=0.9053 avg=1.2154 it/s=405.5\n",
      "[e1 b1156/2315] loss=1.1431 avg=1.1746 it/s=410.9\n",
      "[e1 b1387/2315] loss=0.7161 avg=1.1443 it/s=413.3\n",
      "[e1 b1618/2315] loss=1.4147 avg=1.1236 it/s=408.9\n",
      "[e1 b1849/2315] loss=0.8793 avg=1.1067 it/s=405.9\n",
      "[e1 b2080/2315] loss=1.2571 avg=1.0936 it/s=404.0\n",
      "[e1 b2311/2315] loss=1.2362 avg=1.0809 it/s=403.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.0805 | val_acc=0.6331 | val_f1=0.6324 | time=96.9s\n",
      "[e2 b1/2315] loss=0.8607 avg=0.8607 it/s=440.6\n",
      "[e2 b2/2315] loss=1.0145 avg=0.9376 it/s=387.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.1006 avg=1.0439 it/s=382.1\n",
      "[e2 b463/2315] loss=0.9930 avg=1.1233 it/s=391.6\n",
      "[e2 b694/2315] loss=1.6917 avg=1.1545 it/s=401.0\n",
      "[e2 b925/2315] loss=1.4815 avg=1.1952 it/s=400.8\n",
      "[e2 b1156/2315] loss=1.5614 avg=1.2741 it/s=402.0\n",
      "[e2 b1387/2315] loss=1.5988 avg=1.3264 it/s=406.1\n",
      "[e2 b1618/2315] loss=1.6429 avg=1.3617 it/s=410.2\n",
      "[e2 b1849/2315] loss=1.5705 avg=1.3877 it/s=412.8\n",
      "[e2 b2080/2315] loss=1.6240 avg=1.4094 it/s=407.0\n",
      "[e2 b2311/2315] loss=1.7310 avg=1.4266 it/s=403.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.4266 | val_acc=0.2775 | val_f1=0.0869 | time=97.0s\n",
      "[e3 b1/2315] loss=1.7584 avg=1.7584 it/s=446.1\n",
      "[e3 b2/2315] loss=1.6628 avg=1.7106 it/s=399.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5122 avg=1.5793 it/s=407.5\n",
      "[e3 b463/2315] loss=1.4597 avg=1.5777 it/s=424.2\n",
      "[e3 b694/2315] loss=1.6354 avg=1.5791 it/s=424.6\n",
      "[e3 b925/2315] loss=1.6445 avg=1.5785 it/s=425.5\n",
      "[e3 b1156/2315] loss=1.5605 avg=1.5783 it/s=425.6\n",
      "[e3 b1387/2315] loss=1.6008 avg=1.5778 it/s=423.5\n",
      "[e3 b1618/2315] loss=1.5495 avg=1.5779 it/s=420.4\n",
      "[e3 b1849/2315] loss=1.5468 avg=1.5774 it/s=422.8\n",
      "[e3 b2080/2315] loss=1.7009 avg=1.5769 it/s=424.8\n",
      "[e3 b2311/2315] loss=1.5441 avg=1.5770 it/s=426.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5770 | val_acc=0.2775 | val_f1=0.0869 | time=91.6s\n",
      "[e4 b1/2315] loss=1.5588 avg=1.5588 it/s=648.3\n",
      "[e4 b2/2315] loss=1.4715 avg=1.5151 it/s=566.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.4596 avg=1.5695 it/s=394.2\n",
      "[e4 b463/2315] loss=1.5038 avg=1.5727 it/s=386.2\n",
      "[e4 b694/2315] loss=1.4902 avg=1.5774 it/s=387.2\n",
      "[e4 b925/2315] loss=1.5758 avg=1.5766 it/s=393.0\n",
      "[e4 b1156/2315] loss=1.5407 avg=1.5767 it/s=398.9\n",
      "[e4 b1387/2315] loss=1.6416 avg=1.5760 it/s=404.0\n",
      "[e4 b1618/2315] loss=1.5858 avg=1.5756 it/s=407.9\n",
      "[e4 b1849/2315] loss=1.6745 avg=1.5752 it/s=409.9\n",
      "[e4 b2080/2315] loss=1.5896 avg=1.5755 it/s=409.0\n",
      "[e4 b2311/2315] loss=1.6489 avg=1.5760 it/s=409.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5760 | val_acc=0.2775 | val_f1=0.0869 | time=95.4s\n",
      "[e5 b1/2315] loss=1.6295 avg=1.6295 it/s=441.4\n",
      "[e5 b2/2315] loss=1.5107 avg=1.5701 it/s=457.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.6117 avg=1.5742 it/s=448.8\n",
      "[e5 b463/2315] loss=1.6349 avg=1.5755 it/s=451.8\n",
      "[e5 b694/2315] loss=1.5316 avg=1.5770 it/s=449.1\n",
      "[e5 b925/2315] loss=1.5955 avg=1.5771 it/s=440.6\n",
      "[e5 b1156/2315] loss=1.6136 avg=1.5775 it/s=435.5\n",
      "[e5 b1387/2315] loss=1.5701 avg=1.5770 it/s=428.3\n",
      "[e5 b1618/2315] loss=1.5398 avg=1.5771 it/s=422.5\n",
      "[e5 b1849/2315] loss=1.5450 avg=1.5766 it/s=415.2\n",
      "[e5 b2080/2315] loss=1.5623 avg=1.5763 it/s=411.9\n",
      "[e5 b2311/2315] loss=1.5766 avg=1.5760 it/s=411.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñà‚ñÅ‚ñÜ‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>val/acc</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.63235</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00012</td></tr><tr><td>params/ratio</td><td>0.2055</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>57297413</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>95.08015</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57602</td></tr><tr><td>train/epoch_loss</td><td>1.57593</td></tr><tr><td>train/items_per_sec</td><td>411.29398</td></tr><tr><td>train/loss</td><td>1.52041</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.63235</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_12</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/kunb7na1' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/kunb7na1</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_044932-kunb7na1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 11. Best value: 0.873175:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:34:05<1:28:24, 757.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 12] f1=0.6324 | unfreeze_k=8 lr=1.88e-04 wd=4.6e-06 suggested_bs=64\n",
      "[I 2025-08-16 04:57:36,613] Trial 12 finished with value: 0.6323546484809632 and parameters: {'num_unfreeze_last_layers': 8, 'lr': 0.00018765508178568579, 'weight_decay': 4.610599546574859e-06, 'batch_size': 64}. Best is trial 11 with value: 0.873174802426709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_045743-f7495oqe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/f7495oqe' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_13</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/f7495oqe' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/f7495oqe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.6643 avg=1.6643 it/s=138.1\n",
      "[e1 b2/2315] loss=1.6148 avg=1.6396 it/s=204.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5087 avg=1.5796 it/s=399.6\n",
      "[e1 b463/2315] loss=1.0805 avg=1.4803 it/s=404.8\n",
      "[e1 b694/2315] loss=0.7252 avg=1.3590 it/s=405.9\n",
      "[e1 b925/2315] loss=0.9780 avg=1.2641 it/s=405.2\n",
      "[e1 b1156/2315] loss=0.9122 avg=1.1943 it/s=396.7\n",
      "[e1 b1387/2315] loss=1.5636 avg=1.1391 it/s=390.1\n",
      "[e1 b1618/2315] loss=0.6595 avg=1.0977 it/s=387.3\n",
      "[e1 b1849/2315] loss=0.7155 avg=1.0691 it/s=386.9\n",
      "[e1 b2080/2315] loss=0.9394 avg=1.0399 it/s=386.4\n",
      "[e1 b2311/2315] loss=0.6763 avg=1.0142 it/s=386.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.0139 | val_acc=0.7004 | val_f1=0.7132 | time=100.9s\n",
      "[e2 b1/2315] loss=0.9537 avg=0.9537 it/s=409.4\n",
      "[e2 b2/2315] loss=0.9445 avg=0.9491 it/s=437.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.0091 avg=0.7547 it/s=413.1\n",
      "[e2 b463/2315] loss=0.6166 avg=0.7336 it/s=399.2\n",
      "[e2 b694/2315] loss=0.9079 avg=0.7254 it/s=396.0\n",
      "[e2 b925/2315] loss=0.4089 avg=0.7285 it/s=399.0\n",
      "[e2 b1156/2315] loss=0.9085 avg=0.7309 it/s=402.0\n",
      "[e2 b1387/2315] loss=1.0751 avg=0.7368 it/s=403.8\n",
      "[e2 b1618/2315] loss=0.4731 avg=0.7342 it/s=400.1\n",
      "[e2 b1849/2315] loss=0.9180 avg=0.7310 it/s=399.4\n",
      "[e2 b2080/2315] loss=0.6241 avg=0.7256 it/s=396.8\n",
      "[e2 b2311/2315] loss=0.5531 avg=0.7193 it/s=395.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=0.7192 | val_acc=0.7536 | val_f1=0.7620 | time=98.7s\n",
      "[e3 b1/2315] loss=0.7985 avg=0.7985 it/s=257.9\n",
      "[e3 b2/2315] loss=0.5778 avg=0.6882 it/s=302.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.5719 avg=0.6406 it/s=395.3\n",
      "[e3 b463/2315] loss=0.6252 avg=0.6176 it/s=383.9\n",
      "[e3 b694/2315] loss=0.4259 avg=0.6125 it/s=374.9\n",
      "[e3 b925/2315] loss=1.0189 avg=0.6129 it/s=372.0\n",
      "[e3 b1156/2315] loss=0.2949 avg=0.6219 it/s=375.6\n",
      "[e3 b1387/2315] loss=0.1999 avg=0.6198 it/s=379.9\n",
      "[e3 b1618/2315] loss=0.6791 avg=0.6140 it/s=383.8\n",
      "[e3 b1849/2315] loss=0.3884 avg=0.6092 it/s=385.2\n",
      "[e3 b2080/2315] loss=0.6907 avg=0.6069 it/s=385.3\n",
      "[e3 b2311/2315] loss=0.9315 avg=0.6064 it/s=384.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=0.6062 | val_acc=0.7966 | val_f1=0.8041 | time=101.5s\n",
      "[e4 b1/2315] loss=0.5959 avg=0.5959 it/s=349.5\n",
      "[e4 b2/2315] loss=0.4786 avg=0.5372 it/s=386.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.3838 avg=0.5613 it/s=371.3\n",
      "[e4 b463/2315] loss=0.4415 avg=0.5748 it/s=373.9\n",
      "[e4 b694/2315] loss=0.1319 avg=0.5640 it/s=372.9\n",
      "[e4 b925/2315] loss=0.1890 avg=0.5618 it/s=370.2\n",
      "[e4 b1156/2315] loss=0.2809 avg=0.5590 it/s=374.7\n",
      "[e4 b1387/2315] loss=0.8235 avg=0.5545 it/s=375.9\n",
      "[e4 b1618/2315] loss=0.4138 avg=0.5505 it/s=378.3\n",
      "[e4 b1849/2315] loss=0.7312 avg=0.5461 it/s=382.0\n",
      "[e4 b2080/2315] loss=0.5785 avg=0.5451 it/s=384.6\n",
      "[e4 b2311/2315] loss=0.7393 avg=0.5443 it/s=386.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=0.5442 | val_acc=0.8049 | val_f1=0.8098 | time=101.0s\n",
      "[e5 b1/2315] loss=0.4777 avg=0.4777 it/s=401.2\n",
      "[e5 b2/2315] loss=0.3832 avg=0.4304 it/s=372.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.4118 avg=0.5102 it/s=359.9\n",
      "[e5 b463/2315] loss=0.5125 avg=0.4846 it/s=357.8\n",
      "[e5 b694/2315] loss=0.4881 avg=0.4910 it/s=369.6\n",
      "[e5 b925/2315] loss=0.7798 avg=0.4915 it/s=374.8\n",
      "[e5 b1156/2315] loss=0.4517 avg=0.4869 it/s=379.9\n",
      "[e5 b1387/2315] loss=0.5577 avg=0.4841 it/s=382.8\n",
      "[e5 b1618/2315] loss=0.3298 avg=0.4833 it/s=383.5\n",
      "[e5 b1849/2315] loss=0.3817 avg=0.4807 it/s=384.2\n",
      "[e5 b2080/2315] loss=0.3213 avg=0.4802 it/s=386.4\n",
      "[e5 b2311/2315] loss=0.7108 avg=0.4805 it/s=389.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 | loss=0.4808 | val_acc=0.7884 | val_f1=0.7979 | time=100.0s\n",
      "[e6 b1/2315] loss=0.3405 avg=0.3405 it/s=488.9\n",
      "[e6 b2/2315] loss=0.6285 avg=0.4845 it/s=433.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.3355 avg=0.3933 it/s=416.1\n",
      "[e6 b463/2315] loss=0.2684 avg=0.4156 it/s=394.0\n",
      "[e6 b694/2315] loss=0.5801 avg=0.4188 it/s=380.1\n",
      "[e6 b925/2315] loss=0.4097 avg=0.4182 it/s=374.2\n",
      "[e6 b1156/2315] loss=0.4919 avg=0.4144 it/s=375.7\n",
      "[e6 b1387/2315] loss=0.3866 avg=0.4157 it/s=378.1\n",
      "[e6 b1618/2315] loss=0.3029 avg=0.4150 it/s=381.4\n",
      "[e6 b1849/2315] loss=0.4736 avg=0.4146 it/s=381.8\n",
      "[e6 b2080/2315] loss=0.4332 avg=0.4125 it/s=378.7\n",
      "[e6 b2311/2315] loss=0.5124 avg=0.4120 it/s=378.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/12 | loss=0.4125 | val_acc=0.8251 | val_f1=0.8308 | time=102.6s\n",
      "[e7 b1/2315] loss=0.0703 avg=0.0703 it/s=525.8\n",
      "[e7 b2/2315] loss=0.8526 avg=0.4615 it/s=380.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.2702 avg=0.4126 it/s=420.0\n",
      "[e7 b463/2315] loss=0.4423 avg=0.3908 it/s=413.8\n",
      "[e7 b694/2315] loss=0.6819 avg=0.3815 it/s=416.8\n",
      "[e7 b925/2315] loss=0.8062 avg=0.3817 it/s=410.6\n",
      "[e7 b1156/2315] loss=0.2033 avg=0.3794 it/s=403.4\n",
      "[e7 b1387/2315] loss=0.2902 avg=0.3757 it/s=394.6\n",
      "[e7 b1618/2315] loss=0.7532 avg=0.3738 it/s=391.1\n",
      "[e7 b1849/2315] loss=0.0738 avg=0.3708 it/s=389.6\n",
      "[e7 b2080/2315] loss=0.5361 avg=0.3684 it/s=388.3\n",
      "[e7 b2311/2315] loss=0.9529 avg=0.3676 it/s=387.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/12 | loss=0.3676 | val_acc=0.8333 | val_f1=0.8379 | time=100.3s\n",
      "[e8 b1/2315] loss=0.3930 avg=0.3930 it/s=286.3\n",
      "[e8 b2/2315] loss=0.1965 avg=0.2948 it/s=297.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e8 b232/2315] loss=0.2969 avg=0.3145 it/s=376.8\n",
      "[e8 b463/2315] loss=0.5247 avg=0.3231 it/s=388.3\n",
      "[e8 b694/2315] loss=0.1069 avg=0.3229 it/s=395.1\n",
      "[e8 b925/2315] loss=0.2114 avg=0.3225 it/s=397.7\n",
      "[e8 b1156/2315] loss=0.4742 avg=0.3200 it/s=400.6\n",
      "[e8 b1387/2315] loss=0.4421 avg=0.3210 it/s=395.6\n",
      "[e8 b1618/2315] loss=0.3975 avg=0.3155 it/s=386.9\n",
      "[e8 b1849/2315] loss=0.5935 avg=0.3151 it/s=381.1\n",
      "[e8 b2080/2315] loss=0.5876 avg=0.3156 it/s=381.7\n",
      "[e8 b2311/2315] loss=0.5540 avg=0.3127 it/s=381.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/12 | loss=0.3125 | val_acc=0.8550 | val_f1=0.8588 | time=102.0s\n",
      "[e9 b1/2315] loss=0.0186 avg=0.0186 it/s=339.4\n",
      "[e9 b2/2315] loss=0.4054 avg=0.2120 it/s=341.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e9 b232/2315] loss=0.7347 avg=0.2574 it/s=386.8\n",
      "[e9 b463/2315] loss=0.7491 avg=0.2545 it/s=389.8\n",
      "[e9 b694/2315] loss=0.4518 avg=0.2692 it/s=384.0\n",
      "[e9 b925/2315] loss=0.0222 avg=0.2720 it/s=384.8\n",
      "[e9 b1156/2315] loss=0.0440 avg=0.2695 it/s=387.1\n",
      "[e9 b1387/2315] loss=0.2967 avg=0.2692 it/s=389.4\n",
      "[e9 b1618/2315] loss=0.4574 avg=0.2708 it/s=390.3\n",
      "[e9 b1849/2315] loss=0.0261 avg=0.2692 it/s=386.7\n",
      "[e9 b2080/2315] loss=0.0546 avg=0.2677 it/s=386.0\n",
      "[e9 b2311/2315] loss=0.0468 avg=0.2668 it/s=385.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/12 | loss=0.2666 | val_acc=0.8605 | val_f1=0.8649 | time=100.9s\n",
      "[e10 b1/2315] loss=0.1991 avg=0.1991 it/s=360.2\n",
      "[e10 b2/2315] loss=0.0182 avg=0.1086 it/s=406.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e10 b232/2315] loss=0.1639 avg=0.2118 it/s=386.0\n",
      "[e10 b463/2315] loss=0.5413 avg=0.2343 it/s=391.0\n",
      "[e10 b694/2315] loss=0.1627 avg=0.2377 it/s=391.4\n",
      "[e10 b925/2315] loss=0.0202 avg=0.2341 it/s=386.6\n",
      "[e10 b1156/2315] loss=0.1209 avg=0.2322 it/s=381.9\n",
      "[e10 b1387/2315] loss=0.3029 avg=0.2324 it/s=384.3\n",
      "[e10 b1618/2315] loss=0.5801 avg=0.2330 it/s=389.3\n",
      "[e10 b1849/2315] loss=0.0296 avg=0.2321 it/s=392.6\n",
      "[e10 b2080/2315] loss=0.1710 avg=0.2288 it/s=392.6\n",
      "[e10 b2311/2315] loss=0.2548 avg=0.2290 it/s=391.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/12 | loss=0.2288 | val_acc=0.8632 | val_f1=0.8668 | time=99.7s\n",
      "[e11 b1/2315] loss=0.0471 avg=0.0471 it/s=325.0\n",
      "[e11 b2/2315] loss=0.0551 avg=0.0511 it/s=341.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e11 b232/2315] loss=0.1704 avg=0.1982 it/s=363.3\n",
      "[e11 b463/2315] loss=0.0103 avg=0.1928 it/s=357.9\n",
      "[e11 b694/2315] loss=0.1265 avg=0.2061 it/s=354.3\n",
      "[e11 b925/2315] loss=0.1211 avg=0.2041 it/s=358.7\n",
      "[e11 b1156/2315] loss=0.2088 avg=0.2035 it/s=363.4\n",
      "[e11 b1387/2315] loss=0.0127 avg=0.1997 it/s=367.6\n",
      "[e11 b1618/2315] loss=0.3502 avg=0.1996 it/s=369.6\n",
      "[e11 b1849/2315] loss=0.5753 avg=0.1975 it/s=374.4\n",
      "[e11 b2080/2315] loss=0.1779 avg=0.1956 it/s=378.1\n",
      "[e11 b2311/2315] loss=0.4049 avg=0.1954 it/s=381.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/12 | loss=0.1954 | val_acc=0.8627 | val_f1=0.8662 | time=101.9s\n",
      "[e12 b1/2315] loss=0.1049 avg=0.1049 it/s=407.4\n",
      "[e12 b2/2315] loss=0.0378 avg=0.0714 it/s=429.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e12 b232/2315] loss=0.4572 avg=0.1775 it/s=368.6\n",
      "[e12 b463/2315] loss=0.3929 avg=0.1645 it/s=368.5\n",
      "[e12 b694/2315] loss=0.0509 avg=0.1697 it/s=365.3\n",
      "[e12 b925/2315] loss=0.4747 avg=0.1681 it/s=367.3\n",
      "[e12 b1156/2315] loss=0.0461 avg=0.1660 it/s=369.2\n",
      "[e12 b1387/2315] loss=0.1163 avg=0.1684 it/s=370.3\n",
      "[e12 b1618/2315] loss=0.0070 avg=0.1678 it/s=373.2\n",
      "[e12 b1849/2315] loss=0.0210 avg=0.1678 it/s=375.9\n",
      "[e12 b2080/2315] loss=0.0162 avg=0.1665 it/s=376.5\n",
      "[e12 b2311/2315] loss=0.3777 avg=0.1683 it/s=380.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/12 | loss=0.1681 | val_acc=0.8703 | val_f1=0.8733 | time=102.1s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÖ‚ñÅ‚ñÜ‚ñÖ‚ñÉ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñá‚ñá</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñá‚ñÉ‚ñÑ‚ñá‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.87325</td></tr><tr><td>epoch</td><td>12</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>27776</td></tr><tr><td>time/epoch_sec</td><td>102.14034</td></tr><tr><td>train/avg_loss_so_far</td><td>0.16833</td></tr><tr><td>train/epoch_loss</td><td>0.16806</td></tr><tr><td>train/items_per_sec</td><td>380.18742</td></tr><tr><td>train/loss</td><td>0.00579</td></tr><tr><td>val/acc</td><td>0.87026</td></tr><tr><td>val/best_epoch</td><td>12</td></tr><tr><td>val/best_f1_so_far</td><td>0.87325</td></tr><tr><td>val/f1</td><td>0.87325</td></tr><tr><td>val/precision</td><td>0.86889</td></tr><tr><td>val/recall</td><td>0.87873</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_13</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/f7495oqe' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/f7495oqe</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_045743-f7495oqe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 13. Best value: 0.873252:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:54:42<1:30:14, 902.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 13] f1=0.8733 | unfreeze_k=9 lr=1.03e-04 wd=3.4e-05 suggested_bs=8\n",
      "[I 2025-08-16 05:18:13,480] Trial 13 finished with value: 0.8732520507590781 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 0.00010344194727767704, 'weight_decay': 3.4396049618939587e-05, 'batch_size': 8}. Best is trial 13 with value: 0.8732520507590781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_051814-6frv5ixf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/6frv5ixf' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_14</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/6frv5ixf' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/6frv5ixf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.6409 avg=1.6409 it/s=136.7\n",
      "[e1 b2/2315] loss=1.5748 avg=1.6079 it/s=188.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6867 avg=1.5434 it/s=395.4\n",
      "[e1 b463/2315] loss=1.6168 avg=1.4051 it/s=382.3\n",
      "[e1 b694/2315] loss=0.9030 avg=1.3068 it/s=376.2\n",
      "[e1 b925/2315] loss=1.3521 avg=1.2678 it/s=372.1\n",
      "[e1 b1156/2315] loss=1.2296 avg=1.2808 it/s=377.7\n",
      "[e1 b1387/2315] loss=1.6142 avg=1.3012 it/s=381.6\n",
      "[e1 b1618/2315] loss=1.4930 avg=1.3424 it/s=384.7\n",
      "[e1 b1849/2315] loss=1.5065 avg=1.3733 it/s=386.4\n",
      "[e1 b2080/2315] loss=1.4994 avg=1.3964 it/s=389.2\n",
      "[e1 b2311/2315] loss=1.5668 avg=1.4149 it/s=390.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.4153 | val_acc=0.2775 | val_f1=0.0869 | time=99.5s\n",
      "[e2 b1/2315] loss=1.5454 avg=1.5454 it/s=392.8\n",
      "[e2 b2/2315] loss=1.5869 avg=1.5662 it/s=385.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6426 avg=1.5827 it/s=420.7\n",
      "[e2 b463/2315] loss=1.5391 avg=1.5796 it/s=420.3\n",
      "[e2 b694/2315] loss=1.5538 avg=1.5796 it/s=420.7\n",
      "[e2 b925/2315] loss=1.5417 avg=1.5789 it/s=411.4\n",
      "[e2 b1156/2315] loss=1.5335 avg=1.5779 it/s=401.3\n",
      "[e2 b1387/2315] loss=1.5434 avg=1.5772 it/s=396.8\n",
      "[e2 b1618/2315] loss=1.6870 avg=1.5780 it/s=394.5\n",
      "[e2 b1849/2315] loss=1.6454 avg=1.5787 it/s=390.5\n",
      "[e2 b2080/2315] loss=1.6884 avg=1.5780 it/s=388.8\n",
      "[e2 b2311/2315] loss=1.4932 avg=1.5777 it/s=389.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5777 | val_acc=0.2775 | val_f1=0.0869 | time=100.0s\n",
      "[e3 b1/2315] loss=1.5056 avg=1.5056 it/s=459.8\n",
      "[e3 b2/2315] loss=1.4698 avg=1.4877 it/s=373.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.6377 avg=1.5727 it/s=393.3\n",
      "[e3 b463/2315] loss=1.5586 avg=1.5787 it/s=389.4\n",
      "[e3 b694/2315] loss=1.4082 avg=1.5743 it/s=395.8\n",
      "[e3 b925/2315] loss=1.6990 avg=1.5760 it/s=399.6\n",
      "[e3 b1156/2315] loss=1.5059 avg=1.5752 it/s=400.9\n",
      "[e3 b1387/2315] loss=1.6535 avg=1.5757 it/s=394.5\n",
      "[e3 b1618/2315] loss=1.5527 avg=1.5762 it/s=389.1\n",
      "[e3 b1849/2315] loss=1.6100 avg=1.5756 it/s=388.2\n",
      "[e3 b2080/2315] loss=1.5211 avg=1.5761 it/s=388.7\n",
      "[e3 b2311/2315] loss=1.6318 avg=1.5764 it/s=387.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5764 | val_acc=0.2775 | val_f1=0.0869 | time=100.4s\n",
      "[e4 b1/2315] loss=1.6071 avg=1.6071 it/s=531.4\n",
      "[e4 b2/2315] loss=1.6536 avg=1.6304 it/s=435.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.4876 avg=1.5763 it/s=394.7\n",
      "[e4 b463/2315] loss=1.5368 avg=1.5759 it/s=397.4\n",
      "[e4 b694/2315] loss=1.5368 avg=1.5754 it/s=398.4\n",
      "[e4 b925/2315] loss=1.5404 avg=1.5760 it/s=397.3\n",
      "[e4 b1156/2315] loss=1.6068 avg=1.5755 it/s=400.2\n",
      "[e4 b1387/2315] loss=1.5427 avg=1.5768 it/s=402.0\n",
      "[e4 b1618/2315] loss=1.5924 avg=1.5775 it/s=403.1\n",
      "[e4 b1849/2315] loss=1.5744 avg=1.5766 it/s=400.5\n",
      "[e4 b2080/2315] loss=1.5598 avg=1.5760 it/s=398.4\n",
      "[e4 b2311/2315] loss=1.5969 avg=1.5761 it/s=396.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5761 | val_acc=0.2775 | val_f1=0.0869 | time=98.4s\n",
      "[e5 b1/2315] loss=1.6475 avg=1.6475 it/s=304.5\n",
      "[e5 b2/2315] loss=1.6320 avg=1.6398 it/s=361.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5366 avg=1.5768 it/s=396.1\n",
      "[e5 b463/2315] loss=1.4937 avg=1.5767 it/s=404.1\n",
      "[e5 b694/2315] loss=1.6736 avg=1.5779 it/s=407.1\n",
      "[e5 b925/2315] loss=1.5456 avg=1.5754 it/s=399.4\n",
      "[e5 b1156/2315] loss=1.5562 avg=1.5756 it/s=397.0\n",
      "[e5 b1387/2315] loss=1.5684 avg=1.5765 it/s=398.1\n",
      "[e5 b1618/2315] loss=1.6767 avg=1.5769 it/s=400.8\n",
      "[e5 b1849/2315] loss=1.7046 avg=1.5768 it/s=403.3\n",
      "[e5 b2080/2315] loss=1.5137 avg=1.5769 it/s=405.6\n",
      "[e5 b2311/2315] loss=1.4953 avg=1.5759 it/s=406.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñá‚ñá‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÅ‚ñà‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00018</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>95.82265</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57592</td></tr><tr><td>train/epoch_loss</td><td>1.57592</td></tr><tr><td>train/items_per_sec</td><td>406.77788</td></tr><tr><td>train/loss</td><td>1.54844</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_14</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/6frv5ixf' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/6frv5ixf</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_051814-6frv5ixf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 13. Best value: 0.873252:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [3:03:06<1:05:12, 782.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 14] f1=0.0869 | unfreeze_k=9 lr=2.87e-04 wd=4.2e-06 suggested_bs=8\n",
      "[I 2025-08-16 05:26:37,992] Trial 14 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 0.00028664415955693967, 'weight_decay': 4.169196508305699e-06, 'batch_size': 8}. Best is trial 13 with value: 0.8732520507590781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_052639-me0cpv5y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/me0cpv5y' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_15</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/me0cpv5y' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/me0cpv5y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.5346 avg=1.5346 it/s=137.7\n",
      "[e1 b2/2315] loss=1.5595 avg=1.5471 it/s=191.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5076 avg=1.5410 it/s=374.7\n",
      "[e1 b463/2315] loss=1.5271 avg=1.5104 it/s=379.0\n",
      "[e1 b694/2315] loss=1.6102 avg=1.5351 it/s=383.0\n",
      "[e1 b925/2315] loss=1.7241 avg=1.5477 it/s=385.9\n",
      "[e1 b1156/2315] loss=1.4760 avg=1.5557 it/s=384.8\n",
      "[e1 b1387/2315] loss=1.5452 avg=1.5608 it/s=384.9\n",
      "[e1 b1618/2315] loss=1.4913 avg=1.5647 it/s=387.3\n",
      "[e1 b1849/2315] loss=1.6120 avg=1.5669 it/s=390.2\n",
      "[e1 b2080/2315] loss=1.5931 avg=1.5684 it/s=391.8\n",
      "[e1 b2311/2315] loss=1.7207 avg=1.5701 it/s=393.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5701 | val_acc=0.2775 | val_f1=0.0869 | time=99.0s\n",
      "[e2 b1/2315] loss=1.6194 avg=1.6194 it/s=366.7\n",
      "[e2 b2/2315] loss=1.5991 avg=1.6092 it/s=377.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6668 avg=1.5816 it/s=379.4\n",
      "[e2 b463/2315] loss=1.5586 avg=1.5816 it/s=364.7\n",
      "[e2 b694/2315] loss=1.6562 avg=1.5815 it/s=359.2\n",
      "[e2 b925/2315] loss=1.5274 avg=1.5822 it/s=364.2\n",
      "[e2 b1156/2315] loss=1.5360 avg=1.5815 it/s=366.5\n",
      "[e2 b1387/2315] loss=1.4732 avg=1.5801 it/s=370.3\n",
      "[e2 b1618/2315] loss=1.5180 avg=1.5803 it/s=372.7\n",
      "[e2 b1849/2315] loss=1.4818 avg=1.5797 it/s=374.3\n",
      "[e2 b2080/2315] loss=1.4829 avg=1.5798 it/s=376.9\n",
      "[e2 b2311/2315] loss=1.5224 avg=1.5792 it/s=379.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5793 | val_acc=0.2775 | val_f1=0.0869 | time=102.5s\n",
      "[e3 b1/2315] loss=1.5624 avg=1.5624 it/s=412.6\n",
      "[e3 b2/2315] loss=1.5599 avg=1.5611 it/s=424.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.7709 avg=1.5758 it/s=403.7\n",
      "[e3 b463/2315] loss=1.5236 avg=1.5800 it/s=400.5\n",
      "[e3 b694/2315] loss=1.6480 avg=1.5804 it/s=386.0\n",
      "[e3 b925/2315] loss=1.6711 avg=1.5793 it/s=382.5\n",
      "[e3 b1156/2315] loss=1.5469 avg=1.5795 it/s=384.4\n",
      "[e3 b1387/2315] loss=1.6139 avg=1.5800 it/s=384.3\n",
      "[e3 b1618/2315] loss=1.5051 avg=1.5787 it/s=384.2\n",
      "[e3 b1849/2315] loss=1.5730 avg=1.5790 it/s=385.0\n",
      "[e3 b2080/2315] loss=1.6501 avg=1.5783 it/s=387.0\n",
      "[e3 b2311/2315] loss=1.5629 avg=1.5783 it/s=387.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5784 | val_acc=0.2775 | val_f1=0.0869 | time=100.4s\n",
      "[e4 b1/2315] loss=1.5756 avg=1.5756 it/s=386.2\n",
      "[e4 b2/2315] loss=1.6278 avg=1.6017 it/s=407.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.5228 avg=1.5798 it/s=394.0\n",
      "[e4 b463/2315] loss=1.5855 avg=1.5767 it/s=400.5\n",
      "[e4 b694/2315] loss=1.5701 avg=1.5772 it/s=402.3\n",
      "[e4 b925/2315] loss=1.5775 avg=1.5792 it/s=405.2\n",
      "[e4 b1156/2315] loss=1.5559 avg=1.5789 it/s=402.2\n",
      "[e4 b1387/2315] loss=1.5649 avg=1.5789 it/s=395.5\n",
      "[e4 b1618/2315] loss=1.6140 avg=1.5786 it/s=391.4\n",
      "[e4 b1849/2315] loss=1.6234 avg=1.5788 it/s=386.7\n",
      "[e4 b2080/2315] loss=1.6995 avg=1.5775 it/s=386.5\n",
      "[e4 b2311/2315] loss=1.6051 avg=1.5777 it/s=387.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5776 | val_acc=0.2775 | val_f1=0.0869 | time=100.5s\n",
      "[e5 b1/2315] loss=1.5206 avg=1.5206 it/s=278.2\n",
      "[e5 b2/2315] loss=1.6115 avg=1.5661 it/s=308.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.4688 avg=1.5657 it/s=379.5\n",
      "[e5 b463/2315] loss=1.5617 avg=1.5723 it/s=384.9\n",
      "[e5 b694/2315] loss=1.5938 avg=1.5731 it/s=386.8\n",
      "[e5 b925/2315] loss=1.5695 avg=1.5748 it/s=393.8\n",
      "[e5 b1156/2315] loss=1.6238 avg=1.5753 it/s=397.5\n",
      "[e5 b1387/2315] loss=1.6349 avg=1.5754 it/s=399.3\n",
      "[e5 b1618/2315] loss=1.6670 avg=1.5758 it/s=398.5\n",
      "[e5 b1849/2315] loss=1.6098 avg=1.5768 it/s=391.3\n",
      "[e5 b2080/2315] loss=1.4929 avg=1.5771 it/s=386.9\n",
      "[e5 b2311/2315] loss=1.6209 avg=1.5772 it/s=385.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÖ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÅ‚ñà‚ñÑ‚ñÑ‚ñÖ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñá‚ñá‚ñÜ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÑ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÖ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00034</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>100.89559</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57719</td></tr><tr><td>train/epoch_loss</td><td>1.57713</td></tr><tr><td>train/items_per_sec</td><td>385.61822</td></tr><tr><td>train/loss</td><td>1.59601</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_15</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/me0cpv5y' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/me0cpv5y</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_052639-me0cpv5y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 13. Best value: 0.873252:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [3:11:40<46:46, 701.68s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 15] f1=0.0869 | unfreeze_k=9 lr=5.41e-04 wd=8.3e-06 suggested_bs=8\n",
      "[I 2025-08-16 05:35:11,967] Trial 15 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 0.0005412386442534567, 'weight_decay': 8.343157852100824e-06, 'batch_size': 8}. Best is trial 13 with value: 0.8732520507590781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_053513-gjzjdsrr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/gjzjdsrr' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_16</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/gjzjdsrr' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/gjzjdsrr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.5989 avg=1.5989 it/s=292.1\n",
      "[e1 b2/2315] loss=1.7152 avg=1.6570 it/s=343.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6101 avg=1.5693 it/s=379.1\n",
      "[e1 b463/2315] loss=0.9628 avg=1.4449 it/s=384.7\n",
      "[e1 b694/2315] loss=0.9013 avg=1.3227 it/s=389.7\n",
      "[e1 b925/2315] loss=0.9742 avg=1.2395 it/s=393.2\n",
      "[e1 b1156/2315] loss=0.7871 avg=1.1903 it/s=397.5\n",
      "[e1 b1387/2315] loss=0.3955 avg=1.1538 it/s=401.9\n",
      "[e1 b1618/2315] loss=1.8110 avg=1.1318 it/s=404.4\n",
      "[e1 b1849/2315] loss=1.6801 avg=1.1289 it/s=404.1\n",
      "[e1 b2080/2315] loss=1.6100 avg=1.1681 it/s=398.6\n",
      "[e1 b2311/2315] loss=1.5448 avg=1.2098 it/s=393.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.2105 | val_acc=0.2775 | val_f1=0.0869 | time=99.1s\n",
      "[e2 b1/2315] loss=1.7106 avg=1.7106 it/s=443.1\n",
      "[e2 b2/2315] loss=1.6065 avg=1.6585 it/s=446.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6524 avg=1.5785 it/s=400.1\n",
      "[e2 b463/2315] loss=1.6319 avg=1.5789 it/s=407.2\n",
      "[e2 b694/2315] loss=1.6252 avg=1.5773 it/s=404.1\n",
      "[e2 b925/2315] loss=1.4666 avg=1.5776 it/s=400.4\n",
      "[e2 b1156/2315] loss=1.5610 avg=1.5782 it/s=394.9\n",
      "[e2 b1387/2315] loss=1.6402 avg=1.5783 it/s=388.1\n",
      "[e2 b1618/2315] loss=1.6044 avg=1.5790 it/s=391.5\n",
      "[e2 b1849/2315] loss=1.6924 avg=1.5791 it/s=394.9\n",
      "[e2 b2080/2315] loss=1.5308 avg=1.5794 it/s=397.7\n",
      "[e2 b2311/2315] loss=1.6010 avg=1.5792 it/s=398.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5791 | val_acc=0.2775 | val_f1=0.0869 | time=98.0s\n",
      "[e3 b1/2315] loss=1.6315 avg=1.6315 it/s=285.9\n",
      "[e3 b2/2315] loss=1.5511 avg=1.5913 it/s=314.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5250 avg=1.5762 it/s=347.1\n",
      "[e3 b463/2315] loss=1.5635 avg=1.5762 it/s=360.6\n",
      "[e3 b694/2315] loss=1.5632 avg=1.5798 it/s=366.1\n",
      "[e3 b925/2315] loss=1.3925 avg=1.5785 it/s=366.2\n",
      "[e3 b1156/2315] loss=1.4867 avg=1.5773 it/s=369.1\n",
      "[e3 b1387/2315] loss=1.5945 avg=1.5779 it/s=372.4\n",
      "[e3 b1618/2315] loss=1.6295 avg=1.5775 it/s=374.4\n",
      "[e3 b1849/2315] loss=1.5891 avg=1.5774 it/s=375.2\n",
      "[e3 b2080/2315] loss=1.5631 avg=1.5774 it/s=377.6\n",
      "[e3 b2311/2315] loss=1.4891 avg=1.5771 it/s=379.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5772 | val_acc=0.2775 | val_f1=0.0869 | time=102.2s\n",
      "[e4 b1/2315] loss=1.5638 avg=1.5638 it/s=414.2\n",
      "[e4 b2/2315] loss=1.5372 avg=1.5505 it/s=439.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.5291 avg=1.5760 it/s=396.9\n",
      "[e4 b463/2315] loss=1.6895 avg=1.5769 it/s=376.5\n",
      "[e4 b694/2315] loss=1.5361 avg=1.5753 it/s=367.5\n",
      "[e4 b925/2315] loss=1.5203 avg=1.5754 it/s=362.7\n",
      "[e4 b1156/2315] loss=1.4371 avg=1.5747 it/s=368.6\n",
      "[e4 b1387/2315] loss=1.5221 avg=1.5749 it/s=370.4\n",
      "[e4 b1618/2315] loss=1.5479 avg=1.5760 it/s=374.9\n",
      "[e4 b1849/2315] loss=1.6011 avg=1.5763 it/s=376.4\n",
      "[e4 b2080/2315] loss=1.6386 avg=1.5766 it/s=379.6\n",
      "[e4 b2311/2315] loss=1.5849 avg=1.5765 it/s=380.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5764 | val_acc=0.2775 | val_f1=0.0869 | time=101.9s\n",
      "[e5 b1/2315] loss=1.5676 avg=1.5676 it/s=689.2\n",
      "[e5 b2/2315] loss=1.6071 avg=1.5873 it/s=429.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5603 avg=1.5734 it/s=402.8\n",
      "[e5 b463/2315] loss=1.6199 avg=1.5744 it/s=398.2\n",
      "[e5 b694/2315] loss=1.5838 avg=1.5771 it/s=397.2\n",
      "[e5 b925/2315] loss=1.4901 avg=1.5784 it/s=395.8\n",
      "[e5 b1156/2315] loss=1.6344 avg=1.5767 it/s=385.0\n",
      "[e5 b1387/2315] loss=1.5528 avg=1.5773 it/s=381.3\n",
      "[e5 b1618/2315] loss=1.6099 avg=1.5773 it/s=381.6\n",
      "[e5 b1849/2315] loss=1.5161 avg=1.5768 it/s=381.6\n",
      "[e5 b2080/2315] loss=1.6195 avg=1.5769 it/s=380.4\n",
      "[e5 b2311/2315] loss=1.5403 avg=1.5760 it/s=380.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñÉ‚ñÅ‚ñà‚ñá‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñá‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00013</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>102.09456</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57601</td></tr><tr><td>train/epoch_loss</td><td>1.5761</td></tr><tr><td>train/items_per_sec</td><td>380.8949</td></tr><tr><td>train/loss</td><td>1.52892</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_16</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/gjzjdsrr' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/gjzjdsrr</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_053513-gjzjdsrr\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 13. Best value: 0.873252:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:20:13<32:14, 644.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 16] f1=0.0869 | unfreeze_k=9 lr=2.14e-04 wd=5.2e-05 suggested_bs=8\n",
      "[I 2025-08-16 05:43:44,825] Trial 16 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 0.00021406994305558442, 'weight_decay': 5.182521308036029e-05, 'batch_size': 8}. Best is trial 13 with value: 0.8732520507590781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_054346-4kupuz2y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4kupuz2y' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_17</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4kupuz2y' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4kupuz2y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.6203 avg=1.6203 it/s=124.6\n",
      "[e1 b2/2315] loss=1.6982 avg=1.6593 it/s=177.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6761 avg=1.5913 it/s=386.3\n",
      "[e1 b463/2315] loss=1.5770 avg=1.5957 it/s=393.2\n",
      "[e1 b694/2315] loss=1.5673 avg=1.5933 it/s=394.3\n",
      "[e1 b925/2315] loss=1.6086 avg=1.5914 it/s=394.2\n",
      "[e1 b1156/2315] loss=1.5075 avg=1.5901 it/s=390.6\n",
      "[e1 b1387/2315] loss=1.5861 avg=1.5900 it/s=384.3\n",
      "[e1 b1618/2315] loss=1.5267 avg=1.5888 it/s=384.1\n",
      "[e1 b1849/2315] loss=1.6928 avg=1.5871 it/s=385.0\n",
      "[e1 b2080/2315] loss=1.4814 avg=1.5866 it/s=382.2\n",
      "[e1 b2311/2315] loss=1.5306 avg=1.5857 it/s=379.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5857 | val_acc=0.2775 | val_f1=0.0869 | time=102.5s\n",
      "[e2 b1/2315] loss=1.6595 avg=1.6595 it/s=316.6\n",
      "[e2 b2/2315] loss=1.5931 avg=1.6263 it/s=330.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.6060 avg=1.5800 it/s=390.2\n",
      "[e2 b463/2315] loss=1.6108 avg=1.5809 it/s=385.2\n",
      "[e2 b694/2315] loss=1.5600 avg=1.5817 it/s=392.3\n",
      "[e2 b925/2315] loss=1.5686 avg=1.5803 it/s=396.3\n",
      "[e2 b1156/2315] loss=1.5358 avg=1.5798 it/s=397.9\n",
      "[e2 b1387/2315] loss=1.5457 avg=1.5805 it/s=396.5\n",
      "[e2 b1618/2315] loss=1.5262 avg=1.5791 it/s=389.1\n",
      "[e2 b1849/2315] loss=1.5435 avg=1.5788 it/s=385.0\n",
      "[e2 b2080/2315] loss=1.7093 avg=1.5781 it/s=384.1\n",
      "[e2 b2311/2315] loss=1.5414 avg=1.5769 it/s=384.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5770 | val_acc=0.2775 | val_f1=0.0869 | time=101.2s\n",
      "[e3 b1/2315] loss=1.5864 avg=1.5864 it/s=416.3\n",
      "[e3 b2/2315] loss=1.4722 avg=1.5293 it/s=449.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5167 avg=1.5704 it/s=394.8\n",
      "[e3 b463/2315] loss=1.6761 avg=1.5754 it/s=400.7\n",
      "[e3 b694/2315] loss=1.4544 avg=1.5741 it/s=401.5\n",
      "[e3 b925/2315] loss=1.5622 avg=1.5744 it/s=402.2\n",
      "[e3 b1156/2315] loss=1.5375 avg=1.5744 it/s=402.6\n",
      "[e3 b1387/2315] loss=1.7032 avg=1.5740 it/s=405.9\n",
      "[e3 b1618/2315] loss=1.5970 avg=1.5745 it/s=408.3\n",
      "[e3 b1849/2315] loss=1.5934 avg=1.5757 it/s=408.6\n",
      "[e3 b2080/2315] loss=1.5098 avg=1.5751 it/s=404.5\n",
      "[e3 b2311/2315] loss=1.5430 avg=1.5754 it/s=400.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5752 | val_acc=0.2775 | val_f1=0.0869 | time=97.5s\n",
      "[e4 b1/2315] loss=1.5548 avg=1.5548 it/s=479.9\n",
      "[e4 b2/2315] loss=1.5427 avg=1.5488 it/s=401.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.6602 avg=1.5711 it/s=378.7\n",
      "[e4 b463/2315] loss=1.4953 avg=1.5733 it/s=393.7\n",
      "[e4 b694/2315] loss=1.4728 avg=1.5752 it/s=391.4\n",
      "[e4 b925/2315] loss=1.6954 avg=1.5743 it/s=391.4\n",
      "[e4 b1156/2315] loss=1.6849 avg=1.5755 it/s=387.0\n",
      "[e4 b1387/2315] loss=1.5441 avg=1.5750 it/s=388.5\n",
      "[e4 b1618/2315] loss=1.5284 avg=1.5750 it/s=390.8\n",
      "[e4 b1849/2315] loss=1.4916 avg=1.5748 it/s=394.3\n",
      "[e4 b2080/2315] loss=1.5810 avg=1.5752 it/s=397.3\n",
      "[e4 b2311/2315] loss=1.5450 avg=1.5756 it/s=399.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5757 | val_acc=0.2775 | val_f1=0.0869 | time=97.5s\n",
      "[e5 b1/2315] loss=1.4974 avg=1.4974 it/s=327.3\n",
      "[e5 b2/2315] loss=1.5242 avg=1.5108 it/s=398.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5675 avg=1.5732 it/s=374.2\n",
      "[e5 b463/2315] loss=1.5198 avg=1.5744 it/s=373.9\n",
      "[e5 b694/2315] loss=1.6139 avg=1.5741 it/s=370.2\n",
      "[e5 b925/2315] loss=1.7067 avg=1.5733 it/s=371.5\n",
      "[e5 b1156/2315] loss=1.7079 avg=1.5744 it/s=366.8\n",
      "[e5 b1387/2315] loss=1.6235 avg=1.5747 it/s=365.5\n",
      "[e5 b1618/2315] loss=1.5591 avg=1.5750 it/s=368.2\n",
      "[e5 b1849/2315] loss=1.5382 avg=1.5747 it/s=370.8\n",
      "[e5 b2080/2315] loss=1.6275 avg=1.5748 it/s=372.9\n",
      "[e5 b2311/2315] loss=1.5980 avg=1.5752 it/s=376.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñá‚ñÜ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñÅ‚ñÉ‚ñà‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñÇ‚ñà‚ñÜ‚ñÇ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.00113</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>103.1572</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57524</td></tr><tr><td>train/epoch_loss</td><td>1.57531</td></tr><tr><td>train/items_per_sec</td><td>376.38906</td></tr><tr><td>train/loss</td><td>1.55137</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_17</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4kupuz2y' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/4kupuz2y</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_054346-4kupuz2y\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 13. Best value: 0.873252:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:28:46<20:10, 605.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 17] f1=0.0869 | unfreeze_k=9 lr=1.82e-03 wd=1.9e-05 suggested_bs=8\n",
      "[I 2025-08-16 05:52:17,111] Trial 17 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 0.0018193251310341153, 'weight_decay': 1.8921760002592513e-05, 'batch_size': 8}. Best is trial 13 with value: 0.8732520507590781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_055218-2wwtyn8k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2wwtyn8k' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_18</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2wwtyn8k' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2wwtyn8k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 57,297,413 / 278,813,189 (20.55%) ; unfreeze_last_k=8\n",
      "[e1 b1/2315] loss=1.6091 avg=1.6091 it/s=294.9\n",
      "[e1 b2/2315] loss=1.6361 avg=1.6226 it/s=340.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.7800 avg=1.6097 it/s=412.4\n",
      "[e1 b463/2315] loss=1.5634 avg=1.6074 it/s=406.6\n",
      "[e1 b694/2315] loss=1.6055 avg=1.6009 it/s=398.2\n",
      "[e1 b925/2315] loss=1.7620 avg=1.6009 it/s=394.7\n",
      "[e1 b1156/2315] loss=1.4718 avg=1.6035 it/s=400.6\n",
      "[e1 b1387/2315] loss=1.5404 avg=1.5998 it/s=403.3\n",
      "[e1 b1618/2315] loss=1.6599 avg=1.5955 it/s=406.1\n",
      "[e1 b1849/2315] loss=1.6199 avg=1.5925 it/s=406.7\n",
      "[e1 b2080/2315] loss=1.6979 avg=1.5908 it/s=406.9\n",
      "[e1 b2311/2315] loss=1.6889 avg=1.5894 it/s=405.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.5893 | val_acc=0.2775 | val_f1=0.0869 | time=96.1s\n",
      "[e2 b1/2315] loss=1.4611 avg=1.4611 it/s=353.5\n",
      "[e2 b2/2315] loss=1.5447 avg=1.5029 it/s=416.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.5334 avg=1.5752 it/s=439.2\n",
      "[e2 b463/2315] loss=1.5187 avg=1.5803 it/s=439.7\n",
      "[e2 b694/2315] loss=1.5387 avg=1.5792 it/s=442.3\n",
      "[e2 b925/2315] loss=1.6701 avg=1.5786 it/s=435.0\n",
      "[e2 b1156/2315] loss=1.5515 avg=1.5784 it/s=419.5\n",
      "[e2 b1387/2315] loss=1.6537 avg=1.5766 it/s=414.9\n",
      "[e2 b1618/2315] loss=1.4913 avg=1.5767 it/s=416.7\n",
      "[e2 b1849/2315] loss=1.5390 avg=1.5760 it/s=420.3\n",
      "[e2 b2080/2315] loss=1.5108 avg=1.5759 it/s=421.9\n",
      "[e2 b2311/2315] loss=1.5044 avg=1.5761 it/s=422.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5761 | val_acc=0.2775 | val_f1=0.0869 | time=92.4s\n",
      "[e3 b1/2315] loss=1.5542 avg=1.5542 it/s=626.1\n",
      "[e3 b2/2315] loss=1.5984 avg=1.5763 it/s=363.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5297 avg=1.5759 it/s=421.7\n",
      "[e3 b463/2315] loss=1.4875 avg=1.5737 it/s=422.3\n",
      "[e3 b694/2315] loss=1.5618 avg=1.5750 it/s=424.6\n",
      "[e3 b925/2315] loss=1.5809 avg=1.5749 it/s=428.1\n",
      "[e3 b1156/2315] loss=1.6322 avg=1.5763 it/s=431.0\n",
      "[e3 b1387/2315] loss=1.6224 avg=1.5765 it/s=432.9\n",
      "[e3 b1618/2315] loss=1.5372 avg=1.5767 it/s=430.9\n",
      "[e3 b1849/2315] loss=1.6010 avg=1.5766 it/s=424.0\n",
      "[e3 b2080/2315] loss=1.5068 avg=1.5772 it/s=420.1\n",
      "[e3 b2311/2315] loss=1.6376 avg=1.5763 it/s=418.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5762 | val_acc=0.2775 | val_f1=0.0869 | time=93.4s\n",
      "[e4 b1/2315] loss=1.5334 avg=1.5334 it/s=411.0\n",
      "[e4 b2/2315] loss=1.7078 avg=1.6206 it/s=443.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.6972 avg=1.5719 it/s=406.0\n",
      "[e4 b463/2315] loss=1.5466 avg=1.5753 it/s=405.9\n",
      "[e4 b694/2315] loss=1.6076 avg=1.5763 it/s=406.9\n",
      "[e4 b925/2315] loss=1.6387 avg=1.5747 it/s=407.8\n",
      "[e4 b1156/2315] loss=1.5913 avg=1.5734 it/s=406.6\n",
      "[e4 b1387/2315] loss=1.5058 avg=1.5740 it/s=408.4\n",
      "[e4 b1618/2315] loss=1.6621 avg=1.5737 it/s=412.2\n",
      "[e4 b1849/2315] loss=1.5889 avg=1.5749 it/s=415.1\n",
      "[e4 b2080/2315] loss=1.4853 avg=1.5752 it/s=416.7\n",
      "[e4 b2311/2315] loss=1.5928 avg=1.5758 it/s=411.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5758 | val_acc=0.2775 | val_f1=0.0869 | time=95.2s\n",
      "[e5 b1/2315] loss=1.4977 avg=1.4977 it/s=371.9\n",
      "[e5 b2/2315] loss=1.5720 avg=1.5348 it/s=427.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.5121 avg=1.5688 it/s=392.3\n",
      "[e5 b463/2315] loss=1.4893 avg=1.5723 it/s=397.9\n",
      "[e5 b694/2315] loss=1.5403 avg=1.5724 it/s=404.6\n",
      "[e5 b925/2315] loss=1.5469 avg=1.5730 it/s=405.8\n",
      "[e5 b1156/2315] loss=1.5044 avg=1.5747 it/s=406.2\n",
      "[e5 b1387/2315] loss=1.6834 avg=1.5754 it/s=409.3\n",
      "[e5 b1618/2315] loss=1.5578 avg=1.5758 it/s=413.1\n",
      "[e5 b1849/2315] loss=1.5486 avg=1.5772 it/s=413.6\n",
      "[e5 b2080/2315] loss=1.5842 avg=1.5772 it/s=415.8\n",
      "[e5 b2311/2315] loss=1.5270 avg=1.5771 it/s=417.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 5\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÖ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñÅ‚ñÉ‚ñÜ‚ñÉ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÅ‚ñÜ‚ñÇ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñá‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÅ‚ñÑ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>lr</td><td>0.0056</td></tr><tr><td>params/ratio</td><td>0.2055</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>57297413</td></tr><tr><td>step</td><td>11571</td></tr><tr><td>time/epoch_sec</td><td>93.60675</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57706</td></tr><tr><td>train/epoch_loss</td><td>1.57708</td></tr><tr><td>train/items_per_sec</td><td>417.68997</td></tr><tr><td>train/loss</td><td>1.62999</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_18</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2wwtyn8k' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/2wwtyn8k</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_055218-2wwtyn8k\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 13. Best value: 0.873252:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:36:46<09:27, 567.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 18] f1=0.0869 | unfreeze_k=8 lr=9.02e-03 wd=7.0e-06 suggested_bs=64\n",
      "[I 2025-08-16 06:00:17,417] Trial 18 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 8, 'lr': 0.009024939590437785, 'weight_decay': 7.007463850573143e-06, 'batch_size': 64}. Best is trial 13 with value: 0.8732520507590781.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:205: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_060024-y1fzjq8q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/y1fzjq8q' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_19</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/y1fzjq8q' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/y1fzjq8q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.5885 avg=1.5885 it/s=104.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b2/2315] loss=1.6242 avg=1.6063 it/s=152.5\n",
      "[e1 b232/2315] loss=1.4133 avg=1.5542 it/s=365.1\n",
      "[e1 b463/2315] loss=0.9630 avg=1.4388 it/s=357.6\n",
      "[e1 b694/2315] loss=1.3027 avg=1.3151 it/s=358.2\n",
      "[e1 b925/2315] loss=1.5001 avg=1.2336 it/s=369.1\n",
      "[e1 b1156/2315] loss=1.3670 avg=1.1842 it/s=375.7\n",
      "[e1 b1387/2315] loss=1.0184 avg=1.1482 it/s=378.7\n",
      "[e1 b1618/2315] loss=0.7770 avg=1.1379 it/s=382.0\n",
      "[e1 b1849/2315] loss=1.3047 avg=1.1313 it/s=381.3\n",
      "[e1 b2080/2315] loss=1.6167 avg=1.1463 it/s=380.4\n",
      "[e1 b2311/2315] loss=1.7675 avg=1.1893 it/s=383.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12 | loss=1.1901 | val_acc=0.2410 | val_f1=0.0777 | time=101.3s\n",
      "[e2 b1/2315] loss=1.7647 avg=1.7647 it/s=424.9\n",
      "[e2 b2/2315] loss=1.5540 avg=1.6593 it/s=416.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.7031 avg=1.5835 it/s=402.6\n",
      "[e2 b463/2315] loss=1.4999 avg=1.5803 it/s=395.3\n",
      "[e2 b694/2315] loss=1.5068 avg=1.5820 it/s=385.7\n",
      "[e2 b925/2315] loss=1.4707 avg=1.5807 it/s=385.2\n",
      "[e2 b1156/2315] loss=1.5956 avg=1.5807 it/s=382.1\n",
      "[e2 b1387/2315] loss=1.6491 avg=1.5797 it/s=376.2\n",
      "[e2 b1618/2315] loss=1.6159 avg=1.5801 it/s=373.9\n",
      "[e2 b1849/2315] loss=1.6397 avg=1.5800 it/s=373.4\n",
      "[e2 b2080/2315] loss=1.5258 avg=1.5794 it/s=375.2\n",
      "[e2 b2311/2315] loss=1.6676 avg=1.5787 it/s=377.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/12 | loss=1.5787 | val_acc=0.2775 | val_f1=0.0869 | time=103.0s\n",
      "[e3 b1/2315] loss=1.4797 avg=1.4797 it/s=419.3\n",
      "[e3 b2/2315] loss=1.5734 avg=1.5266 it/s=355.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5181 avg=1.5735 it/s=387.1\n",
      "[e3 b463/2315] loss=1.3943 avg=1.5773 it/s=397.1\n",
      "[e3 b694/2315] loss=1.5834 avg=1.5784 it/s=399.2\n",
      "[e3 b925/2315] loss=1.5910 avg=1.5777 it/s=392.7\n",
      "[e3 b1156/2315] loss=1.5814 avg=1.5783 it/s=387.3\n",
      "[e3 b1387/2315] loss=1.5577 avg=1.5790 it/s=383.8\n",
      "[e3 b1618/2315] loss=1.5417 avg=1.5789 it/s=383.0\n",
      "[e3 b1849/2315] loss=1.5345 avg=1.5786 it/s=384.4\n",
      "[e3 b2080/2315] loss=1.5718 avg=1.5789 it/s=385.7\n",
      "[e3 b2311/2315] loss=1.5160 avg=1.5777 it/s=385.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/12 | loss=1.5776 | val_acc=0.2775 | val_f1=0.0869 | time=101.2s\n",
      "[e4 b1/2315] loss=1.4977 avg=1.4977 it/s=368.2\n",
      "[e4 b2/2315] loss=1.6063 avg=1.5520 it/s=365.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.5655 avg=1.5744 it/s=360.7\n",
      "[e4 b463/2315] loss=1.5943 avg=1.5766 it/s=373.9\n",
      "[e4 b694/2315] loss=1.5698 avg=1.5772 it/s=384.4\n",
      "[e4 b925/2315] loss=1.4477 avg=1.5760 it/s=389.1\n",
      "[e4 b1156/2315] loss=1.5392 avg=1.5762 it/s=392.8\n",
      "[e4 b1387/2315] loss=1.6798 avg=1.5764 it/s=390.9\n",
      "[e4 b1618/2315] loss=1.5141 avg=1.5758 it/s=385.2\n",
      "[e4 b1849/2315] loss=1.5716 avg=1.5762 it/s=379.9\n",
      "[e4 b2080/2315] loss=1.5255 avg=1.5767 it/s=381.6\n",
      "[e4 b2311/2315] loss=1.5878 avg=1.5769 it/s=383.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/12 | loss=1.5770 | val_acc=0.2775 | val_f1=0.0869 | time=101.3s\n",
      "[e5 b1/2315] loss=1.5030 avg=1.5030 it/s=348.3\n",
      "[e5 b2/2315] loss=1.6374 avg=1.5702 it/s=420.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.7917 avg=1.5735 it/s=391.4\n",
      "[e5 b463/2315] loss=1.5473 avg=1.5747 it/s=393.4\n",
      "[e5 b694/2315] loss=1.5807 avg=1.5758 it/s=396.2\n",
      "[e5 b925/2315] loss=1.5103 avg=1.5757 it/s=397.7\n",
      "[e5 b1156/2315] loss=1.5807 avg=1.5750 it/s=401.5\n",
      "[e5 b1387/2315] loss=1.4670 avg=1.5760 it/s=405.0\n",
      "[e5 b1618/2315] loss=1.5665 avg=1.5761 it/s=407.0\n",
      "[e5 b1849/2315] loss=1.6256 avg=1.5767 it/s=406.1\n",
      "[e5 b2080/2315] loss=1.5934 avg=1.5771 it/s=401.3\n",
      "[e5 b2311/2315] loss=1.5977 avg=1.5766 it/s=398.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/12 | loss=1.5767 | val_acc=0.2775 | val_f1=0.0869 | time=98.1s\n",
      "[e6 b1/2315] loss=1.5630 avg=1.5630 it/s=408.5\n",
      "[e6 b2/2315] loss=1.5874 avg=1.5752 it/s=398.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:258: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=1.6675 avg=1.5692 it/s=401.1\n",
      "[e6 b463/2315] loss=1.5862 avg=1.5727 it/s=391.5\n",
      "[e6 b694/2315] loss=1.4707 avg=1.5766 it/s=387.7\n",
      "[e6 b925/2315] loss=1.6371 avg=1.5762 it/s=391.4\n",
      "[e6 b1156/2315] loss=1.6673 avg=1.5755 it/s=391.5\n",
      "[e6 b1387/2315] loss=1.5551 avg=1.5755 it/s=390.2\n",
      "[e6 b1618/2315] loss=1.4295 avg=1.5762 it/s=393.1\n",
      "[e6 b1849/2315] loss=1.5656 avg=1.5760 it/s=395.9\n",
      "[e6 b2080/2315] loss=1.5262 avg=1.5763 it/s=397.4\n",
      "[e6 b2311/2315] loss=1.5687 avg=1.5764 it/s=397.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 6\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñÅ‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÅ‚ñÜ‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.08688</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>13886</td></tr><tr><td>time/epoch_sec</td><td>98.2173</td></tr><tr><td>train/avg_loss_so_far</td><td>1.57637</td></tr><tr><td>train/epoch_loss</td><td>1.57637</td></tr><tr><td>train/items_per_sec</td><td>397.80294</td></tr><tr><td>train/loss</td><td>1.638</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>2</td></tr><tr><td>val/best_f1_so_far</td><td>0.08688</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_optuna_trial_19</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/y1fzjq8q' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/y1fzjq8q</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_060024-y1fzjq8q\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_28084\\2837239188.py:180: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 13. Best value: 0.873252: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:47:06<00:00, 681.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 19] f1=0.0869 | unfreeze_k=9 lr=1.91e-04 wd=2.5e-06 suggested_bs=8\n",
      "[I 2025-08-16 06:10:37,417] Trial 19 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 0.00019148701280271666, 'weight_decay': 2.4790993754320025e-06, 'batch_size': 8}. Best is trial 13 with value: 0.8732520507590781.\n",
      "Best trial: 13 F1: 0.8732520507590781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)\n",
    "print(\"Best trial:\", study.best_trial.number, \"F1:\", study.best_value)\n",
    "best_params = {\"run_name\": f\"{BASE_RUN_NAME}_best_optuna\", **study.best_trial.params}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808a39f7-4c03-4bfe-bec4-f7f5a162da4b",
   "metadata": {},
   "source": [
    "# üìä Discussion of Results\n",
    "\n",
    "Looking at the full Optuna study, we can clearly split trials into two groups:\n",
    "\n",
    "- ‚úÖ **Good trials**: ~6 runs went the distance (12 epochs) and reached **val/F1 > 0.86**  \n",
    "- ‚ùå **Bad trials**: the rest crashed early with **F1 ‚âà 0.08**, meaning some hyperparameter combos are just not stable for fine-tuning mDeBERTa  \n",
    "\n",
    "---\n",
    "\n",
    "## üîç Patterns we noticed\n",
    "- **Learning rate (LR):**  \n",
    "  - Best runs are always in the **1e-5 ‚Äì 1e-4 range**  \n",
    "  - Larger LRs ‚Üí quick divergence and failure  \n",
    "- **Weight decay:**  \n",
    "  - When **WD > 1e-5**, performance dropped  \n",
    "  - Small WD values (‚âà `1e-5` or less) worked much better  \n",
    "- **Unfreezing layers:**  \n",
    "  - Strong runs came from **8‚Äì12 layers unfrozen**  \n",
    "  - The **absolute best run** unfreezed **all 12 layers**, showing deeper fine-tuning pays off for this dataset  \n",
    "\n",
    "---\n",
    "\n",
    "## üèÜ Best run\n",
    "The top score came from **Trial 2 (first run)** with:  \n",
    "- **val/F1 = 0.88022**  \n",
    "- LR = `3.5e-5`  \n",
    "- Batch size = `8`  \n",
    "- Weight decay = `9.4e-5`  \n",
    "- Unfrozen layers = **12** (full encoder)  \n",
    "\n",
    "üí° This result is basically on par with the **official English benchmark for mDeBERTa (~88.2 F1)** ‚Üí so our setup is hitting the model‚Äôs expected ceiling.  \n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next steps\n",
    "- Zoom in on the **LR sweet spot (1e-5 ‚Äì 5e-5)**  \n",
    "- Stick with **low weight decay (<1e-5)**  \n",
    "- Keep **full unfreezing (12 layers)**, or at least ‚â•9  \n",
    "- Retrain in this refined space to get a more stable checkpoint ‚Üí then move on to the **clean vs noisy test comparison**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ab1063-6545-46d2-b741-fd9a91686caf",
   "metadata": {},
   "source": [
    "# üíæ Saving and Reloading Best Hyperparameters\n",
    "\n",
    "After finishing the Optuna search, we don‚Äôt want to lose the best hyperparameters.  \n",
    "To make the process reproducible, we **save them into a JSON file** so we can load them again later.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35662c6c2d76d2b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:38:44.323116Z",
     "start_time": "2025-08-16T08:38:44.317506Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best hparams to: checkpoints\\best_hparams_optuna_2.json\n",
      "Best trial number: 13  value: 0.8732520507590781\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# best hparams from Optuna (only suggested ones live here)\n",
    "best_hparams = study.best_trial.params\n",
    "\n",
    "# if your train_one_run expects epochs/patience and they were fixed (not suggested),\n",
    "# add them explicitly:\n",
    "best_hparams_complete = {\n",
    "    **best_hparams,\n",
    "    \"epochs\": FIXED_EPOCHS,       # or whatever you used\n",
    "    \"patience\": FIXED_PATIENCE,   # \"\n",
    "}\n",
    "# hp_path = os.path.join(\"checkpoints\", \"best_hparams_optuna.json\")\n",
    "hp_path = os.path.join(\"checkpoints\", \"best_hparams_optuna_2.json\")\n",
    "with open(hp_path, \"w\") as f:\n",
    "    json.dump(best_hparams_complete, f, indent=2)\n",
    "print(\"Saved best hparams to:\", hp_path)\n",
    "print(\"Best trial number:\", study.best_trial.number, \" value:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738c17421fdcfadb",
   "metadata": {},
   "source": [
    "## üöÄLoad & Train Best Hyperparameters\n",
    "\n",
    "Now we load the saved hyperparameters from the best Optuna run.  \n",
    "The best trial reached **val/F1 = 0.88022** (trial 2), with LR ‚âà 3.5e-5, weight decay ‚âà 9.4e-5, batch size 8, and all 12 layers unfrozen.  \n",
    "We will use these settings to retrain the model in a clean run and later evaluate it on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18100f6-293b-45df-ad6d-7d7b7fa0b3ca",
   "metadata": {},
   "source": [
    "# Load Best Model EX4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58c724bb98e8bdee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T10:05:00.998770Z",
     "start_time": "2025-08-17T10:05:00.985196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_unfreeze_last_layers': 12,\n",
       " 'lr': 3.496909962515421e-05,\n",
       " 'weight_decay': 9.403805231949854e-05,\n",
       " 'batch_size': 8,\n",
       " 'epochs': 10,\n",
       " 'patience': 4,\n",
       " 'run_name': 'microsoft/mdeberta-v3-base_full_ex_4_best_optuna_retrain'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json, os\n",
    "BASE_RUN_NAME = \"microsoft/mdeberta-v3-base_full_ex_4\"\n",
    "with open(os.path.join(\"checkpoints\", \"best_hparams_optuna.json\")) as f:\n",
    "    best_hparams = json.load(f)\n",
    "\n",
    "# give a distinct name for the final run\n",
    "best_hparams[\"run_name\"] = f\"{BASE_RUN_NAME}_best_optuna_retrain\"\n",
    "\n",
    "# (optional) bump epochs here; see guidance below\n",
    "best_hparams[\"epochs\"] = 10      # higher cap\n",
    "best_hparams[\"patience\"] = 4     # unchanged\n",
    "\n",
    "best_hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "194a4ac845e9731c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T10:23:55.534328Z",
     "start_time": "2025-08-17T10:05:03.391617Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:207: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_130505-uak8z5kw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/uak8z5kw' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_best_optuna_retrain</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/uak8z5kw' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/uak8z5kw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b1/2315] loss=1.5977 avg=1.5977 it/s=30.4\n",
      "[e1 b2/2315] loss=1.6135 avg=1.6056 it/s=44.5\n",
      "[e1 b232/2315] loss=1.5696 avg=1.5877 it/s=315.9\n",
      "[e1 b463/2315] loss=1.3081 avg=1.5419 it/s=332.9\n",
      "[e1 b694/2315] loss=1.2160 avg=1.4361 it/s=341.6\n",
      "[e1 b925/2315] loss=1.0303 avg=1.3292 it/s=345.2\n",
      "[e1 b1156/2315] loss=1.4365 avg=1.2427 it/s=346.7\n",
      "[e1 b1387/2315] loss=1.0453 avg=1.1761 it/s=349.4\n",
      "[e1 b1618/2315] loss=0.6425 avg=1.1249 it/s=350.5\n",
      "[e1 b1849/2315] loss=0.3707 avg=1.0793 it/s=349.3\n",
      "[e1 b2080/2315] loss=0.5823 avg=1.0368 it/s=347.1\n",
      "[e1 b2311/2315] loss=0.6791 avg=1.0018 it/s=346.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | loss=1.0015 | val_acc=0.7629 | val_f1=0.7729 | time=111.6s\n",
      "[e2 b1/2315] loss=0.6484 avg=0.6484 it/s=285.2\n",
      "[e2 b2/2315] loss=0.4790 avg=0.5637 it/s=292.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.5898 avg=0.5615 it/s=356.4\n",
      "[e2 b463/2315] loss=0.5841 avg=0.5753 it/s=359.0\n",
      "[e2 b694/2315] loss=0.4356 avg=0.5742 it/s=355.0\n",
      "[e2 b925/2315] loss=0.6761 avg=0.5715 it/s=352.6\n",
      "[e2 b1156/2315] loss=0.4915 avg=0.5667 it/s=348.1\n",
      "[e2 b1387/2315] loss=0.3386 avg=0.5631 it/s=350.5\n",
      "[e2 b1618/2315] loss=0.8995 avg=0.5565 it/s=352.7\n",
      "[e2 b1849/2315] loss=0.5608 avg=0.5487 it/s=353.2\n",
      "[e2 b2080/2315] loss=0.6693 avg=0.5434 it/s=352.9\n",
      "[e2 b2311/2315] loss=0.4592 avg=0.5396 it/s=352.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | loss=0.5397 | val_acc=0.8117 | val_f1=0.8161 | time=109.8s\n",
      "[e3 b1/2315] loss=0.3494 avg=0.3494 it/s=318.7\n",
      "[e3 b2/2315] loss=0.6386 avg=0.4940 it/s=325.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.6826 avg=0.4223 it/s=344.5\n",
      "[e3 b463/2315] loss=0.7476 avg=0.4230 it/s=345.5\n",
      "[e3 b694/2315] loss=0.3135 avg=0.4291 it/s=345.5\n",
      "[e3 b925/2315] loss=0.5662 avg=0.4208 it/s=348.9\n",
      "[e3 b1156/2315] loss=0.6292 avg=0.4194 it/s=351.1\n",
      "[e3 b1387/2315] loss=0.5126 avg=0.4187 it/s=352.3\n",
      "[e3 b1618/2315] loss=0.1508 avg=0.4158 it/s=353.9\n",
      "[e3 b1849/2315] loss=0.4077 avg=0.4167 it/s=355.1\n",
      "[e3 b2080/2315] loss=0.3348 avg=0.4157 it/s=356.2\n",
      "[e3 b2311/2315] loss=0.4474 avg=0.4147 it/s=354.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | loss=0.4144 | val_acc=0.8406 | val_f1=0.8455 | time=109.3s\n",
      "[e4 b1/2315] loss=0.1458 avg=0.1458 it/s=331.7\n",
      "[e4 b2/2315] loss=0.1385 avg=0.1422 it/s=361.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.7823 avg=0.3397 it/s=347.7\n",
      "[e4 b463/2315] loss=0.1889 avg=0.3351 it/s=345.1\n",
      "[e4 b694/2315] loss=0.6018 avg=0.3302 it/s=348.9\n",
      "[e4 b925/2315] loss=0.6548 avg=0.3313 it/s=348.9\n",
      "[e4 b1156/2315] loss=0.7217 avg=0.3349 it/s=349.0\n",
      "[e4 b1387/2315] loss=0.1390 avg=0.3356 it/s=351.1\n",
      "[e4 b1618/2315] loss=0.3413 avg=0.3347 it/s=352.9\n",
      "[e4 b1849/2315] loss=0.5281 avg=0.3331 it/s=352.8\n",
      "[e4 b2080/2315] loss=0.6994 avg=0.3313 it/s=352.3\n",
      "[e4 b2311/2315] loss=0.2343 avg=0.3319 it/s=351.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | loss=0.3318 | val_acc=0.8571 | val_f1=0.8608 | time=110.4s\n",
      "[e5 b1/2315] loss=0.0911 avg=0.0911 it/s=314.9\n",
      "[e5 b2/2315] loss=0.0329 avg=0.0620 it/s=318.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.2877 avg=0.2698 it/s=308.2\n",
      "[e5 b463/2315] loss=0.0988 avg=0.2716 it/s=313.3\n",
      "[e5 b694/2315] loss=0.0654 avg=0.2780 it/s=319.9\n",
      "[e5 b925/2315] loss=0.0156 avg=0.2767 it/s=323.2\n",
      "[e5 b1156/2315] loss=0.3445 avg=0.2793 it/s=324.2\n",
      "[e5 b1387/2315] loss=0.5519 avg=0.2792 it/s=326.5\n",
      "[e5 b1618/2315] loss=0.1366 avg=0.2754 it/s=329.6\n",
      "[e5 b1849/2315] loss=0.0335 avg=0.2742 it/s=331.4\n",
      "[e5 b2080/2315] loss=0.1646 avg=0.2733 it/s=331.4\n",
      "[e5 b2311/2315] loss=0.5911 avg=0.2717 it/s=330.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | loss=0.2718 | val_acc=0.8564 | val_f1=0.8597 | time=116.5s\n",
      "[e6 b1/2315] loss=0.3264 avg=0.3264 it/s=362.1\n",
      "[e6 b2/2315] loss=0.1784 avg=0.2524 it/s=365.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2758 avg=0.2333 it/s=354.6\n",
      "[e6 b463/2315] loss=0.5765 avg=0.2383 it/s=351.0\n",
      "[e6 b694/2315] loss=0.3511 avg=0.2383 it/s=350.5\n",
      "[e6 b925/2315] loss=0.1891 avg=0.2292 it/s=348.4\n",
      "[e6 b1156/2315] loss=0.0097 avg=0.2320 it/s=347.1\n",
      "[e6 b1387/2315] loss=0.1764 avg=0.2315 it/s=349.5\n",
      "[e6 b1618/2315] loss=0.9665 avg=0.2298 it/s=351.0\n",
      "[e6 b1849/2315] loss=0.7269 avg=0.2280 it/s=351.9\n",
      "[e6 b2080/2315] loss=0.0307 avg=0.2273 it/s=353.5\n",
      "[e6 b2311/2315] loss=0.3955 avg=0.2269 it/s=354.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | loss=0.2271 | val_acc=0.8435 | val_f1=0.8471 | time=109.1s\n",
      "[e7 b1/2315] loss=0.1288 avg=0.1288 it/s=319.8\n",
      "[e7 b2/2315] loss=0.0141 avg=0.0715 it/s=370.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.3033 avg=0.1611 it/s=354.6\n",
      "[e7 b463/2315] loss=0.0198 avg=0.1728 it/s=349.7\n",
      "[e7 b694/2315] loss=0.1329 avg=0.1760 it/s=348.5\n",
      "[e7 b925/2315] loss=0.1271 avg=0.1807 it/s=351.7\n",
      "[e7 b1156/2315] loss=0.2476 avg=0.1851 it/s=352.9\n",
      "[e7 b1387/2315] loss=0.0093 avg=0.1865 it/s=354.5\n",
      "[e7 b1618/2315] loss=0.3550 avg=0.1870 it/s=355.3\n",
      "[e7 b1849/2315] loss=0.0303 avg=0.1844 it/s=356.0\n",
      "[e7 b2080/2315] loss=0.0439 avg=0.1852 it/s=357.1\n",
      "[e7 b2311/2315] loss=0.0064 avg=0.1845 it/s=358.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | loss=0.1844 | val_acc=0.8591 | val_f1=0.8640 | time=107.9s\n",
      "[e8 b1/2315] loss=0.0114 avg=0.0114 it/s=287.5\n",
      "[e8 b2/2315] loss=0.0111 avg=0.0113 it/s=331.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e8 b232/2315] loss=0.0040 avg=0.1512 it/s=363.5\n",
      "[e8 b463/2315] loss=0.0079 avg=0.1689 it/s=357.3\n",
      "[e8 b694/2315] loss=0.0034 avg=0.1681 it/s=349.5\n",
      "[e8 b925/2315] loss=0.0100 avg=0.1653 it/s=342.9\n",
      "[e8 b1156/2315] loss=0.3326 avg=0.1646 it/s=337.1\n",
      "[e8 b1387/2315] loss=0.0347 avg=0.1602 it/s=334.6\n",
      "[e8 b1618/2315] loss=0.0741 avg=0.1577 it/s=336.1\n",
      "[e8 b1849/2315] loss=0.4028 avg=0.1563 it/s=335.6\n",
      "[e8 b2080/2315] loss=0.8694 avg=0.1554 it/s=333.7\n",
      "[e8 b2311/2315] loss=0.0094 avg=0.1546 it/s=334.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | loss=0.1545 | val_acc=0.8586 | val_f1=0.8628 | time=115.6s\n",
      "[e9 b1/2315] loss=0.0048 avg=0.0048 it/s=480.7\n",
      "[e9 b2/2315] loss=0.0041 avg=0.0045 it/s=402.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e9 b232/2315] loss=0.0029 avg=0.1017 it/s=358.5\n",
      "[e9 b463/2315] loss=0.0048 avg=0.1248 it/s=360.8\n",
      "[e9 b694/2315] loss=0.0027 avg=0.1183 it/s=347.4\n",
      "[e9 b925/2315] loss=0.6540 avg=0.1184 it/s=342.8\n",
      "[e9 b1156/2315] loss=0.0092 avg=0.1184 it/s=338.7\n",
      "[e9 b1387/2315] loss=0.1919 avg=0.1209 it/s=338.4\n",
      "[e9 b1618/2315] loss=0.2756 avg=0.1202 it/s=337.5\n",
      "[e9 b1849/2315] loss=0.0063 avg=0.1228 it/s=339.7\n",
      "[e9 b2080/2315] loss=0.0092 avg=0.1222 it/s=338.2\n",
      "[e9 b2311/2315] loss=0.8849 avg=0.1234 it/s=336.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | loss=0.1233 | val_acc=0.8676 | val_f1=0.8713 | time=114.9s\n",
      "[e10 b1/2315] loss=0.0022 avg=0.0022 it/s=298.2\n",
      "[e10 b2/2315] loss=0.7794 avg=0.3908 it/s=287.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:260: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e10 b232/2315] loss=0.0031 avg=0.1081 it/s=352.8\n",
      "[e10 b463/2315] loss=0.0052 avg=0.1049 it/s=358.1\n",
      "[e10 b694/2315] loss=0.0366 avg=0.1007 it/s=352.3\n",
      "[e10 b925/2315] loss=0.0026 avg=0.1015 it/s=351.9\n",
      "[e10 b1156/2315] loss=0.0023 avg=0.1016 it/s=352.2\n",
      "[e10 b1387/2315] loss=0.0039 avg=0.1013 it/s=348.6\n",
      "[e10 b1618/2315] loss=0.4062 avg=0.1009 it/s=347.3\n",
      "[e10 b1849/2315] loss=0.0101 avg=0.1008 it/s=348.6\n",
      "[e10 b2080/2315] loss=0.1211 avg=0.1012 it/s=349.4\n",
      "[e10 b2311/2315] loss=0.5247 avg=0.0987 it/s=350.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | loss=0.0987 | val_acc=0.8635 | val_f1=0.8672 | time=110.3s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÅ‚ñá‚ñá‚ñÉ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>best_val_f1</td><td>0.87125</td></tr><tr><td>epoch</td><td>10</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>23146</td></tr><tr><td>time/epoch_sec</td><td>110.2989</td></tr><tr><td>train/avg_loss_so_far</td><td>0.09875</td></tr><tr><td>train/epoch_loss</td><td>0.09872</td></tr><tr><td>train/items_per_sec</td><td>350.59028</td></tr><tr><td>train/loss</td><td>0.00541</td></tr><tr><td>val/acc</td><td>0.86346</td></tr><tr><td>val/best_epoch</td><td>9</td></tr><tr><td>val/best_f1_so_far</td><td>0.87125</td></tr><tr><td>val/f1</td><td>0.86723</td></tr><tr><td>val/precision</td><td>0.86194</td></tr><tr><td>val/recall</td><td>0.87403</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_best_optuna_retrain</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/uak8z5kw' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full/runs/uak8z5kw</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-full</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_130505-uak8z5kw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\1725332257.py:182: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Retrain best config to get a clean checkpoint\n",
    "best_ckpt, _ = train_one_run(best_hparams)\n",
    "best_path = best_ckpt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4dfdf237f48ba2",
   "metadata": {},
   "source": [
    "## üíæSave Best Model & Test\n",
    "\n",
    "After loading the best hyperparameters, we retrained the model from scratch to get a **clean checkpoint**.  \n",
    "This ensures that the final model is trained only with the best settings found by Optuna (val/F1 ‚âà 0.88022).  \n",
    "\n",
    "For evaluation, we tested on the **clean translated test set**, since the model was also trained on **cleaned and translated data**.  \n",
    "This way, the evaluation setup matches the training conditions.  \n",
    "\n",
    "### üìä Final Results (Validation / Test)\n",
    "- **Best validation F1:** 0.87125 (epoch 9)  \n",
    "- **Validation Accuracy:** 0.86346  \n",
    "- **Validation Precision:** 0.86194  \n",
    "- **Validation Recall:** 0.87403  \n",
    "- **Validation F1 (last epoch):** 0.86723  \n",
    "\n",
    "The best checkpoint was saved at:  \n",
    "`checkpoints/best_microsoft_mdeberta-v3-base_full_ex_4_best_optuna_retrain.pt`  \n",
    "\n",
    "### Training Stats\n",
    "- Trainable parameters: **85.6M / 278.8M** (~30.7%)  \n",
    "- Average training loss (per epoch): **0.0987**  \n",
    "- Throughput: ~**350 items/sec**  \n",
    "- Time per epoch: ~**110 sec**  \n",
    "\n",
    "These results confirm that our setup is close to the reported performance of **mDeBERTa-v3-base on English (‚âà88.2 F1)** and provides a strong baseline.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07069c8-2fde-4c45-8c22-0161a09f06ba",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95d68fa40fc7f364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T10:36:09.034084Z",
     "start_time": "2025-08-17T10:35:57.248010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35636\\582332933.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST | acc=0.8662 | f1_macro=0.8685 | precision_macro=0.8680 | recall_macro=0.8697\n",
      "\n",
      "Per-class report (ids map to labels):\n",
      "{0: 'extremely negative', 1: 'negative', 2: 'neutral', 3: 'positive', 4: 'extremely positive'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative       0.89      0.89      0.89       592\n",
      "          negative       0.85      0.88      0.87      1041\n",
      "           neutral       0.85      0.87      0.86       619\n",
      "          positive       0.88      0.81      0.84       947\n",
      "extremely positive       0.87      0.89      0.88       599\n",
      "\n",
      "          accuracy                           0.87      3798\n",
      "         macro avg       0.87      0.87      0.87      3798\n",
      "      weighted avg       0.87      0.87      0.87      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Retrain best config to get a clean checkpoint\n",
    "# best_ckpt, _ = train_one_run(best_params)\n",
    "# best_path = best_ckpt\n",
    "best_params=best_hparams\n",
    "# -------------------------\n",
    "# Final evaluation on TEST (+ W&B logging)\n",
    "# -------------------------\n",
    "model = build_model(best_params[\"num_unfreeze_last_layers\"])\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "        with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "                      enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "            logits = model(**batch).logits\n",
    "        all_preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "print(f\"\\nTEST | acc={acc:.4f} | f1_macro={f1:.4f} | precision_macro={p:.4f} | recall_macro={r:.4f}\\n\")\n",
    "\n",
    "print(\"Per-class report (ids map to labels):\")\n",
    "print(ID2LABEL)\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# # ---- W&B: log test metrics, per-class scores, and confusion matrix ----\n",
    "# test_run = wandb.init(project=PROJECT, name=f\"{BASE_RUN_NAME}_test\", resume=\"allow\", reinit=True)\n",
    "# log_payload = {\n",
    "#     \"test/acc\": acc,\n",
    "#     \"test/precision_macro\": p,\n",
    "#     \"test/recall_macro\": r,\n",
    "#     \"test/f1_macro\": f1,\n",
    "# }\n",
    "# for cls_name in ORDER:\n",
    "#     if cls_name in report:\n",
    "#         log_payload[f\"test/{cls_name}/precision\"] = report[cls_name][\"precision\"]\n",
    "#         log_payload[f\"test/{cls_name}/recall\"]    = report[cls_name][\"recall\"]\n",
    "#         log_payload[f\"test/{cls_name}/f1\"]        = report[cls_name][\"f1-score\"]\n",
    "#\n",
    "# wandb.log(log_payload)\n",
    "#\n",
    "# cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(ORDER))))\n",
    "# wandb.log({\n",
    "#     \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "#         y_true=all_labels,\n",
    "#         preds=all_preds,\n",
    "#         class_names=[ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "#     )\n",
    "# })\n",
    "# test_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "# test_run.summary[\"test_f1_macro\"] = f1\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d0eee5-7dd8-4684-9647-142440e3dc6e",
   "metadata": {},
   "source": [
    "# ‚úÖ Test Results ‚Äì Clean Translated Set\n",
    "\n",
    "When evaluating on the **clean translated test set** (same distribution as training data), the model reached:\n",
    "\n",
    "- üéØ **Accuracy:** 0.8662  \n",
    "- üìä **Macro F1:** 0.8685  \n",
    "- üßÆ **Macro Precision:** 0.8680  \n",
    "- üîÑ **Macro Recall:** 0.8697  \n",
    "\n",
    "---\n",
    "\n",
    "### üîé Per-class breakdown\n",
    "- üò° **Extremely Negative:** F1 = **0.89** ‚Üí very strong & consistent  \n",
    "- üôÅ **Negative:** F1 = **0.87**  \n",
    "- üòê **Neutral:** F1 = **0.86**  \n",
    "- üôÇ **Positive:** F1 = **0.84** ‚Üí slightly weaker, recall drop  \n",
    "- ü§© **Extremely Positive:** F1 = **0.88** ‚Üí very strong & consistent  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Takeaway\n",
    "The model performs **best on the extreme sentiment classes** (extremely negative / extremely positive), where the signal is more clear.  \n",
    "Performance is a bit weaker for **positive vs neutral**, which makes sense since these classes are often harder to separate in real tweets.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e1782-7a1e-42b0-b88e-42e0e2ba4ca3",
   "metadata": {},
   "source": [
    "## Export: Model, Tokenizer, Metrics & Artifacts (Exercise 4)\n",
    "\n",
    "We export the **retrained best model** and all artifacts so the run is fully reproducible and easy to reload elsewhere.\n",
    "\n",
    "**What gets saved:**\n",
    "- ü§ó **HF model + tokenizer** (config + weights + tokenizer files)\n",
    "- üß† **Raw best state_dict** snapshot (`best_state_dict.pt`) from early stopping\n",
    "- üìä **Test metrics** (`test_metrics.json`)\n",
    "- üìà **Classification report** CSV and **confusion matrix** CSV\n",
    "- üè∑Ô∏è **Label mapping** (`labels.json`) with `order`, `label2id`, `id2label`\n",
    "- ‚öôÔ∏è **Best hyperparameters** (`best_hparams_ex4.json`)\n",
    "- üìÑ **README** with reload instructions\n",
    "\n",
    "**Output directory:**\n",
    "`ex_4_model/microsoft__mdeberta-v3-base_full_ex_4_YYYYMMDD_HHMMSS`\n",
    "\n",
    "This makes it simple to reload the exact model and tokenizer later, or to load the raw `state_dict` into a fresh model with the same head and label mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7402e8e011c079dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T10:36:22.477211Z",
     "start_time": "2025-08-17T10:36:20.966740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EX.4 EXPORT DONE ===\n",
      "Saved to: ex_4_model\\microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\n",
      " ‚Ä¢ HF model + tokenizer\n",
      " ‚Ä¢ Original best state_dict snapshot ‚Üí ex_4_model\\microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\\best_state_dict.pt\n",
      " ‚Ä¢ Test metrics JSON ‚Üí ex_4_model\\microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\\test_metrics.json\n",
      " ‚Ä¢ Classification report CSV ‚Üí ex_4_model\\microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\\classification_report_test.csv\n",
      " ‚Ä¢ Confusion matrix CSV ‚Üí ex_4_model\\microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\\confusion_matrix_test.csv\n",
      " ‚Ä¢ Label mapping JSON ‚Üí ex_4_model\\microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\\labels.json\n",
      " ‚Ä¢ Best hparams JSON ‚Üí ex_4_model\\microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\\best_hparams_ex4.json\n"
     ]
    }
   ],
   "source": [
    "# === Save full EX.4 export (model, tokenizer, metrics, hparams) ===\n",
    "import os, json, time, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Root folder + unique subdir (so you can export multiple times safely)\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_stub  = (BASE_RUN_NAME if 'BASE_RUN_NAME' in globals() else 'ex4').replace(\"/\", \"__\").replace(\"\\\\\", \"__\")\n",
    "export_dir = os.path.join(\"ex_4_model\", f\"{run_stub}_{timestamp}\")\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# 1) Save the model in Hugging Face format + tokenizer\n",
    "# (Your current `model` already has id2label/label2id in its config from build_model)\n",
    "model.save_pretrained(export_dir)          # writes config.json + pytorch_model.bin\n",
    "tokenizer.save_pretrained(export_dir)      # writes tokenizer files into same folder\n",
    "\n",
    "# Also keep the raw state_dict snapshot that early stopping wrote\n",
    "try:\n",
    "    shutil.copy2(best_path, os.path.join(export_dir, \"best_state_dict.pt\"))\n",
    "except Exception as e:\n",
    "    print(f\"[export] Warning: couldn't copy raw state_dict from {best_path}: {e}\")\n",
    "\n",
    "# 2) Save test artifacts (report CSV, confusion matrix CSV, metrics JSON)\n",
    "rep_df = pd.DataFrame(report).transpose()\n",
    "cm     = confusion_matrix(all_labels, all_preds, labels=list(range(len(ORDER))))\n",
    "cm_df  = pd.DataFrame(cm, index=[f\"true_{c}\" for c in ORDER], columns=[f\"pred_{c}\" for c in ORDER])\n",
    "\n",
    "rep_path   = os.path.join(export_dir, \"classification_report_test.csv\")\n",
    "cm_path    = os.path.join(export_dir, \"confusion_matrix_test.csv\")\n",
    "metrics_js = os.path.join(export_dir, \"test_metrics.json\")\n",
    "\n",
    "rep_df.to_csv(rep_path, index=True)\n",
    "cm_df.to_csv(cm_path, index=True)\n",
    "with open(metrics_js, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"test_accuracy\": float(acc),\n",
    "            \"test_precision_macro\": float(p),\n",
    "            \"test_recall_macro\": float(r),\n",
    "            \"test_f1_macro\": float(f1),\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "# 3) Save label mapping + order (useful when reloading elsewhere)\n",
    "labels_js = os.path.join(export_dir, \"labels.json\")\n",
    "with open(labels_js, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(\n",
    "        {\n",
    "            \"order\": ORDER,\n",
    "            \"label2id\": LABEL2ID,\n",
    "            \"id2label\": {int(k): v for k, v in ID2LABEL.items()},\n",
    "        },\n",
    "        f,\n",
    "        indent=2,\n",
    "    )\n",
    "\n",
    "# 4) Save best hyperparameters that produced the checkpoint\n",
    "hparams = {}\n",
    "if 'best_params'   in globals(): hparams = best_params\n",
    "elif 'best_hparams' in globals(): hparams = best_hparams\n",
    "\n",
    "hparams_js = os.path.join(export_dir, \"best_hparams_ex4.json\")\n",
    "with open(hparams_js, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hparams, f, indent=2)\n",
    "\n",
    "# 5) Tiny README with reload instructions\n",
    "readme_txt = os.path.join(export_dir, \"README.txt\")\n",
    "with open(readme_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        \"Exercise-4 export\\n\"\n",
    "        f\"Model: {MODEL_NAME}\\n\"\n",
    "        f\"Exported at: {timestamp}\\n\"\n",
    "        f\"Run stub: {run_stub}\\n\"\n",
    "        f\"Labels (order): {ORDER}\\n\"\n",
    "        f\"Best hparams: {hparams}\\n\"\n",
    "        \"\\n\"\n",
    "        \"How to reload:\\n\"\n",
    "        \"----------------\\n\"\n",
    "        \"from transformers import AutoModelForSequenceClassification, AutoTokenizer\\n\"\n",
    "        f\"model = AutoModelForSequenceClassification.from_pretrained(r\\\"{export_dir}\\\")\\n\"\n",
    "        f\"tokenizer = AutoTokenizer.from_pretrained(r\\\"{export_dir}\\\")\\n\"\n",
    "        \"\\n\"\n",
    "        \"You can also load the raw state_dict (best_state_dict.pt) into a fresh model created\\n\"\n",
    "        \"with the same head and id2label/label2id mapping.\\n\"\n",
    "    )\n",
    "\n",
    "print(\"\\n=== EX.4 EXPORT DONE ===\")\n",
    "print(\"Saved to:\", export_dir)\n",
    "print(\" ‚Ä¢ HF model + tokenizer\")\n",
    "print(\" ‚Ä¢ Original best state_dict snapshot ‚Üí\", os.path.join(export_dir, \"best_state_dict.pt\"))\n",
    "print(\" ‚Ä¢ Test metrics JSON ‚Üí\", metrics_js)\n",
    "print(\" ‚Ä¢ Classification report CSV ‚Üí\", rep_path)\n",
    "print(\" ‚Ä¢ Confusion matrix CSV ‚Üí\", cm_path)\n",
    "print(\" ‚Ä¢ Label mapping JSON ‚Üí\", labels_js)\n",
    "print(\" ‚Ä¢ Best hparams JSON ‚Üí\", hparams_js)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81651d4ead374540",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T18:37:07.379545Z",
     "start_time": "2025-08-15T18:37:02.835353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250815_213702-t6gj6ixw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/t6gj6ixw' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_test</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/t6gj6ixw' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/t6gj6ixw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/acc</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/f1</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/precision</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/recall</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/f1</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/precision</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/recall</td><td>‚ñÅ</td></tr><tr><td>test/f1_macro</td><td>‚ñÅ</td></tr><tr><td>test/negative/f1</td><td>‚ñÅ</td></tr><tr><td>test/negative/precision</td><td>‚ñÅ</td></tr><tr><td>test/negative/recall</td><td>‚ñÅ</td></tr><tr><td>test/neutral/f1</td><td>‚ñÅ</td></tr><tr><td>test/neutral/precision</td><td>‚ñÅ</td></tr><tr><td>test/neutral/recall</td><td>‚ñÅ</td></tr><tr><td>test/positive/f1</td><td>‚ñÅ</td></tr><tr><td>test/positive/precision</td><td>‚ñÅ</td></tr><tr><td>test/positive/recall</td><td>‚ñÅ</td></tr><tr><td>test/precision_macro</td><td>‚ñÅ</td></tr><tr><td>test/recall_macro</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints\\best_mic...</td></tr><tr><td>test/acc</td><td>0.85703</td></tr><tr><td>test/extremely negative/f1</td><td>0.86469</td></tr><tr><td>test/extremely negative/precision</td><td>0.82192</td></tr><tr><td>test/extremely negative/recall</td><td>0.91216</td></tr><tr><td>test/extremely positive/f1</td><td>0.89069</td></tr><tr><td>test/extremely positive/precision</td><td>0.86478</td></tr><tr><td>test/extremely positive/recall</td><td>0.9182</td></tr><tr><td>test/f1_macro</td><td>0.85991</td></tr><tr><td>test/negative/f1</td><td>0.84676</td></tr><tr><td>test/negative/precision</td><td>0.86633</td></tr><tr><td>test/negative/recall</td><td>0.82805</td></tr><tr><td>test/neutral/f1</td><td>0.85737</td></tr><tr><td>test/neutral/precision</td><td>0.83692</td></tr><tr><td>test/neutral/recall</td><td>0.87884</td></tr><tr><td>test/positive/f1</td><td>0.84007</td></tr><tr><td>test/positive/precision</td><td>0.88256</td></tr><tr><td>test/positive/recall</td><td>0.80148</td></tr><tr><td>test/precision_macro</td><td>0.8545</td></tr><tr><td>test/recall_macro</td><td>0.86774</td></tr><tr><td>test_f1_macro</td><td>0.85991</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_test</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/t6gj6ixw' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/t6gj6ixw</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250815_213702-t6gj6ixw\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- W&B: log test metrics, per-class scores, and confusion matrix ----\n",
    "test_run = wandb.init(project=PROJECT, name=f\"{BASE_RUN_NAME}_test\", resume=\"allow\", reinit=True)\n",
    "log_payload = {\n",
    "    \"test/acc\": acc,\n",
    "    \"test/precision_macro\": p,\n",
    "    \"test/recall_macro\": r,\n",
    "    \"test/f1_macro\": f1,\n",
    "}\n",
    "for cls_name in ORDER:\n",
    "    if cls_name in report:\n",
    "        log_payload[f\"test/{cls_name}/precision\"] = report[cls_name][\"precision\"]\n",
    "        log_payload[f\"test/{cls_name}/recall\"]    = report[cls_name][\"recall\"]\n",
    "        log_payload[f\"test/{cls_name}/f1\"]        = report[cls_name][\"f1-score\"]\n",
    "\n",
    "wandb.log(log_payload)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(ORDER))))\n",
    "wandb.log({\n",
    "    \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        y_true=all_labels,\n",
    "        preds=all_preds,\n",
    "        class_names=[ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "    )\n",
    "})\n",
    "test_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "test_run.summary[\"test_f1_macro\"] = f1\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905cac46a480ae6",
   "metadata": {},
   "source": [
    "# üöÄ Exercise 5 ‚Äì Training with HuggingFace Libraries\n",
    "\n",
    "In this part we re-implement our training pipeline using the **HuggingFace `transformers` library**.  \n",
    "Instead of writing a custom training loop, we use the built-in **`Trainer` API**, which makes fine-tuning large models simpler and more reproducible.  \n",
    "\n",
    "### üîë What‚Äôs different here\n",
    "- Use of **`TrainingArguments`** and **`Trainer`** for training, evaluation, and logging.  \n",
    "- Automatic support for **mixed precision (fp16/bf16)**, **gradient accumulation**, and **checkpointing**.  \n",
    "- Direct integration with **evaluation metrics (accuracy, F1, precision, recall)**.  \n",
    "- Cleaner and more modular workflow compared to Exercise 4.  \n",
    "\n",
    "### üéØ Goal\n",
    "Train the **mDeBERTa-v3-base** model on our **clean translated dataset**, with the best hyperparameters found earlier, and validate that the HuggingFace training pipeline gives consistent results with our custom Optuna-based loop.  \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea6b5e-dba2-4959-a8e8-c52ffed680a8",
   "metadata": {},
   "source": [
    "# üöÄ Ex.5 ‚Äì First HF Trainer Run (Best Hyperparameters from Before)\n",
    "\n",
    "In this section we move from our **custom training loop** to the **HuggingFace `Trainer` API**,  \n",
    "using the *best hyperparameters we found earlier with Optuna*:\n",
    "\n",
    "- Model: **mDeBERTa-v3-base**  \n",
    "- Batch size = 16  \n",
    "- LR = 3e-5  \n",
    "- Weight decay = 0.05  \n",
    "- Epochs = 12  \n",
    "- Unfreeze last **10 layers**  \n",
    "\n",
    "We also keep all the nice features:\n",
    "- ‚úÖ `wandb` logging  \n",
    "- ‚úÖ Early stopping via patience  \n",
    "- ‚úÖ Mixed precision (fp16/bf16)  \n",
    "- ‚úÖ Print-friendly callback (for live tracking)  \n",
    "\n",
    "üëâ The goal here is to reproduce the strong setup we had before, but now inside HuggingFace‚Äôs official training loop.  \n",
    "Later, we will **discuss why the results weren‚Äôt as good as expected** and what might explain the gap compared to our previous loop.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bc0ce-65f3-4080-b756-583ad05df229",
   "metadata": {},
   "source": [
    "# Training EX5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e5666025bd90a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:46:49.843705Z",
     "start_time": "2025-08-16T08:46:43.689517Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Ex.5: HF Trainer version with W&B + prints (Windows/RTX 4090 friendly) ---\n",
    "\n",
    "import os, math, random, time, json\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import wandb\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, Trainer, TrainingArguments\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# ---- Fast CUDA defaults (4090) ----\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "# ---- W&B defaults ----\n",
    "os.environ.setdefault(\"WANDB_MODE\", \"online\")\n",
    "os.environ.setdefault(\"WANDB_PROJECT\", \"adv-dl-p2\")\n",
    "os.environ.setdefault(\"WANDB_NOTEBOOK_NAME\", \"ex5_trainer.ipynb\")\n",
    "WANDB_PROJECT = os.environ[\"WANDB_PROJECT\"]\n",
    "\n",
    "# ---- Constants ----\n",
    "MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "BASE_RUN_NAME = MODEL_NAME.replace(\"/\", \"__\") + \"_ex5_trainer\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "LR = 3e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.06\n",
    "GRAD_ACCUM = 1\n",
    "NUM_WORKERS = 0  # Windows-safe; raise to 2 if stable\n",
    "\n",
    "# ---- Label mapping (5-way) ----\n",
    "CANON = {\n",
    "    \"extremely negative\": \"extremely negative\",\n",
    "    \"negative\": \"negative\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"positive\": \"positive\",\n",
    "    \"extremely positive\": \"extremely positive\",\n",
    "}\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return CANON.get(s, s)\n",
    "\n",
    "# ---- Prep dataframes from df_train/df_test already in memory ----\n",
    "assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns\n",
    "assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"]).copy()\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"labels\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\", \"labels\", \"label_name\"]]\n",
    "\n",
    "dftrain_ = prep_df(df_train)\n",
    "dftest_  = prep_df(df_test)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    dftrain_, test_size=0.1, stratify=dftrain_[\"labels\"], random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1cefc11b66d72e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T08:46:49.846194800Z",
     "start_time": "2025-08-15T19:30:05.563319Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37039/37039 [00:01<00:00, 28196.86 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4116/4116 [00:00<00:00, 37462.23 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3798/3798 [00:00<00:00, 36229.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=False, truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "def keep_only(ds, cols):\n",
    "    # Use select_columns if available, else remove the others\n",
    "    if hasattr(ds, \"select_columns\"):\n",
    "        return ds.select_columns(cols)\n",
    "    drop = [c for c in ds.column_names if c not in cols]\n",
    "    return ds.remove_columns(drop)\n",
    "\n",
    "# Build datasets (avoid adding __index_level_0__ with preserve_index=False)\n",
    "train_ds = Dataset.from_pandas(train_df,  preserve_index=False)\n",
    "val_ds   = Dataset.from_pandas(val_df,    preserve_index=False)\n",
    "test_ds  = Dataset.from_pandas(dftest_,   preserve_index=False)\n",
    "\n",
    "# Keep just text + labels before tokenization\n",
    "cols_keep = [\"text\", \"labels\"]\n",
    "train_ds = keep_only(train_ds, cols_keep)\n",
    "val_ds   = keep_only(val_ds,   cols_keep)\n",
    "test_ds  = keep_only(test_ds,  cols_keep)\n",
    "\n",
    "# Tokenize\n",
    "train_ds = train_ds.map(tok_fn, batched=True)\n",
    "val_ds   = val_ds.map(tok_fn,   batched=True)\n",
    "test_ds  = test_ds.map(tok_fn,  batched=True)\n",
    "\n",
    "# Remove raw text after tokenization, keep labels + token IDs\n",
    "train_ds = train_ds.remove_columns([\"text\"])\n",
    "val_ds   = val_ds.remove_columns([\"text\"])\n",
    "test_ds  = test_ds.remove_columns([\"text\"])\n",
    "\n",
    "# Ensure final columns are exactly what Trainer expects\n",
    "final_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds = keep_only(train_ds, final_cols)\n",
    "val_ds   = keep_only(val_ds,   final_cols)\n",
    "test_ds  = keep_only(test_ds,  final_cols)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "886fe6afe78f6822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T19:30:45.262333Z",
     "start_time": "2025-08-15T19:30:43.325006Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "UNFREEZE_LAST_K = 10  # 1..12 are sensible for DeBERTa-v3-base\n",
    "\n",
    "def build_model(unfreeze_last_k=UNFREEZE_LAST_K):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(ORDER),\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else None),\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    base = getattr(model, \"deberta\", None) or getattr(model, \"roberta\", None) or getattr(model, \"bert\", None)\n",
    "    if base is not None and hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "        # freeze all\n",
    "        for p in base.parameters(): p.requires_grad = False\n",
    "        # unfreeze last k transformer blocks\n",
    "        for layer in base.encoder.layer[-int(unfreeze_last_k):]:\n",
    "            for p in layer.parameters(): p.requires_grad = True\n",
    "    # classifier head always trainable\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = build_model()\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%) ; unfreeze_last_k={UNFREEZE_LAST_K}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "999ff4f314c7be01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-15T19:52:19.520464Z",
     "start_time": "2025-08-15T19:32:32.334416Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 4.20kB [00:00, 5.53MB/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250815_223234-hbl8jumj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/hbl8jumj' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/hbl8jumj' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/hbl8jumj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_21192\\3450032475.py:102: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trainer] Starting fine-tune ‚Üí output_dir=hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer\n",
      "[Run] epochs=12 bs=16 lr=3.00e-05 wd=5.0e-02 warmup_ratio=0.06 grad_accum=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='27780' max='27780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [27780/27780 19:42, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.213200</td>\n",
       "      <td>1.232292</td>\n",
       "      <td>0.482750</td>\n",
       "      <td>0.500943</td>\n",
       "      <td>0.507310</td>\n",
       "      <td>0.496841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.080600</td>\n",
       "      <td>1.058841</td>\n",
       "      <td>0.561467</td>\n",
       "      <td>0.584520</td>\n",
       "      <td>0.592781</td>\n",
       "      <td>0.577479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.962200</td>\n",
       "      <td>0.983838</td>\n",
       "      <td>0.596210</td>\n",
       "      <td>0.615364</td>\n",
       "      <td>0.619935</td>\n",
       "      <td>0.611747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.895000</td>\n",
       "      <td>0.967778</td>\n",
       "      <td>0.605199</td>\n",
       "      <td>0.623989</td>\n",
       "      <td>0.630168</td>\n",
       "      <td>0.619882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.954000</td>\n",
       "      <td>0.949300</td>\n",
       "      <td>0.618319</td>\n",
       "      <td>0.632191</td>\n",
       "      <td>0.640670</td>\n",
       "      <td>0.632150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.939800</td>\n",
       "      <td>0.952293</td>\n",
       "      <td>0.613703</td>\n",
       "      <td>0.634493</td>\n",
       "      <td>0.636548</td>\n",
       "      <td>0.628234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.953200</td>\n",
       "      <td>0.948852</td>\n",
       "      <td>0.611516</td>\n",
       "      <td>0.631063</td>\n",
       "      <td>0.634211</td>\n",
       "      <td>0.626338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.999500</td>\n",
       "      <td>0.942999</td>\n",
       "      <td>0.613703</td>\n",
       "      <td>0.630220</td>\n",
       "      <td>0.638575</td>\n",
       "      <td>0.628589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.902200</td>\n",
       "      <td>0.946464</td>\n",
       "      <td>0.616861</td>\n",
       "      <td>0.636011</td>\n",
       "      <td>0.638370</td>\n",
       "      <td>0.630863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.953400</td>\n",
       "      <td>0.946236</td>\n",
       "      <td>0.615889</td>\n",
       "      <td>0.633949</td>\n",
       "      <td>0.638359</td>\n",
       "      <td>0.630118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.990800</td>\n",
       "      <td>0.946013</td>\n",
       "      <td>0.617104</td>\n",
       "      <td>0.634913</td>\n",
       "      <td>0.640290</td>\n",
       "      <td>0.631097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.950500</td>\n",
       "      <td>0.945905</td>\n",
       "      <td>0.615403</td>\n",
       "      <td>0.634264</td>\n",
       "      <td>0.637055</td>\n",
       "      <td>0.629484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e0 b20/2315] loss=1.6226 lr=3.42e-07\n",
      "[e0 b40/2315] loss=1.6173 lr=7.02e-07\n",
      "[e0 b60/2315] loss=1.6075 lr=1.06e-06\n",
      "[e0 b80/2315] loss=1.6150 lr=1.42e-06\n",
      "[e0 b100/2315] loss=1.6179 lr=1.78e-06\n",
      "[e0 b120/2315] loss=1.6180 lr=2.14e-06\n",
      "[e0 b140/2315] loss=1.6063 lr=2.50e-06\n",
      "[e0 b160/2315] loss=1.6032 lr=2.86e-06\n",
      "[e0 b180/2315] loss=1.6073 lr=3.22e-06\n",
      "[e0 b200/2315] loss=1.6209 lr=3.58e-06\n",
      "[e0 b220/2315] loss=1.6040 lr=3.94e-06\n",
      "[e0 b240/2315] loss=1.6119 lr=4.30e-06\n",
      "[e0 b260/2315] loss=1.6084 lr=4.66e-06\n",
      "[e0 b280/2315] loss=1.5997 lr=5.02e-06\n",
      "[e0 b300/2315] loss=1.6078 lr=5.38e-06\n",
      "[e0 b320/2315] loss=1.6032 lr=5.74e-06\n",
      "[e0 b340/2315] loss=1.6153 lr=6.10e-06\n",
      "[e0 b360/2315] loss=1.6092 lr=6.46e-06\n",
      "[e0 b380/2315] loss=1.6029 lr=6.82e-06\n",
      "[e0 b400/2315] loss=1.6028 lr=7.18e-06\n",
      "[e0 b420/2315] loss=1.5912 lr=7.54e-06\n",
      "[e0 b440/2315] loss=1.5979 lr=7.90e-06\n",
      "[e0 b460/2315] loss=1.6232 lr=8.26e-06\n",
      "[e0 b480/2315] loss=1.5998 lr=8.62e-06\n",
      "[e0 b500/2315] loss=1.6050 lr=8.98e-06\n",
      "[e0 b520/2315] loss=1.5995 lr=9.34e-06\n",
      "[e0 b540/2315] loss=1.5932 lr=9.70e-06\n",
      "[e0 b560/2315] loss=1.5978 lr=1.01e-05\n",
      "[e0 b580/2315] loss=1.5872 lr=1.04e-05\n",
      "[e0 b600/2315] loss=1.5772 lr=1.08e-05\n",
      "[e0 b620/2315] loss=1.5760 lr=1.11e-05\n",
      "[e0 b640/2315] loss=1.5607 lr=1.15e-05\n",
      "[e0 b660/2315] loss=1.5733 lr=1.19e-05\n",
      "[e0 b680/2315] loss=1.5722 lr=1.22e-05\n",
      "[e0 b700/2315] loss=1.5712 lr=1.26e-05\n",
      "[e0 b720/2315] loss=1.5660 lr=1.29e-05\n",
      "[e0 b740/2315] loss=1.5852 lr=1.33e-05\n",
      "[e0 b760/2315] loss=1.5344 lr=1.37e-05\n",
      "[e0 b780/2315] loss=1.5726 lr=1.40e-05\n",
      "[e0 b800/2315] loss=1.5457 lr=1.44e-05\n",
      "[e0 b820/2315] loss=1.5380 lr=1.47e-05\n",
      "[e0 b840/2315] loss=1.5338 lr=1.51e-05\n",
      "[e0 b860/2315] loss=1.5359 lr=1.55e-05\n",
      "[e0 b880/2315] loss=1.5488 lr=1.58e-05\n",
      "[e0 b900/2315] loss=1.5496 lr=1.62e-05\n",
      "[e0 b920/2315] loss=1.5357 lr=1.65e-05\n",
      "[e0 b940/2315] loss=1.5509 lr=1.69e-05\n",
      "[e0 b960/2315] loss=1.5278 lr=1.73e-05\n",
      "[e0 b980/2315] loss=1.5400 lr=1.76e-05\n",
      "[e0 b1000/2315] loss=1.5564 lr=1.80e-05\n",
      "[e0 b1020/2315] loss=1.5378 lr=1.83e-05\n",
      "[e0 b1040/2315] loss=1.5081 lr=1.87e-05\n",
      "[e0 b1060/2315] loss=1.5294 lr=1.91e-05\n",
      "[e0 b1080/2315] loss=1.5374 lr=1.94e-05\n",
      "[e0 b1100/2315] loss=1.5483 lr=1.98e-05\n",
      "[e0 b1120/2315] loss=1.5301 lr=2.01e-05\n",
      "[e0 b1140/2315] loss=1.5493 lr=2.05e-05\n",
      "[e0 b1160/2315] loss=1.5501 lr=2.09e-05\n",
      "[e0 b1180/2315] loss=1.5409 lr=2.12e-05\n",
      "[e0 b1200/2315] loss=1.5306 lr=2.16e-05\n",
      "[e0 b1220/2315] loss=1.5110 lr=2.19e-05\n",
      "[e0 b1240/2315] loss=1.5034 lr=2.23e-05\n",
      "[e0 b1260/2315] loss=1.5044 lr=2.27e-05\n",
      "[e0 b1280/2315] loss=1.5025 lr=2.30e-05\n",
      "[e0 b1300/2315] loss=1.5294 lr=2.34e-05\n",
      "[e0 b1320/2315] loss=1.5439 lr=2.37e-05\n",
      "[e0 b1340/2315] loss=1.4953 lr=2.41e-05\n",
      "[e0 b1360/2315] loss=1.4854 lr=2.45e-05\n",
      "[e0 b1380/2315] loss=1.5200 lr=2.48e-05\n",
      "[e0 b1400/2315] loss=1.4870 lr=2.52e-05\n",
      "[e0 b1420/2315] loss=1.4884 lr=2.55e-05\n",
      "[e0 b1440/2315] loss=1.4843 lr=2.59e-05\n",
      "[e0 b1460/2315] loss=1.5154 lr=2.63e-05\n",
      "[e0 b1480/2315] loss=1.5222 lr=2.66e-05\n",
      "[e0 b1500/2315] loss=1.4711 lr=2.70e-05\n",
      "[e0 b1520/2315] loss=1.5352 lr=2.73e-05\n",
      "[e0 b1540/2315] loss=1.4932 lr=2.77e-05\n",
      "[e0 b1560/2315] loss=1.4510 lr=2.81e-05\n",
      "[e0 b1580/2315] loss=1.4951 lr=2.84e-05\n",
      "[e0 b1600/2315] loss=1.4636 lr=2.88e-05\n",
      "[e0 b1620/2315] loss=1.4558 lr=2.91e-05\n",
      "[e0 b1640/2315] loss=1.4987 lr=2.95e-05\n",
      "[e0 b1660/2315] loss=1.4552 lr=2.99e-05\n",
      "[e0 b1680/2315] loss=1.4494 lr=3.00e-05\n",
      "[e0 b1700/2315] loss=1.4653 lr=3.00e-05\n",
      "[e0 b1720/2315] loss=1.4191 lr=2.99e-05\n",
      "[e0 b1740/2315] loss=1.3942 lr=2.99e-05\n",
      "[e0 b1760/2315] loss=1.4777 lr=2.99e-05\n",
      "[e0 b1780/2315] loss=1.4472 lr=2.99e-05\n",
      "[e0 b1800/2315] loss=1.3785 lr=2.98e-05\n",
      "[e0 b1820/2315] loss=1.4144 lr=2.98e-05\n",
      "[e0 b1840/2315] loss=1.3683 lr=2.98e-05\n",
      "[e0 b1860/2315] loss=1.3950 lr=2.98e-05\n",
      "[e0 b1880/2315] loss=1.3001 lr=2.98e-05\n",
      "[e0 b1900/2315] loss=1.2777 lr=2.97e-05\n",
      "[e0 b1920/2315] loss=1.3870 lr=2.97e-05\n",
      "[e0 b1940/2315] loss=1.3679 lr=2.97e-05\n",
      "[e0 b1960/2315] loss=1.3076 lr=2.97e-05\n",
      "[e0 b1980/2315] loss=1.3254 lr=2.96e-05\n",
      "[e0 b2000/2315] loss=1.3451 lr=2.96e-05\n",
      "[e0 b2020/2315] loss=1.2596 lr=2.96e-05\n",
      "[e0 b2040/2315] loss=1.3086 lr=2.96e-05\n",
      "[e0 b2060/2315] loss=1.3041 lr=2.95e-05\n",
      "[e0 b2080/2315] loss=1.2333 lr=2.95e-05\n",
      "[e0 b2100/2315] loss=1.3424 lr=2.95e-05\n",
      "[e0 b2120/2315] loss=1.3069 lr=2.95e-05\n",
      "[e0 b2140/2315] loss=1.3056 lr=2.95e-05\n",
      "[e0 b2160/2315] loss=1.3141 lr=2.94e-05\n",
      "[e0 b2180/2315] loss=1.2639 lr=2.94e-05\n",
      "[e0 b2200/2315] loss=1.2584 lr=2.94e-05\n",
      "[e0 b2220/2315] loss=1.3349 lr=2.94e-05\n",
      "[e0 b2240/2315] loss=1.2489 lr=2.93e-05\n",
      "[e0 b2260/2315] loss=1.2215 lr=2.93e-05\n",
      "[e0 b2280/2315] loss=1.2287 lr=2.93e-05\n",
      "[e0 b2300/2315] loss=1.2132 lr=2.93e-05\n",
      "[val @ epoch 1] acc=0.4828 f1=0.4968 p=0.5009 r=0.5073\n",
      "[val @ epoch 2] acc=0.5615 f1=0.5775 p=0.5845 r=0.5928\n",
      "[val @ epoch 3] acc=0.5962 f1=0.6117 p=0.6154 r=0.6199\n",
      "[val @ epoch 4] acc=0.6052 f1=0.6199 p=0.6240 r=0.6302\n",
      "[e4 b20/2315] loss=0.8973 lr=2.13e-05\n",
      "[e4 b40/2315] loss=0.8554 lr=2.12e-05\n",
      "[e4 b60/2315] loss=1.0102 lr=2.12e-05\n",
      "[e4 b80/2315] loss=0.8720 lr=2.12e-05\n",
      "[e4 b100/2315] loss=0.8621 lr=2.12e-05\n",
      "[e4 b120/2315] loss=0.9924 lr=2.11e-05\n",
      "[e4 b140/2315] loss=1.0173 lr=2.11e-05\n",
      "[e4 b160/2315] loss=1.0694 lr=2.11e-05\n",
      "[e4 b180/2315] loss=1.0150 lr=2.11e-05\n",
      "[e4 b200/2315] loss=0.9821 lr=2.10e-05\n",
      "[e4 b220/2315] loss=0.9794 lr=2.10e-05\n",
      "[e4 b240/2315] loss=1.0295 lr=2.10e-05\n",
      "[e4 b260/2315] loss=0.9194 lr=2.10e-05\n",
      "[e4 b280/2315] loss=0.9797 lr=2.10e-05\n",
      "[e4 b300/2315] loss=0.9390 lr=2.09e-05\n",
      "[e4 b320/2315] loss=0.9393 lr=2.09e-05\n",
      "[e4 b340/2315] loss=0.9223 lr=2.09e-05\n",
      "[e4 b360/2315] loss=0.9258 lr=2.09e-05\n",
      "[e4 b380/2315] loss=0.9445 lr=2.08e-05\n",
      "[e4 b400/2315] loss=0.9336 lr=2.08e-05\n",
      "[e4 b420/2315] loss=0.9274 lr=2.08e-05\n",
      "[e4 b440/2315] loss=0.9727 lr=2.08e-05\n",
      "[e4 b460/2315] loss=0.9687 lr=2.07e-05\n",
      "[e4 b480/2315] loss=0.9350 lr=2.07e-05\n",
      "[e4 b500/2315] loss=0.9609 lr=2.07e-05\n",
      "[e4 b520/2315] loss=0.9780 lr=2.07e-05\n",
      "[e4 b540/2315] loss=0.9846 lr=2.07e-05\n",
      "[e4 b560/2315] loss=0.9423 lr=2.06e-05\n",
      "[e4 b580/2315] loss=0.9490 lr=2.06e-05\n",
      "[e4 b600/2315] loss=0.8946 lr=2.06e-05\n",
      "[e4 b620/2315] loss=0.9918 lr=2.06e-05\n",
      "[e4 b640/2315] loss=0.9787 lr=2.05e-05\n",
      "[e4 b660/2315] loss=0.8821 lr=2.05e-05\n",
      "[e4 b680/2315] loss=0.9316 lr=2.05e-05\n",
      "[e4 b700/2315] loss=0.9481 lr=2.05e-05\n",
      "[e4 b720/2315] loss=0.9791 lr=2.05e-05\n",
      "[e4 b740/2315] loss=0.8731 lr=2.04e-05\n",
      "[e4 b760/2315] loss=0.9048 lr=2.04e-05\n",
      "[e4 b780/2315] loss=0.9756 lr=2.04e-05\n",
      "[e4 b800/2315] loss=0.8817 lr=2.04e-05\n",
      "[e4 b820/2315] loss=0.8910 lr=2.03e-05\n",
      "[e4 b840/2315] loss=0.9894 lr=2.03e-05\n",
      "[e4 b860/2315] loss=1.0006 lr=2.03e-05\n",
      "[e4 b880/2315] loss=0.9576 lr=2.03e-05\n",
      "[e4 b900/2315] loss=0.9294 lr=2.02e-05\n",
      "[e4 b920/2315] loss=0.9076 lr=2.02e-05\n",
      "[e4 b940/2315] loss=0.9673 lr=2.02e-05\n",
      "[e4 b960/2315] loss=1.0346 lr=2.02e-05\n",
      "[e4 b980/2315] loss=0.9235 lr=2.02e-05\n",
      "[e4 b1000/2315] loss=0.8861 lr=2.01e-05\n",
      "[e4 b1020/2315] loss=0.9194 lr=2.01e-05\n",
      "[e4 b1040/2315] loss=0.9087 lr=2.01e-05\n",
      "[e4 b1060/2315] loss=0.9513 lr=2.01e-05\n",
      "[e4 b1080/2315] loss=0.9324 lr=2.00e-05\n",
      "[e4 b1100/2315] loss=0.9160 lr=2.00e-05\n",
      "[e4 b1120/2315] loss=0.8666 lr=2.00e-05\n",
      "[e4 b1140/2315] loss=0.9665 lr=2.00e-05\n",
      "[e4 b1160/2315] loss=0.9184 lr=1.99e-05\n",
      "[e4 b1180/2315] loss=0.9661 lr=1.99e-05\n",
      "[e4 b1200/2315] loss=0.9127 lr=1.99e-05\n",
      "[e4 b1220/2315] loss=0.9819 lr=1.99e-05\n",
      "[e4 b1240/2315] loss=0.8698 lr=1.99e-05\n",
      "[e4 b1260/2315] loss=0.8796 lr=1.98e-05\n",
      "[e4 b1280/2315] loss=0.8989 lr=1.98e-05\n",
      "[e4 b1300/2315] loss=1.0140 lr=1.98e-05\n",
      "[e4 b1320/2315] loss=0.9406 lr=1.98e-05\n",
      "[e4 b1340/2315] loss=0.8378 lr=1.97e-05\n",
      "[e4 b1360/2315] loss=0.8700 lr=1.97e-05\n",
      "[e4 b1380/2315] loss=0.9729 lr=1.97e-05\n",
      "[e4 b1400/2315] loss=1.0009 lr=1.97e-05\n",
      "[e4 b1420/2315] loss=0.8697 lr=1.96e-05\n",
      "[e4 b1440/2315] loss=1.0029 lr=1.96e-05\n",
      "[e4 b1460/2315] loss=0.8920 lr=1.96e-05\n",
      "[e4 b1480/2315] loss=0.8949 lr=1.96e-05\n",
      "[e4 b1500/2315] loss=0.9807 lr=1.96e-05\n",
      "[e4 b1520/2315] loss=0.9377 lr=1.95e-05\n",
      "[e4 b1540/2315] loss=0.9347 lr=1.95e-05\n",
      "[e4 b1560/2315] loss=0.9520 lr=1.95e-05\n",
      "[e4 b1580/2315] loss=0.9301 lr=1.95e-05\n",
      "[e4 b1600/2315] loss=0.9702 lr=1.94e-05\n",
      "[e4 b1620/2315] loss=0.8706 lr=1.94e-05\n",
      "[e4 b1640/2315] loss=0.9881 lr=1.94e-05\n",
      "[e4 b1660/2315] loss=0.9614 lr=1.94e-05\n",
      "[e4 b1680/2315] loss=0.9474 lr=1.93e-05\n",
      "[e4 b1700/2315] loss=0.9155 lr=1.93e-05\n",
      "[e4 b1720/2315] loss=0.9552 lr=1.93e-05\n",
      "[e4 b1740/2315] loss=0.9372 lr=1.93e-05\n",
      "[e4 b1760/2315] loss=0.9778 lr=1.93e-05\n",
      "[e4 b1780/2315] loss=0.9932 lr=1.92e-05\n",
      "[e4 b1800/2315] loss=0.9669 lr=1.92e-05\n",
      "[e4 b1820/2315] loss=0.9544 lr=1.92e-05\n",
      "[e4 b1840/2315] loss=0.9653 lr=1.92e-05\n",
      "[e4 b1860/2315] loss=0.9262 lr=1.91e-05\n",
      "[e4 b1880/2315] loss=0.9686 lr=1.91e-05\n",
      "[e4 b1900/2315] loss=0.8965 lr=1.91e-05\n",
      "[e4 b1920/2315] loss=0.8837 lr=1.91e-05\n",
      "[e4 b1940/2315] loss=0.9166 lr=1.90e-05\n",
      "[e4 b1960/2315] loss=0.9247 lr=1.90e-05\n",
      "[e4 b1980/2315] loss=0.9259 lr=1.90e-05\n",
      "[e4 b2000/2315] loss=0.9989 lr=1.90e-05\n",
      "[e4 b2020/2315] loss=0.9575 lr=1.90e-05\n",
      "[e4 b2040/2315] loss=0.9625 lr=1.89e-05\n",
      "[e4 b2060/2315] loss=0.9091 lr=1.89e-05\n",
      "[e4 b2080/2315] loss=0.9299 lr=1.89e-05\n",
      "[e4 b2100/2315] loss=0.9620 lr=1.89e-05\n",
      "[e4 b2120/2315] loss=0.9382 lr=1.88e-05\n",
      "[e4 b2140/2315] loss=0.9639 lr=1.88e-05\n",
      "[e4 b2160/2315] loss=0.9201 lr=1.88e-05\n",
      "[e4 b2180/2315] loss=0.9555 lr=1.88e-05\n",
      "[e4 b2200/2315] loss=0.9856 lr=1.88e-05\n",
      "[e4 b2220/2315] loss=0.9232 lr=1.87e-05\n",
      "[e4 b2240/2315] loss=1.0330 lr=1.87e-05\n",
      "[e4 b2260/2315] loss=1.0120 lr=1.87e-05\n",
      "[e4 b2280/2315] loss=0.9335 lr=1.87e-05\n",
      "[e4 b2300/2315] loss=0.9540 lr=1.86e-05\n",
      "[val @ epoch 5] acc=0.6183 f1=0.6322 p=0.6322 r=0.6407\n",
      "[val @ epoch 6] acc=0.6137 f1=0.6282 p=0.6345 r=0.6365\n",
      "[val @ epoch 7] acc=0.6115 f1=0.6263 p=0.6311 r=0.6342\n",
      "[val @ epoch 8] acc=0.6137 f1=0.6286 p=0.6302 r=0.6386\n",
      "[e8 b20/2315] loss=0.9503 lr=1.06e-05\n",
      "[e8 b40/2315] loss=0.8866 lr=1.06e-05\n",
      "[e8 b60/2315] loss=0.9405 lr=1.06e-05\n",
      "[e8 b80/2315] loss=0.9701 lr=1.05e-05\n",
      "[e8 b100/2315] loss=0.9922 lr=1.05e-05\n",
      "[e8 b120/2315] loss=0.9102 lr=1.05e-05\n",
      "[e8 b140/2315] loss=0.9567 lr=1.05e-05\n",
      "[e8 b160/2315] loss=0.8818 lr=1.05e-05\n",
      "[e8 b180/2315] loss=0.8506 lr=1.04e-05\n",
      "[e8 b200/2315] loss=0.8594 lr=1.04e-05\n",
      "[e8 b220/2315] loss=0.9790 lr=1.04e-05\n",
      "[e8 b240/2315] loss=0.9181 lr=1.04e-05\n",
      "[e8 b260/2315] loss=0.9417 lr=1.03e-05\n",
      "[e8 b280/2315] loss=0.8850 lr=1.03e-05\n",
      "[e8 b300/2315] loss=0.8916 lr=1.03e-05\n",
      "[e8 b320/2315] loss=0.9481 lr=1.03e-05\n",
      "[e8 b340/2315] loss=0.8776 lr=1.02e-05\n",
      "[e8 b360/2315] loss=0.9273 lr=1.02e-05\n",
      "[e8 b380/2315] loss=0.8911 lr=1.02e-05\n",
      "[e8 b400/2315] loss=0.9462 lr=1.02e-05\n",
      "[e8 b420/2315] loss=0.9108 lr=1.02e-05\n",
      "[e8 b440/2315] loss=0.9469 lr=1.01e-05\n",
      "[e8 b460/2315] loss=0.9093 lr=1.01e-05\n",
      "[e8 b480/2315] loss=0.9365 lr=1.01e-05\n",
      "[e8 b500/2315] loss=0.8908 lr=1.01e-05\n",
      "[e8 b520/2315] loss=0.9365 lr=1.00e-05\n",
      "[e8 b540/2315] loss=0.9012 lr=1.00e-05\n",
      "[e8 b560/2315] loss=0.8885 lr=1.00e-05\n",
      "[e8 b580/2315] loss=0.8561 lr=9.97e-06\n",
      "[e8 b600/2315] loss=0.8840 lr=9.95e-06\n",
      "[e8 b620/2315] loss=0.9619 lr=9.93e-06\n",
      "[e8 b640/2315] loss=0.9729 lr=9.90e-06\n",
      "[e8 b660/2315] loss=0.9301 lr=9.88e-06\n",
      "[e8 b680/2315] loss=0.9844 lr=9.86e-06\n",
      "[e8 b700/2315] loss=0.8641 lr=9.84e-06\n",
      "[e8 b720/2315] loss=0.9580 lr=9.81e-06\n",
      "[e8 b740/2315] loss=0.9277 lr=9.79e-06\n",
      "[e8 b760/2315] loss=0.8776 lr=9.77e-06\n",
      "[e8 b780/2315] loss=0.9657 lr=9.74e-06\n",
      "[e8 b800/2315] loss=0.8898 lr=9.72e-06\n",
      "[e8 b820/2315] loss=0.9551 lr=9.70e-06\n",
      "[e8 b840/2315] loss=0.9468 lr=9.67e-06\n",
      "[e8 b860/2315] loss=0.9515 lr=9.65e-06\n",
      "[e8 b880/2315] loss=0.9402 lr=9.63e-06\n",
      "[e8 b900/2315] loss=0.9909 lr=9.61e-06\n",
      "[e8 b920/2315] loss=0.8671 lr=9.58e-06\n",
      "[e8 b940/2315] loss=0.9262 lr=9.56e-06\n",
      "[e8 b960/2315] loss=0.9085 lr=9.54e-06\n",
      "[e8 b980/2315] loss=0.8671 lr=9.51e-06\n",
      "[e8 b1000/2315] loss=0.9084 lr=9.49e-06\n",
      "[e8 b1020/2315] loss=0.9663 lr=9.47e-06\n",
      "[e8 b1040/2315] loss=0.9230 lr=9.44e-06\n",
      "[e8 b1060/2315] loss=0.9735 lr=9.42e-06\n",
      "[e8 b1080/2315] loss=0.9764 lr=9.40e-06\n",
      "[e8 b1100/2315] loss=0.9518 lr=9.38e-06\n",
      "[e8 b1120/2315] loss=0.9536 lr=9.35e-06\n",
      "[e8 b1140/2315] loss=0.8800 lr=9.33e-06\n",
      "[e8 b1160/2315] loss=1.0289 lr=9.31e-06\n",
      "[e8 b1180/2315] loss=0.9202 lr=9.28e-06\n",
      "[e8 b1200/2315] loss=0.9223 lr=9.26e-06\n",
      "[e8 b1220/2315] loss=1.0170 lr=9.24e-06\n",
      "[e8 b1240/2315] loss=0.8639 lr=9.21e-06\n",
      "[e8 b1260/2315] loss=0.8935 lr=9.19e-06\n",
      "[e8 b1280/2315] loss=0.8683 lr=9.17e-06\n",
      "[e8 b1300/2315] loss=0.8787 lr=9.15e-06\n",
      "[e8 b1320/2315] loss=0.9019 lr=9.12e-06\n",
      "[e8 b1340/2315] loss=0.9216 lr=9.10e-06\n",
      "[e8 b1360/2315] loss=0.9036 lr=9.08e-06\n",
      "[e8 b1380/2315] loss=0.9197 lr=9.05e-06\n",
      "[e8 b1400/2315] loss=0.9917 lr=9.03e-06\n",
      "[e8 b1420/2315] loss=0.9502 lr=9.01e-06\n",
      "[e8 b1440/2315] loss=0.8800 lr=8.99e-06\n",
      "[e8 b1460/2315] loss=0.9156 lr=8.96e-06\n",
      "[e8 b1480/2315] loss=0.8860 lr=8.94e-06\n",
      "[e8 b1500/2315] loss=0.9635 lr=8.92e-06\n",
      "[e8 b1520/2315] loss=0.9475 lr=8.89e-06\n",
      "[e8 b1540/2315] loss=0.9574 lr=8.87e-06\n",
      "[e8 b1560/2315] loss=0.9371 lr=8.85e-06\n",
      "[e8 b1580/2315] loss=0.9383 lr=8.82e-06\n",
      "[e8 b1600/2315] loss=0.9291 lr=8.80e-06\n",
      "[e8 b1620/2315] loss=0.9030 lr=8.78e-06\n",
      "[e8 b1640/2315] loss=0.9036 lr=8.76e-06\n",
      "[e8 b1660/2315] loss=0.9404 lr=8.73e-06\n",
      "[e8 b1680/2315] loss=0.9809 lr=8.71e-06\n",
      "[e8 b1700/2315] loss=0.9315 lr=8.69e-06\n",
      "[e8 b1720/2315] loss=0.9109 lr=8.66e-06\n",
      "[e8 b1740/2315] loss=0.9105 lr=8.64e-06\n",
      "[e8 b1760/2315] loss=0.9264 lr=8.62e-06\n",
      "[e8 b1780/2315] loss=0.9005 lr=8.59e-06\n",
      "[e8 b1800/2315] loss=0.9089 lr=8.57e-06\n",
      "[e8 b1820/2315] loss=1.0100 lr=8.55e-06\n",
      "[e8 b1840/2315] loss=0.8987 lr=8.53e-06\n",
      "[e8 b1860/2315] loss=0.9231 lr=8.50e-06\n",
      "[e8 b1880/2315] loss=0.9430 lr=8.48e-06\n",
      "[e8 b1900/2315] loss=0.9595 lr=8.46e-06\n",
      "[e8 b1920/2315] loss=0.9107 lr=8.43e-06\n",
      "[e8 b1940/2315] loss=1.0202 lr=8.41e-06\n",
      "[e8 b1960/2315] loss=0.9330 lr=8.39e-06\n",
      "[e8 b1980/2315] loss=0.9947 lr=8.36e-06\n",
      "[e8 b2000/2315] loss=0.9356 lr=8.34e-06\n",
      "[e8 b2020/2315] loss=0.9266 lr=8.32e-06\n",
      "[e8 b2040/2315] loss=0.9442 lr=8.30e-06\n",
      "[e8 b2060/2315] loss=0.8838 lr=8.27e-06\n",
      "[e8 b2080/2315] loss=0.8951 lr=8.25e-06\n",
      "[e8 b2100/2315] loss=0.8630 lr=8.23e-06\n",
      "[e8 b2120/2315] loss=0.9422 lr=8.20e-06\n",
      "[e8 b2140/2315] loss=0.8959 lr=8.18e-06\n",
      "[e8 b2160/2315] loss=0.9309 lr=8.16e-06\n",
      "[e8 b2180/2315] loss=0.9532 lr=8.14e-06\n",
      "[e8 b2200/2315] loss=0.9233 lr=8.11e-06\n",
      "[e8 b2220/2315] loss=0.9345 lr=8.09e-06\n",
      "[e8 b2240/2315] loss=0.8555 lr=8.07e-06\n",
      "[e8 b2260/2315] loss=0.8898 lr=8.04e-06\n",
      "[e8 b2280/2315] loss=1.0007 lr=8.02e-06\n",
      "[e8 b2300/2315] loss=0.9022 lr=8.00e-06\n",
      "[val @ epoch 9] acc=0.6169 f1=0.6309 p=0.6360 r=0.6384\n",
      "[val @ epoch 10] acc=0.6159 f1=0.6301 p=0.6339 r=0.6384\n",
      "[val @ epoch 11] acc=0.6171 f1=0.6311 p=0.6349 r=0.6403\n",
      "[val @ epoch 12] acc=0.6154 f1=0.6295 p=0.6343 r=0.6371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainOutput(global_step=27780, training_loss=1.0007773465952314, metrics={'train_runtime': 1182.5271, 'train_samples_per_second': 375.863, 'train_steps_per_second': 23.492, 'total_flos': 1.887617915188579e+16, 'train_loss': 1.0007773465952314, 'epoch': 12.0})\n",
      "Best checkpoint dir: hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer\\checkpoint-11575\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÇ‚ñá‚ñà‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÜ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñÜ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_dir</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.6154</td></tr><tr><td>eval/f1</td><td>0.62948</td></tr><tr><td>eval/loss</td><td>0.94591</td></tr><tr><td>eval/precision</td><td>0.63426</td></tr><tr><td>eval/recall</td><td>0.63706</td></tr><tr><td>eval/runtime</td><td>4.0429</td></tr><tr><td>eval/samples_per_second</td><td>1018.092</td></tr><tr><td>eval/steps_per_second</td><td>63.816</td></tr><tr><td>total_flos</td><td>1.887617915188579e+16</td></tr><tr><td>train/epoch</td><td>12</td></tr><tr><td>train/global_step</td><td>27780</td></tr><tr><td>train/grad_norm</td><td>19.125</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9505</td></tr><tr><td>train_loss</td><td>1.00078</td></tr><tr><td>train_runtime</td><td>1182.5271</td></tr><tr><td>train_samples_per_second</td><td>375.863</td></tr><tr><td>train_steps_per_second</td><td>23.492</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/hbl8jumj' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2/runs/hbl8jumj</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250815_223234-hbl8jumj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import evaluate\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "class PrintAndWBCallback(TrainerCallback):\n",
    "    def __init__(self, print_every=20):\n",
    "        self.print_every = print_every\n",
    "        self.steps_per_epoch = None\n",
    "        self.last_print_step = -1\n",
    "\n",
    "    def on_train_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # steps/epoch computed via train_dataloader length\n",
    "        trainer = kwargs.get(\"model\")  # not available here; compute later in on_step_begin\n",
    "        print(f\"[Run] epochs={args.num_train_epochs} bs={args.per_device_train_batch_size} \"\n",
    "              f\"lr={args.learning_rate:.2e} wd={args.weight_decay:.1e} \"\n",
    "              f\"warmup_ratio={args.warmup_ratio} grad_accum={args.gradient_accumulation_steps}\")\n",
    "\n",
    "    def on_step_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # set steps_per_epoch once we have dataloader info exposed in state\n",
    "        if self.steps_per_epoch is None and state.max_steps is not None and state.num_train_epochs:\n",
    "            approx_steps_total = state.max_steps\n",
    "            self.steps_per_epoch = max(1, int(approx_steps_total / math.ceil(state.num_train_epochs)))\n",
    "\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is None: return\n",
    "        # mimic: [e1 b123/...] loss=... it/s=...\n",
    "        if self.steps_per_epoch:\n",
    "            step_in_epoch = (state.global_step % self.steps_per_epoch) or self.steps_per_epoch\n",
    "            if step_in_epoch % self.print_every == 0 and state.global_step != self.last_print_step:\n",
    "                loss = logs.get(\"loss\", logs.get(\"train_loss\", None))\n",
    "                lr = logs.get(\"learning_rate\", None)\n",
    "                sps = logs.get(\"train_samples_per_second\", None) or logs.get(\"samples_per_second\", None)\n",
    "                txt = f\"[e{int(state.epoch or 0)} b{step_in_epoch}/{self.steps_per_epoch}]\"\n",
    "                if loss is not None: txt += f\" loss={loss:.4f}\"\n",
    "                if lr   is not None: txt += f\" lr={lr:.2e}\"\n",
    "                if sps  is not None: txt += f\" samp/s={sps:.1f}\"\n",
    "                print(txt, flush=True)\n",
    "                self.last_print_step = state.global_step\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n",
    "        if metrics:\n",
    "            msg = (f\"[val @ epoch {int(state.epoch or 0)}] \"\n",
    "                   f\"acc={metrics.get('eval_accuracy',0):.4f} \"\n",
    "                   f\"f1={metrics.get('eval_f1',0):.4f} \"\n",
    "                   f\"p={metrics.get('eval_precision',0):.4f} \"\n",
    "                   f\"r={metrics.get('eval_recall',0):.4f}\")\n",
    "            print(msg, flush=True)\n",
    "\n",
    "safe_name = BASE_RUN_NAME\n",
    "out_dir = os.path.join(\"hf_ckpts\", safe_name)\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=out_dir,\n",
    "    run_name=safe_name,\n",
    "    report_to=[\"wandb\"],\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    dataloader_num_workers=NUM_WORKERS,\n",
    "    fp16=not bf16_ok and torch.cuda.is_available(),\n",
    "    bf16=bf16_ok,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    eval_strategy=\"epoch\",            # (new name replacing evaluation_strategy)\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "wandb_run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    name=safe_name,\n",
    "    config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"max_len\": MAX_LEN,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"grad_accum\": GRAD_ACCUM,\n",
    "        \"unfreeze_last_k\": UNFREEZE_LAST_K,\n",
    "    },\n",
    "    reinit=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrintAndWBCallback(print_every=20)],\n",
    ")\n",
    "\n",
    "print(f\"[Trainer] Starting fine-tune ‚Üí output_dir={out_dir}\")\n",
    "train_out = trainer.train()\n",
    "print(train_out)\n",
    "\n",
    "# best checkpoint path on disk\n",
    "best_path = trainer.state.best_model_checkpoint\n",
    "print(\"Best checkpoint dir:\", best_path)\n",
    "wandb_run.summary[\"best_checkpoint_dir\"] = best_path\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250968a91581fd33",
   "metadata": {},
   "source": [
    "## HP Tuning Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d740d75fabfdfe8",
   "metadata": {},
   "source": [
    "# üß™ Ex.5 ‚Äî Hyperparameter Tuning with HuggingFace `Trainer` (+ Optuna)\n",
    "\n",
    "Now that the basic HF `Trainer` pipeline works, we‚Äôre doing a **focused HP search** directly on top of it.  \n",
    "Goal: keep the Trainer setup clean and stable while letting **Optuna** explore a *small, sensible* space around our earlier best settings.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß What we‚Äôre tuning (student-style plan)\n",
    "- **Learning rate**: `1e-5 ‚Üí 1e-4` (log scale)  \n",
    "  *Why*: this is the sweet spot we found before; we‚Äôll confirm it under the Trainer loop.\n",
    "- **Weight decay**: `1e-6 ‚Üí 1e-4`  \n",
    "  *Why*: larger values hurt before; we constrain to the ‚Äúsafe‚Äù zone.\n",
    "- **Unfreezing depth**: **8‚Äì12** last layers  \n",
    "  *Why*: deeper unfreezing won in Ex.4; we test if full (12) still dominates here.\n",
    "- **Batch size**: **{4, 8, 16, 32}**  \n",
    "  \n",
    "---\n",
    "\n",
    "## üèóÔ∏è How we run it\n",
    "- **Trainer factory per trial** ‚Üí builds a fresh model and `TrainingArguments` for the sampled HPs.\n",
    "- **Callbacks**:  \n",
    "  - Custom print logger (lighter than full tqdm).  \n",
    "  - **EarlyStoppingCallback** with patience = **4**.  \n",
    "- **bf16/fp16** auto-detection for speed, dynamic padding (pad to 8), and fused AdamW where possible.\n",
    "- **W&B**: each trial is a separate run, grouped under a single **study** for easy comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What we expect\n",
    "- Best trials should cluster again around **LR ‚âà few√ó1e-5**, **low weight decay**, and **deep unfreezing** (‚â•9, often 12).  \n",
    "- Smaller batches (**8‚Äì16**) typically remain more stable than very large ones.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ After the study\n",
    "We **save the best params to JSON** (`hf_ckpts/best_params_optuna{i}.json`) so we can retrain cleanly later  \n",
    "(with possibly more epochs) without depending on the Optuna object.\n",
    " ---\n",
    "## Wadb Link\n",
    "https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7d45381b160a787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T13:21:58.580089Z",
     "start_time": "2025-08-17T13:21:58.426291Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Load CSVs (your files have columns: ['UserName','ScreenName','Location','TweetAt','OriginalTweet','Sentiment'])\n",
    "TRAIN_CSV = \"Corona_NLP_train_cleaned_translated.csv\"   # or \"Corona_NLP_train.csv\"\n",
    "TEST_CSV  = \"Corona_NLP_test_cleaned_translated.csv\"    # or \"Corona_NLP_test.csv\"\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV, encoding=\"utf-8\", engine=\"python\")\n",
    "df_test  = pd.read_csv(TEST_CSV,  encoding=\"utf-8\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b940c3dc586219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T07:11:20.464647Z",
     "start_time": "2025-08-17T07:10:57.540661Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 37039/4116/3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37039/37039 [00:01<00:00, 28774.57 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4116/4116 [00:00<00:00, 25847.50 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3798/3798 [00:00<00:00, 24327.51 examples/s]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1329: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n"
     ]
    }
   ],
   "source": [
    "# --- Ex.5: HF Trainer version with W&B + prints (Windows/RTX 4090 friendly) ---\n",
    "\n",
    "import os, math, random, time, json\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "import wandb\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, Trainer, TrainingArguments\n",
    ")\n",
    "from transformers.trainer_callback import TrainerCallback, TrainerState, TrainerControl\n",
    "\n",
    "# ---- Fast CUDA defaults (4090) ----\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    try:\n",
    "        torch.set_float32_matmul_precision(\"high\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s)\n",
    "    torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "set_seed(SEED)\n",
    "\n",
    "# ---- W&B defaults ----\n",
    "os.environ.setdefault(\"WANDB_MODE\", \"online\")\n",
    "os.environ.setdefault(\"WANDB_PROJECT\", \"adv-dl-p2-ex-5-deberta-16-08-try\")\n",
    "os.environ.setdefault(\"WANDB_NOTEBOOK_NAME\", \"ex5_trainer_new.ipynb\")\n",
    "WANDB_PROJECT = os.environ[\"WANDB_PROJECT\"]\n",
    "\n",
    "# ---- Constants ----\n",
    "MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "BASE_RUN_NAME = MODEL_NAME.replace(\"/\", \"__\") + \"_ex5_trainer-try\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "LR = 3e-5\n",
    "WEIGHT_DECAY = 0.05\n",
    "WARMUP_RATIO = 0.06\n",
    "GRAD_ACCUM = 1\n",
    "NUM_WORKERS = 0  # Windows-safe; raise to 2 if stable\n",
    "\n",
    "# ---- Label mapping (5-way) ----\n",
    "CANON = {\n",
    "    \"extremely negative\": \"extremely negative\",\n",
    "    \"negative\": \"negative\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"positive\": \"positive\",\n",
    "    \"extremely positive\": \"extremely positive\",\n",
    "}\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return CANON.get(s, s)\n",
    "\n",
    "# ---- Prep dataframes from df_train/df_test already in memory ----\n",
    "assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns\n",
    "assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"]).copy()\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"labels\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\", \"labels\", \"label_name\"]]\n",
    "\n",
    "dftrain_ = prep_df(df_train)\n",
    "dftest_  = prep_df(df_test)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    dftrain_, test_size=0.1, stratify=dftrain_[\"labels\"], random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def tok_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=False, truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "def keep_only(ds, cols):\n",
    "    # Use select_columns if available, else remove the others\n",
    "    if hasattr(ds, \"select_columns\"):\n",
    "        return ds.select_columns(cols)\n",
    "    drop = [c for c in ds.column_names if c not in cols]\n",
    "    return ds.remove_columns(drop)\n",
    "\n",
    "# Build datasets (avoid adding __index_level_0__ with preserve_index=False)\n",
    "train_ds = Dataset.from_pandas(train_df,  preserve_index=False)\n",
    "val_ds   = Dataset.from_pandas(val_df,    preserve_index=False)\n",
    "test_ds  = Dataset.from_pandas(dftest_,   preserve_index=False)\n",
    "\n",
    "# Keep just text + labels before tokenization\n",
    "cols_keep = [\"text\", \"labels\"]\n",
    "train_ds = keep_only(train_ds, cols_keep)\n",
    "val_ds   = keep_only(val_ds,   cols_keep)\n",
    "test_ds  = keep_only(test_ds,  cols_keep)\n",
    "\n",
    "# Tokenize\n",
    "train_ds = train_ds.map(tok_fn, batched=True)\n",
    "val_ds   = val_ds.map(tok_fn,   batched=True)\n",
    "test_ds  = test_ds.map(tok_fn,  batched=True)\n",
    "\n",
    "# Remove raw text after tokenization, keep labels + token IDs\n",
    "train_ds = train_ds.remove_columns([\"text\"])\n",
    "val_ds   = val_ds.remove_columns([\"text\"])\n",
    "test_ds  = test_ds.remove_columns([\"text\"])\n",
    "\n",
    "# Ensure final columns are exactly what Trainer expects\n",
    "final_cols = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
    "train_ds = keep_only(train_ds, final_cols)\n",
    "val_ds   = keep_only(val_ds,   final_cols)\n",
    "test_ds  = keep_only(test_ds,  final_cols)\n",
    "\n",
    "# # --- Speed tip: length stats + length column for bucketing ---\n",
    "# def _len_map(batch):\n",
    "#     return {\"input_length\": [len(x) for x in batch[\"input_ids\"]]}\n",
    "#\n",
    "# train_ds = train_ds.map(_len_map, batched=True)\n",
    "# val_ds   = val_ds.map(_len_map,   batched=True)\n",
    "# test_ds  = test_ds.map(_len_map,  batched=True)\n",
    "\n",
    "# Optional: quick distribution print so you can decide a smaller MAX_LEN later\n",
    "import numpy as np\n",
    "# lens = np.array(train_ds[\"input_length\"])\n",
    "# print(f\"[len] p50={np.percentile(lens,50)} p90={np.percentile(lens,90)} \"\n",
    "#       f\"p95={np.percentile(lens,95)} max={lens.max()}\")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "UNFREEZE_LAST_K = 10  # 1..12 are sensible for DeBERTa-v3-base\n",
    "\n",
    "def build_model(unfreeze_last_k=UNFREEZE_LAST_K):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(ORDER),\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else None),\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    base = getattr(model, \"deberta\", None) or getattr(model, \"roberta\", None) or getattr(model, \"bert\", None)\n",
    "    if base is not None and hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "        # freeze all\n",
    "        for p in base.parameters(): p.requires_grad = False\n",
    "        # unfreeze last k transformer blocks\n",
    "        for layer in base.encoder.layer[-int(unfreeze_last_k):]:\n",
    "            for p in layer.parameters(): p.requires_grad = True\n",
    "    # classifier head always trainable\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# # after model = build_model()\n",
    "# if torch.cuda.is_available():\n",
    "#     model.gradient_checkpointing_enable()          # compute‚Äìmemory tradeoff ‚Üí larger batch\n",
    "\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%) ; unfreeze_last_k={UNFREEZE_LAST_K}\")\n",
    "import evaluate\n",
    "metric_acc = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "class PrintAndWBCallback(TrainerCallback):\n",
    "    def __init__(self, print_every=100):\n",
    "        self.print_every = print_every\n",
    "        self.steps_per_epoch = None\n",
    "        self.last_print_step = -1\n",
    "\n",
    "    def on_train_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # steps/epoch computed via train_dataloader length\n",
    "        trainer = kwargs.get(\"model\")  # not available here; compute later in on_step_begin\n",
    "        print(f\"[Run] epochs={args.num_train_epochs} bs={args.per_device_train_batch_size} \"\n",
    "              f\"lr={args.learning_rate:.2e} wd={args.weight_decay:.1e} \"\n",
    "              f\"warmup_ratio={args.warmup_ratio} grad_accum={args.gradient_accumulation_steps}\")\n",
    "\n",
    "    def on_step_begin(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # set steps_per_epoch once we have dataloader info exposed in state\n",
    "        if self.steps_per_epoch is None and state.max_steps is not None and state.num_train_epochs:\n",
    "            approx_steps_total = state.max_steps\n",
    "            self.steps_per_epoch = max(1, int(approx_steps_total / math.ceil(state.num_train_epochs)))\n",
    "\n",
    "    def on_log(self, args, state: TrainerState, control: TrainerControl, logs=None, **kwargs):\n",
    "        if logs is None: return\n",
    "        # mimic: [e1 b123/...] loss=... it/s=...\n",
    "        if self.steps_per_epoch:\n",
    "            step_in_epoch = (state.global_step % self.steps_per_epoch) or self.steps_per_epoch\n",
    "            if step_in_epoch % self.print_every == 0 and state.global_step != self.last_print_step:\n",
    "                loss = logs.get(\"loss\", logs.get(\"train_loss\", None))\n",
    "                lr = logs.get(\"learning_rate\", None)\n",
    "                sps = logs.get(\"train_samples_per_second\", None) or logs.get(\"samples_per_second\", None)\n",
    "                txt = f\"[e{int(state.epoch or 0)} b{step_in_epoch}/{self.steps_per_epoch}]\"\n",
    "                if loss is not None: txt += f\" loss={loss:.4f}\"\n",
    "                if lr   is not None: txt += f\" lr={lr:.2e}\"\n",
    "                if sps  is not None: txt += f\" samp/s={sps:.1f}\"\n",
    "                print(txt, flush=True)\n",
    "                self.last_print_step = state.global_step\n",
    "\n",
    "    def on_evaluate(self, args, state: TrainerState, control: TrainerControl, metrics, **kwargs):\n",
    "        if metrics:\n",
    "            msg = (f\"[val @ epoch {int(state.epoch or 0)}] \"\n",
    "                   f\"acc={metrics.get('eval_accuracy',0):.4f} \"\n",
    "                   f\"f1={metrics.get('eval_f1',0):.4f} \"\n",
    "                   f\"p={metrics.get('eval_precision',0):.4f} \"\n",
    "                   f\"r={metrics.get('eval_recall',0):.4f}\")\n",
    "            print(msg, flush=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adbacf278188eb4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T01:34:12.035753Z",
     "start_time": "2025-08-17T01:34:12.032513Z"
    }
   },
   "outputs": [],
   "source": [
    "UNFREEZE_LAST_K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a142c0dded0db55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-16T22:59:27.937390Z",
     "start_time": "2025-08-16T18:41:10.545753Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-16 21:41:10,602] A new study created in memory with name: no-name-3b13a031-3415-41ed-8e90-8e6b61be55de\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Study] Starting Optuna: trials=10, epochs/trial=8 ; centered at lr=3.00e-05, wd=5.0e-02, k=10, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find ex5_trainer_new.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_214111-6bssnddt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/6bssnddt' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t0</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/6bssnddt' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/6bssnddt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=0 | epochs=8 bs=8 lr=1.20e-05 wd=6.1e-06 k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=8 lr=1.20e-05 wd=6.1e-06 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/4630] loss=1.6069 lr=5.34e-07\n",
      "{'loss': 1.6069, 'grad_norm': 3.875, 'learning_rate': 5.336356965817732e-07, 'epoch': 0.02159827213822894}\n",
      "[e0 b200/4630] loss=1.5959 lr=1.07e-06\n",
      "{'loss': 1.5959, 'grad_norm': 4.78125, 'learning_rate': 1.0726616527249786e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b300/4630] loss=1.6043 lr=1.61e-06\n",
      "{'loss': 1.6043, 'grad_norm': 3.28125, 'learning_rate': 1.6116876088681837e-06, 'epoch': 0.06479481641468683}\n",
      "[e0 b400/4630] loss=1.6024 lr=2.15e-06\n",
      "{'loss': 1.6024, 'grad_norm': 1.953125, 'learning_rate': 2.150713565011389e-06, 'epoch': 0.08639308855291576}\n",
      "[e0 b500/4630] loss=1.6011 lr=2.69e-06\n",
      "{'loss': 1.6011, 'grad_norm': 3.890625, 'learning_rate': 2.6897395211545945e-06, 'epoch': 0.1079913606911447}\n",
      "[e0 b600/4630] loss=1.6014 lr=3.23e-06\n",
      "{'loss': 1.6014, 'grad_norm': 3.046875, 'learning_rate': 3.2287654772977998e-06, 'epoch': 0.12958963282937366}\n",
      "[e0 b700/4630] loss=1.5988 lr=3.77e-06\n",
      "{'loss': 1.5988, 'grad_norm': 3.984375, 'learning_rate': 3.7677914334410055e-06, 'epoch': 0.1511879049676026}\n",
      "[e0 b800/4630] loss=1.6007 lr=4.31e-06\n",
      "{'loss': 1.6007, 'grad_norm': 3.546875, 'learning_rate': 4.306817389584211e-06, 'epoch': 0.17278617710583152}\n",
      "[e0 b900/4630] loss=1.5967 lr=4.85e-06\n",
      "{'loss': 1.5967, 'grad_norm': 2.1875, 'learning_rate': 4.845843345727415e-06, 'epoch': 0.19438444924406048}\n",
      "[e0 b1000/4630] loss=1.5980 lr=5.38e-06\n",
      "{'loss': 1.598, 'grad_norm': 4.375, 'learning_rate': 5.384869301870621e-06, 'epoch': 0.2159827213822894}\n",
      "[e0 b1100/4630] loss=1.5983 lr=5.92e-06\n",
      "{'loss': 1.5983, 'grad_norm': 2.15625, 'learning_rate': 5.923895258013827e-06, 'epoch': 0.23758099352051837}\n",
      "[e0 b1200/4630] loss=1.5965 lr=6.46e-06\n",
      "{'loss': 1.5965, 'grad_norm': 4.53125, 'learning_rate': 6.462921214157031e-06, 'epoch': 0.2591792656587473}\n",
      "[e0 b1300/4630] loss=1.5879 lr=7.00e-06\n",
      "{'loss': 1.5879, 'grad_norm': 3.171875, 'learning_rate': 7.001947170300236e-06, 'epoch': 0.28077753779697623}\n",
      "[e0 b1400/4630] loss=1.5981 lr=7.54e-06\n",
      "{'loss': 1.5981, 'grad_norm': 3.03125, 'learning_rate': 7.5409731264434425e-06, 'epoch': 0.3023758099352052}\n",
      "[e0 b1500/4630] loss=1.5955 lr=8.08e-06\n",
      "{'loss': 1.5955, 'grad_norm': 3.0625, 'learning_rate': 8.079999082586649e-06, 'epoch': 0.32397408207343414}\n",
      "[e0 b1600/4630] loss=1.5850 lr=8.62e-06\n",
      "{'loss': 1.585, 'grad_norm': 5.34375, 'learning_rate': 8.619025038729853e-06, 'epoch': 0.34557235421166305}\n",
      "[e0 b1700/4630] loss=1.5805 lr=9.16e-06\n",
      "{'loss': 1.5805, 'grad_norm': 2.796875, 'learning_rate': 9.158050994873058e-06, 'epoch': 0.367170626349892}\n",
      "[e0 b1800/4630] loss=1.5840 lr=9.70e-06\n",
      "{'loss': 1.584, 'grad_norm': 2.265625, 'learning_rate': 9.697076951016264e-06, 'epoch': 0.38876889848812096}\n",
      "[e0 b1900/4630] loss=1.5869 lr=1.02e-05\n",
      "{'loss': 1.5869, 'grad_norm': 3.890625, 'learning_rate': 1.0236102907159468e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b2000/4630] loss=1.5892 lr=1.08e-05\n",
      "{'loss': 1.5892, 'grad_norm': 1.984375, 'learning_rate': 1.0775128863302674e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b2100/4630] loss=1.5856 lr=1.13e-05\n",
      "{'loss': 1.5856, 'grad_norm': 3.765625, 'learning_rate': 1.131415481944588e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b2200/4630] loss=1.5859 lr=1.19e-05\n",
      "{'loss': 1.5859, 'grad_norm': 2.84375, 'learning_rate': 1.1853180775589085e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b2300/4630] loss=1.5777 lr=1.20e-05\n",
      "{'loss': 1.5777, 'grad_norm': 5.46875, 'learning_rate': 1.1956391001605808e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b2400/4630] loss=1.5774 lr=1.19e-05\n",
      "{'loss': 1.5774, 'grad_norm': 7.0, 'learning_rate': 1.1921975207582592e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b2500/4630] loss=1.5620 lr=1.19e-05\n",
      "{'loss': 1.562, 'grad_norm': 5.3125, 'learning_rate': 1.1887559413559375e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b2600/4630] loss=1.5615 lr=1.19e-05\n",
      "{'loss': 1.5615, 'grad_norm': 9.875, 'learning_rate': 1.1853143619536158e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b2700/4630] loss=1.5602 lr=1.18e-05\n",
      "{'loss': 1.5602, 'grad_norm': 8.875, 'learning_rate': 1.1818727825512942e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b2800/4630] loss=1.5568 lr=1.18e-05\n",
      "{'loss': 1.5568, 'grad_norm': 17.5, 'learning_rate': 1.1784312031489723e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b2900/4630] loss=1.5470 lr=1.17e-05\n",
      "{'loss': 1.547, 'grad_norm': 9.0, 'learning_rate': 1.1749896237466507e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b3000/4630] loss=1.5557 lr=1.17e-05\n",
      "{'loss': 1.5557, 'grad_norm': 11.5, 'learning_rate': 1.171548044344329e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b3100/4630] loss=1.5579 lr=1.17e-05\n",
      "{'loss': 1.5579, 'grad_norm': 18.875, 'learning_rate': 1.1681064649420073e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b3200/4630] loss=1.5191 lr=1.16e-05\n",
      "{'loss': 1.5191, 'grad_norm': 10.9375, 'learning_rate': 1.1646648855396856e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b3300/4630] loss=1.5400 lr=1.16e-05\n",
      "{'loss': 1.54, 'grad_norm': 13.8125, 'learning_rate': 1.161223306137364e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b3400/4630] loss=1.5322 lr=1.16e-05\n",
      "{'loss': 1.5322, 'grad_norm': 13.125, 'learning_rate': 1.1577817267350421e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b3500/4630] loss=1.5226 lr=1.15e-05\n",
      "{'loss': 1.5226, 'grad_norm': 10.375, 'learning_rate': 1.1543401473327205e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b3600/4630] loss=1.5301 lr=1.15e-05\n",
      "{'loss': 1.5301, 'grad_norm': 16.0, 'learning_rate': 1.1508985679303988e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b3700/4630] loss=1.5596 lr=1.15e-05\n",
      "{'loss': 1.5596, 'grad_norm': 16.125, 'learning_rate': 1.1474569885280771e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b3800/4630] loss=1.5438 lr=1.14e-05\n",
      "{'loss': 1.5438, 'grad_norm': 11.75, 'learning_rate': 1.1440154091257555e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b3900/4630] loss=1.5167 lr=1.14e-05\n",
      "{'loss': 1.5167, 'grad_norm': 15.0625, 'learning_rate': 1.1405738297234338e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b4000/4630] loss=1.5266 lr=1.14e-05\n",
      "{'loss': 1.5266, 'grad_norm': 12.8125, 'learning_rate': 1.1371322503211121e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b4100/4630] loss=1.5312 lr=1.13e-05\n",
      "{'loss': 1.5312, 'grad_norm': 14.8125, 'learning_rate': 1.1336906709187903e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b4200/4630] loss=1.5180 lr=1.13e-05\n",
      "{'loss': 1.518, 'grad_norm': 10.625, 'learning_rate': 1.1302490915164686e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b4300/4630] loss=1.5147 lr=1.13e-05\n",
      "{'loss': 1.5147, 'grad_norm': 12.625, 'learning_rate': 1.126807512114147e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b4400/4630] loss=1.5280 lr=1.12e-05\n",
      "{'loss': 1.528, 'grad_norm': 19.125, 'learning_rate': 1.1233659327118253e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b4500/4630] loss=1.5268 lr=1.12e-05\n",
      "{'loss': 1.5268, 'grad_norm': 12.9375, 'learning_rate': 1.1199243533095036e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b4600/4630] loss=1.5269 lr=1.12e-05\n",
      "{'loss': 1.5269, 'grad_norm': 8.8125, 'learning_rate': 1.116482773907182e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.5211037397384644, 'eval_accuracy': 0.30903790087463556, 'eval_precision': 0.2153358925120586, 'eval_recall': 0.24014835285233432, 'eval_f1': 0.18013912823644196, 'eval_runtime': 7.8032, 'eval_samples_per_second': 527.474, 'eval_steps_per_second': 65.998, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.3090 f1=0.1801 p=0.2153 r=0.2401\n",
      "{'loss': 1.516, 'grad_norm': 10.4375, 'learning_rate': 1.1130411945048601e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 1.5261, 'grad_norm': 17.625, 'learning_rate': 1.1095996151025384e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.5152, 'grad_norm': 9.3125, 'learning_rate': 1.1061580357002167e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 1.5071, 'grad_norm': 17.25, 'learning_rate': 1.102716456297895e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.5086, 'grad_norm': 10.25, 'learning_rate': 1.0992748768955734e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 1.5175, 'grad_norm': 22.25, 'learning_rate': 1.0958332974932517e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.4968, 'grad_norm': 11.0625, 'learning_rate': 1.0923917180909299e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 1.513, 'grad_norm': 13.25, 'learning_rate': 1.0889501386886082e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.5003, 'grad_norm': 12.0625, 'learning_rate': 1.0855085592862866e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 1.494, 'grad_norm': 11.8125, 'learning_rate': 1.0820669798839649e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.5076, 'grad_norm': 14.5625, 'learning_rate': 1.0786254004816432e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 1.5063, 'grad_norm': 14.0625, 'learning_rate': 1.0751838210793215e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.5151, 'grad_norm': 15.25, 'learning_rate': 1.0717422416769999e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 1.5023, 'grad_norm': 20.75, 'learning_rate': 1.068300662274678e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.5035, 'grad_norm': 16.5, 'learning_rate': 1.0648590828723564e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 1.5193, 'grad_norm': 15.5, 'learning_rate': 1.0614175034700347e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.5137, 'grad_norm': 11.8125, 'learning_rate': 1.057975924067713e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 1.5068, 'grad_norm': 9.875, 'learning_rate': 1.0545343446653914e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.5004, 'grad_norm': 10.3125, 'learning_rate': 1.0510927652630697e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 1.5042, 'grad_norm': 11.9375, 'learning_rate': 1.0476511858607478e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.5057, 'grad_norm': 12.875, 'learning_rate': 1.0442096064584262e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 1.4756, 'grad_norm': 13.4375, 'learning_rate': 1.0407680270561045e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.5158, 'grad_norm': 14.5, 'learning_rate': 1.0373264476537828e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 1.5039, 'grad_norm': 12.5, 'learning_rate': 1.0338848682514612e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.5058, 'grad_norm': 15.5, 'learning_rate': 1.0304432888491395e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 1.5061, 'grad_norm': 12.0625, 'learning_rate': 1.0270017094468177e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.5181, 'grad_norm': 19.5, 'learning_rate': 1.023560130044496e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 1.4893, 'grad_norm': 10.0625, 'learning_rate': 1.0201185506421743e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.4917, 'grad_norm': 12.125, 'learning_rate': 1.0166769712398527e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 1.5121, 'grad_norm': 9.375, 'learning_rate': 1.013235391837531e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.4848, 'grad_norm': 10.1875, 'learning_rate': 1.0097938124352093e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 1.4826, 'grad_norm': 10.5, 'learning_rate': 1.0063522330328875e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.5086, 'grad_norm': 15.25, 'learning_rate': 1.0029106536305658e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 1.4809, 'grad_norm': 14.5, 'learning_rate': 9.994690742282441e-06, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.5162, 'grad_norm': 13.5, 'learning_rate': 9.960274948259225e-06, 'epoch': 1.7494600431965441}\n",
      "{'loss': 1.4898, 'grad_norm': 16.0, 'learning_rate': 9.925859154236008e-06, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.5013, 'grad_norm': 15.1875, 'learning_rate': 9.891443360212791e-06, 'epoch': 1.7926565874730023}\n",
      "{'loss': 1.5077, 'grad_norm': 14.4375, 'learning_rate': 9.857027566189575e-06, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.5029, 'grad_norm': 15.6875, 'learning_rate': 9.822611772166356e-06, 'epoch': 1.83585313174946}\n",
      "{'loss': 1.4864, 'grad_norm': 13.1875, 'learning_rate': 9.78819597814314e-06, 'epoch': 1.857451403887689}\n",
      "{'loss': 1.485, 'grad_norm': 9.875, 'learning_rate': 9.753780184119923e-06, 'epoch': 1.8790496760259179}\n",
      "{'loss': 1.4872, 'grad_norm': 21.25, 'learning_rate': 9.719364390096706e-06, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.5045, 'grad_norm': 11.0625, 'learning_rate': 9.68494859607349e-06, 'epoch': 1.9222462203023758}\n",
      "{'loss': 1.501, 'grad_norm': 15.875, 'learning_rate': 9.650532802050273e-06, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.4799, 'grad_norm': 11.25, 'learning_rate': 9.616117008027054e-06, 'epoch': 1.9654427645788337}\n",
      "{'loss': 1.4852, 'grad_norm': 107.5, 'learning_rate': 9.581701214003838e-06, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 1.4953222274780273, 'eval_accuracy': 0.33430515063168126, 'eval_precision': 0.46189775829529484, 'eval_recall': 0.27292720202814647, 'eval_f1': 0.24014202607735496, 'eval_runtime': 8.5251, 'eval_samples_per_second': 482.808, 'eval_steps_per_second': 60.41, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.3343 f1=0.2401 p=0.4619 r=0.2729\n",
      "{'loss': 1.5147, 'grad_norm': 15.375, 'learning_rate': 9.54728541998062e-06, 'epoch': 2.0086393088552916}\n",
      "{'loss': 1.4841, 'grad_norm': 17.625, 'learning_rate': 9.512869625957404e-06, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.5039, 'grad_norm': 14.3125, 'learning_rate': 9.478453831934187e-06, 'epoch': 2.0518358531317493}\n",
      "{'loss': 1.497, 'grad_norm': 15.9375, 'learning_rate': 9.44403803791097e-06, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.4784, 'grad_norm': 13.3125, 'learning_rate': 9.409622243887752e-06, 'epoch': 2.0950323974082075}\n",
      "{'loss': 1.4978, 'grad_norm': 10.8125, 'learning_rate': 9.375206449864536e-06, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.4775, 'grad_norm': 12.9375, 'learning_rate': 9.340790655841319e-06, 'epoch': 2.138228941684665}\n",
      "{'loss': 1.5112, 'grad_norm': 12.5625, 'learning_rate': 9.306374861818102e-06, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.4796, 'grad_norm': 15.375, 'learning_rate': 9.271959067794886e-06, 'epoch': 2.1814254859611233}\n",
      "{'loss': 1.4845, 'grad_norm': 21.625, 'learning_rate': 9.237543273771669e-06, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.4938, 'grad_norm': 10.0625, 'learning_rate': 9.203127479748452e-06, 'epoch': 2.224622030237581}\n",
      "{'loss': 1.4982, 'grad_norm': 12.875, 'learning_rate': 9.168711685725234e-06, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.4864, 'grad_norm': 12.25, 'learning_rate': 9.134295891702017e-06, 'epoch': 2.267818574514039}\n",
      "{'loss': 1.4889, 'grad_norm': 23.875, 'learning_rate': 9.0998800976788e-06, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.4872, 'grad_norm': 18.25, 'learning_rate': 9.065464303655584e-06, 'epoch': 2.311015118790497}\n",
      "{'loss': 1.4758, 'grad_norm': 14.8125, 'learning_rate': 9.031048509632367e-06, 'epoch': 2.3326133909287257}\n",
      "{'loss': 1.5017, 'grad_norm': 12.8125, 'learning_rate': 8.99663271560915e-06, 'epoch': 2.3542116630669545}\n",
      "{'loss': 1.4913, 'grad_norm': 12.6875, 'learning_rate': 8.962216921585932e-06, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.5115, 'grad_norm': 12.4375, 'learning_rate': 8.927801127562715e-06, 'epoch': 2.3974082073434126}\n",
      "{'loss': 1.5028, 'grad_norm': 11.0625, 'learning_rate': 8.893385333539498e-06, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.487, 'grad_norm': 13.25, 'learning_rate': 8.858969539516282e-06, 'epoch': 2.4406047516198703}\n",
      "{'loss': 1.485, 'grad_norm': 29.625, 'learning_rate': 8.824553745493065e-06, 'epoch': 2.462203023758099}\n",
      "{'loss': 1.4839, 'grad_norm': 10.75, 'learning_rate': 8.790137951469848e-06, 'epoch': 2.4838012958963285}\n",
      "{'loss': 1.4732, 'grad_norm': 22.75, 'learning_rate': 8.75572215744663e-06, 'epoch': 2.5053995680345573}\n",
      "{'loss': 1.4778, 'grad_norm': 16.5, 'learning_rate': 8.721306363423413e-06, 'epoch': 2.526997840172786}\n",
      "{'loss': 1.491, 'grad_norm': 11.875, 'learning_rate': 8.686890569400197e-06, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.4753, 'grad_norm': 9.625, 'learning_rate': 8.65247477537698e-06, 'epoch': 2.570194384449244}\n",
      "{'loss': 1.4735, 'grad_norm': 11.8125, 'learning_rate': 8.618058981353763e-06, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.4884, 'grad_norm': 11.75, 'learning_rate': 8.583643187330546e-06, 'epoch': 2.613390928725702}\n",
      "{'loss': 1.4827, 'grad_norm': 13.375, 'learning_rate': 8.549227393307328e-06, 'epoch': 2.634989200863931}\n",
      "{'loss': 1.4997, 'grad_norm': 22.125, 'learning_rate': 8.514811599284111e-06, 'epoch': 2.6565874730021597}\n",
      "{'loss': 1.4951, 'grad_norm': 12.4375, 'learning_rate': 8.480395805260895e-06, 'epoch': 2.6781857451403885}\n",
      "{'loss': 1.5041, 'grad_norm': 16.75, 'learning_rate': 8.445980011237678e-06, 'epoch': 2.699784017278618}\n",
      "{'loss': 1.4839, 'grad_norm': 12.5, 'learning_rate': 8.411564217214461e-06, 'epoch': 2.7213822894168467}\n",
      "{'loss': 1.495, 'grad_norm': 18.0, 'learning_rate': 8.377148423191245e-06, 'epoch': 2.7429805615550755}\n",
      "{'loss': 1.483, 'grad_norm': 11.5625, 'learning_rate': 8.342732629168028e-06, 'epoch': 2.7645788336933044}\n",
      "{'loss': 1.4941, 'grad_norm': 16.375, 'learning_rate': 8.30831683514481e-06, 'epoch': 2.786177105831533}\n",
      "{'loss': 1.47, 'grad_norm': 16.375, 'learning_rate': 8.273901041121593e-06, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.483, 'grad_norm': 12.625, 'learning_rate': 8.239485247098376e-06, 'epoch': 2.8293736501079914}\n",
      "{'loss': 1.4855, 'grad_norm': 16.375, 'learning_rate': 8.20506945307516e-06, 'epoch': 2.85097192224622}\n",
      "{'loss': 1.5036, 'grad_norm': 24.625, 'learning_rate': 8.170653659051943e-06, 'epoch': 2.8725701943844495}\n",
      "{'loss': 1.4754, 'grad_norm': 31.625, 'learning_rate': 8.136237865028726e-06, 'epoch': 2.8941684665226783}\n",
      "{'loss': 1.4781, 'grad_norm': 12.0, 'learning_rate': 8.101822071005508e-06, 'epoch': 2.915766738660907}\n",
      "{'loss': 1.4859, 'grad_norm': 16.125, 'learning_rate': 8.067406276982291e-06, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.4866, 'grad_norm': 11.625, 'learning_rate': 8.032990482959074e-06, 'epoch': 2.958963282937365}\n",
      "{'loss': 1.49, 'grad_norm': 14.125, 'learning_rate': 7.998574688935858e-06, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 1.490586519241333, 'eval_accuracy': 0.33090379008746357, 'eval_precision': 0.4276859189871531, 'eval_recall': 0.2703495444036783, 'eval_f1': 0.2361467519490656, 'eval_runtime': 9.4908, 'eval_samples_per_second': 433.681, 'eval_steps_per_second': 54.263, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.3309 f1=0.2361 p=0.4277 r=0.2703\n",
      "{'loss': 1.4691, 'grad_norm': 15.5, 'learning_rate': 7.96415889491264e-06, 'epoch': 3.002159827213823}\n",
      "{'loss': 1.4937, 'grad_norm': 13.5, 'learning_rate': 7.929743100889424e-06, 'epoch': 3.023758099352052}\n",
      "{'loss': 1.4858, 'grad_norm': 45.75, 'learning_rate': 7.895327306866206e-06, 'epoch': 3.0453563714902807}\n",
      "{'loss': 1.4893, 'grad_norm': 11.375, 'learning_rate': 7.860911512842989e-06, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.4689, 'grad_norm': 14.625, 'learning_rate': 7.826495718819772e-06, 'epoch': 3.088552915766739}\n",
      "{'loss': 1.4767, 'grad_norm': 8.875, 'learning_rate': 7.792079924796556e-06, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.4993, 'grad_norm': 9.9375, 'learning_rate': 7.757664130773339e-06, 'epoch': 3.1317494600431965}\n",
      "{'loss': 1.4712, 'grad_norm': 12.8125, 'learning_rate': 7.723248336750122e-06, 'epoch': 3.1533477321814254}\n",
      "{'loss': 1.507, 'grad_norm': 12.3125, 'learning_rate': 7.688832542726906e-06, 'epoch': 3.1749460043196542}\n",
      "{'loss': 1.5116, 'grad_norm': 12.1875, 'learning_rate': 7.654416748703687e-06, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.4825, 'grad_norm': 15.6875, 'learning_rate': 7.6200009546804704e-06, 'epoch': 3.2181425485961124}\n",
      "{'loss': 1.4962, 'grad_norm': 13.0625, 'learning_rate': 7.585585160657253e-06, 'epoch': 3.239740820734341}\n",
      "{'loss': 1.4685, 'grad_norm': 16.25, 'learning_rate': 7.551169366634036e-06, 'epoch': 3.26133909287257}\n",
      "{'loss': 1.4708, 'grad_norm': 12.4375, 'learning_rate': 7.5167535726108195e-06, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.4966, 'grad_norm': 16.75, 'learning_rate': 7.482337778587603e-06, 'epoch': 3.304535637149028}\n",
      "{'loss': 1.4751, 'grad_norm': 11.375, 'learning_rate': 7.447921984564385e-06, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.4797, 'grad_norm': 13.5, 'learning_rate': 7.413506190541168e-06, 'epoch': 3.347732181425486}\n",
      "{'loss': 1.4914, 'grad_norm': 13.75, 'learning_rate': 7.379090396517951e-06, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.4719, 'grad_norm': 18.5, 'learning_rate': 7.344674602494734e-06, 'epoch': 3.390928725701944}\n",
      "{'loss': 1.5068, 'grad_norm': 16.5, 'learning_rate': 7.310258808471518e-06, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.4773, 'grad_norm': 16.375, 'learning_rate': 7.275843014448301e-06, 'epoch': 3.4341252699784017}\n",
      "{'loss': 1.4886, 'grad_norm': 10.375, 'learning_rate': 7.241427220425083e-06, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.4799, 'grad_norm': 9.625, 'learning_rate': 7.207011426401866e-06, 'epoch': 3.4773218142548594}\n",
      "{'loss': 1.4796, 'grad_norm': 17.125, 'learning_rate': 7.172595632378649e-06, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.4904, 'grad_norm': 10.8125, 'learning_rate': 7.138179838355432e-06, 'epoch': 3.5205183585313176}\n",
      "{'loss': 1.4963, 'grad_norm': 16.375, 'learning_rate': 7.103764044332216e-06, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.4729, 'grad_norm': 23.5, 'learning_rate': 7.069348250308999e-06, 'epoch': 3.5637149028077753}\n",
      "{'loss': 1.4868, 'grad_norm': 15.125, 'learning_rate': 7.0349324562857815e-06, 'epoch': 3.5853131749460045}\n",
      "{'loss': 1.4911, 'grad_norm': 14.8125, 'learning_rate': 7.000516662262564e-06, 'epoch': 3.6069114470842334}\n",
      "{'loss': 1.492, 'grad_norm': 20.25, 'learning_rate': 6.966100868239347e-06, 'epoch': 3.6285097192224622}\n",
      "{'loss': 1.4862, 'grad_norm': 19.625, 'learning_rate': 6.9316850742161305e-06, 'epoch': 3.650107991360691}\n",
      "{'loss': 1.4781, 'grad_norm': 11.375, 'learning_rate': 6.897269280192914e-06, 'epoch': 3.67170626349892}\n",
      "{'loss': 1.4939, 'grad_norm': 13.8125, 'learning_rate': 6.862853486169697e-06, 'epoch': 3.693304535637149}\n",
      "{'loss': 1.4695, 'grad_norm': 27.125, 'learning_rate': 6.8284376921464804e-06, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.4691, 'grad_norm': 12.0625, 'learning_rate': 6.794021898123262e-06, 'epoch': 3.736501079913607}\n",
      "{'loss': 1.4672, 'grad_norm': 13.25, 'learning_rate': 6.759606104100045e-06, 'epoch': 3.7580993520518358}\n",
      "{'loss': 1.4845, 'grad_norm': 12.0, 'learning_rate': 6.725190310076829e-06, 'epoch': 3.7796976241900646}\n",
      "{'loss': 1.4915, 'grad_norm': 16.375, 'learning_rate': 6.690774516053612e-06, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.5077, 'grad_norm': 12.1875, 'learning_rate': 6.656358722030395e-06, 'epoch': 3.8228941684665227}\n",
      "{'loss': 1.4942, 'grad_norm': 13.3125, 'learning_rate': 6.6219429280071785e-06, 'epoch': 3.8444924406047516}\n",
      "{'loss': 1.4816, 'grad_norm': 24.5, 'learning_rate': 6.58752713398396e-06, 'epoch': 3.8660907127429804}\n",
      "{'loss': 1.4971, 'grad_norm': 13.375, 'learning_rate': 6.5531113399607435e-06, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.4821, 'grad_norm': 14.1875, 'learning_rate': 6.518695545937527e-06, 'epoch': 3.9092872570194386}\n",
      "{'loss': 1.4846, 'grad_norm': 12.5, 'learning_rate': 6.48427975191431e-06, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.4974, 'grad_norm': 12.5, 'learning_rate': 6.449863957891093e-06, 'epoch': 3.9524838012958963}\n",
      "{'loss': 1.4907, 'grad_norm': 25.75, 'learning_rate': 6.415448163867877e-06, 'epoch': 3.974082073434125}\n",
      "{'loss': 1.4801, 'grad_norm': 12.0, 'learning_rate': 6.381032369844658e-06, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 1.4886293411254883, 'eval_accuracy': 0.34207968901846453, 'eval_precision': 0.423023969550225, 'eval_recall': 0.2817489073685374, 'eval_f1': 0.25340324101829476, 'eval_runtime': 7.7486, 'eval_samples_per_second': 531.19, 'eval_steps_per_second': 66.463, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.3421 f1=0.2534 p=0.4230 r=0.2817\n",
      "{'loss': 1.473, 'grad_norm': 12.1875, 'learning_rate': 6.3466165758214416e-06, 'epoch': 4.017278617710583}\n",
      "{'loss': 1.4607, 'grad_norm': 10.375, 'learning_rate': 6.312200781798225e-06, 'epoch': 4.038876889848812}\n",
      "{'loss': 1.5001, 'grad_norm': 15.5625, 'learning_rate': 6.277784987775008e-06, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.4915, 'grad_norm': 13.8125, 'learning_rate': 6.2433691937517915e-06, 'epoch': 4.08207343412527}\n",
      "{'loss': 1.5043, 'grad_norm': 16.5, 'learning_rate': 6.208953399728575e-06, 'epoch': 4.103671706263499}\n",
      "{'loss': 1.465, 'grad_norm': 14.5625, 'learning_rate': 6.174537605705358e-06, 'epoch': 4.125269978401728}\n",
      "{'loss': 1.4965, 'grad_norm': 14.375, 'learning_rate': 6.14012181168214e-06, 'epoch': 4.146868250539957}\n",
      "{'loss': 1.493, 'grad_norm': 13.5625, 'learning_rate': 6.105706017658923e-06, 'epoch': 4.168466522678186}\n",
      "{'loss': 1.4876, 'grad_norm': 12.8125, 'learning_rate': 6.071290223635706e-06, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.5009, 'grad_norm': 10.9375, 'learning_rate': 6.03687442961249e-06, 'epoch': 4.211663066954643}\n",
      "{'loss': 1.4685, 'grad_norm': 14.5, 'learning_rate': 6.002458635589273e-06, 'epoch': 4.233261339092873}\n",
      "{'loss': 1.4746, 'grad_norm': 12.4375, 'learning_rate': 5.968042841566055e-06, 'epoch': 4.254859611231102}\n",
      "{'loss': 1.4827, 'grad_norm': 15.0625, 'learning_rate': 5.933627047542839e-06, 'epoch': 4.27645788336933}\n",
      "{'loss': 1.479, 'grad_norm': 14.4375, 'learning_rate': 5.899211253519621e-06, 'epoch': 4.29805615550756}\n",
      "{'loss': 1.4806, 'grad_norm': 13.1875, 'learning_rate': 5.864795459496404e-06, 'epoch': 4.319654427645788}\n",
      "{'loss': 1.4781, 'grad_norm': 17.0, 'learning_rate': 5.830379665473188e-06, 'epoch': 4.341252699784017}\n",
      "{'loss': 1.4774, 'grad_norm': 14.25, 'learning_rate': 5.795963871449971e-06, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.494, 'grad_norm': 13.1875, 'learning_rate': 5.7615480774267534e-06, 'epoch': 4.384449244060475}\n",
      "{'loss': 1.4754, 'grad_norm': 13.4375, 'learning_rate': 5.727132283403537e-06, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.5037, 'grad_norm': 12.1875, 'learning_rate': 5.69271648938032e-06, 'epoch': 4.427645788336933}\n",
      "{'loss': 1.4685, 'grad_norm': 26.5, 'learning_rate': 5.6583006953571025e-06, 'epoch': 4.449244060475162}\n",
      "{'loss': 1.4842, 'grad_norm': 16.75, 'learning_rate': 5.623884901333886e-06, 'epoch': 4.470842332613391}\n",
      "{'loss': 1.471, 'grad_norm': 17.125, 'learning_rate': 5.589469107310669e-06, 'epoch': 4.49244060475162}\n",
      "{'loss': 1.483, 'grad_norm': 11.6875, 'learning_rate': 5.5550533132874516e-06, 'epoch': 4.514038876889849}\n",
      "{'loss': 1.4883, 'grad_norm': 25.5, 'learning_rate': 5.520637519264235e-06, 'epoch': 4.535637149028078}\n",
      "{'loss': 1.4835, 'grad_norm': 10.0625, 'learning_rate': 5.486221725241018e-06, 'epoch': 4.557235421166307}\n",
      "{'loss': 1.4767, 'grad_norm': 12.75, 'learning_rate': 5.451805931217801e-06, 'epoch': 4.578833693304536}\n",
      "{'loss': 1.4847, 'grad_norm': 13.9375, 'learning_rate': 5.417390137194584e-06, 'epoch': 4.600431965442764}\n",
      "{'loss': 1.4704, 'grad_norm': 11.25, 'learning_rate': 5.382974343171367e-06, 'epoch': 4.622030237580994}\n",
      "{'loss': 1.4928, 'grad_norm': 12.3125, 'learning_rate': 5.34855854914815e-06, 'epoch': 4.643628509719223}\n",
      "{'loss': 1.4998, 'grad_norm': 14.9375, 'learning_rate': 5.314142755124933e-06, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.497, 'grad_norm': 11.8125, 'learning_rate': 5.279726961101716e-06, 'epoch': 4.686825053995681}\n",
      "{'loss': 1.4735, 'grad_norm': 14.5625, 'learning_rate': 5.245311167078499e-06, 'epoch': 4.708423326133909}\n",
      "{'loss': 1.4677, 'grad_norm': 13.3125, 'learning_rate': 5.210895373055282e-06, 'epoch': 4.730021598272138}\n",
      "{'loss': 1.4806, 'grad_norm': 11.3125, 'learning_rate': 5.176479579032065e-06, 'epoch': 4.751619870410368}\n",
      "{'loss': 1.4959, 'grad_norm': 10.1875, 'learning_rate': 5.142063785008849e-06, 'epoch': 4.773218142548596}\n",
      "{'loss': 1.4865, 'grad_norm': 18.625, 'learning_rate': 5.107647990985631e-06, 'epoch': 4.794816414686825}\n",
      "{'loss': 1.4928, 'grad_norm': 13.6875, 'learning_rate': 5.073232196962414e-06, 'epoch': 4.816414686825054}\n",
      "{'loss': 1.4625, 'grad_norm': 12.8125, 'learning_rate': 5.038816402939198e-06, 'epoch': 4.838012958963283}\n",
      "{'loss': 1.491, 'grad_norm': 11.6875, 'learning_rate': 5.00440060891598e-06, 'epoch': 4.859611231101512}\n",
      "{'loss': 1.5365, 'grad_norm': 15.0625, 'learning_rate': 4.9699848148927634e-06, 'epoch': 4.881209503239741}\n",
      "{'loss': 1.4779, 'grad_norm': 24.375, 'learning_rate': 4.935569020869547e-06, 'epoch': 4.90280777537797}\n",
      "{'loss': 1.4709, 'grad_norm': 11.375, 'learning_rate': 4.901153226846329e-06, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.5029, 'grad_norm': 17.125, 'learning_rate': 4.8667374328231125e-06, 'epoch': 4.946004319654428}\n",
      "{'loss': 1.4976, 'grad_norm': 15.8125, 'learning_rate': 4.832321638799896e-06, 'epoch': 4.967602591792657}\n",
      "{'loss': 1.5034, 'grad_norm': 16.375, 'learning_rate': 4.797905844776678e-06, 'epoch': 4.989200863930885}\n",
      "{'eval_loss': 1.488826870918274, 'eval_accuracy': 0.3391642371234208, 'eval_precision': 0.4642726672296108, 'eval_recall': 0.2778416658852286, 'eval_f1': 0.2474881281233554, 'eval_runtime': 7.7026, 'eval_samples_per_second': 534.367, 'eval_steps_per_second': 66.861, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.3392 f1=0.2475 p=0.4643 r=0.2778\n",
      "{'loss': 1.4739, 'grad_norm': 15.625, 'learning_rate': 4.7634900507534616e-06, 'epoch': 5.010799136069115}\n",
      "{'loss': 1.4866, 'grad_norm': 13.3125, 'learning_rate': 4.729074256730245e-06, 'epoch': 5.032397408207343}\n",
      "{'loss': 1.4726, 'grad_norm': 10.75, 'learning_rate': 4.694658462707027e-06, 'epoch': 5.053995680345572}\n",
      "{'loss': 1.4709, 'grad_norm': 10.4375, 'learning_rate': 4.660242668683811e-06, 'epoch': 5.075593952483802}\n",
      "{'loss': 1.4953, 'grad_norm': 11.625, 'learning_rate': 4.625826874660594e-06, 'epoch': 5.09719222462203}\n",
      "{'loss': 1.5109, 'grad_norm': 12.0, 'learning_rate': 4.591411080637376e-06, 'epoch': 5.118790496760259}\n",
      "{'loss': 1.4943, 'grad_norm': 13.375, 'learning_rate': 4.55699528661416e-06, 'epoch': 5.140388768898488}\n",
      "{'loss': 1.4973, 'grad_norm': 15.875, 'learning_rate': 4.522579492590943e-06, 'epoch': 5.161987041036717}\n",
      "{'loss': 1.4671, 'grad_norm': 10.375, 'learning_rate': 4.488163698567725e-06, 'epoch': 5.183585313174946}\n",
      "{'loss': 1.4958, 'grad_norm': 18.875, 'learning_rate': 4.453747904544509e-06, 'epoch': 5.205183585313175}\n",
      "{'loss': 1.4906, 'grad_norm': 10.875, 'learning_rate': 4.419332110521292e-06, 'epoch': 5.226781857451404}\n",
      "{'loss': 1.4705, 'grad_norm': 15.8125, 'learning_rate': 4.384916316498075e-06, 'epoch': 5.248380129589632}\n",
      "{'loss': 1.4638, 'grad_norm': 11.6875, 'learning_rate': 4.350500522474858e-06, 'epoch': 5.269978401727862}\n",
      "{'loss': 1.4828, 'grad_norm': 13.6875, 'learning_rate': 4.316084728451641e-06, 'epoch': 5.291576673866091}\n",
      "{'loss': 1.4789, 'grad_norm': 13.6875, 'learning_rate': 4.281668934428424e-06, 'epoch': 5.313174946004319}\n",
      "{'loss': 1.4933, 'grad_norm': 11.5, 'learning_rate': 4.247253140405207e-06, 'epoch': 5.334773218142549}\n",
      "{'loss': 1.4953, 'grad_norm': 10.125, 'learning_rate': 4.21283734638199e-06, 'epoch': 5.356371490280777}\n",
      "{'loss': 1.4636, 'grad_norm': 13.125, 'learning_rate': 4.1784215523587734e-06, 'epoch': 5.377969762419006}\n",
      "{'loss': 1.4733, 'grad_norm': 19.375, 'learning_rate': 4.144005758335556e-06, 'epoch': 5.399568034557236}\n",
      "{'loss': 1.4822, 'grad_norm': 16.5, 'learning_rate': 4.109589964312339e-06, 'epoch': 5.421166306695464}\n",
      "{'loss': 1.4627, 'grad_norm': 13.125, 'learning_rate': 4.0751741702891225e-06, 'epoch': 5.442764578833693}\n",
      "{'loss': 1.4967, 'grad_norm': 12.75, 'learning_rate': 4.040758376265905e-06, 'epoch': 5.464362850971923}\n",
      "{'loss': 1.4851, 'grad_norm': 22.375, 'learning_rate': 4.006342582242688e-06, 'epoch': 5.485961123110151}\n",
      "{'loss': 1.4964, 'grad_norm': 17.875, 'learning_rate': 3.9719267882194715e-06, 'epoch': 5.50755939524838}\n",
      "{'loss': 1.4869, 'grad_norm': 12.8125, 'learning_rate': 3.937510994196254e-06, 'epoch': 5.529157667386609}\n",
      "{'loss': 1.4668, 'grad_norm': 10.9375, 'learning_rate': 3.903095200173037e-06, 'epoch': 5.550755939524838}\n",
      "{'loss': 1.4982, 'grad_norm': 25.125, 'learning_rate': 3.868679406149821e-06, 'epoch': 5.572354211663067}\n",
      "{'loss': 1.4824, 'grad_norm': 13.4375, 'learning_rate': 3.834263612126603e-06, 'epoch': 5.593952483801296}\n",
      "{'loss': 1.4908, 'grad_norm': 12.625, 'learning_rate': 3.799847818103386e-06, 'epoch': 5.615550755939525}\n",
      "{'loss': 1.4976, 'grad_norm': 15.4375, 'learning_rate': 3.7654320240801692e-06, 'epoch': 5.637149028077753}\n",
      "{'loss': 1.4852, 'grad_norm': 12.375, 'learning_rate': 3.7310162300569517e-06, 'epoch': 5.658747300215983}\n",
      "{'loss': 1.4952, 'grad_norm': 17.875, 'learning_rate': 3.696600436033735e-06, 'epoch': 5.680345572354212}\n",
      "{'loss': 1.4712, 'grad_norm': 10.5625, 'learning_rate': 3.6621846420105183e-06, 'epoch': 5.70194384449244}\n",
      "{'loss': 1.4871, 'grad_norm': 12.9375, 'learning_rate': 3.6277688479873016e-06, 'epoch': 5.72354211663067}\n",
      "{'loss': 1.4921, 'grad_norm': 15.375, 'learning_rate': 3.593353053964084e-06, 'epoch': 5.745140388768899}\n",
      "{'loss': 1.5045, 'grad_norm': 12.6875, 'learning_rate': 3.5589372599408673e-06, 'epoch': 5.766738660907127}\n",
      "{'loss': 1.488, 'grad_norm': 20.625, 'learning_rate': 3.5245214659176506e-06, 'epoch': 5.788336933045357}\n",
      "{'loss': 1.4951, 'grad_norm': 13.3125, 'learning_rate': 3.490105671894433e-06, 'epoch': 5.809935205183585}\n",
      "{'loss': 1.4875, 'grad_norm': 10.375, 'learning_rate': 3.4556898778712164e-06, 'epoch': 5.831533477321814}\n",
      "{'loss': 1.4725, 'grad_norm': 11.1875, 'learning_rate': 3.4212740838479997e-06, 'epoch': 5.853131749460044}\n",
      "{'loss': 1.4901, 'grad_norm': 11.125, 'learning_rate': 3.386858289824782e-06, 'epoch': 5.874730021598272}\n",
      "{'loss': 1.4811, 'grad_norm': 24.0, 'learning_rate': 3.3524424958015655e-06, 'epoch': 5.896328293736501}\n",
      "{'loss': 1.4758, 'grad_norm': 16.375, 'learning_rate': 3.3180267017783488e-06, 'epoch': 5.91792656587473}\n",
      "{'loss': 1.4967, 'grad_norm': 15.0, 'learning_rate': 3.283610907755131e-06, 'epoch': 5.939524838012959}\n",
      "{'loss': 1.4894, 'grad_norm': 18.875, 'learning_rate': 3.2491951137319145e-06, 'epoch': 5.961123110151188}\n",
      "{'loss': 1.5041, 'grad_norm': 14.4375, 'learning_rate': 3.214779319708698e-06, 'epoch': 5.982721382289417}\n",
      "{'eval_loss': 1.4886659383773804, 'eval_accuracy': 0.33867832847424684, 'eval_precision': 0.4592843293217461, 'eval_recall': 0.2782837799102221, 'eval_f1': 0.24921748617089837, 'eval_runtime': 8.1583, 'eval_samples_per_second': 504.52, 'eval_steps_per_second': 63.126, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.3387 f1=0.2492 p=0.4593 r=0.2783\n",
      "{'loss': 1.4874, 'grad_norm': 14.3125, 'learning_rate': 3.1803635256854803e-06, 'epoch': 6.004319654427646}\n",
      "{'loss': 1.4875, 'grad_norm': 18.5, 'learning_rate': 3.1459477316622636e-06, 'epoch': 6.025917926565874}\n",
      "{'loss': 1.4824, 'grad_norm': 21.625, 'learning_rate': 3.111531937639047e-06, 'epoch': 6.047516198704104}\n",
      "{'loss': 1.4432, 'grad_norm': 12.4375, 'learning_rate': 3.0771161436158293e-06, 'epoch': 6.069114470842333}\n",
      "{'loss': 1.4641, 'grad_norm': 9.875, 'learning_rate': 3.0427003495926126e-06, 'epoch': 6.090712742980561}\n",
      "{'loss': 1.4645, 'grad_norm': 13.125, 'learning_rate': 3.008284555569396e-06, 'epoch': 6.112311015118791}\n",
      "{'loss': 1.4954, 'grad_norm': 17.75, 'learning_rate': 2.973868761546179e-06, 'epoch': 6.133909287257019}\n",
      "{'loss': 1.4971, 'grad_norm': 15.5625, 'learning_rate': 2.9394529675229617e-06, 'epoch': 6.155507559395248}\n",
      "{'loss': 1.4731, 'grad_norm': 12.625, 'learning_rate': 2.905037173499745e-06, 'epoch': 6.177105831533478}\n",
      "{'loss': 1.4951, 'grad_norm': 19.875, 'learning_rate': 2.870621379476528e-06, 'epoch': 6.198704103671706}\n",
      "{'loss': 1.4867, 'grad_norm': 7.9375, 'learning_rate': 2.8362055854533107e-06, 'epoch': 6.220302375809935}\n",
      "{'loss': 1.4883, 'grad_norm': 15.0625, 'learning_rate': 2.801789791430094e-06, 'epoch': 6.241900647948164}\n",
      "{'loss': 1.4852, 'grad_norm': 11.4375, 'learning_rate': 2.767373997406877e-06, 'epoch': 6.263498920086393}\n",
      "{'loss': 1.4586, 'grad_norm': 13.8125, 'learning_rate': 2.73295820338366e-06, 'epoch': 6.285097192224622}\n",
      "{'loss': 1.4838, 'grad_norm': 11.5, 'learning_rate': 2.698542409360443e-06, 'epoch': 6.306695464362851}\n",
      "{'loss': 1.5033, 'grad_norm': 14.6875, 'learning_rate': 2.664126615337226e-06, 'epoch': 6.32829373650108}\n",
      "{'loss': 1.4783, 'grad_norm': 25.875, 'learning_rate': 2.6297108213140093e-06, 'epoch': 6.3498920086393085}\n",
      "{'loss': 1.4965, 'grad_norm': 20.75, 'learning_rate': 2.595295027290792e-06, 'epoch': 6.371490280777538}\n",
      "{'loss': 1.4881, 'grad_norm': 13.0625, 'learning_rate': 2.560879233267575e-06, 'epoch': 6.393088552915767}\n",
      "{'loss': 1.4801, 'grad_norm': 16.25, 'learning_rate': 2.5264634392443583e-06, 'epoch': 6.4146868250539955}\n",
      "{'loss': 1.5051, 'grad_norm': 21.75, 'learning_rate': 2.492047645221141e-06, 'epoch': 6.436285097192225}\n",
      "{'loss': 1.4884, 'grad_norm': 11.9375, 'learning_rate': 2.457631851197924e-06, 'epoch': 6.457883369330453}\n",
      "{'loss': 1.4902, 'grad_norm': 12.0625, 'learning_rate': 2.4232160571747074e-06, 'epoch': 6.479481641468682}\n",
      "{'loss': 1.4921, 'grad_norm': 13.625, 'learning_rate': 2.3888002631514903e-06, 'epoch': 6.501079913606912}\n",
      "{'loss': 1.4877, 'grad_norm': 13.625, 'learning_rate': 2.354384469128273e-06, 'epoch': 6.52267818574514}\n",
      "{'loss': 1.5065, 'grad_norm': 9.75, 'learning_rate': 2.3199686751050564e-06, 'epoch': 6.544276457883369}\n",
      "{'loss': 1.4897, 'grad_norm': 13.4375, 'learning_rate': 2.2855528810818393e-06, 'epoch': 6.565874730021598}\n",
      "{'loss': 1.4833, 'grad_norm': 14.5, 'learning_rate': 2.2511370870586226e-06, 'epoch': 6.587473002159827}\n",
      "{'loss': 1.4989, 'grad_norm': 13.5625, 'learning_rate': 2.2167212930354055e-06, 'epoch': 6.609071274298056}\n",
      "{'loss': 1.4841, 'grad_norm': 11.0625, 'learning_rate': 2.1823054990121884e-06, 'epoch': 6.630669546436285}\n",
      "{'loss': 1.4892, 'grad_norm': 29.25, 'learning_rate': 2.1478897049889717e-06, 'epoch': 6.652267818574514}\n",
      "{'loss': 1.4955, 'grad_norm': 15.125, 'learning_rate': 2.1134739109657545e-06, 'epoch': 6.6738660907127425}\n",
      "{'loss': 1.495, 'grad_norm': 20.375, 'learning_rate': 2.0790581169425374e-06, 'epoch': 6.695464362850972}\n",
      "{'loss': 1.4962, 'grad_norm': 15.8125, 'learning_rate': 2.0446423229193207e-06, 'epoch': 6.717062634989201}\n",
      "{'loss': 1.478, 'grad_norm': 19.5, 'learning_rate': 2.0102265288961036e-06, 'epoch': 6.7386609071274295}\n",
      "{'loss': 1.4848, 'grad_norm': 13.1875, 'learning_rate': 1.9758107348728865e-06, 'epoch': 6.760259179265659}\n",
      "{'loss': 1.472, 'grad_norm': 14.0625, 'learning_rate': 1.9413949408496698e-06, 'epoch': 6.781857451403888}\n",
      "{'loss': 1.4819, 'grad_norm': 13.5625, 'learning_rate': 1.9069791468264524e-06, 'epoch': 6.8034557235421165}\n",
      "{'loss': 1.4877, 'grad_norm': 16.5, 'learning_rate': 1.8725633528032357e-06, 'epoch': 6.825053995680346}\n",
      "{'loss': 1.4775, 'grad_norm': 15.1875, 'learning_rate': 1.8381475587800186e-06, 'epoch': 6.846652267818574}\n",
      "{'loss': 1.494, 'grad_norm': 24.875, 'learning_rate': 1.8037317647568015e-06, 'epoch': 6.8682505399568035}\n",
      "{'loss': 1.4581, 'grad_norm': 9.625, 'learning_rate': 1.7693159707335848e-06, 'epoch': 6.889848812095033}\n",
      "{'loss': 1.4878, 'grad_norm': 20.25, 'learning_rate': 1.7349001767103677e-06, 'epoch': 6.911447084233261}\n",
      "{'loss': 1.4887, 'grad_norm': 10.5, 'learning_rate': 1.7004843826871506e-06, 'epoch': 6.93304535637149}\n",
      "{'loss': 1.485, 'grad_norm': 13.125, 'learning_rate': 1.6660685886639339e-06, 'epoch': 6.954643628509719}\n",
      "{'loss': 1.4795, 'grad_norm': 13.75, 'learning_rate': 1.6316527946407167e-06, 'epoch': 6.976241900647948}\n",
      "{'loss': 1.484, 'grad_norm': 10.625, 'learning_rate': 1.5972370006174996e-06, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 1.4890917539596558, 'eval_accuracy': 0.3391642371234208, 'eval_precision': 0.4571478880519268, 'eval_recall': 0.2774988700237486, 'eval_f1': 0.24612313155045057, 'eval_runtime': 8.116, 'eval_samples_per_second': 507.147, 'eval_steps_per_second': 63.455, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.3392 f1=0.2461 p=0.4571 r=0.2775\n",
      "{'loss': 1.4773, 'grad_norm': 11.25, 'learning_rate': 1.562821206594283e-06, 'epoch': 7.019438444924406}\n",
      "{'loss': 1.4893, 'grad_norm': 13.6875, 'learning_rate': 1.5284054125710658e-06, 'epoch': 7.041036717062635}\n",
      "{'loss': 1.4896, 'grad_norm': 13.0, 'learning_rate': 1.4939896185478489e-06, 'epoch': 7.0626349892008635}\n",
      "{'loss': 1.4763, 'grad_norm': 22.5, 'learning_rate': 1.459573824524632e-06, 'epoch': 7.084233261339093}\n",
      "{'loss': 1.5017, 'grad_norm': 17.125, 'learning_rate': 1.425158030501415e-06, 'epoch': 7.105831533477322}\n",
      "{'loss': 1.485, 'grad_norm': 22.625, 'learning_rate': 1.390742236478198e-06, 'epoch': 7.1274298056155505}\n",
      "{'loss': 1.4971, 'grad_norm': 16.25, 'learning_rate': 1.356326442454981e-06, 'epoch': 7.14902807775378}\n",
      "{'loss': 1.4839, 'grad_norm': 11.9375, 'learning_rate': 1.3219106484317641e-06, 'epoch': 7.170626349892009}\n",
      "{'loss': 1.4618, 'grad_norm': 14.3125, 'learning_rate': 1.287494854408547e-06, 'epoch': 7.1922246220302375}\n",
      "{'loss': 1.4795, 'grad_norm': 14.75, 'learning_rate': 1.25307906038533e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 1.4692, 'grad_norm': 15.375, 'learning_rate': 1.2186632663621132e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 1.4828, 'grad_norm': 11.0, 'learning_rate': 1.184247472338896e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 1.4748, 'grad_norm': 14.75, 'learning_rate': 1.1498316783156791e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 1.5095, 'grad_norm': 13.625, 'learning_rate': 1.115415884292462e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 1.4601, 'grad_norm': 11.9375, 'learning_rate': 1.0810000902692451e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 1.4686, 'grad_norm': 19.125, 'learning_rate': 1.0465842962460282e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 1.4944, 'grad_norm': 10.125, 'learning_rate': 1.012168502222811e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 1.4898, 'grad_norm': 11.6875, 'learning_rate': 9.777527081995942e-07, 'epoch': 7.386609071274298}\n",
      "{'loss': 1.4988, 'grad_norm': 11.6875, 'learning_rate': 9.433369141763773e-07, 'epoch': 7.408207343412527}\n",
      "{'loss': 1.4757, 'grad_norm': 13.0625, 'learning_rate': 9.089211201531603e-07, 'epoch': 7.429805615550756}\n",
      "{'loss': 1.4682, 'grad_norm': 13.3125, 'learning_rate': 8.745053261299433e-07, 'epoch': 7.4514038876889845}\n",
      "{'loss': 1.4878, 'grad_norm': 11.25, 'learning_rate': 8.400895321067263e-07, 'epoch': 7.473002159827214}\n",
      "{'loss': 1.4864, 'grad_norm': 16.625, 'learning_rate': 8.056737380835094e-07, 'epoch': 7.494600431965443}\n",
      "{'loss': 1.4775, 'grad_norm': 19.375, 'learning_rate': 7.712579440602923e-07, 'epoch': 7.5161987041036715}\n",
      "{'loss': 1.4725, 'grad_norm': 15.125, 'learning_rate': 7.368421500370754e-07, 'epoch': 7.537796976241901}\n",
      "{'loss': 1.4716, 'grad_norm': 21.125, 'learning_rate': 7.024263560138585e-07, 'epoch': 7.559395248380129}\n",
      "{'loss': 1.4872, 'grad_norm': 13.375, 'learning_rate': 6.680105619906414e-07, 'epoch': 7.5809935205183585}\n",
      "{'loss': 1.4791, 'grad_norm': 12.0, 'learning_rate': 6.335947679674245e-07, 'epoch': 7.602591792656588}\n",
      "{'loss': 1.4919, 'grad_norm': 11.0, 'learning_rate': 5.991789739442075e-07, 'epoch': 7.624190064794816}\n",
      "{'loss': 1.4706, 'grad_norm': 12.8125, 'learning_rate': 5.647631799209906e-07, 'epoch': 7.6457883369330455}\n",
      "{'loss': 1.494, 'grad_norm': 14.25, 'learning_rate': 5.303473858977736e-07, 'epoch': 7.667386609071274}\n",
      "{'loss': 1.4949, 'grad_norm': 14.3125, 'learning_rate': 4.959315918745567e-07, 'epoch': 7.688984881209503}\n",
      "{'loss': 1.4933, 'grad_norm': 24.875, 'learning_rate': 4.6151579785133965e-07, 'epoch': 7.7105831533477325}\n",
      "{'loss': 1.483, 'grad_norm': 17.375, 'learning_rate': 4.2710000382812264e-07, 'epoch': 7.732181425485961}\n",
      "{'loss': 1.4753, 'grad_norm': 9.75, 'learning_rate': 3.9268420980490567e-07, 'epoch': 7.75377969762419}\n",
      "{'loss': 1.4984, 'grad_norm': 17.625, 'learning_rate': 3.582684157816887e-07, 'epoch': 7.775377969762419}\n",
      "{'loss': 1.495, 'grad_norm': 13.9375, 'learning_rate': 3.2385262175847174e-07, 'epoch': 7.796976241900648}\n",
      "{'loss': 1.4838, 'grad_norm': 13.0625, 'learning_rate': 2.8943682773525473e-07, 'epoch': 7.818574514038877}\n",
      "{'loss': 1.4914, 'grad_norm': 15.75, 'learning_rate': 2.5502103371203776e-07, 'epoch': 7.840172786177106}\n",
      "{'loss': 1.5025, 'grad_norm': 15.625, 'learning_rate': 2.206052396888208e-07, 'epoch': 7.861771058315335}\n",
      "{'loss': 1.482, 'grad_norm': 12.875, 'learning_rate': 1.861894456656038e-07, 'epoch': 7.883369330453563}\n",
      "{'loss': 1.4965, 'grad_norm': 10.0625, 'learning_rate': 1.5177365164238685e-07, 'epoch': 7.9049676025917925}\n",
      "{'loss': 1.4959, 'grad_norm': 11.375, 'learning_rate': 1.1735785761916988e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 1.4837, 'grad_norm': 17.125, 'learning_rate': 8.29420635959529e-08, 'epoch': 7.94816414686825}\n",
      "{'loss': 1.4949, 'grad_norm': 16.375, 'learning_rate': 4.852626957273593e-08, 'epoch': 7.9697624190064795}\n",
      "{'loss': 1.4876, 'grad_norm': 12.25, 'learning_rate': 1.4110475549518959e-08, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 1.4881889820098877, 'eval_accuracy': 0.33843537414965985, 'eval_precision': 0.4348872793193303, 'eval_recall': 0.27747837594570546, 'eval_f1': 0.24691389674279435, 'eval_runtime': 7.7502, 'eval_samples_per_second': 531.083, 'eval_steps_per_second': 66.45, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.3384 f1=0.2469 p=0.4349 r=0.2775\n",
      "{'train_runtime': 1728.7481, 'train_samples_per_second': 171.403, 'train_steps_per_second': 21.426, 'train_loss': 1.49803965735384, 'epoch': 8.0}\n",
      "{'eval_loss': 1.4886293411254883, 'eval_accuracy': 0.34207968901846453, 'eval_precision': 0.423023969550225, 'eval_recall': 0.2817489073685374, 'eval_f1': 0.25340324101829476, 'eval_runtime': 8.4055, 'eval_samples_per_second': 489.679, 'eval_steps_per_second': 61.269, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.3421 f1=0.2534 p=0.4230 r=0.2817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñá‚ñÜ‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñá‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÑ‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÑ‚ñÅ‚ñà‚ñà‚ñÜ‚ñÜ‚ñà‚ñÖ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÑ‚ñÅ‚ñà‚ñà‚ñÜ‚ñÜ‚ñà‚ñÖ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÖ‚ñÖ</td></tr><tr><td>train/learning_rate</td><td>‚ñÑ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÇ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.2534</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.34208</td></tr><tr><td>eval/f1</td><td>0.2534</td></tr><tr><td>eval/loss</td><td>1.48863</td></tr><tr><td>eval/precision</td><td>0.42302</td></tr><tr><td>eval/recall</td><td>0.28175</td></tr><tr><td>eval/runtime</td><td>8.4055</td></tr><tr><td>eval/samples_per_second</td><td>489.679</td></tr><tr><td>eval/steps_per_second</td><td>61.269</td></tr><tr><td>total_flos</td><td>1.1916090232884816e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>37040</td></tr><tr><td>train/grad_norm</td><td>12.25</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.4876</td></tr><tr><td>train_loss</td><td>1.49804</td></tr><tr><td>train_runtime</td><td>1728.7481</td></tr><tr><td>train_samples_per_second</td><td>171.403</td></tr><tr><td>train_steps_per_second</td><td>21.426</td></tr><tr><td>trial/accuracy</td><td>0.34208</td></tr><tr><td>trial/f1</td><td>0.2534</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t0</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/6bssnddt' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/6bssnddt</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_214111-6bssnddt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.253403:  10%|‚ñà         | 1/10 [29:02<4:21:25, 1742.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=0 f1=0.2534\n",
      "[I 2025-08-16 22:10:13,407] Trial 0 finished with value: 0.25340324101829476 and parameters: {'lr': 1.1982547005063454e-05, 'weight_decay': 6.101718790027908e-06, 'unfreeze_last_k': 12, 'batch_size': 8}. Best is trial 0 with value: 0.25340324101829476.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_221013-nvrj9dmp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nvrj9dmp' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t1</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nvrj9dmp' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nvrj9dmp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=1 | epochs=8 bs=8 lr=2.86e-05 wd=1.4e-06 k=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[Run] epochs=8 bs=8 lr=2.86e-05 wd=1.4e-06 warmup_ratio=0.06 grad_accum=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e0 b100/4630] loss=1.6398 lr=1.27e-06\n",
      "{'loss': 1.6398, 'grad_norm': 4.0625, 'learning_rate': 1.273024361730603e-06, 'epoch': 0.02159827213822894}\n",
      "[e0 b200/4630] loss=1.6456 lr=2.56e-06\n",
      "{'loss': 1.6456, 'grad_norm': 5.09375, 'learning_rate': 2.558907555397879e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b300/4630] loss=1.6495 lr=3.84e-06\n",
      "{'loss': 1.6495, 'grad_norm': 3.546875, 'learning_rate': 3.844790749065155e-06, 'epoch': 0.06479481641468683}\n",
      "[e0 b400/4630] loss=1.6410 lr=5.13e-06\n",
      "{'loss': 1.641, 'grad_norm': 2.0625, 'learning_rate': 5.1306739427324315e-06, 'epoch': 0.08639308855291576}\n",
      "[e0 b500/4630] loss=1.6399 lr=6.42e-06\n",
      "{'loss': 1.6399, 'grad_norm': 4.375, 'learning_rate': 6.416557136399707e-06, 'epoch': 0.1079913606911447}\n",
      "[e0 b600/4630] loss=1.6351 lr=7.70e-06\n",
      "{'loss': 1.6351, 'grad_norm': 3.34375, 'learning_rate': 7.702440330066983e-06, 'epoch': 0.12958963282937366}\n",
      "[e0 b700/4630] loss=1.6268 lr=8.99e-06\n",
      "{'loss': 1.6268, 'grad_norm': 4.03125, 'learning_rate': 8.98832352373426e-06, 'epoch': 0.1511879049676026}\n",
      "[e0 b800/4630] loss=1.6217 lr=1.03e-05\n",
      "{'loss': 1.6217, 'grad_norm': 3.84375, 'learning_rate': 1.0274206717401536e-05, 'epoch': 0.17278617710583152}\n",
      "[e0 b900/4630] loss=1.6080 lr=1.16e-05\n",
      "{'loss': 1.608, 'grad_norm': 4.96875, 'learning_rate': 1.156008991106881e-05, 'epoch': 0.19438444924406048}\n",
      "[e0 b1000/4630] loss=1.5802 lr=1.28e-05\n",
      "{'loss': 1.5802, 'grad_norm': 10.625, 'learning_rate': 1.2845973104736088e-05, 'epoch': 0.2159827213822894}\n",
      "[e0 b1100/4630] loss=1.5766 lr=1.41e-05\n",
      "{'loss': 1.5766, 'grad_norm': 4.65625, 'learning_rate': 1.4131856298403364e-05, 'epoch': 0.23758099352051837}\n",
      "[e0 b1200/4630] loss=1.5524 lr=1.54e-05\n",
      "{'loss': 1.5524, 'grad_norm': 8.125, 'learning_rate': 1.5417739492070638e-05, 'epoch': 0.2591792656587473}\n",
      "[e0 b1300/4630] loss=1.5436 lr=1.67e-05\n",
      "{'loss': 1.5436, 'grad_norm': 11.6875, 'learning_rate': 1.6703622685737915e-05, 'epoch': 0.28077753779697623}\n",
      "[e0 b1400/4630] loss=1.5441 lr=1.80e-05\n",
      "{'loss': 1.5441, 'grad_norm': 11.5625, 'learning_rate': 1.7989505879405193e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b1500/4630] loss=1.5551 lr=1.93e-05\n",
      "{'loss': 1.5551, 'grad_norm': 7.6875, 'learning_rate': 1.9275389073072467e-05, 'epoch': 0.32397408207343414}\n",
      "[e0 b1600/4630] loss=1.5182 lr=2.06e-05\n",
      "{'loss': 1.5182, 'grad_norm': 10.625, 'learning_rate': 2.0561272266739744e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b1700/4630] loss=1.4953 lr=2.18e-05\n",
      "{'loss': 1.4953, 'grad_norm': 14.5625, 'learning_rate': 2.184715546040702e-05, 'epoch': 0.367170626349892}\n",
      "[e0 b1800/4630] loss=1.5082 lr=2.31e-05\n",
      "{'loss': 1.5082, 'grad_norm': 12.375, 'learning_rate': 2.3133038654074296e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1900/4630] loss=1.4806 lr=2.44e-05\n",
      "{'loss': 1.4806, 'grad_norm': 17.125, 'learning_rate': 2.441892184774157e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b2000/4630] loss=1.4813 lr=2.57e-05\n",
      "{'loss': 1.4813, 'grad_norm': 25.75, 'learning_rate': 2.5704805041408847e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b2100/4630] loss=1.4377 lr=2.70e-05\n",
      "{'loss': 1.4377, 'grad_norm': 16.125, 'learning_rate': 2.6990688235076125e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b2200/4630] loss=1.4356 lr=2.83e-05\n",
      "{'loss': 1.4356, 'grad_norm': 16.5, 'learning_rate': 2.82765714287434e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b2300/4630] loss=1.3945 lr=2.85e-05\n",
      "{'loss': 1.3945, 'grad_norm': 12.5625, 'learning_rate': 2.8522786464470265e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b2400/4630] loss=1.3791 lr=2.84e-05\n",
      "{'loss': 1.3791, 'grad_norm': 15.5, 'learning_rate': 2.8440685239794894e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b2500/4630] loss=1.3402 lr=2.84e-05\n",
      "{'loss': 1.3402, 'grad_norm': 15.8125, 'learning_rate': 2.8358584015119524e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b2600/4630] loss=1.3363 lr=2.83e-05\n",
      "{'loss': 1.3363, 'grad_norm': 22.375, 'learning_rate': 2.8276482790444157e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b2700/4630] loss=1.2920 lr=2.82e-05\n",
      "{'loss': 1.292, 'grad_norm': 36.75, 'learning_rate': 2.8194381565768786e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b2800/4630] loss=1.2980 lr=2.81e-05\n",
      "{'loss': 1.298, 'grad_norm': 31.75, 'learning_rate': 2.8112280341093412e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b2900/4630] loss=1.2895 lr=2.80e-05\n",
      "{'loss': 1.2895, 'grad_norm': 17.75, 'learning_rate': 2.8030179116418045e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b3000/4630] loss=1.2813 lr=2.79e-05\n",
      "{'loss': 1.2813, 'grad_norm': 27.0, 'learning_rate': 2.7948077891742675e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b3100/4630] loss=1.2539 lr=2.79e-05\n",
      "{'loss': 1.2539, 'grad_norm': 16.375, 'learning_rate': 2.7865976667067304e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b3200/4630] loss=1.2421 lr=2.78e-05\n",
      "{'loss': 1.2421, 'grad_norm': 22.875, 'learning_rate': 2.7783875442391937e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b3300/4630] loss=1.2379 lr=2.77e-05\n",
      "{'loss': 1.2379, 'grad_norm': 19.25, 'learning_rate': 2.7701774217716567e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b3400/4630] loss=1.1909 lr=2.76e-05\n",
      "{'loss': 1.1909, 'grad_norm': 39.5, 'learning_rate': 2.7619672993041193e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b3500/4630] loss=1.1937 lr=2.75e-05\n",
      "{'loss': 1.1937, 'grad_norm': 19.625, 'learning_rate': 2.7537571768365826e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b3600/4630] loss=1.2175 lr=2.75e-05\n",
      "{'loss': 1.2175, 'grad_norm': 24.875, 'learning_rate': 2.7455470543690455e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b3700/4630] loss=1.2264 lr=2.74e-05\n",
      "{'loss': 1.2264, 'grad_norm': 23.5, 'learning_rate': 2.7373369319015085e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b3800/4630] loss=1.1479 lr=2.73e-05\n",
      "{'loss': 1.1479, 'grad_norm': 24.125, 'learning_rate': 2.7291268094339717e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b3900/4630] loss=1.2407 lr=2.72e-05\n",
      "{'loss': 1.2407, 'grad_norm': 30.0, 'learning_rate': 2.7209166869664347e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b4000/4630] loss=1.1998 lr=2.71e-05\n",
      "{'loss': 1.1998, 'grad_norm': 21.875, 'learning_rate': 2.7127065644988976e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b4100/4630] loss=1.1746 lr=2.70e-05\n",
      "{'loss': 1.1746, 'grad_norm': 27.375, 'learning_rate': 2.7044964420313606e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b4200/4630] loss=1.1488 lr=2.70e-05\n",
      "{'loss': 1.1488, 'grad_norm': 17.0, 'learning_rate': 2.6962863195638235e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b4300/4630] loss=1.2025 lr=2.69e-05\n",
      "{'loss': 1.2025, 'grad_norm': 18.25, 'learning_rate': 2.6880761970962865e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b4400/4630] loss=1.1341 lr=2.68e-05\n",
      "{'loss': 1.1341, 'grad_norm': 27.125, 'learning_rate': 2.6798660746287498e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b4500/4630] loss=1.1790 lr=2.67e-05\n",
      "{'loss': 1.179, 'grad_norm': 24.5, 'learning_rate': 2.6716559521612127e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b4600/4630] loss=1.1017 lr=2.66e-05\n",
      "{'loss': 1.1017, 'grad_norm': 48.75, 'learning_rate': 2.6634458296936757e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.124897837638855, 'eval_accuracy': 0.5344995140913509, 'eval_precision': 0.5485265175986054, 'eval_recall': 0.557540224142007, 'eval_f1': 0.5498834006601097, 'eval_runtime': 7.6106, 'eval_samples_per_second': 540.825, 'eval_steps_per_second': 67.669, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.5345 f1=0.5499 p=0.5485 r=0.5575\n",
      "{'loss': 1.0824, 'grad_norm': 13.375, 'learning_rate': 2.6552357072261386e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 1.1303, 'grad_norm': 22.375, 'learning_rate': 2.6470255847586016e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.0944, 'grad_norm': 28.0, 'learning_rate': 2.6388154622910645e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 1.1278, 'grad_norm': 25.625, 'learning_rate': 2.6306053398235278e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.094, 'grad_norm': 19.625, 'learning_rate': 2.6223952173559907e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 1.1697, 'grad_norm': 28.5, 'learning_rate': 2.6141850948884537e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.1042, 'grad_norm': 15.5625, 'learning_rate': 2.6059749724209166e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 1.0472, 'grad_norm': 27.75, 'learning_rate': 2.5977648499533796e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.0925, 'grad_norm': 29.5, 'learning_rate': 2.5895547274858425e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 1.0852, 'grad_norm': 27.625, 'learning_rate': 2.5813446050183058e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.0815, 'grad_norm': 26.625, 'learning_rate': 2.5731344825507688e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 1.0725, 'grad_norm': 32.25, 'learning_rate': 2.5649243600832317e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.1202, 'grad_norm': 24.75, 'learning_rate': 2.556714237615695e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 1.0917, 'grad_norm': 22.75, 'learning_rate': 2.5485041151481576e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.1563, 'grad_norm': 19.875, 'learning_rate': 2.5402939926806206e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 1.1054, 'grad_norm': 38.75, 'learning_rate': 2.532083870213084e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.0804, 'grad_norm': 27.875, 'learning_rate': 2.5238737477455468e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 1.0473, 'grad_norm': 27.0, 'learning_rate': 2.5156636252780097e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.1027, 'grad_norm': 10.875, 'learning_rate': 2.507453502810473e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 1.0689, 'grad_norm': 26.0, 'learning_rate': 2.4992433803429356e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.0643, 'grad_norm': 28.375, 'learning_rate': 2.4910332578753986e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 1.063, 'grad_norm': 22.25, 'learning_rate': 2.482823135407862e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.0819, 'grad_norm': 27.875, 'learning_rate': 2.4746130129403248e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 1.0685, 'grad_norm': 12.3125, 'learning_rate': 2.4664028904727878e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.1199, 'grad_norm': 18.75, 'learning_rate': 2.458192768005251e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 1.0989, 'grad_norm': 32.75, 'learning_rate': 2.4499826455377137e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.084, 'grad_norm': 22.375, 'learning_rate': 2.4417725230701766e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 1.0441, 'grad_norm': 13.375, 'learning_rate': 2.43356240060264e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.0289, 'grad_norm': 25.125, 'learning_rate': 2.425352278135103e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 1.1009, 'grad_norm': 20.875, 'learning_rate': 2.4171421556675658e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.06, 'grad_norm': 15.4375, 'learning_rate': 2.408932033200029e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 1.0489, 'grad_norm': 24.5, 'learning_rate': 2.4007219107324917e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.0915, 'grad_norm': 16.25, 'learning_rate': 2.3925117882649547e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 1.0459, 'grad_norm': 24.625, 'learning_rate': 2.384301665797418e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.0518, 'grad_norm': 30.5, 'learning_rate': 2.376091543329881e-05, 'epoch': 1.7494600431965441}\n",
      "{'loss': 1.0269, 'grad_norm': 27.125, 'learning_rate': 2.367881420862344e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.0741, 'grad_norm': 30.75, 'learning_rate': 2.359671298394807e-05, 'epoch': 1.7926565874730023}\n",
      "{'loss': 1.0545, 'grad_norm': 27.875, 'learning_rate': 2.35146117592727e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.0123, 'grad_norm': 23.0, 'learning_rate': 2.3432510534597327e-05, 'epoch': 1.83585313174946}\n",
      "{'loss': 1.0307, 'grad_norm': 18.0, 'learning_rate': 2.335040930992196e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 0.9876, 'grad_norm': 30.75, 'learning_rate': 2.326830808524659e-05, 'epoch': 1.8790496760259179}\n",
      "{'loss': 1.0659, 'grad_norm': 22.875, 'learning_rate': 2.318620686057122e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.0695, 'grad_norm': 34.75, 'learning_rate': 2.310410563589585e-05, 'epoch': 1.9222462203023758}\n",
      "{'loss': 1.0697, 'grad_norm': 40.5, 'learning_rate': 2.302200441122048e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.0183, 'grad_norm': 35.5, 'learning_rate': 2.2939903186545107e-05, 'epoch': 1.9654427645788337}\n",
      "{'loss': 0.9821, 'grad_norm': 37.25, 'learning_rate': 2.285780196186974e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 1.0730704069137573, 'eval_accuracy': 0.5517492711370262, 'eval_precision': 0.5754618056534884, 'eval_recall': 0.584021137074166, 'eval_f1': 0.5644614175445518, 'eval_runtime': 7.7026, 'eval_samples_per_second': 534.366, 'eval_steps_per_second': 66.861, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.5517 f1=0.5645 p=0.5755 r=0.5840\n",
      "{'loss': 1.0706, 'grad_norm': 21.125, 'learning_rate': 2.277570073719437e-05, 'epoch': 2.0086393088552916}\n",
      "{'loss': 1.055, 'grad_norm': 15.6875, 'learning_rate': 2.2693599512519e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.0534, 'grad_norm': 22.625, 'learning_rate': 2.2611498287843632e-05, 'epoch': 2.0518358531317493}\n",
      "{'loss': 1.0086, 'grad_norm': 16.25, 'learning_rate': 2.252939706316826e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.0135, 'grad_norm': 15.375, 'learning_rate': 2.2447295838492887e-05, 'epoch': 2.0950323974082075}\n",
      "{'loss': 1.0295, 'grad_norm': 42.75, 'learning_rate': 2.236519461381752e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.0283, 'grad_norm': 39.5, 'learning_rate': 2.228309338914215e-05, 'epoch': 2.138228941684665}\n",
      "{'loss': 1.0207, 'grad_norm': 40.25, 'learning_rate': 2.220099216446678e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.0366, 'grad_norm': 37.0, 'learning_rate': 2.2118890939791412e-05, 'epoch': 2.1814254859611233}\n",
      "{'loss': 1.005, 'grad_norm': 19.5, 'learning_rate': 2.203678971511604e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.0387, 'grad_norm': 29.75, 'learning_rate': 2.195468849044067e-05, 'epoch': 2.224622030237581}\n",
      "{'loss': 1.0142, 'grad_norm': 18.125, 'learning_rate': 2.18725872657653e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.0388, 'grad_norm': 31.375, 'learning_rate': 2.179048604108993e-05, 'epoch': 2.267818574514039}\n",
      "{'loss': 1.0309, 'grad_norm': 20.75, 'learning_rate': 2.170838481641456e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.0339, 'grad_norm': 30.625, 'learning_rate': 2.1626283591739192e-05, 'epoch': 2.311015118790497}\n",
      "{'loss': 1.0028, 'grad_norm': 34.25, 'learning_rate': 2.1544182367063822e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 0.9909, 'grad_norm': 29.25, 'learning_rate': 2.146208114238845e-05, 'epoch': 2.3542116630669545}\n",
      "{'loss': 0.9907, 'grad_norm': 53.5, 'learning_rate': 2.137997991771308e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.0186, 'grad_norm': 31.625, 'learning_rate': 2.129787869303771e-05, 'epoch': 2.3974082073434126}\n",
      "{'loss': 1.0416, 'grad_norm': 33.75, 'learning_rate': 2.121577746836234e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.0428, 'grad_norm': 20.5, 'learning_rate': 2.1133676243686973e-05, 'epoch': 2.4406047516198703}\n",
      "{'loss': 1.021, 'grad_norm': 28.125, 'learning_rate': 2.1051575019011602e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 0.9777, 'grad_norm': 49.5, 'learning_rate': 2.096947379433623e-05, 'epoch': 2.4838012958963285}\n",
      "{'loss': 1.0318, 'grad_norm': 33.5, 'learning_rate': 2.088737256966086e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 0.9998, 'grad_norm': 13.3125, 'learning_rate': 2.080527134498549e-05, 'epoch': 2.526997840172786}\n",
      "{'loss': 1.0696, 'grad_norm': 22.125, 'learning_rate': 2.072317012031012e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.0441, 'grad_norm': 31.5, 'learning_rate': 2.0641068895634753e-05, 'epoch': 2.570194384449244}\n",
      "{'loss': 0.9963, 'grad_norm': 28.125, 'learning_rate': 2.0558967670959382e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.0235, 'grad_norm': 33.0, 'learning_rate': 2.0476866446284012e-05, 'epoch': 2.613390928725702}\n",
      "{'loss': 0.9928, 'grad_norm': 24.125, 'learning_rate': 2.039476522160864e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 0.9725, 'grad_norm': 30.5, 'learning_rate': 2.031266399693327e-05, 'epoch': 2.6565874730021597}\n",
      "{'loss': 1.0447, 'grad_norm': 20.625, 'learning_rate': 2.0230562772257904e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 0.9899, 'grad_norm': 22.0, 'learning_rate': 2.0148461547582533e-05, 'epoch': 2.699784017278618}\n",
      "{'loss': 1.029, 'grad_norm': 34.0, 'learning_rate': 2.0066360322907163e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 0.9751, 'grad_norm': 33.25, 'learning_rate': 1.9984259098231792e-05, 'epoch': 2.7429805615550755}\n",
      "{'loss': 1.0034, 'grad_norm': 21.875, 'learning_rate': 1.9902157873556425e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 0.9863, 'grad_norm': 41.5, 'learning_rate': 1.982005664888105e-05, 'epoch': 2.786177105831533}\n",
      "{'loss': 0.9946, 'grad_norm': 26.25, 'learning_rate': 1.9737955424205684e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.0575, 'grad_norm': 27.25, 'learning_rate': 1.9655854199530313e-05, 'epoch': 2.8293736501079914}\n",
      "{'loss': 0.9784, 'grad_norm': 28.125, 'learning_rate': 1.9573752974854943e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 0.9821, 'grad_norm': 55.75, 'learning_rate': 1.9491651750179572e-05, 'epoch': 2.8725701943844495}\n",
      "{'loss': 1.0495, 'grad_norm': 43.25, 'learning_rate': 1.9409550525504205e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 0.9987, 'grad_norm': 29.875, 'learning_rate': 1.932744930082883e-05, 'epoch': 2.915766738660907}\n",
      "{'loss': 1.0085, 'grad_norm': 38.0, 'learning_rate': 1.9245348076153464e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.0562, 'grad_norm': 17.875, 'learning_rate': 1.9163246851478094e-05, 'epoch': 2.958963282937365}\n",
      "{'loss': 1.0093, 'grad_norm': 23.125, 'learning_rate': 1.9081145626802723e-05, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 1.0268843173980713, 'eval_accuracy': 0.577259475218659, 'eval_precision': 0.60151654423489, 'eval_recall': 0.601772411389183, 'eval_f1': 0.591546155140336, 'eval_runtime': 8.3896, 'eval_samples_per_second': 490.606, 'eval_steps_per_second': 61.385, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.5773 f1=0.5915 p=0.6015 r=0.6018\n",
      "{'loss': 0.9857, 'grad_norm': 26.875, 'learning_rate': 1.8999044402127356e-05, 'epoch': 3.002159827213823}\n",
      "{'loss': 1.0182, 'grad_norm': 19.75, 'learning_rate': 1.8916943177451986e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 0.9898, 'grad_norm': 19.625, 'learning_rate': 1.883484195277661e-05, 'epoch': 3.0453563714902807}\n",
      "{'loss': 1.0335, 'grad_norm': 40.0, 'learning_rate': 1.8752740728101245e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.0224, 'grad_norm': 34.25, 'learning_rate': 1.8670639503425874e-05, 'epoch': 3.088552915766739}\n",
      "{'loss': 0.9774, 'grad_norm': 33.0, 'learning_rate': 1.8588538278750504e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.0075, 'grad_norm': 22.25, 'learning_rate': 1.8506437054075136e-05, 'epoch': 3.1317494600431965}\n",
      "{'loss': 1.0158, 'grad_norm': 22.625, 'learning_rate': 1.8424335829399766e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 0.9795, 'grad_norm': 27.875, 'learning_rate': 1.8342234604724395e-05, 'epoch': 3.1749460043196542}\n",
      "{'loss': 1.0242, 'grad_norm': 20.625, 'learning_rate': 1.8260133380049025e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.0017, 'grad_norm': 17.75, 'learning_rate': 1.8178032155373654e-05, 'epoch': 3.2181425485961124}\n",
      "{'loss': 0.9587, 'grad_norm': 23.375, 'learning_rate': 1.8095930930698284e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 0.9848, 'grad_norm': 25.0, 'learning_rate': 1.8013829706022917e-05, 'epoch': 3.26133909287257}\n",
      "{'loss': 0.9809, 'grad_norm': 36.5, 'learning_rate': 1.7931728481347546e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.0247, 'grad_norm': 26.125, 'learning_rate': 1.7849627256672176e-05, 'epoch': 3.304535637149028}\n",
      "{'loss': 1.0192, 'grad_norm': 40.0, 'learning_rate': 1.7767526031996805e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.0045, 'grad_norm': 23.25, 'learning_rate': 1.7685424807321435e-05, 'epoch': 3.347732181425486}\n",
      "{'loss': 0.9619, 'grad_norm': 30.125, 'learning_rate': 1.7603323582646064e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.0182, 'grad_norm': 18.125, 'learning_rate': 1.7521222357970697e-05, 'epoch': 3.390928725701944}\n",
      "{'loss': 1.0181, 'grad_norm': 39.75, 'learning_rate': 1.7439121133295326e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.0037, 'grad_norm': 27.5, 'learning_rate': 1.7357019908619956e-05, 'epoch': 3.4341252699784017}\n",
      "{'loss': 1.0157, 'grad_norm': 31.125, 'learning_rate': 1.7274918683944585e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.0, 'grad_norm': 27.75, 'learning_rate': 1.7192817459269215e-05, 'epoch': 3.4773218142548594}\n",
      "{'loss': 1.0117, 'grad_norm': 26.875, 'learning_rate': 1.7110716234593844e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.0602, 'grad_norm': 22.75, 'learning_rate': 1.7028615009918477e-05, 'epoch': 3.5205183585313176}\n",
      "{'loss': 1.0224, 'grad_norm': 19.625, 'learning_rate': 1.6946513785243107e-05, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.0403, 'grad_norm': 32.5, 'learning_rate': 1.6864412560567736e-05, 'epoch': 3.5637149028077753}\n",
      "{'loss': 0.9797, 'grad_norm': 48.0, 'learning_rate': 1.6782311335892366e-05, 'epoch': 3.5853131749460045}\n",
      "{'loss': 0.995, 'grad_norm': 29.75, 'learning_rate': 1.6700210111216995e-05, 'epoch': 3.6069114470842334}\n",
      "{'loss': 1.022, 'grad_norm': 31.375, 'learning_rate': 1.6618108886541625e-05, 'epoch': 3.6285097192224622}\n",
      "{'loss': 0.9869, 'grad_norm': 16.0, 'learning_rate': 1.6536007661866258e-05, 'epoch': 3.650107991360691}\n",
      "{'loss': 0.9735, 'grad_norm': 38.75, 'learning_rate': 1.6453906437190887e-05, 'epoch': 3.67170626349892}\n",
      "{'loss': 0.9956, 'grad_norm': 21.875, 'learning_rate': 1.6371805212515516e-05, 'epoch': 3.693304535637149}\n",
      "{'loss': 0.9306, 'grad_norm': 20.875, 'learning_rate': 1.628970398784015e-05, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.0147, 'grad_norm': 30.5, 'learning_rate': 1.6207602763164775e-05, 'epoch': 3.736501079913607}\n",
      "{'loss': 0.9989, 'grad_norm': 42.25, 'learning_rate': 1.6125501538489405e-05, 'epoch': 3.7580993520518358}\n",
      "{'loss': 0.9603, 'grad_norm': 16.875, 'learning_rate': 1.6043400313814038e-05, 'epoch': 3.7796976241900646}\n",
      "{'loss': 1.0311, 'grad_norm': 30.375, 'learning_rate': 1.5961299089138667e-05, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.0474, 'grad_norm': 33.5, 'learning_rate': 1.5879197864463297e-05, 'epoch': 3.8228941684665227}\n",
      "{'loss': 0.9801, 'grad_norm': 22.875, 'learning_rate': 1.579709663978793e-05, 'epoch': 3.8444924406047516}\n",
      "{'loss': 0.9773, 'grad_norm': 32.5, 'learning_rate': 1.5714995415112556e-05, 'epoch': 3.8660907127429804}\n",
      "{'loss': 1.0098, 'grad_norm': 22.25, 'learning_rate': 1.5632894190437185e-05, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.019, 'grad_norm': 21.0, 'learning_rate': 1.5550792965761818e-05, 'epoch': 3.9092872570194386}\n",
      "{'loss': 0.9899, 'grad_norm': 22.25, 'learning_rate': 1.5468691741086448e-05, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.0326, 'grad_norm': 23.5, 'learning_rate': 1.5386590516411077e-05, 'epoch': 3.9524838012958963}\n",
      "{'loss': 0.9977, 'grad_norm': 20.625, 'learning_rate': 1.530448929173571e-05, 'epoch': 3.974082073434125}\n",
      "{'loss': 0.9425, 'grad_norm': 27.625, 'learning_rate': 1.5222388067060336e-05, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 1.037163257598877, 'eval_accuracy': 0.5663265306122449, 'eval_precision': 0.5908790796166622, 'eval_recall': 0.5952628538947596, 'eval_f1': 0.5795248708964496, 'eval_runtime': 8.0071, 'eval_samples_per_second': 514.045, 'eval_steps_per_second': 64.318, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.5663 f1=0.5795 p=0.5909 r=0.5953\n",
      "{'loss': 0.9242, 'grad_norm': 24.125, 'learning_rate': 1.5140286842384967e-05, 'epoch': 4.017278617710583}\n",
      "{'loss': 0.9686, 'grad_norm': 31.375, 'learning_rate': 1.5058185617709597e-05, 'epoch': 4.038876889848812}\n",
      "{'loss': 1.0024, 'grad_norm': 20.375, 'learning_rate': 1.4976084393034228e-05, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.062, 'grad_norm': 39.25, 'learning_rate': 1.4893983168358857e-05, 'epoch': 4.08207343412527}\n",
      "{'loss': 1.0594, 'grad_norm': 25.625, 'learning_rate': 1.4811881943683489e-05, 'epoch': 4.103671706263499}\n",
      "{'loss': 0.998, 'grad_norm': 30.875, 'learning_rate': 1.472978071900812e-05, 'epoch': 4.125269978401728}\n",
      "{'loss': 0.9696, 'grad_norm': 34.5, 'learning_rate': 1.4647679494332747e-05, 'epoch': 4.146868250539957}\n",
      "{'loss': 0.9504, 'grad_norm': 27.75, 'learning_rate': 1.4565578269657377e-05, 'epoch': 4.168466522678186}\n",
      "{'loss': 1.0059, 'grad_norm': 33.75, 'learning_rate': 1.4483477044982008e-05, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.0127, 'grad_norm': 26.625, 'learning_rate': 1.4401375820306638e-05, 'epoch': 4.211663066954643}\n",
      "{'loss': 1.0282, 'grad_norm': 25.875, 'learning_rate': 1.4319274595631269e-05, 'epoch': 4.233261339092873}\n",
      "{'loss': 0.9855, 'grad_norm': 21.375, 'learning_rate': 1.4237173370955898e-05, 'epoch': 4.254859611231102}\n",
      "{'loss': 1.0062, 'grad_norm': 31.0, 'learning_rate': 1.415507214628053e-05, 'epoch': 4.27645788336933}\n",
      "{'loss': 0.9635, 'grad_norm': 25.625, 'learning_rate': 1.4072970921605157e-05, 'epoch': 4.29805615550756}\n",
      "{'loss': 0.9943, 'grad_norm': 39.5, 'learning_rate': 1.3990869696929788e-05, 'epoch': 4.319654427645788}\n",
      "{'loss': 0.9984, 'grad_norm': 41.5, 'learning_rate': 1.390876847225442e-05, 'epoch': 4.341252699784017}\n",
      "{'loss': 0.9644, 'grad_norm': 37.75, 'learning_rate': 1.3826667247579049e-05, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.0473, 'grad_norm': 38.0, 'learning_rate': 1.3744566022903679e-05, 'epoch': 4.384449244060475}\n",
      "{'loss': 0.9901, 'grad_norm': 22.625, 'learning_rate': 1.366246479822831e-05, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.0233, 'grad_norm': 31.875, 'learning_rate': 1.358036357355294e-05, 'epoch': 4.427645788336933}\n",
      "{'loss': 0.9575, 'grad_norm': 16.875, 'learning_rate': 1.3498262348877569e-05, 'epoch': 4.449244060475162}\n",
      "{'loss': 0.9832, 'grad_norm': 21.5, 'learning_rate': 1.34161611242022e-05, 'epoch': 4.470842332613391}\n",
      "{'loss': 0.9841, 'grad_norm': 28.625, 'learning_rate': 1.333405989952683e-05, 'epoch': 4.49244060475162}\n",
      "{'loss': 0.9869, 'grad_norm': 24.5, 'learning_rate': 1.3251958674851459e-05, 'epoch': 4.514038876889849}\n",
      "{'loss': 0.9856, 'grad_norm': 46.75, 'learning_rate': 1.316985745017609e-05, 'epoch': 4.535637149028078}\n",
      "{'loss': 0.9616, 'grad_norm': 19.25, 'learning_rate': 1.308775622550072e-05, 'epoch': 4.557235421166307}\n",
      "{'loss': 0.9464, 'grad_norm': 27.5, 'learning_rate': 1.3005655000825349e-05, 'epoch': 4.578833693304536}\n",
      "{'loss': 0.9898, 'grad_norm': 22.375, 'learning_rate': 1.292355377614998e-05, 'epoch': 4.600431965442764}\n",
      "{'loss': 0.9775, 'grad_norm': 29.0, 'learning_rate': 1.284145255147461e-05, 'epoch': 4.622030237580994}\n",
      "{'loss': 0.9583, 'grad_norm': 43.0, 'learning_rate': 1.2759351326799239e-05, 'epoch': 4.643628509719223}\n",
      "{'loss': 0.9995, 'grad_norm': 26.5, 'learning_rate': 1.267725010212387e-05, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.0071, 'grad_norm': 34.0, 'learning_rate': 1.25951488774485e-05, 'epoch': 4.686825053995681}\n",
      "{'loss': 1.002, 'grad_norm': 26.75, 'learning_rate': 1.251304765277313e-05, 'epoch': 4.708423326133909}\n",
      "{'loss': 0.9817, 'grad_norm': 37.25, 'learning_rate': 1.243094642809776e-05, 'epoch': 4.730021598272138}\n",
      "{'loss': 0.9869, 'grad_norm': 24.5, 'learning_rate': 1.234884520342239e-05, 'epoch': 4.751619870410368}\n",
      "{'loss': 0.995, 'grad_norm': 29.125, 'learning_rate': 1.2266743978747021e-05, 'epoch': 4.773218142548596}\n",
      "{'loss': 0.9923, 'grad_norm': 33.0, 'learning_rate': 1.218464275407165e-05, 'epoch': 4.794816414686825}\n",
      "{'loss': 0.9944, 'grad_norm': 29.75, 'learning_rate': 1.210254152939628e-05, 'epoch': 4.816414686825054}\n",
      "{'loss': 0.9623, 'grad_norm': 28.625, 'learning_rate': 1.2020440304720911e-05, 'epoch': 4.838012958963283}\n",
      "{'loss': 0.9888, 'grad_norm': 29.75, 'learning_rate': 1.193833908004554e-05, 'epoch': 4.859611231101512}\n",
      "{'loss': 0.9795, 'grad_norm': 39.0, 'learning_rate': 1.185623785537017e-05, 'epoch': 4.881209503239741}\n",
      "{'loss': 0.9863, 'grad_norm': 19.5, 'learning_rate': 1.1774136630694801e-05, 'epoch': 4.90280777537797}\n",
      "{'loss': 1.0055, 'grad_norm': 26.875, 'learning_rate': 1.1692035406019431e-05, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.0227, 'grad_norm': 32.0, 'learning_rate': 1.160993418134406e-05, 'epoch': 4.946004319654428}\n",
      "{'loss': 1.0522, 'grad_norm': 24.0, 'learning_rate': 1.1527832956668692e-05, 'epoch': 4.967602591792657}\n",
      "{'loss': 1.0269, 'grad_norm': 37.75, 'learning_rate': 1.1445731731993321e-05, 'epoch': 4.989200863930885}\n",
      "{'eval_loss': 1.0155688524246216, 'eval_accuracy': 0.5816326530612245, 'eval_precision': 0.6016641455421565, 'eval_recall': 0.6052385331870939, 'eval_f1': 0.5954355520238496, 'eval_runtime': 10.3254, 'eval_samples_per_second': 398.627, 'eval_steps_per_second': 49.877, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.5816 f1=0.5954 p=0.6017 r=0.6052\n",
      "{'loss': 0.9582, 'grad_norm': 30.375, 'learning_rate': 1.136363050731795e-05, 'epoch': 5.010799136069115}\n",
      "{'loss': 1.0302, 'grad_norm': 15.5625, 'learning_rate': 1.1281529282642582e-05, 'epoch': 5.032397408207343}\n",
      "{'loss': 0.9947, 'grad_norm': 33.0, 'learning_rate': 1.1199428057967211e-05, 'epoch': 5.053995680345572}\n",
      "{'loss': 0.972, 'grad_norm': 22.0, 'learning_rate': 1.111732683329184e-05, 'epoch': 5.075593952483802}\n",
      "{'loss': 0.9797, 'grad_norm': 24.125, 'learning_rate': 1.1035225608616472e-05, 'epoch': 5.09719222462203}\n",
      "{'loss': 0.9666, 'grad_norm': 24.25, 'learning_rate': 1.0953124383941101e-05, 'epoch': 5.118790496760259}\n",
      "{'loss': 1.0015, 'grad_norm': 36.0, 'learning_rate': 1.087102315926573e-05, 'epoch': 5.140388768898488}\n",
      "{'loss': 0.9418, 'grad_norm': 22.0, 'learning_rate': 1.0788921934590362e-05, 'epoch': 5.161987041036717}\n",
      "{'loss': 1.0126, 'grad_norm': 28.625, 'learning_rate': 1.0706820709914991e-05, 'epoch': 5.183585313174946}\n",
      "{'loss': 1.0013, 'grad_norm': 38.75, 'learning_rate': 1.0624719485239621e-05, 'epoch': 5.205183585313175}\n",
      "{'loss': 0.9801, 'grad_norm': 16.375, 'learning_rate': 1.0542618260564252e-05, 'epoch': 5.226781857451404}\n",
      "{'loss': 0.9805, 'grad_norm': 30.875, 'learning_rate': 1.0460517035888883e-05, 'epoch': 5.248380129589632}\n",
      "{'loss': 0.9544, 'grad_norm': 32.75, 'learning_rate': 1.0378415811213511e-05, 'epoch': 5.269978401727862}\n",
      "{'loss': 0.9697, 'grad_norm': 21.25, 'learning_rate': 1.0296314586538142e-05, 'epoch': 5.291576673866091}\n",
      "{'loss': 0.9431, 'grad_norm': 12.5, 'learning_rate': 1.0214213361862773e-05, 'epoch': 5.313174946004319}\n",
      "{'loss': 0.9906, 'grad_norm': 25.875, 'learning_rate': 1.0132112137187401e-05, 'epoch': 5.334773218142549}\n",
      "{'loss': 0.9968, 'grad_norm': 26.5, 'learning_rate': 1.0050010912512032e-05, 'epoch': 5.356371490280777}\n",
      "{'loss': 0.9406, 'grad_norm': 30.0, 'learning_rate': 9.967909687836664e-06, 'epoch': 5.377969762419006}\n",
      "{'loss': 0.9683, 'grad_norm': 28.25, 'learning_rate': 9.885808463161291e-06, 'epoch': 5.399568034557236}\n",
      "{'loss': 1.0174, 'grad_norm': 35.75, 'learning_rate': 9.803707238485923e-06, 'epoch': 5.421166306695464}\n",
      "{'loss': 0.9649, 'grad_norm': 23.375, 'learning_rate': 9.721606013810554e-06, 'epoch': 5.442764578833693}\n",
      "{'loss': 1.0365, 'grad_norm': 17.25, 'learning_rate': 9.639504789135181e-06, 'epoch': 5.464362850971923}\n",
      "{'loss': 0.9575, 'grad_norm': 37.0, 'learning_rate': 9.557403564459813e-06, 'epoch': 5.485961123110151}\n",
      "{'loss': 0.9624, 'grad_norm': 32.5, 'learning_rate': 9.475302339784444e-06, 'epoch': 5.50755939524838}\n",
      "{'loss': 0.9925, 'grad_norm': 27.5, 'learning_rate': 9.393201115109072e-06, 'epoch': 5.529157667386609}\n",
      "{'loss': 1.0101, 'grad_norm': 28.375, 'learning_rate': 9.311099890433703e-06, 'epoch': 5.550755939524838}\n",
      "{'loss': 0.982, 'grad_norm': 25.75, 'learning_rate': 9.228998665758334e-06, 'epoch': 5.572354211663067}\n",
      "{'loss': 0.9865, 'grad_norm': 53.25, 'learning_rate': 9.146897441082962e-06, 'epoch': 5.593952483801296}\n",
      "{'loss': 0.9693, 'grad_norm': 15.6875, 'learning_rate': 9.064796216407593e-06, 'epoch': 5.615550755939525}\n",
      "{'loss': 1.018, 'grad_norm': 29.875, 'learning_rate': 8.982694991732224e-06, 'epoch': 5.637149028077753}\n",
      "{'loss': 0.9508, 'grad_norm': 31.75, 'learning_rate': 8.900593767056852e-06, 'epoch': 5.658747300215983}\n",
      "{'loss': 1.0093, 'grad_norm': 22.625, 'learning_rate': 8.818492542381483e-06, 'epoch': 5.680345572354212}\n",
      "{'loss': 0.9552, 'grad_norm': 19.5, 'learning_rate': 8.736391317706114e-06, 'epoch': 5.70194384449244}\n",
      "{'loss': 1.0116, 'grad_norm': 19.125, 'learning_rate': 8.654290093030744e-06, 'epoch': 5.72354211663067}\n",
      "{'loss': 0.9567, 'grad_norm': 21.625, 'learning_rate': 8.572188868355373e-06, 'epoch': 5.745140388768899}\n",
      "{'loss': 0.9755, 'grad_norm': 22.75, 'learning_rate': 8.490087643680004e-06, 'epoch': 5.766738660907127}\n",
      "{'loss': 0.9761, 'grad_norm': 23.25, 'learning_rate': 8.407986419004634e-06, 'epoch': 5.788336933045357}\n",
      "{'loss': 1.0165, 'grad_norm': 37.0, 'learning_rate': 8.325885194329263e-06, 'epoch': 5.809935205183585}\n",
      "{'loss': 1.0036, 'grad_norm': 20.0, 'learning_rate': 8.243783969653895e-06, 'epoch': 5.831533477321814}\n",
      "{'loss': 0.9835, 'grad_norm': 19.375, 'learning_rate': 8.161682744978524e-06, 'epoch': 5.853131749460044}\n",
      "{'loss': 0.9793, 'grad_norm': 23.125, 'learning_rate': 8.079581520303154e-06, 'epoch': 5.874730021598272}\n",
      "{'loss': 1.0267, 'grad_norm': 17.75, 'learning_rate': 7.997480295627785e-06, 'epoch': 5.896328293736501}\n",
      "{'loss': 0.9751, 'grad_norm': 20.75, 'learning_rate': 7.915379070952414e-06, 'epoch': 5.91792656587473}\n",
      "{'loss': 1.0289, 'grad_norm': 43.5, 'learning_rate': 7.833277846277044e-06, 'epoch': 5.939524838012959}\n",
      "{'loss': 0.9688, 'grad_norm': 32.25, 'learning_rate': 7.751176621601675e-06, 'epoch': 5.961123110151188}\n",
      "{'loss': 1.0289, 'grad_norm': 24.625, 'learning_rate': 7.669075396926304e-06, 'epoch': 5.982721382289417}\n",
      "{'eval_loss': 1.0207500457763672, 'eval_accuracy': 0.5787172011661808, 'eval_precision': 0.6006939883851876, 'eval_recall': 0.6045651564726396, 'eval_f1': 0.5930172600238156, 'eval_runtime': 10.5992, 'eval_samples_per_second': 388.333, 'eval_steps_per_second': 48.589, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.5787 f1=0.5930 p=0.6007 r=0.6046\n",
      "{'loss': 0.9583, 'grad_norm': 30.5, 'learning_rate': 7.586974172250934e-06, 'epoch': 6.004319654427646}\n",
      "{'loss': 0.9607, 'grad_norm': 25.625, 'learning_rate': 7.504872947575564e-06, 'epoch': 6.025917926565874}\n",
      "{'loss': 0.9689, 'grad_norm': 34.75, 'learning_rate': 7.422771722900195e-06, 'epoch': 6.047516198704104}\n",
      "{'loss': 0.9714, 'grad_norm': 34.5, 'learning_rate': 7.340670498224824e-06, 'epoch': 6.069114470842333}\n",
      "{'loss': 1.0085, 'grad_norm': 19.25, 'learning_rate': 7.258569273549455e-06, 'epoch': 6.090712742980561}\n",
      "{'loss': 0.9327, 'grad_norm': 44.5, 'learning_rate': 7.1764680488740854e-06, 'epoch': 6.112311015118791}\n",
      "{'loss': 0.9651, 'grad_norm': 40.25, 'learning_rate': 7.094366824198715e-06, 'epoch': 6.133909287257019}\n",
      "{'loss': 0.9789, 'grad_norm': 27.875, 'learning_rate': 7.012265599523345e-06, 'epoch': 6.155507559395248}\n",
      "{'loss': 0.9743, 'grad_norm': 37.75, 'learning_rate': 6.930164374847976e-06, 'epoch': 6.177105831533478}\n",
      "{'loss': 0.9939, 'grad_norm': 29.0, 'learning_rate': 6.848063150172605e-06, 'epoch': 6.198704103671706}\n",
      "{'loss': 0.9999, 'grad_norm': 32.25, 'learning_rate': 6.765961925497235e-06, 'epoch': 6.220302375809935}\n",
      "{'loss': 0.9929, 'grad_norm': 31.375, 'learning_rate': 6.683860700821866e-06, 'epoch': 6.241900647948164}\n",
      "{'loss': 0.9591, 'grad_norm': 27.75, 'learning_rate': 6.601759476146495e-06, 'epoch': 6.263498920086393}\n",
      "{'loss': 0.9684, 'grad_norm': 23.875, 'learning_rate': 6.5196582514711255e-06, 'epoch': 6.285097192224622}\n",
      "{'loss': 1.0082, 'grad_norm': 16.0, 'learning_rate': 6.437557026795756e-06, 'epoch': 6.306695464362851}\n",
      "{'loss': 0.9739, 'grad_norm': 24.5, 'learning_rate': 6.355455802120385e-06, 'epoch': 6.32829373650108}\n",
      "{'loss': 0.9911, 'grad_norm': 51.75, 'learning_rate': 6.2733545774450165e-06, 'epoch': 6.3498920086393085}\n",
      "{'loss': 0.9908, 'grad_norm': 34.25, 'learning_rate': 6.191253352769646e-06, 'epoch': 6.371490280777538}\n",
      "{'loss': 1.0048, 'grad_norm': 26.0, 'learning_rate': 6.1091521280942755e-06, 'epoch': 6.393088552915767}\n",
      "{'loss': 0.9716, 'grad_norm': 26.0, 'learning_rate': 6.027050903418907e-06, 'epoch': 6.4146868250539955}\n",
      "{'loss': 0.9755, 'grad_norm': 31.75, 'learning_rate': 5.944949678743536e-06, 'epoch': 6.436285097192225}\n",
      "{'loss': 0.9972, 'grad_norm': 29.75, 'learning_rate': 5.862848454068166e-06, 'epoch': 6.457883369330453}\n",
      "{'loss': 1.0307, 'grad_norm': 22.0, 'learning_rate': 5.780747229392797e-06, 'epoch': 6.479481641468682}\n",
      "{'loss': 0.9967, 'grad_norm': 36.5, 'learning_rate': 5.698646004717426e-06, 'epoch': 6.501079913606912}\n",
      "{'loss': 1.0372, 'grad_norm': 35.0, 'learning_rate': 5.616544780042056e-06, 'epoch': 6.52267818574514}\n",
      "{'loss': 0.9901, 'grad_norm': 16.75, 'learning_rate': 5.534443555366687e-06, 'epoch': 6.544276457883369}\n",
      "{'loss': 0.9792, 'grad_norm': 29.25, 'learning_rate': 5.4523423306913164e-06, 'epoch': 6.565874730021598}\n",
      "{'loss': 0.9934, 'grad_norm': 24.875, 'learning_rate': 5.370241106015947e-06, 'epoch': 6.587473002159827}\n",
      "{'loss': 1.0014, 'grad_norm': 27.0, 'learning_rate': 5.288139881340577e-06, 'epoch': 6.609071274298056}\n",
      "{'loss': 1.0168, 'grad_norm': 34.0, 'learning_rate': 5.2060386566652066e-06, 'epoch': 6.630669546436285}\n",
      "{'loss': 0.9559, 'grad_norm': 36.0, 'learning_rate': 5.123937431989837e-06, 'epoch': 6.652267818574514}\n",
      "{'loss': 1.0117, 'grad_norm': 12.1875, 'learning_rate': 5.041836207314467e-06, 'epoch': 6.6738660907127425}\n",
      "{'loss': 1.0055, 'grad_norm': 31.25, 'learning_rate': 4.959734982639097e-06, 'epoch': 6.695464362850972}\n",
      "{'loss': 1.0184, 'grad_norm': 27.375, 'learning_rate': 4.877633757963727e-06, 'epoch': 6.717062634989201}\n",
      "{'loss': 0.9498, 'grad_norm': 33.0, 'learning_rate': 4.795532533288357e-06, 'epoch': 6.7386609071274295}\n",
      "{'loss': 1.0082, 'grad_norm': 20.0, 'learning_rate': 4.713431308612987e-06, 'epoch': 6.760259179265659}\n",
      "{'loss': 0.9866, 'grad_norm': 29.625, 'learning_rate': 4.631330083937617e-06, 'epoch': 6.781857451403888}\n",
      "{'loss': 0.9526, 'grad_norm': 35.0, 'learning_rate': 4.5492288592622475e-06, 'epoch': 6.8034557235421165}\n",
      "{'loss': 1.0164, 'grad_norm': 23.625, 'learning_rate': 4.467127634586878e-06, 'epoch': 6.825053995680346}\n",
      "{'loss': 0.9714, 'grad_norm': 25.25, 'learning_rate': 4.385026409911507e-06, 'epoch': 6.846652267818574}\n",
      "{'loss': 0.9999, 'grad_norm': 26.25, 'learning_rate': 4.302925185236138e-06, 'epoch': 6.8682505399568035}\n",
      "{'loss': 0.9858, 'grad_norm': 28.875, 'learning_rate': 4.220823960560768e-06, 'epoch': 6.889848812095033}\n",
      "{'loss': 0.9899, 'grad_norm': 25.5, 'learning_rate': 4.1387227358853975e-06, 'epoch': 6.911447084233261}\n",
      "{'loss': 0.9883, 'grad_norm': 22.125, 'learning_rate': 4.056621511210028e-06, 'epoch': 6.93304535637149}\n",
      "{'loss': 0.9488, 'grad_norm': 18.75, 'learning_rate': 3.974520286534658e-06, 'epoch': 6.954643628509719}\n",
      "{'loss': 0.9518, 'grad_norm': 45.0, 'learning_rate': 3.892419061859288e-06, 'epoch': 6.976241900647948}\n",
      "{'loss': 1.0222, 'grad_norm': 24.875, 'learning_rate': 3.8103178371839175e-06, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 1.0188262462615967, 'eval_accuracy': 0.5796890184645287, 'eval_precision': 0.6014496195306508, 'eval_recall': 0.60477531306159, 'eval_f1': 0.5941974977644134, 'eval_runtime': 10.5517, 'eval_samples_per_second': 390.078, 'eval_steps_per_second': 48.807, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.5797 f1=0.5942 p=0.6014 r=0.6048\n",
      "{'loss': 1.0423, 'grad_norm': 25.5, 'learning_rate': 3.7282166125085483e-06, 'epoch': 7.019438444924406}\n",
      "{'loss': 0.9825, 'grad_norm': 27.625, 'learning_rate': 3.646115387833178e-06, 'epoch': 7.041036717062635}\n",
      "{'loss': 1.0108, 'grad_norm': 35.5, 'learning_rate': 3.564014163157808e-06, 'epoch': 7.0626349892008635}\n",
      "{'loss': 0.977, 'grad_norm': 32.5, 'learning_rate': 3.4819129384824384e-06, 'epoch': 7.084233261339093}\n",
      "{'loss': 0.968, 'grad_norm': 17.75, 'learning_rate': 3.3998117138070687e-06, 'epoch': 7.105831533477322}\n",
      "{'loss': 1.0283, 'grad_norm': 33.25, 'learning_rate': 3.3177104891316986e-06, 'epoch': 7.1274298056155505}\n",
      "{'loss': 0.9533, 'grad_norm': 24.0, 'learning_rate': 3.2356092644563285e-06, 'epoch': 7.14902807775378}\n",
      "{'loss': 0.999, 'grad_norm': 28.5, 'learning_rate': 3.153508039780959e-06, 'epoch': 7.170626349892009}\n",
      "{'loss': 0.9814, 'grad_norm': 20.875, 'learning_rate': 3.0714068151055888e-06, 'epoch': 7.1922246220302375}\n",
      "{'loss': 0.992, 'grad_norm': 24.375, 'learning_rate': 2.9893055904302187e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 0.9833, 'grad_norm': 22.25, 'learning_rate': 2.907204365754849e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 0.95, 'grad_norm': 29.75, 'learning_rate': 2.8251031410794793e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 0.9579, 'grad_norm': 48.75, 'learning_rate': 2.743001916404109e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 1.0054, 'grad_norm': 35.25, 'learning_rate': 2.660900691728739e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 0.9793, 'grad_norm': 34.75, 'learning_rate': 2.5787994670533695e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 0.9471, 'grad_norm': 23.625, 'learning_rate': 2.4966982423779994e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 1.0199, 'grad_norm': 22.0, 'learning_rate': 2.4145970177026293e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 1.002, 'grad_norm': 30.375, 'learning_rate': 2.3324957930272596e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 0.937, 'grad_norm': 34.75, 'learning_rate': 2.2503945683518895e-06, 'epoch': 7.408207343412527}\n",
      "{'loss': 1.0135, 'grad_norm': 22.375, 'learning_rate': 2.16829334367652e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 0.9623, 'grad_norm': 40.5, 'learning_rate': 2.0861921190011498e-06, 'epoch': 7.4514038876889845}\n",
      "{'loss': 0.9436, 'grad_norm': 34.5, 'learning_rate': 2.0040908943257797e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 0.9801, 'grad_norm': 24.875, 'learning_rate': 1.92198966965041e-06, 'epoch': 7.494600431965443}\n",
      "{'loss': 1.0003, 'grad_norm': 20.75, 'learning_rate': 1.8398884449750397e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 0.9729, 'grad_norm': 28.75, 'learning_rate': 1.75778722029967e-06, 'epoch': 7.537796976241901}\n",
      "{'loss': 1.0107, 'grad_norm': 40.75, 'learning_rate': 1.6756859956243002e-06, 'epoch': 7.559395248380129}\n",
      "{'loss': 0.9436, 'grad_norm': 28.0, 'learning_rate': 1.59358477094893e-06, 'epoch': 7.5809935205183585}\n",
      "{'loss': 0.9954, 'grad_norm': 22.625, 'learning_rate': 1.5114835462735604e-06, 'epoch': 7.602591792656588}\n",
      "{'loss': 0.9924, 'grad_norm': 27.625, 'learning_rate': 1.4293823215981903e-06, 'epoch': 7.624190064794816}\n",
      "{'loss': 0.9826, 'grad_norm': 22.875, 'learning_rate': 1.3472810969228204e-06, 'epoch': 7.6457883369330455}\n",
      "{'loss': 1.0116, 'grad_norm': 30.75, 'learning_rate': 1.2651798722474505e-06, 'epoch': 7.667386609071274}\n",
      "{'loss': 1.0471, 'grad_norm': 25.625, 'learning_rate': 1.1830786475720806e-06, 'epoch': 7.688984881209503}\n",
      "{'loss': 0.9776, 'grad_norm': 21.75, 'learning_rate': 1.1009774228967108e-06, 'epoch': 7.7105831533477325}\n",
      "{'loss': 0.9863, 'grad_norm': 25.875, 'learning_rate': 1.0188761982213407e-06, 'epoch': 7.732181425485961}\n",
      "{'loss': 1.0006, 'grad_norm': 20.25, 'learning_rate': 9.367749735459709e-07, 'epoch': 7.75377969762419}\n",
      "{'loss': 0.9863, 'grad_norm': 32.5, 'learning_rate': 8.546737488706009e-07, 'epoch': 7.775377969762419}\n",
      "{'loss': 0.9765, 'grad_norm': 29.75, 'learning_rate': 7.72572524195231e-07, 'epoch': 7.796976241900648}\n",
      "{'loss': 0.9487, 'grad_norm': 27.625, 'learning_rate': 6.90471299519861e-07, 'epoch': 7.818574514038877}\n",
      "{'loss': 0.9821, 'grad_norm': 24.125, 'learning_rate': 6.08370074844491e-07, 'epoch': 7.840172786177106}\n",
      "{'loss': 0.9713, 'grad_norm': 29.375, 'learning_rate': 5.262688501691212e-07, 'epoch': 7.861771058315335}\n",
      "{'loss': 1.0032, 'grad_norm': 15.875, 'learning_rate': 4.441676254937513e-07, 'epoch': 7.883369330453563}\n",
      "{'loss': 0.9726, 'grad_norm': 24.375, 'learning_rate': 3.6206640081838135e-07, 'epoch': 7.9049676025917925}\n",
      "{'loss': 0.9633, 'grad_norm': 30.125, 'learning_rate': 2.799651761430114e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 0.9662, 'grad_norm': 32.5, 'learning_rate': 1.9786395146764153e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 1.0337, 'grad_norm': 57.75, 'learning_rate': 1.1576272679227158e-07, 'epoch': 7.9697624190064795}\n",
      "{'loss': 0.9987, 'grad_norm': 26.5, 'learning_rate': 3.3661502116901665e-08, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 1.0187244415283203, 'eval_accuracy': 0.5811467444120505, 'eval_precision': 0.6037056315805353, 'eval_recall': 0.6064207060730797, 'eval_f1': 0.5956587069208666, 'eval_runtime': 7.7637, 'eval_samples_per_second': 530.157, 'eval_steps_per_second': 66.334, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.5811 f1=0.5957 p=0.6037 r=0.6064\n",
      "{'train_runtime': 1624.7896, 'train_samples_per_second': 182.369, 'train_steps_per_second': 22.797, 'train_loss': 1.0543746091377142, 'epoch': 8.0}\n",
      "{'eval_loss': 1.0187244415283203, 'eval_accuracy': 0.5811467444120505, 'eval_precision': 0.6037056315805353, 'eval_recall': 0.6064207060730797, 'eval_f1': 0.5956587069208666, 'eval_runtime': 8.0287, 'eval_samples_per_second': 512.658, 'eval_steps_per_second': 64.145, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.5811 f1=0.5957 p=0.6037 r=0.6064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÉ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÖ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÑ‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñá‚ñà‚ñà‚ñÅ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñà‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñà‚ñÜ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñá</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñá‚ñà‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÜ</td></tr><tr><td>train/learning_rate</td><td>‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.59566</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.58115</td></tr><tr><td>eval/f1</td><td>0.59566</td></tr><tr><td>eval/loss</td><td>1.01872</td></tr><tr><td>eval/precision</td><td>0.60371</td></tr><tr><td>eval/recall</td><td>0.60642</td></tr><tr><td>eval/runtime</td><td>8.0287</td></tr><tr><td>eval/samples_per_second</td><td>512.658</td></tr><tr><td>eval/steps_per_second</td><td>64.145</td></tr><tr><td>total_flos</td><td>1.1916090232884816e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>37040</td></tr><tr><td>train/grad_norm</td><td>26.5</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9987</td></tr><tr><td>train_loss</td><td>1.05437</td></tr><tr><td>train_runtime</td><td>1624.7896</td></tr><tr><td>train_samples_per_second</td><td>182.369</td></tr><tr><td>train_steps_per_second</td><td>22.797</td></tr><tr><td>trial/accuracy</td><td>0.58115</td></tr><tr><td>trial/f1</td><td>0.59566</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t1</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nvrj9dmp' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nvrj9dmp</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_221013-nvrj9dmp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.595659:  20%|‚ñà‚ñà        | 2/10 [56:20<3:44:06, 1680.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=1 f1=0.5957\n",
      "[I 2025-08-16 22:37:30,722] Trial 1 finished with value: 0.5956587069208666 and parameters: {'lr': 2.8585183395223546e-05, 'weight_decay': 1.4467147485609183e-06, 'unfreeze_last_k': 9, 'batch_size': 8}. Best is trial 1 with value: 0.5956587069208666.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_223730-5gaq6qrv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/5gaq6qrv' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t2</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/5gaq6qrv' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/5gaq6qrv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=2 | epochs=8 bs=32 lr=9.03e-05 wd=2.3e-06 k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[Run] epochs=8 bs=32 lr=9.03e-05 wd=2.3e-06 warmup_ratio=0.06 grad_accum=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e0 b100/1158] loss=1.6382 lr=1.61e-05\n",
      "{'loss': 1.6382, 'grad_norm': 1.5078125, 'learning_rate': 1.6086790006477026e-05, 'epoch': 0.08635578583765112}\n",
      "[e0 b200/1158] loss=1.5846 lr=3.23e-05\n",
      "{'loss': 1.5846, 'grad_norm': 4.09375, 'learning_rate': 3.233607284130231e-05, 'epoch': 0.17271157167530224}\n",
      "[e0 b300/1158] loss=1.4865 lr=4.86e-05\n",
      "{'loss': 1.4865, 'grad_norm': 7.0, 'learning_rate': 4.8585355676127585e-05, 'epoch': 0.25906735751295334}\n",
      "[e0 b400/1158] loss=1.3073 lr=6.48e-05\n",
      "{'loss': 1.3073, 'grad_norm': 12.3125, 'learning_rate': 6.483463851095286e-05, 'epoch': 0.3454231433506045}\n",
      "[e0 b500/1158] loss=1.1409 lr=8.11e-05\n",
      "{'loss': 1.1409, 'grad_norm': 17.375, 'learning_rate': 8.108392134577814e-05, 'epoch': 0.4317789291882556}\n",
      "[e0 b600/1158] loss=0.9982 lr=8.99e-05\n",
      "{'loss': 0.9982, 'grad_norm': 11.9375, 'learning_rate': 8.989988503060535e-05, 'epoch': 0.5181347150259067}\n",
      "[e0 b700/1158] loss=0.9290 lr=8.89e-05\n",
      "{'loss': 0.929, 'grad_norm': 13.375, 'learning_rate': 8.88623791445049e-05, 'epoch': 0.6044905008635578}\n",
      "[e0 b800/1158] loss=0.8847 lr=8.78e-05\n",
      "{'loss': 0.8847, 'grad_norm': 12.875, 'learning_rate': 8.782487325840442e-05, 'epoch': 0.690846286701209}\n",
      "[e0 b900/1158] loss=0.8429 lr=8.68e-05\n",
      "{'loss': 0.8429, 'grad_norm': 13.5625, 'learning_rate': 8.678736737230396e-05, 'epoch': 0.7772020725388601}\n",
      "[e0 b1000/1158] loss=0.7780 lr=8.57e-05\n",
      "{'loss': 0.778, 'grad_norm': 14.125, 'learning_rate': 8.57498614862035e-05, 'epoch': 0.8635578583765112}\n",
      "[e0 b1100/1158] loss=0.7190 lr=8.47e-05\n",
      "{'loss': 0.719, 'grad_norm': 13.5625, 'learning_rate': 8.471235560010302e-05, 'epoch': 0.9499136442141624}\n",
      "{'eval_loss': 0.6284911632537842, 'eval_accuracy': 0.7742954324586978, 'eval_precision': 0.7881836477421267, 'eval_recall': 0.7737577319908907, 'eval_f1': 0.7799602049485932, 'eval_runtime': 2.026, 'eval_samples_per_second': 2031.61, 'eval_steps_per_second': 63.673, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.7743 f1=0.7800 p=0.7882 r=0.7738\n",
      "{'loss': 0.6611, 'grad_norm': 8.3125, 'learning_rate': 8.367484971400255e-05, 'epoch': 1.0362694300518134}\n",
      "{'loss': 0.6276, 'grad_norm': 10.75, 'learning_rate': 8.263734382790209e-05, 'epoch': 1.1226252158894645}\n",
      "{'loss': 0.6118, 'grad_norm': 11.4375, 'learning_rate': 8.159983794180163e-05, 'epoch': 1.2089810017271156}\n",
      "{'loss': 0.6155, 'grad_norm': 6.0, 'learning_rate': 8.056233205570116e-05, 'epoch': 1.2953367875647668}\n",
      "{'loss': 0.6118, 'grad_norm': 14.75, 'learning_rate': 7.95248261696007e-05, 'epoch': 1.381692573402418}\n",
      "{'loss': 0.5891, 'grad_norm': 6.6875, 'learning_rate': 7.848732028350022e-05, 'epoch': 1.468048359240069}\n",
      "{'loss': 0.5355, 'grad_norm': 14.0, 'learning_rate': 7.744981439739976e-05, 'epoch': 1.5544041450777202}\n",
      "{'loss': 0.5632, 'grad_norm': 12.8125, 'learning_rate': 7.64123085112993e-05, 'epoch': 1.6407599309153713}\n",
      "{'loss': 0.536, 'grad_norm': 12.5625, 'learning_rate': 7.537480262519883e-05, 'epoch': 1.7271157167530224}\n",
      "{'loss': 0.5477, 'grad_norm': 9.8125, 'learning_rate': 7.433729673909837e-05, 'epoch': 1.8134715025906736}\n",
      "{'loss': 0.5061, 'grad_norm': 8.375, 'learning_rate': 7.329979085299791e-05, 'epoch': 1.8998272884283247}\n",
      "{'loss': 0.4828, 'grad_norm': 7.28125, 'learning_rate': 7.226228496689743e-05, 'epoch': 1.9861830742659758}\n",
      "{'eval_loss': 0.5496842861175537, 'eval_accuracy': 0.8119533527696793, 'eval_precision': 0.825963481097445, 'eval_recall': 0.8209870051523396, 'eval_f1': 0.8202207195128445, 'eval_runtime': 2.0822, 'eval_samples_per_second': 1976.776, 'eval_steps_per_second': 61.954, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.8120 f1=0.8202 p=0.8260 r=0.8210\n",
      "{'loss': 0.4717, 'grad_norm': 11.75, 'learning_rate': 7.122477908079697e-05, 'epoch': 2.0725388601036268}\n",
      "{'loss': 0.4349, 'grad_norm': 5.6875, 'learning_rate': 7.01872731946965e-05, 'epoch': 2.158894645941278}\n",
      "{'loss': 0.427, 'grad_norm': 13.4375, 'learning_rate': 6.914976730859604e-05, 'epoch': 2.245250431778929}\n",
      "{'loss': 0.4204, 'grad_norm': 6.03125, 'learning_rate': 6.811226142249558e-05, 'epoch': 2.33160621761658}\n",
      "{'loss': 0.4244, 'grad_norm': 10.125, 'learning_rate': 6.707475553639511e-05, 'epoch': 2.4179620034542313}\n",
      "{'loss': 0.4297, 'grad_norm': 36.0, 'learning_rate': 6.603724965029465e-05, 'epoch': 2.5043177892918824}\n",
      "{'loss': 0.4268, 'grad_norm': 9.5, 'learning_rate': 6.499974376419417e-05, 'epoch': 2.5906735751295336}\n",
      "{'loss': 0.4106, 'grad_norm': 7.15625, 'learning_rate': 6.396223787809371e-05, 'epoch': 2.6770293609671847}\n",
      "{'loss': 0.4153, 'grad_norm': 6.90625, 'learning_rate': 6.292473199199325e-05, 'epoch': 2.763385146804836}\n",
      "{'loss': 0.3948, 'grad_norm': 12.3125, 'learning_rate': 6.188722610589278e-05, 'epoch': 2.849740932642487}\n",
      "{'loss': 0.4325, 'grad_norm': 10.4375, 'learning_rate': 6.084972021979231e-05, 'epoch': 2.936096718480138}\n",
      "{'eval_loss': 0.520410418510437, 'eval_accuracy': 0.826773566569485, 'eval_precision': 0.8296910557800994, 'eval_recall': 0.8404426020592902, 'eval_f1': 0.8312273376284104, 'eval_runtime': 2.1903, 'eval_samples_per_second': 1879.159, 'eval_steps_per_second': 58.895, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.8268 f1=0.8312 p=0.8297 r=0.8404\n",
      "{'loss': 0.3873, 'grad_norm': 5.6875, 'learning_rate': 5.981221433369185e-05, 'epoch': 3.0224525043177892}\n",
      "{'loss': 0.3481, 'grad_norm': 10.0, 'learning_rate': 5.877470844759138e-05, 'epoch': 3.1088082901554404}\n",
      "{'loss': 0.3821, 'grad_norm': 5.5, 'learning_rate': 5.7737202561490916e-05, 'epoch': 3.1951640759930915}\n",
      "{'loss': 0.3329, 'grad_norm': 5.75, 'learning_rate': 5.669969667539045e-05, 'epoch': 3.2815198618307426}\n",
      "{'loss': 0.3651, 'grad_norm': 12.75, 'learning_rate': 5.566219078928998e-05, 'epoch': 3.3678756476683938}\n",
      "{'loss': 0.3403, 'grad_norm': 6.53125, 'learning_rate': 5.462468490318952e-05, 'epoch': 3.454231433506045}\n",
      "{'loss': 0.3553, 'grad_norm': 10.5625, 'learning_rate': 5.3587179017089056e-05, 'epoch': 3.540587219343696}\n",
      "{'loss': 0.3429, 'grad_norm': 31.125, 'learning_rate': 5.2549673130988586e-05, 'epoch': 3.626943005181347}\n",
      "{'loss': 0.356, 'grad_norm': 7.75, 'learning_rate': 5.151216724488812e-05, 'epoch': 3.7132987910189983}\n",
      "{'loss': 0.3404, 'grad_norm': 11.375, 'learning_rate': 5.047466135878766e-05, 'epoch': 3.7996545768566494}\n",
      "{'loss': 0.3551, 'grad_norm': 14.4375, 'learning_rate': 4.943715547268719e-05, 'epoch': 3.8860103626943006}\n",
      "{'loss': 0.3532, 'grad_norm': 12.4375, 'learning_rate': 4.8399649586586725e-05, 'epoch': 3.9723661485319517}\n",
      "{'eval_loss': 0.4974514842033386, 'eval_accuracy': 0.8408649173955296, 'eval_precision': 0.8411885915092832, 'eval_recall': 0.85444790273983, 'eval_f1': 0.8451843276751516, 'eval_runtime': 2.111, 'eval_samples_per_second': 1949.759, 'eval_steps_per_second': 61.108, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.8409 f1=0.8452 p=0.8412 r=0.8544\n",
      "{'loss': 0.316, 'grad_norm': 10.125, 'learning_rate': 4.736214370048626e-05, 'epoch': 4.058721934369602}\n",
      "{'loss': 0.3187, 'grad_norm': 13.0625, 'learning_rate': 4.63246378143858e-05, 'epoch': 4.1450777202072535}\n",
      "{'loss': 0.322, 'grad_norm': 5.875, 'learning_rate': 4.528713192828533e-05, 'epoch': 4.231433506044905}\n",
      "{'loss': 0.3233, 'grad_norm': 11.6875, 'learning_rate': 4.4249626042184865e-05, 'epoch': 4.317789291882556}\n",
      "{'loss': 0.3261, 'grad_norm': 11.5, 'learning_rate': 4.3212120156084395e-05, 'epoch': 4.404145077720207}\n",
      "{'loss': 0.3105, 'grad_norm': 9.0625, 'learning_rate': 4.217461426998393e-05, 'epoch': 4.490500863557858}\n",
      "{'loss': 0.2809, 'grad_norm': 5.0, 'learning_rate': 4.113710838388347e-05, 'epoch': 4.576856649395509}\n",
      "{'loss': 0.3159, 'grad_norm': 12.375, 'learning_rate': 4.0099602497783e-05, 'epoch': 4.66321243523316}\n",
      "{'loss': 0.3174, 'grad_norm': 16.125, 'learning_rate': 3.9062096611682535e-05, 'epoch': 4.7495682210708114}\n",
      "{'loss': 0.3249, 'grad_norm': 8.75, 'learning_rate': 3.802459072558207e-05, 'epoch': 4.835924006908463}\n",
      "{'loss': 0.3042, 'grad_norm': 9.1875, 'learning_rate': 3.69870848394816e-05, 'epoch': 4.922279792746114}\n",
      "{'eval_loss': 0.49683141708374023, 'eval_accuracy': 0.8449951409135082, 'eval_precision': 0.8462598103828807, 'eval_recall': 0.8580295072100828, 'eval_f1': 0.8499332757808699, 'eval_runtime': 2.125, 'eval_samples_per_second': 1936.986, 'eval_steps_per_second': 60.707, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.8450 f1=0.8499 p=0.8463 r=0.8580\n",
      "{'loss': 0.3074, 'grad_norm': 12.9375, 'learning_rate': 3.594957895338114e-05, 'epoch': 5.008635578583765}\n",
      "{'loss': 0.2933, 'grad_norm': 11.0, 'learning_rate': 3.4912073067280674e-05, 'epoch': 5.094991364421416}\n",
      "{'loss': 0.2758, 'grad_norm': 11.25, 'learning_rate': 3.3874567181180204e-05, 'epoch': 5.181347150259067}\n",
      "{'loss': 0.2898, 'grad_norm': 3.953125, 'learning_rate': 3.283706129507974e-05, 'epoch': 5.267702936096718}\n",
      "{'loss': 0.284, 'grad_norm': 25.125, 'learning_rate': 3.179955540897928e-05, 'epoch': 5.354058721934369}\n",
      "{'loss': 0.2925, 'grad_norm': 9.5625, 'learning_rate': 3.076204952287881e-05, 'epoch': 5.4404145077720205}\n",
      "{'loss': 0.2708, 'grad_norm': 9.0625, 'learning_rate': 2.9724543636778344e-05, 'epoch': 5.526770293609672}\n",
      "{'loss': 0.3334, 'grad_norm': 8.75, 'learning_rate': 2.8687037750677877e-05, 'epoch': 5.613126079447323}\n",
      "{'loss': 0.2768, 'grad_norm': 13.1875, 'learning_rate': 2.7649531864577414e-05, 'epoch': 5.699481865284974}\n",
      "{'loss': 0.2856, 'grad_norm': 11.9375, 'learning_rate': 2.6612025978476947e-05, 'epoch': 5.785837651122625}\n",
      "{'loss': 0.3023, 'grad_norm': 11.9375, 'learning_rate': 2.557452009237648e-05, 'epoch': 5.872193436960276}\n",
      "{'loss': 0.3043, 'grad_norm': 13.25, 'learning_rate': 2.4537014206276017e-05, 'epoch': 5.958549222797927}\n",
      "{'eval_loss': 0.4912016987800598, 'eval_accuracy': 0.8459669582118562, 'eval_precision': 0.8456806606416114, 'eval_recall': 0.8597869191102223, 'eval_f1': 0.8505428164164466, 'eval_runtime': 2.1043, 'eval_samples_per_second': 1956.012, 'eval_steps_per_second': 61.304, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.8460 f1=0.8505 p=0.8457 r=0.8598\n",
      "{'loss': 0.2711, 'grad_norm': 16.375, 'learning_rate': 2.349950832017555e-05, 'epoch': 6.0449050086355784}\n",
      "{'loss': 0.285, 'grad_norm': 11.375, 'learning_rate': 2.2462002434075083e-05, 'epoch': 6.13126079447323}\n",
      "{'loss': 0.2668, 'grad_norm': 15.0, 'learning_rate': 2.1424496547974616e-05, 'epoch': 6.217616580310881}\n",
      "{'loss': 0.2939, 'grad_norm': 14.4375, 'learning_rate': 2.038699066187415e-05, 'epoch': 6.303972366148532}\n",
      "{'loss': 0.2898, 'grad_norm': 9.5625, 'learning_rate': 1.9349484775773686e-05, 'epoch': 6.390328151986183}\n",
      "{'loss': 0.2666, 'grad_norm': 12.8125, 'learning_rate': 1.8311978889673223e-05, 'epoch': 6.476683937823834}\n",
      "{'loss': 0.2732, 'grad_norm': 10.125, 'learning_rate': 1.7274473003572753e-05, 'epoch': 6.563039723661485}\n",
      "{'loss': 0.2932, 'grad_norm': 24.25, 'learning_rate': 1.623696711747229e-05, 'epoch': 6.649395509499136}\n",
      "{'loss': 0.2719, 'grad_norm': 12.0625, 'learning_rate': 1.5199461231371824e-05, 'epoch': 6.7357512953367875}\n",
      "{'loss': 0.27, 'grad_norm': 10.8125, 'learning_rate': 1.4161955345271356e-05, 'epoch': 6.822107081174439}\n",
      "{'loss': 0.2953, 'grad_norm': 8.375, 'learning_rate': 1.312444945917089e-05, 'epoch': 6.90846286701209}\n",
      "{'loss': 0.2819, 'grad_norm': 10.4375, 'learning_rate': 1.2086943573070426e-05, 'epoch': 6.994818652849741}\n",
      "{'eval_loss': 0.503679633140564, 'eval_accuracy': 0.847910592808552, 'eval_precision': 0.8469157962708607, 'eval_recall': 0.8626540266588641, 'eval_f1': 0.8522570981538073, 'eval_runtime': 2.0285, 'eval_samples_per_second': 2029.081, 'eval_steps_per_second': 63.594, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.8479 f1=0.8523 p=0.8469 r=0.8627\n",
      "{'loss': 0.2793, 'grad_norm': 13.1875, 'learning_rate': 1.104943768696996e-05, 'epoch': 7.081174438687392}\n",
      "{'loss': 0.2742, 'grad_norm': 7.09375, 'learning_rate': 1.0011931800869495e-05, 'epoch': 7.167530224525043}\n",
      "{'loss': 0.2936, 'grad_norm': 8.6875, 'learning_rate': 8.974425914769029e-06, 'epoch': 7.253886010362694}\n",
      "{'loss': 0.2669, 'grad_norm': 15.1875, 'learning_rate': 7.936920028668564e-06, 'epoch': 7.3402417962003454}\n",
      "{'loss': 0.246, 'grad_norm': 9.5625, 'learning_rate': 6.899414142568097e-06, 'epoch': 7.426597582037997}\n",
      "{'loss': 0.2893, 'grad_norm': 9.375, 'learning_rate': 5.861908256467631e-06, 'epoch': 7.512953367875648}\n",
      "{'loss': 0.2769, 'grad_norm': 4.8125, 'learning_rate': 4.824402370367166e-06, 'epoch': 7.599309153713299}\n",
      "{'loss': 0.2803, 'grad_norm': 17.75, 'learning_rate': 3.7868964842667002e-06, 'epoch': 7.68566493955095}\n",
      "{'loss': 0.2607, 'grad_norm': 10.125, 'learning_rate': 2.7493905981662343e-06, 'epoch': 7.772020725388601}\n",
      "{'loss': 0.2656, 'grad_norm': 24.875, 'learning_rate': 1.7118847120657684e-06, 'epoch': 7.858376511226252}\n",
      "{'loss': 0.2794, 'grad_norm': 8.6875, 'learning_rate': 6.743788259653028e-07, 'epoch': 7.944732297063903}\n",
      "{'eval_loss': 0.5006049871444702, 'eval_accuracy': 0.847910592808552, 'eval_precision': 0.8475659769550237, 'eval_recall': 0.8618369885495382, 'eval_f1': 0.8523413656247504, 'eval_runtime': 2.2071, 'eval_samples_per_second': 1864.879, 'eval_steps_per_second': 58.447, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8479 f1=0.8523 p=0.8476 r=0.8618\n",
      "{'train_runtime': 462.9627, 'train_samples_per_second': 640.034, 'train_steps_per_second': 20.01, 'train_loss': 0.4499531924621635, 'epoch': 8.0}\n",
      "{'eval_loss': 0.5006049871444702, 'eval_accuracy': 0.847910592808552, 'eval_precision': 0.8475659769550237, 'eval_recall': 0.8618369885495382, 'eval_f1': 0.8523413656247504, 'eval_runtime': 2.251, 'eval_samples_per_second': 1828.56, 'eval_steps_per_second': 57.309, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8479 f1=0.8523 p=0.8476 r=0.8618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñá‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.85234</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.84791</td></tr><tr><td>eval/f1</td><td>0.85234</td></tr><tr><td>eval/loss</td><td>0.5006</td></tr><tr><td>eval/precision</td><td>0.84757</td></tr><tr><td>eval/recall</td><td>0.86184</td></tr><tr><td>eval/runtime</td><td>2.251</td></tr><tr><td>eval/samples_per_second</td><td>1828.56</td></tr><tr><td>eval/steps_per_second</td><td>57.309</td></tr><tr><td>total_flos</td><td>1.3154490312328992e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>9264</td></tr><tr><td>train/grad_norm</td><td>8.6875</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2794</td></tr><tr><td>train_loss</td><td>0.44995</td></tr><tr><td>train_runtime</td><td>462.9627</td></tr><tr><td>train_samples_per_second</td><td>640.034</td></tr><tr><td>train_steps_per_second</td><td>20.01</td></tr><tr><td>trial/accuracy</td><td>0.84791</td></tr><tr><td>trial/f1</td><td>0.85234</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t2</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/5gaq6qrv' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/5gaq6qrv</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_223730-5gaq6qrv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341:  30%|‚ñà‚ñà‚ñà       | 3/10 [1:04:09<2:11:33, 1127.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=2 f1=0.8523\n",
      "[I 2025-08-16 22:45:20,312] Trial 2 finished with value: 0.8523413656247504 and parameters: {'lr': 9.034601256162855e-05, 'weight_decay': 2.3378864159216274e-06, 'unfreeze_last_k': 12, 'batch_size': 32}. Best is trial 2 with value: 0.8523413656247504.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_224520-aev6dked</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/aev6dked' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t3</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/aev6dked' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/aev6dked</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=3 | epochs=8 bs=8 lr=1.33e-05 wd=3.5e-05 k=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 57,297,413 / 278,813,189 (20.55%) ; unfreeze_last_k=8\n",
      "[Run] epochs=8 bs=8 lr=1.33e-05 wd=3.5e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/4630] loss=1.6396 lr=5.91e-07\n",
      "{'loss': 1.6396, 'grad_norm': 4.0625, 'learning_rate': 5.910886702551045e-07, 'epoch': 0.02159827213822894}\n",
      "[e0 b200/4630] loss=1.6463 lr=1.19e-06\n",
      "{'loss': 1.6463, 'grad_norm': 5.125, 'learning_rate': 1.1881479331390485e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b300/4630] loss=1.6510 lr=1.79e-06\n",
      "{'loss': 1.651, 'grad_norm': 3.578125, 'learning_rate': 1.7852071960229926e-06, 'epoch': 0.06479481641468683}\n",
      "[e0 b400/4630] loss=1.6430 lr=2.38e-06\n",
      "{'loss': 1.643, 'grad_norm': 2.09375, 'learning_rate': 2.3822664589069365e-06, 'epoch': 0.08639308855291576}\n",
      "[e0 b500/4630] loss=1.6438 lr=2.98e-06\n",
      "{'loss': 1.6438, 'grad_norm': 4.4375, 'learning_rate': 2.979325721790881e-06, 'epoch': 0.1079913606911447}\n",
      "[e0 b600/4630] loss=1.6423 lr=3.58e-06\n",
      "{'loss': 1.6423, 'grad_norm': 3.4375, 'learning_rate': 3.5763849846748247e-06, 'epoch': 0.12958963282937366}\n",
      "[e0 b700/4630] loss=1.6381 lr=4.17e-06\n",
      "{'loss': 1.6381, 'grad_norm': 4.0625, 'learning_rate': 4.173444247558769e-06, 'epoch': 0.1511879049676026}\n",
      "[e0 b800/4630] loss=1.6376 lr=4.77e-06\n",
      "{'loss': 1.6376, 'grad_norm': 3.984375, 'learning_rate': 4.770503510442713e-06, 'epoch': 0.17278617710583152}\n",
      "[e0 b900/4630] loss=1.6388 lr=5.37e-06\n",
      "{'loss': 1.6388, 'grad_norm': 2.03125, 'learning_rate': 5.367562773326657e-06, 'epoch': 0.19438444924406048}\n",
      "[e0 b1000/4630] loss=1.6318 lr=5.96e-06\n",
      "{'loss': 1.6318, 'grad_norm': 4.5, 'learning_rate': 5.9646220362106015e-06, 'epoch': 0.2159827213822894}\n",
      "[e0 b1100/4630] loss=1.6273 lr=6.56e-06\n",
      "{'loss': 1.6273, 'grad_norm': 2.015625, 'learning_rate': 6.561681299094545e-06, 'epoch': 0.23758099352051837}\n",
      "[e0 b1200/4630] loss=1.6283 lr=7.16e-06\n",
      "{'loss': 1.6283, 'grad_norm': 4.8125, 'learning_rate': 7.158740561978489e-06, 'epoch': 0.2591792656587473}\n",
      "[e0 b1300/4630] loss=1.6285 lr=7.76e-06\n",
      "{'loss': 1.6285, 'grad_norm': 3.109375, 'learning_rate': 7.755799824862433e-06, 'epoch': 0.28077753779697623}\n",
      "[e0 b1400/4630] loss=1.6240 lr=8.35e-06\n",
      "{'loss': 1.624, 'grad_norm': 3.421875, 'learning_rate': 8.352859087746378e-06, 'epoch': 0.3023758099352052}\n",
      "[e0 b1500/4630] loss=1.6134 lr=8.95e-06\n",
      "{'loss': 1.6134, 'grad_norm': 3.265625, 'learning_rate': 8.949918350630322e-06, 'epoch': 0.32397408207343414}\n",
      "[e0 b1600/4630] loss=1.6086 lr=9.55e-06\n",
      "{'loss': 1.6086, 'grad_norm': 5.40625, 'learning_rate': 9.546977613514265e-06, 'epoch': 0.34557235421166305}\n",
      "[e0 b1700/4630] loss=1.5838 lr=1.01e-05\n",
      "{'loss': 1.5838, 'grad_norm': 8.3125, 'learning_rate': 1.0144036876398209e-05, 'epoch': 0.367170626349892}\n",
      "[e0 b1800/4630] loss=1.5639 lr=1.07e-05\n",
      "{'loss': 1.5639, 'grad_norm': 7.9375, 'learning_rate': 1.0741096139282153e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1900/4630] loss=1.5706 lr=1.13e-05\n",
      "{'loss': 1.5706, 'grad_norm': 8.25, 'learning_rate': 1.1338155402166096e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b2000/4630] loss=1.5802 lr=1.19e-05\n",
      "{'loss': 1.5802, 'grad_norm': 9.0625, 'learning_rate': 1.1935214665050042e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b2100/4630] loss=1.5587 lr=1.25e-05\n",
      "{'loss': 1.5587, 'grad_norm': 8.3125, 'learning_rate': 1.2532273927933985e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b2200/4630] loss=1.5611 lr=1.31e-05\n",
      "{'loss': 1.5611, 'grad_norm': 8.1875, 'learning_rate': 1.3129333190817929e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b2300/4630] loss=1.5470 lr=1.32e-05\n",
      "{'loss': 1.547, 'grad_norm': 10.125, 'learning_rate': 1.3243655369119969e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b2400/4630] loss=1.5597 lr=1.32e-05\n",
      "{'loss': 1.5597, 'grad_norm': 9.0625, 'learning_rate': 1.3205534257554038e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b2500/4630] loss=1.5458 lr=1.32e-05\n",
      "{'loss': 1.5458, 'grad_norm': 11.6875, 'learning_rate': 1.3167413145988108e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b2600/4630] loss=1.5353 lr=1.31e-05\n",
      "{'loss': 1.5353, 'grad_norm': 8.5625, 'learning_rate': 1.3129292034422177e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b2700/4630] loss=1.5388 lr=1.31e-05\n",
      "{'loss': 1.5388, 'grad_norm': 11.875, 'learning_rate': 1.3091170922856247e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b2800/4630] loss=1.5319 lr=1.31e-05\n",
      "{'loss': 1.5319, 'grad_norm': 19.25, 'learning_rate': 1.3053049811290314e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b2900/4630] loss=1.5331 lr=1.30e-05\n",
      "{'loss': 1.5331, 'grad_norm': 9.125, 'learning_rate': 1.3014928699724385e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b3000/4630] loss=1.5361 lr=1.30e-05\n",
      "{'loss': 1.5361, 'grad_norm': 20.0, 'learning_rate': 1.2976807588158454e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b3100/4630] loss=1.5428 lr=1.29e-05\n",
      "{'loss': 1.5428, 'grad_norm': 13.0625, 'learning_rate': 1.2938686476592524e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b3200/4630] loss=1.5014 lr=1.29e-05\n",
      "{'loss': 1.5014, 'grad_norm': 10.75, 'learning_rate': 1.2900565365026593e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b3300/4630] loss=1.5246 lr=1.29e-05\n",
      "{'loss': 1.5246, 'grad_norm': 10.4375, 'learning_rate': 1.2862444253460663e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b3400/4630] loss=1.5335 lr=1.28e-05\n",
      "{'loss': 1.5335, 'grad_norm': 19.25, 'learning_rate': 1.282432314189473e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b3500/4630] loss=1.5113 lr=1.28e-05\n",
      "{'loss': 1.5113, 'grad_norm': 15.1875, 'learning_rate': 1.2786202030328801e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b3600/4630] loss=1.5248 lr=1.27e-05\n",
      "{'loss': 1.5248, 'grad_norm': 20.625, 'learning_rate': 1.274808091876287e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b3700/4630] loss=1.5478 lr=1.27e-05\n",
      "{'loss': 1.5478, 'grad_norm': 19.0, 'learning_rate': 1.270995980719694e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b3800/4630] loss=1.5204 lr=1.27e-05\n",
      "{'loss': 1.5204, 'grad_norm': 13.75, 'learning_rate': 1.2671838695631009e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b3900/4630] loss=1.5147 lr=1.26e-05\n",
      "{'loss': 1.5147, 'grad_norm': 14.4375, 'learning_rate': 1.263371758406508e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b4000/4630] loss=1.5155 lr=1.26e-05\n",
      "{'loss': 1.5155, 'grad_norm': 9.6875, 'learning_rate': 1.2595596472499148e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b4100/4630] loss=1.5117 lr=1.26e-05\n",
      "{'loss': 1.5117, 'grad_norm': 11.0625, 'learning_rate': 1.2557475360933217e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b4200/4630] loss=1.5079 lr=1.25e-05\n",
      "{'loss': 1.5079, 'grad_norm': 12.25, 'learning_rate': 1.2519354249367286e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b4300/4630] loss=1.5214 lr=1.25e-05\n",
      "{'loss': 1.5214, 'grad_norm': 14.5625, 'learning_rate': 1.2481233137801356e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b4400/4630] loss=1.5116 lr=1.24e-05\n",
      "{'loss': 1.5116, 'grad_norm': 17.0, 'learning_rate': 1.2443112026235425e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b4500/4630] loss=1.5089 lr=1.24e-05\n",
      "{'loss': 1.5089, 'grad_norm': 15.9375, 'learning_rate': 1.2404990914669495e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b4600/4630] loss=1.5052 lr=1.24e-05\n",
      "{'loss': 1.5052, 'grad_norm': 11.0625, 'learning_rate': 1.2366869803103564e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.5100951194763184, 'eval_accuracy': 0.32118561710398447, 'eval_precision': 0.2975323077640658, 'eval_recall': 0.25937255147585, 'eval_f1': 0.22275664089737868, 'eval_runtime': 7.7354, 'eval_samples_per_second': 532.102, 'eval_steps_per_second': 66.577, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.3212 f1=0.2228 p=0.2975 r=0.2594\n",
      "{'loss': 1.5018, 'grad_norm': 11.1875, 'learning_rate': 1.2328748691537633e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 1.5036, 'grad_norm': 14.625, 'learning_rate': 1.2290627579971702e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.5089, 'grad_norm': 8.8125, 'learning_rate': 1.2252506468405772e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 1.4887, 'grad_norm': 16.25, 'learning_rate': 1.2214385356839841e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.4984, 'grad_norm': 12.375, 'learning_rate': 1.2176264245273911e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 1.5094, 'grad_norm': 19.375, 'learning_rate': 1.213814313370798e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.4859, 'grad_norm': 14.0, 'learning_rate': 1.2100022022142049e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 1.5018, 'grad_norm': 11.5625, 'learning_rate': 1.2061900910576118e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.484, 'grad_norm': 13.25, 'learning_rate': 1.2023779799010188e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 1.4739, 'grad_norm': 11.1875, 'learning_rate': 1.1985658687444257e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.4928, 'grad_norm': 11.375, 'learning_rate': 1.1947537575878327e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 1.4853, 'grad_norm': 14.9375, 'learning_rate': 1.1909416464312396e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.5134, 'grad_norm': 13.8125, 'learning_rate': 1.1871295352746465e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 1.4831, 'grad_norm': 23.5, 'learning_rate': 1.1833174241180534e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.4855, 'grad_norm': 13.0, 'learning_rate': 1.1795053129614604e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 1.4968, 'grad_norm': 19.125, 'learning_rate': 1.1756932018048673e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.499, 'grad_norm': 13.75, 'learning_rate': 1.1718810906482743e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 1.493, 'grad_norm': 14.375, 'learning_rate': 1.1680689794916812e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.496, 'grad_norm': 9.5625, 'learning_rate': 1.1642568683350881e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 1.4757, 'grad_norm': 12.75, 'learning_rate': 1.160444757178495e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.4839, 'grad_norm': 10.5, 'learning_rate': 1.156632646021902e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 1.4541, 'grad_norm': 13.5, 'learning_rate': 1.1528205348653089e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.5061, 'grad_norm': 20.375, 'learning_rate': 1.149008423708716e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 1.4891, 'grad_norm': 12.4375, 'learning_rate': 1.1451963125521228e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.4971, 'grad_norm': 11.375, 'learning_rate': 1.1413842013955297e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 1.4893, 'grad_norm': 15.0, 'learning_rate': 1.1375720902389366e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.5041, 'grad_norm': 13.9375, 'learning_rate': 1.1337599790823436e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 1.4652, 'grad_norm': 14.1875, 'learning_rate': 1.1299478679257505e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.4724, 'grad_norm': 14.8125, 'learning_rate': 1.1261357567691574e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 1.5012, 'grad_norm': 9.3125, 'learning_rate': 1.1223236456125644e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.4692, 'grad_norm': 14.25, 'learning_rate': 1.1185115344559713e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 1.4684, 'grad_norm': 15.0, 'learning_rate': 1.1146994232993782e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.4909, 'grad_norm': 13.5625, 'learning_rate': 1.1108873121427852e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 1.4802, 'grad_norm': 22.25, 'learning_rate': 1.1070752009861921e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.496, 'grad_norm': 15.625, 'learning_rate': 1.103263089829599e-05, 'epoch': 1.7494600431965441}\n",
      "{'loss': 1.4711, 'grad_norm': 13.625, 'learning_rate': 1.099450978673006e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.4931, 'grad_norm': 11.25, 'learning_rate': 1.0956388675164129e-05, 'epoch': 1.7926565874730023}\n",
      "{'loss': 1.4881, 'grad_norm': 14.0625, 'learning_rate': 1.09182675635982e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.4777, 'grad_norm': 23.625, 'learning_rate': 1.0880146452032268e-05, 'epoch': 1.83585313174946}\n",
      "{'loss': 1.4729, 'grad_norm': 14.3125, 'learning_rate': 1.0842025340466337e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 1.4672, 'grad_norm': 10.9375, 'learning_rate': 1.0803904228900406e-05, 'epoch': 1.8790496760259179}\n",
      "{'loss': 1.4674, 'grad_norm': 15.3125, 'learning_rate': 1.0765783117334476e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.4906, 'grad_norm': 15.375, 'learning_rate': 1.0727662005768545e-05, 'epoch': 1.9222462203023758}\n",
      "{'loss': 1.4974, 'grad_norm': 18.5, 'learning_rate': 1.0689540894202616e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.4813, 'grad_norm': 10.1875, 'learning_rate': 1.0651419782636683e-05, 'epoch': 1.9654427645788337}\n",
      "{'loss': 1.4641, 'grad_norm': 22.375, 'learning_rate': 1.0613298671070753e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 1.481460452079773, 'eval_accuracy': 0.336491739552964, 'eval_precision': 0.3754867061610681, 'eval_recall': 0.28283764035218706, 'eval_f1': 0.2669739631345677, 'eval_runtime': 7.8248, 'eval_samples_per_second': 526.019, 'eval_steps_per_second': 65.816, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.3365 f1=0.2670 p=0.3755 r=0.2828\n",
      "{'loss': 1.5009, 'grad_norm': 14.3125, 'learning_rate': 1.0575177559504822e-05, 'epoch': 2.0086393088552916}\n",
      "{'loss': 1.4674, 'grad_norm': 17.625, 'learning_rate': 1.0537056447938892e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.4847, 'grad_norm': 19.875, 'learning_rate': 1.0498935336372961e-05, 'epoch': 2.0518358531317493}\n",
      "{'loss': 1.4711, 'grad_norm': 27.0, 'learning_rate': 1.0460814224807032e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.4643, 'grad_norm': 19.5, 'learning_rate': 1.0422693113241099e-05, 'epoch': 2.0950323974082075}\n",
      "{'loss': 1.4826, 'grad_norm': 14.3125, 'learning_rate': 1.038457200167517e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.4702, 'grad_norm': 17.375, 'learning_rate': 1.0346450890109238e-05, 'epoch': 2.138228941684665}\n",
      "{'loss': 1.4865, 'grad_norm': 14.25, 'learning_rate': 1.0308329778543308e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.4598, 'grad_norm': 18.875, 'learning_rate': 1.0270208666977377e-05, 'epoch': 2.1814254859611233}\n",
      "{'loss': 1.4676, 'grad_norm': 14.3125, 'learning_rate': 1.0232087555411448e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.4714, 'grad_norm': 13.0, 'learning_rate': 1.0193966443845516e-05, 'epoch': 2.224622030237581}\n",
      "{'loss': 1.4764, 'grad_norm': 16.5, 'learning_rate': 1.0155845332279585e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.4628, 'grad_norm': 13.5, 'learning_rate': 1.0117724220713654e-05, 'epoch': 2.267818574514039}\n",
      "{'loss': 1.465, 'grad_norm': 39.25, 'learning_rate': 1.0079603109147724e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.4751, 'grad_norm': 24.0, 'learning_rate': 1.0041481997581793e-05, 'epoch': 2.311015118790497}\n",
      "{'loss': 1.4561, 'grad_norm': 23.0, 'learning_rate': 1.0003360886015864e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 1.4776, 'grad_norm': 13.875, 'learning_rate': 9.965239774449932e-06, 'epoch': 2.3542116630669545}\n",
      "{'loss': 1.4789, 'grad_norm': 13.375, 'learning_rate': 9.927118662884001e-06, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.4969, 'grad_norm': 15.1875, 'learning_rate': 9.88899755131807e-06, 'epoch': 2.3974082073434126}\n",
      "{'loss': 1.4911, 'grad_norm': 11.6875, 'learning_rate': 9.85087643975214e-06, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.4768, 'grad_norm': 9.8125, 'learning_rate': 9.81275532818621e-06, 'epoch': 2.4406047516198703}\n",
      "{'loss': 1.4592, 'grad_norm': 14.125, 'learning_rate': 9.77463421662028e-06, 'epoch': 2.462203023758099}\n",
      "{'loss': 1.4572, 'grad_norm': 12.875, 'learning_rate': 9.736513105054348e-06, 'epoch': 2.4838012958963285}\n",
      "{'loss': 1.465, 'grad_norm': 13.75, 'learning_rate': 9.698391993488417e-06, 'epoch': 2.5053995680345573}\n",
      "{'loss': 1.4601, 'grad_norm': 28.0, 'learning_rate': 9.660270881922486e-06, 'epoch': 2.526997840172786}\n",
      "{'loss': 1.4706, 'grad_norm': 11.75, 'learning_rate': 9.622149770356556e-06, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.4669, 'grad_norm': 14.0, 'learning_rate': 9.584028658790625e-06, 'epoch': 2.570194384449244}\n",
      "{'loss': 1.4636, 'grad_norm': 13.0625, 'learning_rate': 9.545907547224696e-06, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.4703, 'grad_norm': 10.6875, 'learning_rate': 9.507786435658764e-06, 'epoch': 2.613390928725702}\n",
      "{'loss': 1.4666, 'grad_norm': 26.25, 'learning_rate': 9.469665324092833e-06, 'epoch': 2.634989200863931}\n",
      "{'loss': 1.4788, 'grad_norm': 17.375, 'learning_rate': 9.431544212526902e-06, 'epoch': 2.6565874730021597}\n",
      "{'loss': 1.4803, 'grad_norm': 21.75, 'learning_rate': 9.393423100960973e-06, 'epoch': 2.6781857451403885}\n",
      "{'loss': 1.4856, 'grad_norm': 10.75, 'learning_rate': 9.355301989395041e-06, 'epoch': 2.699784017278618}\n",
      "{'loss': 1.4697, 'grad_norm': 13.625, 'learning_rate': 9.317180877829112e-06, 'epoch': 2.7213822894168467}\n",
      "{'loss': 1.4695, 'grad_norm': 14.5, 'learning_rate': 9.27905976626318e-06, 'epoch': 2.7429805615550755}\n",
      "{'loss': 1.4669, 'grad_norm': 14.6875, 'learning_rate': 9.240938654697251e-06, 'epoch': 2.7645788336933044}\n",
      "{'loss': 1.4685, 'grad_norm': 17.75, 'learning_rate': 9.202817543131318e-06, 'epoch': 2.786177105831533}\n",
      "{'loss': 1.4531, 'grad_norm': 16.25, 'learning_rate': 9.164696431565389e-06, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.4726, 'grad_norm': 10.3125, 'learning_rate': 9.126575319999457e-06, 'epoch': 2.8293736501079914}\n",
      "{'loss': 1.4692, 'grad_norm': 16.0, 'learning_rate': 9.088454208433528e-06, 'epoch': 2.85097192224622}\n",
      "{'loss': 1.4993, 'grad_norm': 27.0, 'learning_rate': 9.050333096867597e-06, 'epoch': 2.8725701943844495}\n",
      "{'loss': 1.4498, 'grad_norm': 25.25, 'learning_rate': 9.012211985301667e-06, 'epoch': 2.8941684665226783}\n",
      "{'loss': 1.4644, 'grad_norm': 13.3125, 'learning_rate': 8.974090873735734e-06, 'epoch': 2.915766738660907}\n",
      "{'loss': 1.4684, 'grad_norm': 13.375, 'learning_rate': 8.935969762169805e-06, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.4689, 'grad_norm': 16.125, 'learning_rate': 8.897848650603873e-06, 'epoch': 2.958963282937365}\n",
      "{'loss': 1.4741, 'grad_norm': 26.125, 'learning_rate': 8.859727539037944e-06, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 1.4733010530471802, 'eval_accuracy': 0.35131195335276966, 'eval_precision': 0.4347942548407665, 'eval_recall': 0.29968996535693054, 'eval_f1': 0.2859408032779366, 'eval_runtime': 7.6881, 'eval_samples_per_second': 535.371, 'eval_steps_per_second': 66.986, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.3513 f1=0.2859 p=0.4348 r=0.2997\n",
      "{'loss': 1.4474, 'grad_norm': 14.1875, 'learning_rate': 8.821606427472013e-06, 'epoch': 3.002159827213823}\n",
      "{'loss': 1.4868, 'grad_norm': 14.125, 'learning_rate': 8.783485315906083e-06, 'epoch': 3.023758099352052}\n",
      "{'loss': 1.455, 'grad_norm': 18.5, 'learning_rate': 8.74536420434015e-06, 'epoch': 3.0453563714902807}\n",
      "{'loss': 1.4834, 'grad_norm': 13.125, 'learning_rate': 8.70724309277422e-06, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.4591, 'grad_norm': 13.0, 'learning_rate': 8.66912198120829e-06, 'epoch': 3.088552915766739}\n",
      "{'loss': 1.4684, 'grad_norm': 11.75, 'learning_rate': 8.63100086964236e-06, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.4842, 'grad_norm': 11.1875, 'learning_rate': 8.592879758076429e-06, 'epoch': 3.1317494600431965}\n",
      "{'loss': 1.4459, 'grad_norm': 14.9375, 'learning_rate': 8.554758646510499e-06, 'epoch': 3.1533477321814254}\n",
      "{'loss': 1.4837, 'grad_norm': 17.625, 'learning_rate': 8.516637534944568e-06, 'epoch': 3.1749460043196542}\n",
      "{'loss': 1.4907, 'grad_norm': 12.0, 'learning_rate': 8.478516423378637e-06, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.4676, 'grad_norm': 15.5625, 'learning_rate': 8.440395311812705e-06, 'epoch': 3.2181425485961124}\n",
      "{'loss': 1.4662, 'grad_norm': 20.5, 'learning_rate': 8.402274200246776e-06, 'epoch': 3.239740820734341}\n",
      "{'loss': 1.4576, 'grad_norm': 12.3125, 'learning_rate': 8.364153088680845e-06, 'epoch': 3.26133909287257}\n",
      "{'loss': 1.4541, 'grad_norm': 11.375, 'learning_rate': 8.326031977114915e-06, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.4893, 'grad_norm': 14.8125, 'learning_rate': 8.287910865548984e-06, 'epoch': 3.304535637149028}\n",
      "{'loss': 1.4603, 'grad_norm': 15.75, 'learning_rate': 8.249789753983053e-06, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.4669, 'grad_norm': 14.9375, 'learning_rate': 8.211668642417121e-06, 'epoch': 3.347732181425486}\n",
      "{'loss': 1.4834, 'grad_norm': 17.125, 'learning_rate': 8.173547530851192e-06, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.4569, 'grad_norm': 15.125, 'learning_rate': 8.13542641928526e-06, 'epoch': 3.390928725701944}\n",
      "{'loss': 1.4854, 'grad_norm': 20.0, 'learning_rate': 8.097305307719331e-06, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.4581, 'grad_norm': 12.0, 'learning_rate': 8.0591841961534e-06, 'epoch': 3.4341252699784017}\n",
      "{'loss': 1.4808, 'grad_norm': 10.75, 'learning_rate': 8.021063084587469e-06, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.4596, 'grad_norm': 15.375, 'learning_rate': 7.982941973021537e-06, 'epoch': 3.4773218142548594}\n",
      "{'loss': 1.4631, 'grad_norm': 12.0625, 'learning_rate': 7.944820861455608e-06, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.4777, 'grad_norm': 11.5, 'learning_rate': 7.906699749889677e-06, 'epoch': 3.5205183585313176}\n",
      "{'loss': 1.4769, 'grad_norm': 15.75, 'learning_rate': 7.868578638323747e-06, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.4561, 'grad_norm': 16.0, 'learning_rate': 7.830457526757816e-06, 'epoch': 3.5637149028077753}\n",
      "{'loss': 1.4647, 'grad_norm': 16.25, 'learning_rate': 7.792336415191885e-06, 'epoch': 3.5853131749460045}\n",
      "{'loss': 1.4765, 'grad_norm': 15.6875, 'learning_rate': 7.754215303625953e-06, 'epoch': 3.6069114470842334}\n",
      "{'loss': 1.4801, 'grad_norm': 20.0, 'learning_rate': 7.716094192060024e-06, 'epoch': 3.6285097192224622}\n",
      "{'loss': 1.4648, 'grad_norm': 15.375, 'learning_rate': 7.677973080494093e-06, 'epoch': 3.650107991360691}\n",
      "{'loss': 1.4527, 'grad_norm': 13.875, 'learning_rate': 7.639851968928163e-06, 'epoch': 3.67170626349892}\n",
      "{'loss': 1.4756, 'grad_norm': 17.0, 'learning_rate': 7.601730857362232e-06, 'epoch': 3.693304535637149}\n",
      "{'loss': 1.4317, 'grad_norm': 20.25, 'learning_rate': 7.563609745796302e-06, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.4601, 'grad_norm': 13.9375, 'learning_rate': 7.5254886342303695e-06, 'epoch': 3.736501079913607}\n",
      "{'loss': 1.4555, 'grad_norm': 15.375, 'learning_rate': 7.487367522664439e-06, 'epoch': 3.7580993520518358}\n",
      "{'loss': 1.4587, 'grad_norm': 12.6875, 'learning_rate': 7.449246411098509e-06, 'epoch': 3.7796976241900646}\n",
      "{'loss': 1.473, 'grad_norm': 19.375, 'learning_rate': 7.411125299532578e-06, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.4864, 'grad_norm': 12.0625, 'learning_rate': 7.373004187966648e-06, 'epoch': 3.8228941684665227}\n",
      "{'loss': 1.4784, 'grad_norm': 13.875, 'learning_rate': 7.334883076400718e-06, 'epoch': 3.8444924406047516}\n",
      "{'loss': 1.4608, 'grad_norm': 11.4375, 'learning_rate': 7.2967619648347855e-06, 'epoch': 3.8660907127429804}\n",
      "{'loss': 1.4743, 'grad_norm': 12.6875, 'learning_rate': 7.258640853268855e-06, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.4655, 'grad_norm': 14.1875, 'learning_rate': 7.220519741702925e-06, 'epoch': 3.9092872570194386}\n",
      "{'loss': 1.4699, 'grad_norm': 15.8125, 'learning_rate': 7.182398630136994e-06, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.4816, 'grad_norm': 10.625, 'learning_rate': 7.144277518571064e-06, 'epoch': 3.9524838012958963}\n",
      "{'loss': 1.4725, 'grad_norm': 26.0, 'learning_rate': 7.106156407005134e-06, 'epoch': 3.974082073434125}\n",
      "{'loss': 1.4565, 'grad_norm': 12.5, 'learning_rate': 7.0680352954392016e-06, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 1.4722360372543335, 'eval_accuracy': 0.3437803692905734, 'eval_precision': 0.37679487462589323, 'eval_recall': 0.2926739701864473, 'eval_f1': 0.2791750007465702, 'eval_runtime': 7.8031, 'eval_samples_per_second': 527.485, 'eval_steps_per_second': 66.0, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.3438 f1=0.2792 p=0.3768 r=0.2927\n",
      "{'loss': 1.4495, 'grad_norm': 14.3125, 'learning_rate': 7.029914183873271e-06, 'epoch': 4.017278617710583}\n",
      "{'loss': 1.4407, 'grad_norm': 14.3125, 'learning_rate': 6.991793072307341e-06, 'epoch': 4.038876889848812}\n",
      "{'loss': 1.47, 'grad_norm': 12.0, 'learning_rate': 6.9536719607414104e-06, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.4741, 'grad_norm': 12.9375, 'learning_rate': 6.91555084917548e-06, 'epoch': 4.08207343412527}\n",
      "{'loss': 1.5027, 'grad_norm': 11.6875, 'learning_rate': 6.87742973760955e-06, 'epoch': 4.103671706263499}\n",
      "{'loss': 1.4459, 'grad_norm': 12.9375, 'learning_rate': 6.839308626043619e-06, 'epoch': 4.125269978401728}\n",
      "{'loss': 1.4772, 'grad_norm': 21.875, 'learning_rate': 6.801187514477687e-06, 'epoch': 4.146868250539957}\n",
      "{'loss': 1.4768, 'grad_norm': 11.9375, 'learning_rate': 6.763066402911757e-06, 'epoch': 4.168466522678186}\n",
      "{'loss': 1.4728, 'grad_norm': 14.1875, 'learning_rate': 6.7249452913458265e-06, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.4748, 'grad_norm': 12.4375, 'learning_rate': 6.686824179779896e-06, 'epoch': 4.211663066954643}\n",
      "{'loss': 1.4589, 'grad_norm': 15.8125, 'learning_rate': 6.648703068213966e-06, 'epoch': 4.233261339092873}\n",
      "{'loss': 1.4514, 'grad_norm': 13.6875, 'learning_rate': 6.6105819566480345e-06, 'epoch': 4.254859611231102}\n",
      "{'loss': 1.4642, 'grad_norm': 16.875, 'learning_rate': 6.572460845082104e-06, 'epoch': 4.27645788336933}\n",
      "{'loss': 1.4601, 'grad_norm': 16.75, 'learning_rate': 6.534339733516173e-06, 'epoch': 4.29805615550756}\n",
      "{'loss': 1.471, 'grad_norm': 15.5625, 'learning_rate': 6.4962186219502425e-06, 'epoch': 4.319654427645788}\n",
      "{'loss': 1.4588, 'grad_norm': 15.375, 'learning_rate': 6.458097510384312e-06, 'epoch': 4.341252699784017}\n",
      "{'loss': 1.4614, 'grad_norm': 14.4375, 'learning_rate': 6.419976398818382e-06, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.4753, 'grad_norm': 14.125, 'learning_rate': 6.3818552872524505e-06, 'epoch': 4.384449244060475}\n",
      "{'loss': 1.462, 'grad_norm': 18.125, 'learning_rate': 6.34373417568652e-06, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.4916, 'grad_norm': 15.9375, 'learning_rate': 6.30561306412059e-06, 'epoch': 4.427645788336933}\n",
      "{'loss': 1.4493, 'grad_norm': 17.375, 'learning_rate': 6.2674919525546585e-06, 'epoch': 4.449244060475162}\n",
      "{'loss': 1.4711, 'grad_norm': 16.25, 'learning_rate': 6.229370840988728e-06, 'epoch': 4.470842332613391}\n",
      "{'loss': 1.458, 'grad_norm': 20.75, 'learning_rate': 6.191249729422798e-06, 'epoch': 4.49244060475162}\n",
      "{'loss': 1.4518, 'grad_norm': 18.0, 'learning_rate': 6.1531286178568665e-06, 'epoch': 4.514038876889849}\n",
      "{'loss': 1.4775, 'grad_norm': 20.375, 'learning_rate': 6.115007506290936e-06, 'epoch': 4.535637149028078}\n",
      "{'loss': 1.4585, 'grad_norm': 11.375, 'learning_rate': 6.076886394725006e-06, 'epoch': 4.557235421166307}\n",
      "{'loss': 1.4679, 'grad_norm': 18.375, 'learning_rate': 6.0387652831590745e-06, 'epoch': 4.578833693304536}\n",
      "{'loss': 1.468, 'grad_norm': 18.25, 'learning_rate': 6.000644171593144e-06, 'epoch': 4.600431965442764}\n",
      "{'loss': 1.4466, 'grad_norm': 12.3125, 'learning_rate': 5.962523060027214e-06, 'epoch': 4.622030237580994}\n",
      "{'loss': 1.4736, 'grad_norm': 13.3125, 'learning_rate': 5.9244019484612826e-06, 'epoch': 4.643628509719223}\n",
      "{'loss': 1.4682, 'grad_norm': 24.625, 'learning_rate': 5.886280836895352e-06, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.4727, 'grad_norm': 15.75, 'learning_rate': 5.848159725329422e-06, 'epoch': 4.686825053995681}\n",
      "{'loss': 1.4566, 'grad_norm': 10.875, 'learning_rate': 5.8100386137634906e-06, 'epoch': 4.708423326133909}\n",
      "{'loss': 1.4509, 'grad_norm': 16.375, 'learning_rate': 5.77191750219756e-06, 'epoch': 4.730021598272138}\n",
      "{'loss': 1.4621, 'grad_norm': 12.3125, 'learning_rate': 5.73379639063163e-06, 'epoch': 4.751619870410368}\n",
      "{'loss': 1.4795, 'grad_norm': 11.5625, 'learning_rate': 5.6956752790656994e-06, 'epoch': 4.773218142548596}\n",
      "{'loss': 1.4737, 'grad_norm': 17.5, 'learning_rate': 5.657554167499768e-06, 'epoch': 4.794816414686825}\n",
      "{'loss': 1.4698, 'grad_norm': 15.4375, 'learning_rate': 5.619433055933838e-06, 'epoch': 4.816414686825054}\n",
      "{'loss': 1.4546, 'grad_norm': 13.5625, 'learning_rate': 5.5813119443679074e-06, 'epoch': 4.838012958963283}\n",
      "{'loss': 1.4721, 'grad_norm': 13.25, 'learning_rate': 5.543190832801976e-06, 'epoch': 4.859611231101512}\n",
      "{'loss': 1.5002, 'grad_norm': 13.4375, 'learning_rate': 5.505069721236046e-06, 'epoch': 4.881209503239741}\n",
      "{'loss': 1.4522, 'grad_norm': 15.5, 'learning_rate': 5.4669486096701155e-06, 'epoch': 4.90280777537797}\n",
      "{'loss': 1.4596, 'grad_norm': 11.0, 'learning_rate': 5.428827498104184e-06, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.4928, 'grad_norm': 20.75, 'learning_rate': 5.390706386538254e-06, 'epoch': 4.946004319654428}\n",
      "{'loss': 1.4712, 'grad_norm': 15.5, 'learning_rate': 5.3525852749723235e-06, 'epoch': 4.967602591792657}\n",
      "{'loss': 1.4799, 'grad_norm': 12.125, 'learning_rate': 5.314464163406392e-06, 'epoch': 4.989200863930885}\n",
      "{'eval_loss': 1.4699398279190063, 'eval_accuracy': 0.35374149659863946, 'eval_precision': 0.4318315224260284, 'eval_recall': 0.3014409015138352, 'eval_f1': 0.2888584099189607, 'eval_runtime': 7.7497, 'eval_samples_per_second': 531.119, 'eval_steps_per_second': 66.454, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.3537 f1=0.2889 p=0.4318 r=0.3014\n",
      "{'loss': 1.4637, 'grad_norm': 14.5625, 'learning_rate': 5.276343051840462e-06, 'epoch': 5.010799136069115}\n",
      "{'loss': 1.4678, 'grad_norm': 15.0625, 'learning_rate': 5.2382219402745315e-06, 'epoch': 5.032397408207343}\n",
      "{'loss': 1.4562, 'grad_norm': 16.5, 'learning_rate': 5.2001008287086e-06, 'epoch': 5.053995680345572}\n",
      "{'loss': 1.4469, 'grad_norm': 11.625, 'learning_rate': 5.16197971714267e-06, 'epoch': 5.075593952483802}\n",
      "{'loss': 1.485, 'grad_norm': 12.0, 'learning_rate': 5.1238586055767395e-06, 'epoch': 5.09719222462203}\n",
      "{'loss': 1.4922, 'grad_norm': 9.75, 'learning_rate': 5.085737494010808e-06, 'epoch': 5.118790496760259}\n",
      "{'loss': 1.4768, 'grad_norm': 21.0, 'learning_rate': 5.047616382444878e-06, 'epoch': 5.140388768898488}\n",
      "{'loss': 1.481, 'grad_norm': 13.5625, 'learning_rate': 5.0094952708789475e-06, 'epoch': 5.161987041036717}\n",
      "{'loss': 1.4647, 'grad_norm': 12.125, 'learning_rate': 4.971374159313016e-06, 'epoch': 5.183585313174946}\n",
      "{'loss': 1.4741, 'grad_norm': 13.4375, 'learning_rate': 4.933253047747086e-06, 'epoch': 5.205183585313175}\n",
      "{'loss': 1.4589, 'grad_norm': 12.875, 'learning_rate': 4.8951319361811555e-06, 'epoch': 5.226781857451404}\n",
      "{'loss': 1.452, 'grad_norm': 27.5, 'learning_rate': 4.857010824615225e-06, 'epoch': 5.248380129589632}\n",
      "{'loss': 1.4458, 'grad_norm': 12.4375, 'learning_rate': 4.818889713049294e-06, 'epoch': 5.269978401727862}\n",
      "{'loss': 1.4663, 'grad_norm': 15.9375, 'learning_rate': 4.7807686014833635e-06, 'epoch': 5.291576673866091}\n",
      "{'loss': 1.4632, 'grad_norm': 15.25, 'learning_rate': 4.742647489917433e-06, 'epoch': 5.313174946004319}\n",
      "{'loss': 1.4913, 'grad_norm': 13.5, 'learning_rate': 4.704526378351502e-06, 'epoch': 5.334773218142549}\n",
      "{'loss': 1.485, 'grad_norm': 16.875, 'learning_rate': 4.6664052667855716e-06, 'epoch': 5.356371490280777}\n",
      "{'loss': 1.449, 'grad_norm': 10.1875, 'learning_rate': 4.628284155219641e-06, 'epoch': 5.377969762419006}\n",
      "{'loss': 1.4575, 'grad_norm': 18.125, 'learning_rate': 4.59016304365371e-06, 'epoch': 5.399568034557236}\n",
      "{'loss': 1.4626, 'grad_norm': 14.5, 'learning_rate': 4.5520419320877796e-06, 'epoch': 5.421166306695464}\n",
      "{'loss': 1.4473, 'grad_norm': 11.8125, 'learning_rate': 4.513920820521849e-06, 'epoch': 5.442764578833693}\n",
      "{'loss': 1.4788, 'grad_norm': 12.375, 'learning_rate': 4.475799708955918e-06, 'epoch': 5.464362850971923}\n",
      "{'loss': 1.452, 'grad_norm': 18.0, 'learning_rate': 4.437678597389988e-06, 'epoch': 5.485961123110151}\n",
      "{'loss': 1.4681, 'grad_norm': 17.25, 'learning_rate': 4.399557485824057e-06, 'epoch': 5.50755939524838}\n",
      "{'loss': 1.4643, 'grad_norm': 16.625, 'learning_rate': 4.361436374258126e-06, 'epoch': 5.529157667386609}\n",
      "{'loss': 1.4487, 'grad_norm': 11.3125, 'learning_rate': 4.323315262692196e-06, 'epoch': 5.550755939524838}\n",
      "{'loss': 1.4743, 'grad_norm': 21.375, 'learning_rate': 4.285194151126265e-06, 'epoch': 5.572354211663067}\n",
      "{'loss': 1.4802, 'grad_norm': 21.375, 'learning_rate': 4.247073039560334e-06, 'epoch': 5.593952483801296}\n",
      "{'loss': 1.4724, 'grad_norm': 13.0625, 'learning_rate': 4.208951927994404e-06, 'epoch': 5.615550755939525}\n",
      "{'loss': 1.4677, 'grad_norm': 16.625, 'learning_rate': 4.170830816428473e-06, 'epoch': 5.637149028077753}\n",
      "{'loss': 1.4587, 'grad_norm': 18.125, 'learning_rate': 4.132709704862542e-06, 'epoch': 5.658747300215983}\n",
      "{'loss': 1.4668, 'grad_norm': 25.5, 'learning_rate': 4.094588593296612e-06, 'epoch': 5.680345572354212}\n",
      "{'loss': 1.4417, 'grad_norm': 12.625, 'learning_rate': 4.056467481730681e-06, 'epoch': 5.70194384449244}\n",
      "{'loss': 1.4723, 'grad_norm': 11.375, 'learning_rate': 4.018346370164751e-06, 'epoch': 5.72354211663067}\n",
      "{'loss': 1.4702, 'grad_norm': 19.0, 'learning_rate': 3.98022525859882e-06, 'epoch': 5.745140388768899}\n",
      "{'loss': 1.4739, 'grad_norm': 16.0, 'learning_rate': 3.942104147032889e-06, 'epoch': 5.766738660907127}\n",
      "{'loss': 1.462, 'grad_norm': 12.8125, 'learning_rate': 3.903983035466959e-06, 'epoch': 5.788336933045357}\n",
      "{'loss': 1.4654, 'grad_norm': 12.375, 'learning_rate': 3.865861923901028e-06, 'epoch': 5.809935205183585}\n",
      "{'loss': 1.4607, 'grad_norm': 11.0625, 'learning_rate': 3.827740812335097e-06, 'epoch': 5.831533477321814}\n",
      "{'loss': 1.4431, 'grad_norm': 13.375, 'learning_rate': 3.7896197007691665e-06, 'epoch': 5.853131749460044}\n",
      "{'loss': 1.4777, 'grad_norm': 12.0, 'learning_rate': 3.7514985892032357e-06, 'epoch': 5.874730021598272}\n",
      "{'loss': 1.4683, 'grad_norm': 14.8125, 'learning_rate': 3.713377477637305e-06, 'epoch': 5.896328293736501}\n",
      "{'loss': 1.4648, 'grad_norm': 13.5625, 'learning_rate': 3.6752563660713745e-06, 'epoch': 5.91792656587473}\n",
      "{'loss': 1.4799, 'grad_norm': 15.0625, 'learning_rate': 3.6371352545054437e-06, 'epoch': 5.939524838012959}\n",
      "{'loss': 1.4789, 'grad_norm': 17.875, 'learning_rate': 3.599014142939513e-06, 'epoch': 5.961123110151188}\n",
      "{'loss': 1.4887, 'grad_norm': 13.125, 'learning_rate': 3.5608930313735825e-06, 'epoch': 5.982721382289417}\n",
      "{'eval_loss': 1.4704368114471436, 'eval_accuracy': 0.35471331389698735, 'eval_precision': 0.4429862255711621, 'eval_recall': 0.301544079246622, 'eval_f1': 0.2887327046039334, 'eval_runtime': 7.9645, 'eval_samples_per_second': 516.791, 'eval_steps_per_second': 64.662, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.3547 f1=0.2887 p=0.4430 r=0.3015\n",
      "{'loss': 1.4663, 'grad_norm': 17.875, 'learning_rate': 3.5227719198076517e-06, 'epoch': 6.004319654427646}\n",
      "{'loss': 1.4577, 'grad_norm': 19.625, 'learning_rate': 3.484650808241721e-06, 'epoch': 6.025917926565874}\n",
      "{'loss': 1.4698, 'grad_norm': 15.6875, 'learning_rate': 3.4465296966757905e-06, 'epoch': 6.047516198704104}\n",
      "{'loss': 1.4332, 'grad_norm': 20.5, 'learning_rate': 3.4084085851098593e-06, 'epoch': 6.069114470842333}\n",
      "{'loss': 1.4534, 'grad_norm': 13.25, 'learning_rate': 3.370287473543929e-06, 'epoch': 6.090712742980561}\n",
      "{'loss': 1.4413, 'grad_norm': 11.25, 'learning_rate': 3.3321663619779985e-06, 'epoch': 6.112311015118791}\n",
      "{'loss': 1.4696, 'grad_norm': 23.875, 'learning_rate': 3.2940452504120677e-06, 'epoch': 6.133909287257019}\n",
      "{'loss': 1.4749, 'grad_norm': 15.75, 'learning_rate': 3.255924138846137e-06, 'epoch': 6.155507559395248}\n",
      "{'loss': 1.4491, 'grad_norm': 16.0, 'learning_rate': 3.2178030272802065e-06, 'epoch': 6.177105831533478}\n",
      "{'loss': 1.4748, 'grad_norm': 25.375, 'learning_rate': 3.1796819157142757e-06, 'epoch': 6.198704103671706}\n",
      "{'loss': 1.4711, 'grad_norm': 16.875, 'learning_rate': 3.141560804148345e-06, 'epoch': 6.220302375809935}\n",
      "{'loss': 1.4643, 'grad_norm': 16.625, 'learning_rate': 3.1034396925824146e-06, 'epoch': 6.241900647948164}\n",
      "{'loss': 1.4673, 'grad_norm': 19.875, 'learning_rate': 3.0653185810164838e-06, 'epoch': 6.263498920086393}\n",
      "{'loss': 1.4296, 'grad_norm': 14.0, 'learning_rate': 3.027197469450553e-06, 'epoch': 6.285097192224622}\n",
      "{'loss': 1.4638, 'grad_norm': 15.625, 'learning_rate': 2.9890763578846226e-06, 'epoch': 6.306695464362851}\n",
      "{'loss': 1.4773, 'grad_norm': 14.5625, 'learning_rate': 2.9509552463186918e-06, 'epoch': 6.32829373650108}\n",
      "{'loss': 1.4716, 'grad_norm': 24.0, 'learning_rate': 2.9128341347527614e-06, 'epoch': 6.3498920086393085}\n",
      "{'loss': 1.4746, 'grad_norm': 19.625, 'learning_rate': 2.8747130231868306e-06, 'epoch': 6.371490280777538}\n",
      "{'loss': 1.4786, 'grad_norm': 14.5, 'learning_rate': 2.8365919116208998e-06, 'epoch': 6.393088552915767}\n",
      "{'loss': 1.4672, 'grad_norm': 11.6875, 'learning_rate': 2.7984708000549694e-06, 'epoch': 6.4146868250539955}\n",
      "{'loss': 1.4911, 'grad_norm': 17.125, 'learning_rate': 2.7603496884890386e-06, 'epoch': 6.436285097192225}\n",
      "{'loss': 1.4573, 'grad_norm': 14.1875, 'learning_rate': 2.722228576923108e-06, 'epoch': 6.457883369330453}\n",
      "{'loss': 1.4746, 'grad_norm': 13.25, 'learning_rate': 2.6841074653571774e-06, 'epoch': 6.479481641468682}\n",
      "{'loss': 1.4675, 'grad_norm': 13.3125, 'learning_rate': 2.6459863537912466e-06, 'epoch': 6.501079913606912}\n",
      "{'loss': 1.4754, 'grad_norm': 16.5, 'learning_rate': 2.607865242225316e-06, 'epoch': 6.52267818574514}\n",
      "{'loss': 1.4869, 'grad_norm': 13.75, 'learning_rate': 2.5697441306593854e-06, 'epoch': 6.544276457883369}\n",
      "{'loss': 1.4745, 'grad_norm': 14.125, 'learning_rate': 2.5316230190934546e-06, 'epoch': 6.565874730021598}\n",
      "{'loss': 1.4713, 'grad_norm': 17.0, 'learning_rate': 2.4935019075275242e-06, 'epoch': 6.587473002159827}\n",
      "{'loss': 1.4819, 'grad_norm': 21.875, 'learning_rate': 2.4553807959615934e-06, 'epoch': 6.609071274298056}\n",
      "{'loss': 1.4514, 'grad_norm': 14.8125, 'learning_rate': 2.4172596843956626e-06, 'epoch': 6.630669546436285}\n",
      "{'loss': 1.4652, 'grad_norm': 27.875, 'learning_rate': 2.3791385728297323e-06, 'epoch': 6.652267818574514}\n",
      "{'loss': 1.4887, 'grad_norm': 15.3125, 'learning_rate': 2.3410174612638015e-06, 'epoch': 6.6738660907127425}\n",
      "{'loss': 1.4786, 'grad_norm': 19.375, 'learning_rate': 2.3028963496978707e-06, 'epoch': 6.695464362850972}\n",
      "{'loss': 1.4857, 'grad_norm': 18.375, 'learning_rate': 2.2647752381319403e-06, 'epoch': 6.717062634989201}\n",
      "{'loss': 1.4564, 'grad_norm': 24.5, 'learning_rate': 2.2266541265660095e-06, 'epoch': 6.7386609071274295}\n",
      "{'loss': 1.4609, 'grad_norm': 11.4375, 'learning_rate': 2.1885330150000787e-06, 'epoch': 6.760259179265659}\n",
      "{'loss': 1.4652, 'grad_norm': 13.6875, 'learning_rate': 2.1504119034341483e-06, 'epoch': 6.781857451403888}\n",
      "{'loss': 1.4584, 'grad_norm': 15.6875, 'learning_rate': 2.1122907918682175e-06, 'epoch': 6.8034557235421165}\n",
      "{'loss': 1.4716, 'grad_norm': 15.5, 'learning_rate': 2.074169680302287e-06, 'epoch': 6.825053995680346}\n",
      "{'loss': 1.4581, 'grad_norm': 13.375, 'learning_rate': 2.0360485687363563e-06, 'epoch': 6.846652267818574}\n",
      "{'loss': 1.4817, 'grad_norm': 19.75, 'learning_rate': 1.9979274571704255e-06, 'epoch': 6.8682505399568035}\n",
      "{'loss': 1.4513, 'grad_norm': 10.6875, 'learning_rate': 1.959806345604495e-06, 'epoch': 6.889848812095033}\n",
      "{'loss': 1.4593, 'grad_norm': 16.625, 'learning_rate': 1.9216852340385643e-06, 'epoch': 6.911447084233261}\n",
      "{'loss': 1.4693, 'grad_norm': 12.3125, 'learning_rate': 1.8835641224726335e-06, 'epoch': 6.93304535637149}\n",
      "{'loss': 1.4579, 'grad_norm': 12.25, 'learning_rate': 1.8454430109067031e-06, 'epoch': 6.954643628509719}\n",
      "{'loss': 1.4604, 'grad_norm': 12.0625, 'learning_rate': 1.8073218993407723e-06, 'epoch': 6.976241900647948}\n",
      "{'loss': 1.4665, 'grad_norm': 19.375, 'learning_rate': 1.7692007877748415e-06, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 1.4717094898223877, 'eval_accuracy': 0.35034013605442177, 'eval_precision': 0.4174105218890709, 'eval_recall': 0.296722907477197, 'eval_f1': 0.28202542814576703, 'eval_runtime': 7.8127, 'eval_samples_per_second': 526.835, 'eval_steps_per_second': 65.918, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.3503 f1=0.2820 p=0.4174 r=0.2967\n",
      "{'loss': 1.4685, 'grad_norm': 10.3125, 'learning_rate': 1.7310796762089112e-06, 'epoch': 7.019438444924406}\n",
      "{'loss': 1.462, 'grad_norm': 12.625, 'learning_rate': 1.6929585646429803e-06, 'epoch': 7.041036717062635}\n",
      "{'loss': 1.4731, 'grad_norm': 13.875, 'learning_rate': 1.6548374530770498e-06, 'epoch': 7.0626349892008635}\n",
      "{'loss': 1.4599, 'grad_norm': 16.0, 'learning_rate': 1.6167163415111192e-06, 'epoch': 7.084233261339093}\n",
      "{'loss': 1.4851, 'grad_norm': 13.4375, 'learning_rate': 1.5785952299451886e-06, 'epoch': 7.105831533477322}\n",
      "{'loss': 1.4631, 'grad_norm': 14.9375, 'learning_rate': 1.5404741183792578e-06, 'epoch': 7.1274298056155505}\n",
      "{'loss': 1.4807, 'grad_norm': 12.75, 'learning_rate': 1.5023530068133272e-06, 'epoch': 7.14902807775378}\n",
      "{'loss': 1.4681, 'grad_norm': 13.5, 'learning_rate': 1.4642318952473964e-06, 'epoch': 7.170626349892009}\n",
      "{'loss': 1.4416, 'grad_norm': 20.0, 'learning_rate': 1.4261107836814658e-06, 'epoch': 7.1922246220302375}\n",
      "{'loss': 1.4615, 'grad_norm': 14.625, 'learning_rate': 1.387989672115535e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 1.4549, 'grad_norm': 25.875, 'learning_rate': 1.3498685605496044e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 1.4646, 'grad_norm': 11.375, 'learning_rate': 1.3117474489836738e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 1.4686, 'grad_norm': 15.1875, 'learning_rate': 1.273626337417743e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 1.5022, 'grad_norm': 13.25, 'learning_rate': 1.2355052258518124e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 1.4493, 'grad_norm': 12.625, 'learning_rate': 1.1973841142858818e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 1.4485, 'grad_norm': 20.625, 'learning_rate': 1.1592630027199512e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 1.4764, 'grad_norm': 9.875, 'learning_rate': 1.1211418911540204e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 1.4679, 'grad_norm': 17.875, 'learning_rate': 1.0830207795880898e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 1.4809, 'grad_norm': 11.1875, 'learning_rate': 1.0448996680221592e-06, 'epoch': 7.408207343412527}\n",
      "{'loss': 1.4542, 'grad_norm': 11.0625, 'learning_rate': 1.0067785564562286e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 1.448, 'grad_norm': 17.125, 'learning_rate': 9.686574448902978e-07, 'epoch': 7.4514038876889845}\n",
      "{'loss': 1.4751, 'grad_norm': 17.625, 'learning_rate': 9.305363333243672e-07, 'epoch': 7.473002159827214}\n",
      "{'loss': 1.4562, 'grad_norm': 19.0, 'learning_rate': 8.924152217584367e-07, 'epoch': 7.494600431965443}\n",
      "{'loss': 1.4611, 'grad_norm': 16.25, 'learning_rate': 8.542941101925059e-07, 'epoch': 7.5161987041036715}\n",
      "{'loss': 1.4514, 'grad_norm': 15.0625, 'learning_rate': 8.161729986265753e-07, 'epoch': 7.537796976241901}\n",
      "{'loss': 1.4616, 'grad_norm': 20.625, 'learning_rate': 7.780518870606447e-07, 'epoch': 7.559395248380129}\n",
      "{'loss': 1.4617, 'grad_norm': 17.0, 'learning_rate': 7.39930775494714e-07, 'epoch': 7.5809935205183585}\n",
      "{'loss': 1.4661, 'grad_norm': 12.0, 'learning_rate': 7.018096639287834e-07, 'epoch': 7.602591792656588}\n",
      "{'loss': 1.4731, 'grad_norm': 12.1875, 'learning_rate': 6.636885523628527e-07, 'epoch': 7.624190064794816}\n",
      "{'loss': 1.4577, 'grad_norm': 14.375, 'learning_rate': 6.255674407969221e-07, 'epoch': 7.6457883369330455}\n",
      "{'loss': 1.4851, 'grad_norm': 11.9375, 'learning_rate': 5.874463292309914e-07, 'epoch': 7.667386609071274}\n",
      "{'loss': 1.4795, 'grad_norm': 11.5, 'learning_rate': 5.493252176650608e-07, 'epoch': 7.688984881209503}\n",
      "{'loss': 1.4799, 'grad_norm': 18.25, 'learning_rate': 5.112041060991301e-07, 'epoch': 7.7105831533477325}\n",
      "{'loss': 1.4663, 'grad_norm': 23.625, 'learning_rate': 4.7308299453319936e-07, 'epoch': 7.732181425485961}\n",
      "{'loss': 1.4701, 'grad_norm': 15.0, 'learning_rate': 4.3496188296726877e-07, 'epoch': 7.75377969762419}\n",
      "{'loss': 1.4794, 'grad_norm': 18.375, 'learning_rate': 3.968407714013381e-07, 'epoch': 7.775377969762419}\n",
      "{'loss': 1.4767, 'grad_norm': 14.875, 'learning_rate': 3.587196598354075e-07, 'epoch': 7.796976241900648}\n",
      "{'loss': 1.4586, 'grad_norm': 11.125, 'learning_rate': 3.205985482694768e-07, 'epoch': 7.818574514038877}\n",
      "{'loss': 1.477, 'grad_norm': 19.625, 'learning_rate': 2.8247743670354614e-07, 'epoch': 7.840172786177106}\n",
      "{'loss': 1.4772, 'grad_norm': 13.625, 'learning_rate': 2.443563251376155e-07, 'epoch': 7.861771058315335}\n",
      "{'loss': 1.4675, 'grad_norm': 17.5, 'learning_rate': 2.0623521357168482e-07, 'epoch': 7.883369330453563}\n",
      "{'loss': 1.4643, 'grad_norm': 19.375, 'learning_rate': 1.6811410200575418e-07, 'epoch': 7.9049676025917925}\n",
      "{'loss': 1.469, 'grad_norm': 17.5, 'learning_rate': 1.2999299043982353e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 1.4655, 'grad_norm': 18.75, 'learning_rate': 9.187187887389288e-08, 'epoch': 7.94816414686825}\n",
      "{'loss': 1.4773, 'grad_norm': 12.6875, 'learning_rate': 5.375076730796222e-08, 'epoch': 7.9697624190064795}\n",
      "{'loss': 1.4763, 'grad_norm': 18.0, 'learning_rate': 1.5629655742031568e-08, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 1.471195101737976, 'eval_accuracy': 0.35374149659863946, 'eval_precision': 0.4693579804860297, 'eval_recall': 0.30073396578829736, 'eval_f1': 0.28816659843131937, 'eval_runtime': 7.767, 'eval_samples_per_second': 529.937, 'eval_steps_per_second': 66.307, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.3537 f1=0.2882 p=0.4694 r=0.3007\n",
      "{'train_runtime': 1389.048, 'train_samples_per_second': 213.32, 'train_steps_per_second': 26.666, 'train_loss': 1.4829529767417495, 'epoch': 8.0}\n",
      "{'eval_loss': 1.4699398279190063, 'eval_accuracy': 0.35374149659863946, 'eval_precision': 0.4318315224260284, 'eval_recall': 0.3014409015138352, 'eval_f1': 0.2888584099189607, 'eval_runtime': 7.8713, 'eval_samples_per_second': 522.91, 'eval_steps_per_second': 65.427, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.3537 f1=0.2889 p=0.4318 r=0.3014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÑ‚ñá‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñà‚ñÜ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñà‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÑ‚ñÅ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÜ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÉ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñÑ‚ñà‚ñÖ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÉ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÖ</td></tr><tr><td>train/learning_rate</td><td>‚ñÑ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.28886</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.35374</td></tr><tr><td>eval/f1</td><td>0.28886</td></tr><tr><td>eval/loss</td><td>1.46994</td></tr><tr><td>eval/precision</td><td>0.43183</td></tr><tr><td>eval/recall</td><td>0.30144</td></tr><tr><td>eval/runtime</td><td>7.8713</td></tr><tr><td>eval/samples_per_second</td><td>522.91</td></tr><tr><td>eval/steps_per_second</td><td>65.427</td></tr><tr><td>total_flos</td><td>1.1916090232884816e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>37040</td></tr><tr><td>train/grad_norm</td><td>18</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.4763</td></tr><tr><td>train_loss</td><td>1.48295</td></tr><tr><td>train_runtime</td><td>1389.048</td></tr><tr><td>train_samples_per_second</td><td>213.32</td></tr><tr><td>train_steps_per_second</td><td>26.666</td></tr><tr><td>trial/accuracy</td><td>0.35374</td></tr><tr><td>trial/f1</td><td>0.28886</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t3</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/aev6dked' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/aev6dked</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_224520-aev6dked\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341:  40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [1:27:31<2:03:36, 1236.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=3 f1=0.2889\n",
      "[I 2025-08-16 23:08:42,361] Trial 3 finished with value: 0.2888584099189607 and parameters: {'lr': 1.3272627413910076e-05, 'weight_decay': 3.5086905570042316e-05, 'unfreeze_last_k': 8, 'batch_size': 8}. Best is trial 2 with value: 0.8523413656247504.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_230842-h3tjjl2t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/h3tjjl2t' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t4</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/h3tjjl2t' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/h3tjjl2t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=4 | epochs=8 bs=8 lr=4.68e-05 wd=1.3e-05 k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=8 lr=4.68e-05 wd=1.3e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/4630] loss=1.6396 lr=2.08e-06\n",
      "{'loss': 1.6396, 'grad_norm': 4.0625, 'learning_rate': 2.0841980876303363e-06, 'epoch': 0.02159827213822894}\n",
      "[e0 b200/4630] loss=1.6452 lr=4.19e-06\n",
      "{'loss': 1.6452, 'grad_norm': 5.0625, 'learning_rate': 4.189448681196333e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b300/4630] loss=1.6473 lr=6.29e-06\n",
      "{'loss': 1.6473, 'grad_norm': 3.796875, 'learning_rate': 6.294699274762329e-06, 'epoch': 0.06479481641468683}\n",
      "[e0 b400/4630] loss=1.6355 lr=8.40e-06\n",
      "{'loss': 1.6355, 'grad_norm': 2.09375, 'learning_rate': 8.399949868328326e-06, 'epoch': 0.08639308855291576}\n",
      "[e0 b500/4630] loss=1.6321 lr=1.05e-05\n",
      "{'loss': 1.6321, 'grad_norm': 4.34375, 'learning_rate': 1.0505200461894322e-05, 'epoch': 0.1079913606911447}\n",
      "[e0 b600/4630] loss=1.6199 lr=1.26e-05\n",
      "{'loss': 1.6199, 'grad_norm': 3.546875, 'learning_rate': 1.2610451055460319e-05, 'epoch': 0.12958963282937366}\n",
      "[e0 b700/4630] loss=1.5924 lr=1.47e-05\n",
      "{'loss': 1.5924, 'grad_norm': 7.65625, 'learning_rate': 1.4715701649026317e-05, 'epoch': 0.1511879049676026}\n",
      "[e0 b800/4630] loss=1.5724 lr=1.68e-05\n",
      "{'loss': 1.5724, 'grad_norm': 8.75, 'learning_rate': 1.6820952242592312e-05, 'epoch': 0.17278617710583152}\n",
      "[e0 b900/4630] loss=1.5479 lr=1.89e-05\n",
      "{'loss': 1.5479, 'grad_norm': 11.9375, 'learning_rate': 1.892620283615831e-05, 'epoch': 0.19438444924406048}\n",
      "[e0 b1000/4630] loss=1.5630 lr=2.10e-05\n",
      "{'loss': 1.563, 'grad_norm': 7.71875, 'learning_rate': 2.1031453429724305e-05, 'epoch': 0.2159827213822894}\n",
      "[e0 b1100/4630] loss=1.5426 lr=2.31e-05\n",
      "{'loss': 1.5426, 'grad_norm': 11.5625, 'learning_rate': 2.3136704023290302e-05, 'epoch': 0.23758099352051837}\n",
      "[e0 b1200/4630] loss=1.5119 lr=2.52e-05\n",
      "{'loss': 1.5119, 'grad_norm': 9.75, 'learning_rate': 2.5241954616856298e-05, 'epoch': 0.2591792656587473}\n",
      "[e0 b1300/4630] loss=1.5090 lr=2.73e-05\n",
      "{'loss': 1.509, 'grad_norm': 14.5625, 'learning_rate': 2.734720521042229e-05, 'epoch': 0.28077753779697623}\n",
      "[e0 b1400/4630] loss=1.4997 lr=2.95e-05\n",
      "{'loss': 1.4997, 'grad_norm': 18.25, 'learning_rate': 2.9452455803988295e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b1500/4630] loss=1.5124 lr=3.16e-05\n",
      "{'loss': 1.5124, 'grad_norm': 14.4375, 'learning_rate': 3.155770639755429e-05, 'epoch': 0.32397408207343414}\n",
      "[e0 b1600/4630] loss=1.4515 lr=3.37e-05\n",
      "{'loss': 1.4515, 'grad_norm': 17.125, 'learning_rate': 3.3662956991120284e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b1700/4630] loss=1.3900 lr=3.58e-05\n",
      "{'loss': 1.39, 'grad_norm': 23.25, 'learning_rate': 3.576820758468628e-05, 'epoch': 0.367170626349892}\n",
      "[e0 b1800/4630] loss=1.3682 lr=3.79e-05\n",
      "{'loss': 1.3682, 'grad_norm': 18.5, 'learning_rate': 3.787345817825228e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1900/4630] loss=1.3444 lr=4.00e-05\n",
      "{'loss': 1.3444, 'grad_norm': 35.25, 'learning_rate': 3.997870877181827e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b2000/4630] loss=1.3258 lr=4.21e-05\n",
      "{'loss': 1.3258, 'grad_norm': 20.125, 'learning_rate': 4.208395936538427e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b2100/4630] loss=1.2548 lr=4.42e-05\n",
      "{'loss': 1.2548, 'grad_norm': 18.0, 'learning_rate': 4.418920995895027e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b2200/4630] loss=1.2444 lr=4.63e-05\n",
      "{'loss': 1.2444, 'grad_norm': 34.25, 'learning_rate': 4.6294460552516264e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b2300/4630] loss=1.1964 lr=4.67e-05\n",
      "{'loss': 1.1964, 'grad_norm': 22.375, 'learning_rate': 4.669756431237688e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b2400/4630] loss=1.1911 lr=4.66e-05\n",
      "{'loss': 1.1911, 'grad_norm': 25.0, 'learning_rate': 4.6563148019488425e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b2500/4630] loss=1.1371 lr=4.64e-05\n",
      "{'loss': 1.1371, 'grad_norm': 19.25, 'learning_rate': 4.6428731726599976e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b2600/4630] loss=1.1433 lr=4.63e-05\n",
      "{'loss': 1.1433, 'grad_norm': 24.75, 'learning_rate': 4.629431543371153e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b2700/4630] loss=1.1246 lr=4.62e-05\n",
      "{'loss': 1.1246, 'grad_norm': 31.625, 'learning_rate': 4.615989914082308e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b2800/4630] loss=1.0851 lr=4.60e-05\n",
      "{'loss': 1.0851, 'grad_norm': 39.5, 'learning_rate': 4.6025482847934616e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b2900/4630] loss=1.1100 lr=4.59e-05\n",
      "{'loss': 1.11, 'grad_norm': 27.375, 'learning_rate': 4.589106655504617e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b3000/4630] loss=1.0289 lr=4.58e-05\n",
      "{'loss': 1.0289, 'grad_norm': 18.75, 'learning_rate': 4.575665026215772e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b3100/4630] loss=1.0248 lr=4.56e-05\n",
      "{'loss': 1.0248, 'grad_norm': 23.625, 'learning_rate': 4.562223396926927e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b3200/4630] loss=1.0530 lr=4.55e-05\n",
      "{'loss': 1.053, 'grad_norm': 34.5, 'learning_rate': 4.5487817676380814e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b3300/4630] loss=1.0713 lr=4.54e-05\n",
      "{'loss': 1.0713, 'grad_norm': 30.625, 'learning_rate': 4.5353401383492366e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b3400/4630] loss=0.9576 lr=4.52e-05\n",
      "{'loss': 0.9576, 'grad_norm': 34.75, 'learning_rate': 4.521898509060391e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b3500/4630] loss=0.9880 lr=4.51e-05\n",
      "{'loss': 0.988, 'grad_norm': 17.875, 'learning_rate': 4.508456879771546e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b3600/4630] loss=1.0035 lr=4.50e-05\n",
      "{'loss': 1.0035, 'grad_norm': 18.625, 'learning_rate': 4.495015250482701e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b3700/4630] loss=1.0262 lr=4.48e-05\n",
      "{'loss': 1.0262, 'grad_norm': 27.25, 'learning_rate': 4.481573621193856e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b3800/4630] loss=0.9490 lr=4.47e-05\n",
      "{'loss': 0.949, 'grad_norm': 53.5, 'learning_rate': 4.468131991905011e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b3900/4630] loss=1.0355 lr=4.45e-05\n",
      "{'loss': 1.0355, 'grad_norm': 35.5, 'learning_rate': 4.454690362616166e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b4000/4630] loss=0.9949 lr=4.44e-05\n",
      "{'loss': 0.9949, 'grad_norm': 59.75, 'learning_rate': 4.441248733327321e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b4100/4630] loss=0.9749 lr=4.43e-05\n",
      "{'loss': 0.9749, 'grad_norm': 19.75, 'learning_rate': 4.4278071040384755e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b4200/4630] loss=0.8916 lr=4.41e-05\n",
      "{'loss': 0.8916, 'grad_norm': 38.25, 'learning_rate': 4.41436547474963e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b4300/4630] loss=0.9752 lr=4.40e-05\n",
      "{'loss': 0.9752, 'grad_norm': 23.875, 'learning_rate': 4.400923845460785e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b4400/4630] loss=0.9042 lr=4.39e-05\n",
      "{'loss': 0.9042, 'grad_norm': 25.75, 'learning_rate': 4.38748221617194e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b4500/4630] loss=0.9066 lr=4.37e-05\n",
      "{'loss': 0.9066, 'grad_norm': 30.75, 'learning_rate': 4.374040586883095e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b4600/4630] loss=0.8561 lr=4.36e-05\n",
      "{'loss': 0.8561, 'grad_norm': 31.875, 'learning_rate': 4.36059895759425e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 0.8755600452423096, 'eval_accuracy': 0.6547619047619048, 'eval_precision': 0.6932259042695497, 'eval_recall': 0.6480783179799097, 'eval_f1': 0.6650786117511158, 'eval_runtime': 7.8644, 'eval_samples_per_second': 523.369, 'eval_steps_per_second': 65.485, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.6548 f1=0.6651 p=0.6932 r=0.6481\n",
      "{'loss': 0.8436, 'grad_norm': 19.875, 'learning_rate': 4.347157328305404e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 0.91, 'grad_norm': 15.25, 'learning_rate': 4.333715699016559e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 0.9088, 'grad_norm': 15.6875, 'learning_rate': 4.3202740697277144e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 0.8669, 'grad_norm': 18.75, 'learning_rate': 4.3068324404388696e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 0.8681, 'grad_norm': 32.25, 'learning_rate': 4.293390811150024e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 0.9059, 'grad_norm': 32.0, 'learning_rate': 4.279949181861179e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 0.8283, 'grad_norm': 41.75, 'learning_rate': 4.2665075525723336e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 0.8247, 'grad_norm': 26.25, 'learning_rate': 4.253065923283489e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 0.8543, 'grad_norm': 20.625, 'learning_rate': 4.239624293994644e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 0.8264, 'grad_norm': 48.25, 'learning_rate': 4.226182664705798e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 0.8046, 'grad_norm': 22.625, 'learning_rate': 4.2127410354169534e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 0.8408, 'grad_norm': 15.75, 'learning_rate': 4.1992994061281085e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 0.8542, 'grad_norm': 24.375, 'learning_rate': 4.1858577768392636e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 0.8289, 'grad_norm': 27.625, 'learning_rate': 4.172416147550418e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 0.8984, 'grad_norm': 13.125, 'learning_rate': 4.1589745182615725e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 0.8073, 'grad_norm': 27.875, 'learning_rate': 4.1455328889727276e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 0.8354, 'grad_norm': 30.25, 'learning_rate': 4.132091259683883e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 0.8102, 'grad_norm': 33.25, 'learning_rate': 4.118649630395038e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 0.8479, 'grad_norm': 45.75, 'learning_rate': 4.105208001106192e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 0.8472, 'grad_norm': 25.5, 'learning_rate': 4.091766371817347e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 0.7712, 'grad_norm': 32.75, 'learning_rate': 4.078324742528502e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 0.7788, 'grad_norm': 35.25, 'learning_rate': 4.064883113239657e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 0.8258, 'grad_norm': 83.5, 'learning_rate': 4.0514414839508114e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 0.7856, 'grad_norm': 25.375, 'learning_rate': 4.0379998546619666e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 0.7976, 'grad_norm': 30.25, 'learning_rate': 4.024558225373122e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 0.8334, 'grad_norm': 30.0, 'learning_rate': 4.011116596084276e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 0.8317, 'grad_norm': 28.25, 'learning_rate': 3.997674966795431e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 0.761, 'grad_norm': 23.75, 'learning_rate': 3.984233337506586e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 0.7585, 'grad_norm': 28.5, 'learning_rate': 3.970791708217741e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 0.8219, 'grad_norm': 47.0, 'learning_rate': 3.957350078928896e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 0.801, 'grad_norm': 19.625, 'learning_rate': 3.943908449640051e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 0.7404, 'grad_norm': 23.625, 'learning_rate': 3.9304668203512055e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 0.8068, 'grad_norm': 9.25, 'learning_rate': 3.91702519106236e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 0.8049, 'grad_norm': 25.125, 'learning_rate': 3.903583561773515e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 0.7871, 'grad_norm': 50.25, 'learning_rate': 3.89014193248467e-05, 'epoch': 1.7494600431965441}\n",
      "{'loss': 0.7413, 'grad_norm': 12.1875, 'learning_rate': 3.876700303195825e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 0.7968, 'grad_norm': 30.5, 'learning_rate': 3.86325867390698e-05, 'epoch': 1.7926565874730023}\n",
      "{'loss': 0.7893, 'grad_norm': 53.25, 'learning_rate': 3.849817044618135e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 0.6997, 'grad_norm': 13.625, 'learning_rate': 3.836375415329289e-05, 'epoch': 1.83585313174946}\n",
      "{'loss': 0.7482, 'grad_norm': 19.75, 'learning_rate': 3.8229337860404444e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 0.7425, 'grad_norm': 34.5, 'learning_rate': 3.8094921567515996e-05, 'epoch': 1.8790496760259179}\n",
      "{'loss': 0.7799, 'grad_norm': 30.625, 'learning_rate': 3.796050527462754e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 0.7939, 'grad_norm': 32.75, 'learning_rate': 3.782608898173909e-05, 'epoch': 1.9222462203023758}\n",
      "{'loss': 0.7429, 'grad_norm': 25.0, 'learning_rate': 3.769167268885064e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 0.7408, 'grad_norm': 31.25, 'learning_rate': 3.755725639596219e-05, 'epoch': 1.9654427645788337}\n",
      "{'loss': 0.6354, 'grad_norm': 24.625, 'learning_rate': 3.742284010307374e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 0.7563640475273132, 'eval_accuracy': 0.7162293488824101, 'eval_precision': 0.726422854278243, 'eval_recall': 0.7314918769983996, 'eval_f1': 0.7256264369944323, 'eval_runtime': 7.9136, 'eval_samples_per_second': 520.117, 'eval_steps_per_second': 65.078, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.7162 f1=0.7256 p=0.7264 r=0.7315\n",
      "{'loss': 0.7948, 'grad_norm': 24.5, 'learning_rate': 3.728842381018528e-05, 'epoch': 2.0086393088552916}\n",
      "{'loss': 0.7582, 'grad_norm': 26.875, 'learning_rate': 3.7154007517296834e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 0.7291, 'grad_norm': 44.5, 'learning_rate': 3.7019591224408385e-05, 'epoch': 2.0518358531317493}\n",
      "{'loss': 0.7327, 'grad_norm': 28.75, 'learning_rate': 3.6885174931519936e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 0.6964, 'grad_norm': 7.9375, 'learning_rate': 3.675075863863148e-05, 'epoch': 2.0950323974082075}\n",
      "{'loss': 0.7804, 'grad_norm': 48.25, 'learning_rate': 3.6616342345743025e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 0.7266, 'grad_norm': 33.25, 'learning_rate': 3.6481926052854576e-05, 'epoch': 2.138228941684665}\n",
      "{'loss': 0.7012, 'grad_norm': 101.0, 'learning_rate': 3.634750975996613e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 0.7177, 'grad_norm': 41.5, 'learning_rate': 3.621309346707767e-05, 'epoch': 2.1814254859611233}\n",
      "{'loss': 0.7186, 'grad_norm': 31.0, 'learning_rate': 3.607867717418922e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 0.7264, 'grad_norm': 60.0, 'learning_rate': 3.5944260881300774e-05, 'epoch': 2.224622030237581}\n",
      "{'loss': 0.7014, 'grad_norm': 38.0, 'learning_rate': 3.580984458841232e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 0.7494, 'grad_norm': 48.25, 'learning_rate': 3.567542829552387e-05, 'epoch': 2.267818574514039}\n",
      "{'loss': 0.7042, 'grad_norm': 38.5, 'learning_rate': 3.5541012002635414e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 0.68, 'grad_norm': 25.625, 'learning_rate': 3.5406595709746966e-05, 'epoch': 2.311015118790497}\n",
      "{'loss': 0.6977, 'grad_norm': 36.75, 'learning_rate': 3.527217941685852e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 0.6866, 'grad_norm': 6.90625, 'learning_rate': 3.513776312397007e-05, 'epoch': 2.3542116630669545}\n",
      "{'loss': 0.7051, 'grad_norm': 48.75, 'learning_rate': 3.500334683108161e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 0.7591, 'grad_norm': 16.75, 'learning_rate': 3.486893053819316e-05, 'epoch': 2.3974082073434126}\n",
      "{'loss': 0.751, 'grad_norm': 61.0, 'learning_rate': 3.473451424530471e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 0.7481, 'grad_norm': 37.75, 'learning_rate': 3.460009795241626e-05, 'epoch': 2.4406047516198703}\n",
      "{'loss': 0.7145, 'grad_norm': 36.25, 'learning_rate': 3.446568165952781e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 0.6706, 'grad_norm': 29.0, 'learning_rate': 3.4331265366639355e-05, 'epoch': 2.4838012958963285}\n",
      "{'loss': 0.7431, 'grad_norm': 41.25, 'learning_rate': 3.41968490737509e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 0.7182, 'grad_norm': 29.625, 'learning_rate': 3.406243278086245e-05, 'epoch': 2.526997840172786}\n",
      "{'loss': 0.7404, 'grad_norm': 15.1875, 'learning_rate': 3.3928016487974e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 0.7207, 'grad_norm': 14.6875, 'learning_rate': 3.379360019508555e-05, 'epoch': 2.570194384449244}\n",
      "{'loss': 0.6959, 'grad_norm': 54.25, 'learning_rate': 3.36591839021971e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 0.7005, 'grad_norm': 17.75, 'learning_rate': 3.352476760930865e-05, 'epoch': 2.613390928725702}\n",
      "{'loss': 0.6639, 'grad_norm': 19.0, 'learning_rate': 3.339035131642019e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 0.6706, 'grad_norm': 44.25, 'learning_rate': 3.3255935023531744e-05, 'epoch': 2.6565874730021597}\n",
      "{'loss': 0.6973, 'grad_norm': 34.25, 'learning_rate': 3.3121518730643296e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 0.6761, 'grad_norm': 21.375, 'learning_rate': 3.298710243775484e-05, 'epoch': 2.699784017278618}\n",
      "{'loss': 0.6973, 'grad_norm': 44.25, 'learning_rate': 3.285268614486639e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 0.6853, 'grad_norm': 33.5, 'learning_rate': 3.271826985197794e-05, 'epoch': 2.7429805615550755}\n",
      "{'loss': 0.6768, 'grad_norm': 24.0, 'learning_rate': 3.2583853559089494e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 0.7099, 'grad_norm': 13.9375, 'learning_rate': 3.244943726620104e-05, 'epoch': 2.786177105831533}\n",
      "{'loss': 0.6837, 'grad_norm': 12.75, 'learning_rate': 3.231502097331258e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 0.7618, 'grad_norm': 64.0, 'learning_rate': 3.2180604680424134e-05, 'epoch': 2.8293736501079914}\n",
      "{'loss': 0.7039, 'grad_norm': 8.5, 'learning_rate': 3.2046188387535685e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 0.6585, 'grad_norm': 24.375, 'learning_rate': 3.1911772094647236e-05, 'epoch': 2.8725701943844495}\n",
      "{'loss': 0.725, 'grad_norm': 51.25, 'learning_rate': 3.177735580175878e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 0.6735, 'grad_norm': 34.75, 'learning_rate': 3.1642939508870325e-05, 'epoch': 2.915766738660907}\n",
      "{'loss': 0.6772, 'grad_norm': 55.0, 'learning_rate': 3.1508523215981876e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 0.7491, 'grad_norm': 13.5625, 'learning_rate': 3.137410692309343e-05, 'epoch': 2.958963282937365}\n",
      "{'loss': 0.7104, 'grad_norm': 29.75, 'learning_rate': 3.123969063020497e-05, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 0.74321448802948, 'eval_accuracy': 0.7213313896987367, 'eval_precision': 0.7349089039819809, 'eval_recall': 0.739891739195491, 'eval_f1': 0.7311996071799, 'eval_runtime': 7.8561, 'eval_samples_per_second': 523.927, 'eval_steps_per_second': 65.555, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.7213 f1=0.7312 p=0.7349 r=0.7399\n",
      "{'loss': 0.6849, 'grad_norm': 27.625, 'learning_rate': 3.110527433731652e-05, 'epoch': 3.002159827213823}\n",
      "{'loss': 0.6924, 'grad_norm': 29.0, 'learning_rate': 3.0970858044428074e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 0.6644, 'grad_norm': 41.25, 'learning_rate': 3.083644175153962e-05, 'epoch': 3.0453563714902807}\n",
      "{'loss': 0.6746, 'grad_norm': 18.375, 'learning_rate': 3.070202545865117e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 0.681, 'grad_norm': 71.5, 'learning_rate': 3.0567609165762715e-05, 'epoch': 3.088552915766739}\n",
      "{'loss': 0.6493, 'grad_norm': 17.125, 'learning_rate': 3.0433192872874266e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 0.6631, 'grad_norm': 37.5, 'learning_rate': 3.0298776579985817e-05, 'epoch': 3.1317494600431965}\n",
      "{'loss': 0.6996, 'grad_norm': 17.625, 'learning_rate': 3.0164360287097365e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 0.688, 'grad_norm': 14.25, 'learning_rate': 3.0029943994208916e-05, 'epoch': 3.1749460043196542}\n",
      "{'loss': 0.7207, 'grad_norm': 22.375, 'learning_rate': 2.989552770132046e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 0.7018, 'grad_norm': 32.25, 'learning_rate': 2.9761111408432008e-05, 'epoch': 3.2181425485961124}\n",
      "{'loss': 0.6167, 'grad_norm': 37.0, 'learning_rate': 2.962669511554356e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 0.6943, 'grad_norm': 53.0, 'learning_rate': 2.9492278822655107e-05, 'epoch': 3.26133909287257}\n",
      "{'loss': 0.6848, 'grad_norm': 31.75, 'learning_rate': 2.935786252976666e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 0.681, 'grad_norm': 63.5, 'learning_rate': 2.9223446236878206e-05, 'epoch': 3.304535637149028}\n",
      "{'loss': 0.7003, 'grad_norm': 52.25, 'learning_rate': 2.908902994398975e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 0.7087, 'grad_norm': 46.5, 'learning_rate': 2.8954613651101302e-05, 'epoch': 3.347732181425486}\n",
      "{'loss': 0.6704, 'grad_norm': 77.0, 'learning_rate': 2.882019735821285e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 0.6885, 'grad_norm': 28.75, 'learning_rate': 2.86857810653244e-05, 'epoch': 3.390928725701944}\n",
      "{'loss': 0.7116, 'grad_norm': 84.5, 'learning_rate': 2.855136477243595e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 0.6363, 'grad_norm': 8.375, 'learning_rate': 2.84169484795475e-05, 'epoch': 3.4341252699784017}\n",
      "{'loss': 0.696, 'grad_norm': 45.25, 'learning_rate': 2.8282532186659044e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 0.7004, 'grad_norm': 39.25, 'learning_rate': 2.8148115893770592e-05, 'epoch': 3.4773218142548594}\n",
      "{'loss': 0.6661, 'grad_norm': 15.125, 'learning_rate': 2.8013699600882144e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 0.7253, 'grad_norm': 19.375, 'learning_rate': 2.787928330799369e-05, 'epoch': 3.5205183585313176}\n",
      "{'loss': 0.6364, 'grad_norm': 40.25, 'learning_rate': 2.7744867015105243e-05, 'epoch': 3.5421166306695464}\n",
      "{'loss': 0.6804, 'grad_norm': 79.0, 'learning_rate': 2.761045072221679e-05, 'epoch': 3.5637149028077753}\n",
      "{'loss': 0.6517, 'grad_norm': 20.375, 'learning_rate': 2.7476034429328335e-05, 'epoch': 3.5853131749460045}\n",
      "{'loss': 0.6976, 'grad_norm': 29.625, 'learning_rate': 2.7341618136439886e-05, 'epoch': 3.6069114470842334}\n",
      "{'loss': 0.7018, 'grad_norm': 10.6875, 'learning_rate': 2.7207201843551434e-05, 'epoch': 3.6285097192224622}\n",
      "{'loss': 0.6645, 'grad_norm': 15.0, 'learning_rate': 2.7072785550662985e-05, 'epoch': 3.650107991360691}\n",
      "{'loss': 0.6522, 'grad_norm': 16.125, 'learning_rate': 2.6938369257774533e-05, 'epoch': 3.67170626349892}\n",
      "{'loss': 0.6709, 'grad_norm': 33.5, 'learning_rate': 2.6803952964886084e-05, 'epoch': 3.693304535637149}\n",
      "{'loss': 0.6494, 'grad_norm': 24.625, 'learning_rate': 2.6669536671997632e-05, 'epoch': 3.714902807775378}\n",
      "{'loss': 0.6988, 'grad_norm': 32.5, 'learning_rate': 2.6535120379109176e-05, 'epoch': 3.736501079913607}\n",
      "{'loss': 0.6844, 'grad_norm': 61.25, 'learning_rate': 2.6400704086220724e-05, 'epoch': 3.7580993520518358}\n",
      "{'loss': 0.6305, 'grad_norm': 33.0, 'learning_rate': 2.6266287793332275e-05, 'epoch': 3.7796976241900646}\n",
      "{'loss': 0.7239, 'grad_norm': 33.25, 'learning_rate': 2.6131871500443823e-05, 'epoch': 3.801295896328294}\n",
      "{'loss': 0.7045, 'grad_norm': 62.25, 'learning_rate': 2.5997455207555374e-05, 'epoch': 3.8228941684665227}\n",
      "{'loss': 0.6559, 'grad_norm': 40.0, 'learning_rate': 2.5863038914666922e-05, 'epoch': 3.8444924406047516}\n",
      "{'loss': 0.6399, 'grad_norm': 42.0, 'learning_rate': 2.5728622621778467e-05, 'epoch': 3.8660907127429804}\n",
      "{'loss': 0.6762, 'grad_norm': 23.75, 'learning_rate': 2.5594206328890018e-05, 'epoch': 3.8876889848812093}\n",
      "{'loss': 0.6996, 'grad_norm': 24.25, 'learning_rate': 2.5459790036001566e-05, 'epoch': 3.9092872570194386}\n",
      "{'loss': 0.695, 'grad_norm': 42.75, 'learning_rate': 2.5325373743113117e-05, 'epoch': 3.9308855291576674}\n",
      "{'loss': 0.7053, 'grad_norm': 48.25, 'learning_rate': 2.5190957450224665e-05, 'epoch': 3.9524838012958963}\n",
      "{'loss': 0.6157, 'grad_norm': 13.5, 'learning_rate': 2.5056541157336216e-05, 'epoch': 3.974082073434125}\n",
      "{'loss': 0.6229, 'grad_norm': 18.75, 'learning_rate': 2.492212486444776e-05, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 0.7324348092079163, 'eval_accuracy': 0.728377065111759, 'eval_precision': 0.7368070078599009, 'eval_recall': 0.7495762613240372, 'eval_f1': 0.737260737922255, 'eval_runtime': 7.7981, 'eval_samples_per_second': 527.823, 'eval_steps_per_second': 66.042, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.7284 f1=0.7373 p=0.7368 r=0.7496\n",
      "{'loss': 0.6281, 'grad_norm': 51.5, 'learning_rate': 2.4787708571559308e-05, 'epoch': 4.017278617710583}\n",
      "{'loss': 0.6004, 'grad_norm': 9.0, 'learning_rate': 2.465329227867086e-05, 'epoch': 4.038876889848812}\n",
      "{'loss': 0.744, 'grad_norm': 22.375, 'learning_rate': 2.4518875985782407e-05, 'epoch': 4.060475161987041}\n",
      "{'loss': 0.7555, 'grad_norm': 63.75, 'learning_rate': 2.438445969289396e-05, 'epoch': 4.08207343412527}\n",
      "{'loss': 0.679, 'grad_norm': 29.625, 'learning_rate': 2.4250043400005506e-05, 'epoch': 4.103671706263499}\n",
      "{'loss': 0.7156, 'grad_norm': 39.25, 'learning_rate': 2.4115627107117058e-05, 'epoch': 4.125269978401728}\n",
      "{'loss': 0.6362, 'grad_norm': 30.875, 'learning_rate': 2.3981210814228602e-05, 'epoch': 4.146868250539957}\n",
      "{'loss': 0.5984, 'grad_norm': 25.5, 'learning_rate': 2.384679452134015e-05, 'epoch': 4.168466522678186}\n",
      "{'loss': 0.7056, 'grad_norm': 44.5, 'learning_rate': 2.37123782284517e-05, 'epoch': 4.190064794816415}\n",
      "{'loss': 0.7239, 'grad_norm': 42.5, 'learning_rate': 2.357796193556325e-05, 'epoch': 4.211663066954643}\n",
      "{'loss': 0.6783, 'grad_norm': 40.75, 'learning_rate': 2.34435456426748e-05, 'epoch': 4.233261339092873}\n",
      "{'loss': 0.6461, 'grad_norm': 66.5, 'learning_rate': 2.3309129349786345e-05, 'epoch': 4.254859611231102}\n",
      "{'loss': 0.6513, 'grad_norm': 30.625, 'learning_rate': 2.3174713056897896e-05, 'epoch': 4.27645788336933}\n",
      "{'loss': 0.6371, 'grad_norm': 28.375, 'learning_rate': 2.3040296764009444e-05, 'epoch': 4.29805615550756}\n",
      "{'loss': 0.6261, 'grad_norm': 16.75, 'learning_rate': 2.290588047112099e-05, 'epoch': 4.319654427645788}\n",
      "{'loss': 0.6766, 'grad_norm': 14.8125, 'learning_rate': 2.2771464178232543e-05, 'epoch': 4.341252699784017}\n",
      "{'loss': 0.6375, 'grad_norm': 35.5, 'learning_rate': 2.263704788534409e-05, 'epoch': 4.362850971922247}\n",
      "{'loss': 0.724, 'grad_norm': 22.875, 'learning_rate': 2.2502631592455638e-05, 'epoch': 4.384449244060475}\n",
      "{'loss': 0.702, 'grad_norm': 20.0, 'learning_rate': 2.2368215299567186e-05, 'epoch': 4.406047516198704}\n",
      "{'loss': 0.6547, 'grad_norm': 19.25, 'learning_rate': 2.2233799006678737e-05, 'epoch': 4.427645788336933}\n",
      "{'loss': 0.66, 'grad_norm': 20.625, 'learning_rate': 2.2099382713790285e-05, 'epoch': 4.449244060475162}\n",
      "{'loss': 0.6483, 'grad_norm': 62.75, 'learning_rate': 2.1964966420901833e-05, 'epoch': 4.470842332613391}\n",
      "{'loss': 0.6507, 'grad_norm': 33.0, 'learning_rate': 2.183055012801338e-05, 'epoch': 4.49244060475162}\n",
      "{'loss': 0.6534, 'grad_norm': 37.0, 'learning_rate': 2.169613383512493e-05, 'epoch': 4.514038876889849}\n",
      "{'loss': 0.6532, 'grad_norm': 46.5, 'learning_rate': 2.156171754223648e-05, 'epoch': 4.535637149028078}\n",
      "{'loss': 0.6036, 'grad_norm': 31.375, 'learning_rate': 2.1427301249348028e-05, 'epoch': 4.557235421166307}\n",
      "{'loss': 0.6111, 'grad_norm': 19.125, 'learning_rate': 2.1292884956459575e-05, 'epoch': 4.578833693304536}\n",
      "{'loss': 0.6615, 'grad_norm': 19.375, 'learning_rate': 2.1158468663571123e-05, 'epoch': 4.600431965442764}\n",
      "{'loss': 0.6307, 'grad_norm': 33.0, 'learning_rate': 2.1024052370682674e-05, 'epoch': 4.622030237580994}\n",
      "{'loss': 0.6505, 'grad_norm': 42.0, 'learning_rate': 2.0889636077794222e-05, 'epoch': 4.643628509719223}\n",
      "{'loss': 0.6713, 'grad_norm': 40.0, 'learning_rate': 2.075521978490577e-05, 'epoch': 4.665226781857451}\n",
      "{'loss': 0.6822, 'grad_norm': 64.5, 'learning_rate': 2.062080349201732e-05, 'epoch': 4.686825053995681}\n",
      "{'loss': 0.6731, 'grad_norm': 36.75, 'learning_rate': 2.0486387199128866e-05, 'epoch': 4.708423326133909}\n",
      "{'loss': 0.6522, 'grad_norm': 23.75, 'learning_rate': 2.0351970906240417e-05, 'epoch': 4.730021598272138}\n",
      "{'loss': 0.6505, 'grad_norm': 22.875, 'learning_rate': 2.0217554613351965e-05, 'epoch': 4.751619870410368}\n",
      "{'loss': 0.6932, 'grad_norm': 20.5, 'learning_rate': 2.0083138320463516e-05, 'epoch': 4.773218142548596}\n",
      "{'loss': 0.6918, 'grad_norm': 42.0, 'learning_rate': 1.9948722027575064e-05, 'epoch': 4.794816414686825}\n",
      "{'loss': 0.7023, 'grad_norm': 33.0, 'learning_rate': 1.9814305734686612e-05, 'epoch': 4.816414686825054}\n",
      "{'loss': 0.6795, 'grad_norm': 14.0625, 'learning_rate': 1.9679889441798163e-05, 'epoch': 4.838012958963283}\n",
      "{'loss': 0.6663, 'grad_norm': 34.25, 'learning_rate': 1.9545473148909707e-05, 'epoch': 4.859611231101512}\n",
      "{'loss': 0.6165, 'grad_norm': 32.25, 'learning_rate': 1.941105685602126e-05, 'epoch': 4.881209503239741}\n",
      "{'loss': 0.6582, 'grad_norm': 19.625, 'learning_rate': 1.9276640563132806e-05, 'epoch': 4.90280777537797}\n",
      "{'loss': 0.6434, 'grad_norm': 20.0, 'learning_rate': 1.9142224270244354e-05, 'epoch': 4.924406047516198}\n",
      "{'loss': 0.676, 'grad_norm': 25.625, 'learning_rate': 1.9007807977355902e-05, 'epoch': 4.946004319654428}\n",
      "{'loss': 0.6968, 'grad_norm': 31.625, 'learning_rate': 1.8873391684467453e-05, 'epoch': 4.967602591792657}\n",
      "{'loss': 0.7066, 'grad_norm': 21.375, 'learning_rate': 1.8738975391579e-05, 'epoch': 4.989200863930885}\n",
      "{'eval_loss': 0.7221248745918274, 'eval_accuracy': 0.7359086491739553, 'eval_precision': 0.7465534664797963, 'eval_recall': 0.7528394489649252, 'eval_f1': 0.7445851369147556, 'eval_runtime': 8.0668, 'eval_samples_per_second': 510.239, 'eval_steps_per_second': 63.842, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.7359 f1=0.7446 p=0.7466 r=0.7528\n",
      "{'loss': 0.6653, 'grad_norm': 39.75, 'learning_rate': 1.860455909869055e-05, 'epoch': 5.010799136069115}\n",
      "{'loss': 0.6563, 'grad_norm': 37.0, 'learning_rate': 1.84701428058021e-05, 'epoch': 5.032397408207343}\n",
      "{'loss': 0.6936, 'grad_norm': 23.75, 'learning_rate': 1.8335726512913645e-05, 'epoch': 5.053995680345572}\n",
      "{'loss': 0.6436, 'grad_norm': 55.25, 'learning_rate': 1.8201310220025196e-05, 'epoch': 5.075593952483802}\n",
      "{'loss': 0.6513, 'grad_norm': 23.25, 'learning_rate': 1.8066893927136744e-05, 'epoch': 5.09719222462203}\n",
      "{'loss': 0.6536, 'grad_norm': 48.0, 'learning_rate': 1.793247763424829e-05, 'epoch': 5.118790496760259}\n",
      "{'loss': 0.6683, 'grad_norm': 26.125, 'learning_rate': 1.7798061341359843e-05, 'epoch': 5.140388768898488}\n",
      "{'loss': 0.6396, 'grad_norm': 27.875, 'learning_rate': 1.766364504847139e-05, 'epoch': 5.161987041036717}\n",
      "{'loss': 0.6728, 'grad_norm': 32.5, 'learning_rate': 1.7529228755582938e-05, 'epoch': 5.183585313174946}\n",
      "{'loss': 0.6474, 'grad_norm': 40.5, 'learning_rate': 1.7394812462694486e-05, 'epoch': 5.205183585313175}\n",
      "{'loss': 0.6443, 'grad_norm': 27.25, 'learning_rate': 1.7260396169806037e-05, 'epoch': 5.226781857451404}\n",
      "{'loss': 0.6687, 'grad_norm': 33.0, 'learning_rate': 1.7125979876917585e-05, 'epoch': 5.248380129589632}\n",
      "{'loss': 0.6548, 'grad_norm': 70.0, 'learning_rate': 1.6991563584029133e-05, 'epoch': 5.269978401727862}\n",
      "{'loss': 0.6414, 'grad_norm': 22.75, 'learning_rate': 1.685714729114068e-05, 'epoch': 5.291576673866091}\n",
      "{'loss': 0.624, 'grad_norm': 30.25, 'learning_rate': 1.6722730998252232e-05, 'epoch': 5.313174946004319}\n",
      "{'loss': 0.6784, 'grad_norm': 33.75, 'learning_rate': 1.658831470536378e-05, 'epoch': 5.334773218142549}\n",
      "{'loss': 0.6768, 'grad_norm': 50.25, 'learning_rate': 1.6453898412475328e-05, 'epoch': 5.356371490280777}\n",
      "{'loss': 0.6639, 'grad_norm': 17.5, 'learning_rate': 1.631948211958688e-05, 'epoch': 5.377969762419006}\n",
      "{'loss': 0.6292, 'grad_norm': 39.25, 'learning_rate': 1.6185065826698423e-05, 'epoch': 5.399568034557236}\n",
      "{'loss': 0.6819, 'grad_norm': 32.75, 'learning_rate': 1.6050649533809975e-05, 'epoch': 5.421166306695464}\n",
      "{'loss': 0.6358, 'grad_norm': 17.75, 'learning_rate': 1.5916233240921522e-05, 'epoch': 5.442764578833693}\n",
      "{'loss': 0.6384, 'grad_norm': 24.75, 'learning_rate': 1.578181694803307e-05, 'epoch': 5.464362850971923}\n",
      "{'loss': 0.6324, 'grad_norm': 38.5, 'learning_rate': 1.564740065514462e-05, 'epoch': 5.485961123110151}\n",
      "{'loss': 0.6335, 'grad_norm': 31.25, 'learning_rate': 1.551298436225617e-05, 'epoch': 5.50755939524838}\n",
      "{'loss': 0.6482, 'grad_norm': 25.5, 'learning_rate': 1.5378568069367717e-05, 'epoch': 5.529157667386609}\n",
      "{'loss': 0.6438, 'grad_norm': 46.75, 'learning_rate': 1.5244151776479267e-05, 'epoch': 5.550755939524838}\n",
      "{'loss': 0.7053, 'grad_norm': 11.25, 'learning_rate': 1.5109735483590816e-05, 'epoch': 5.572354211663067}\n",
      "{'loss': 0.6669, 'grad_norm': 15.6875, 'learning_rate': 1.4975319190702362e-05, 'epoch': 5.593952483801296}\n",
      "{'loss': 0.678, 'grad_norm': 40.25, 'learning_rate': 1.4840902897813912e-05, 'epoch': 5.615550755939525}\n",
      "{'loss': 0.6895, 'grad_norm': 25.0, 'learning_rate': 1.4706486604925461e-05, 'epoch': 5.637149028077753}\n",
      "{'loss': 0.5951, 'grad_norm': 35.75, 'learning_rate': 1.4572070312037007e-05, 'epoch': 5.658747300215983}\n",
      "{'loss': 0.6697, 'grad_norm': 48.0, 'learning_rate': 1.4437654019148557e-05, 'epoch': 5.680345572354212}\n",
      "{'loss': 0.6334, 'grad_norm': 46.75, 'learning_rate': 1.4303237726260106e-05, 'epoch': 5.70194384449244}\n",
      "{'loss': 0.6981, 'grad_norm': 24.375, 'learning_rate': 1.4168821433371656e-05, 'epoch': 5.72354211663067}\n",
      "{'loss': 0.6011, 'grad_norm': 25.375, 'learning_rate': 1.4034405140483204e-05, 'epoch': 5.745140388768899}\n",
      "{'loss': 0.6141, 'grad_norm': 19.25, 'learning_rate': 1.3899988847594753e-05, 'epoch': 5.766738660907127}\n",
      "{'loss': 0.6112, 'grad_norm': 49.25, 'learning_rate': 1.3765572554706303e-05, 'epoch': 5.788336933045357}\n",
      "{'loss': 0.7062, 'grad_norm': 35.0, 'learning_rate': 1.3631156261817849e-05, 'epoch': 5.809935205183585}\n",
      "{'loss': 0.6718, 'grad_norm': 19.0, 'learning_rate': 1.3496739968929398e-05, 'epoch': 5.831533477321814}\n",
      "{'loss': 0.6758, 'grad_norm': 38.75, 'learning_rate': 1.3362323676040948e-05, 'epoch': 5.853131749460044}\n",
      "{'loss': 0.6626, 'grad_norm': 25.0, 'learning_rate': 1.3227907383152496e-05, 'epoch': 5.874730021598272}\n",
      "{'loss': 0.6635, 'grad_norm': 72.5, 'learning_rate': 1.3093491090264045e-05, 'epoch': 5.896328293736501}\n",
      "{'loss': 0.6723, 'grad_norm': 49.5, 'learning_rate': 1.2959074797375595e-05, 'epoch': 5.91792656587473}\n",
      "{'loss': 0.6703, 'grad_norm': 54.5, 'learning_rate': 1.2824658504487141e-05, 'epoch': 5.939524838012959}\n",
      "{'loss': 0.645, 'grad_norm': 42.0, 'learning_rate': 1.269024221159869e-05, 'epoch': 5.961123110151188}\n",
      "{'loss': 0.6828, 'grad_norm': 23.5, 'learning_rate': 1.255582591871024e-05, 'epoch': 5.982721382289417}\n",
      "{'eval_loss': 0.7212427854537964, 'eval_accuracy': 0.7305636540330418, 'eval_precision': 0.7399681556965518, 'eval_recall': 0.749951315124095, 'eval_f1': 0.7390363035346947, 'eval_runtime': 7.6533, 'eval_samples_per_second': 537.81, 'eval_steps_per_second': 67.292, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.7306 f1=0.7390 p=0.7400 r=0.7500\n",
      "{'loss': 0.6209, 'grad_norm': 76.0, 'learning_rate': 1.2421409625821788e-05, 'epoch': 6.004319654427646}\n",
      "{'loss': 0.6427, 'grad_norm': 12.8125, 'learning_rate': 1.2286993332933336e-05, 'epoch': 6.025917926565874}\n",
      "{'loss': 0.6727, 'grad_norm': 51.25, 'learning_rate': 1.2152577040044885e-05, 'epoch': 6.047516198704104}\n",
      "{'loss': 0.6457, 'grad_norm': 17.25, 'learning_rate': 1.2018160747156433e-05, 'epoch': 6.069114470842333}\n",
      "{'loss': 0.6811, 'grad_norm': 20.625, 'learning_rate': 1.1883744454267983e-05, 'epoch': 6.090712742980561}\n",
      "{'loss': 0.6247, 'grad_norm': 42.75, 'learning_rate': 1.1749328161379532e-05, 'epoch': 6.112311015118791}\n",
      "{'loss': 0.6627, 'grad_norm': 31.75, 'learning_rate': 1.161491186849108e-05, 'epoch': 6.133909287257019}\n",
      "{'loss': 0.6336, 'grad_norm': 21.0, 'learning_rate': 1.1480495575602628e-05, 'epoch': 6.155507559395248}\n",
      "{'loss': 0.6245, 'grad_norm': 26.625, 'learning_rate': 1.1346079282714177e-05, 'epoch': 6.177105831533478}\n",
      "{'loss': 0.6391, 'grad_norm': 26.0, 'learning_rate': 1.1211662989825727e-05, 'epoch': 6.198704103671706}\n",
      "{'loss': 0.6609, 'grad_norm': 41.0, 'learning_rate': 1.1077246696937275e-05, 'epoch': 6.220302375809935}\n",
      "{'loss': 0.6711, 'grad_norm': 19.875, 'learning_rate': 1.0942830404048824e-05, 'epoch': 6.241900647948164}\n",
      "{'loss': 0.6618, 'grad_norm': 33.5, 'learning_rate': 1.0808414111160372e-05, 'epoch': 6.263498920086393}\n",
      "{'loss': 0.617, 'grad_norm': 27.625, 'learning_rate': 1.067399781827192e-05, 'epoch': 6.285097192224622}\n",
      "{'loss': 0.6559, 'grad_norm': 30.0, 'learning_rate': 1.053958152538347e-05, 'epoch': 6.306695464362851}\n",
      "{'loss': 0.6428, 'grad_norm': 20.625, 'learning_rate': 1.0405165232495017e-05, 'epoch': 6.32829373650108}\n",
      "{'loss': 0.6362, 'grad_norm': 33.25, 'learning_rate': 1.0270748939606567e-05, 'epoch': 6.3498920086393085}\n",
      "{'loss': 0.6408, 'grad_norm': 45.0, 'learning_rate': 1.0136332646718116e-05, 'epoch': 6.371490280777538}\n",
      "{'loss': 0.681, 'grad_norm': 33.25, 'learning_rate': 1.0001916353829664e-05, 'epoch': 6.393088552915767}\n",
      "{'loss': 0.6313, 'grad_norm': 21.375, 'learning_rate': 9.867500060941213e-06, 'epoch': 6.4146868250539955}\n",
      "{'loss': 0.6902, 'grad_norm': 49.5, 'learning_rate': 9.733083768052761e-06, 'epoch': 6.436285097192225}\n",
      "{'loss': 0.6808, 'grad_norm': 59.25, 'learning_rate': 9.598667475164309e-06, 'epoch': 6.457883369330453}\n",
      "{'loss': 0.6281, 'grad_norm': 23.875, 'learning_rate': 9.464251182275859e-06, 'epoch': 6.479481641468682}\n",
      "{'loss': 0.6461, 'grad_norm': 14.375, 'learning_rate': 9.329834889387406e-06, 'epoch': 6.501079913606912}\n",
      "{'loss': 0.6976, 'grad_norm': 4.75, 'learning_rate': 9.195418596498956e-06, 'epoch': 6.52267818574514}\n",
      "{'loss': 0.6364, 'grad_norm': 19.25, 'learning_rate': 9.061002303610505e-06, 'epoch': 6.544276457883369}\n",
      "{'loss': 0.6201, 'grad_norm': 29.375, 'learning_rate': 8.926586010722053e-06, 'epoch': 6.565874730021598}\n",
      "{'loss': 0.6861, 'grad_norm': 39.0, 'learning_rate': 8.792169717833603e-06, 'epoch': 6.587473002159827}\n",
      "{'loss': 0.6591, 'grad_norm': 39.5, 'learning_rate': 8.65775342494515e-06, 'epoch': 6.609071274298056}\n",
      "{'loss': 0.7177, 'grad_norm': 40.75, 'learning_rate': 8.523337132056698e-06, 'epoch': 6.630669546436285}\n",
      "{'loss': 0.6541, 'grad_norm': 22.75, 'learning_rate': 8.388920839168248e-06, 'epoch': 6.652267818574514}\n",
      "{'loss': 0.651, 'grad_norm': 29.125, 'learning_rate': 8.254504546279796e-06, 'epoch': 6.6738660907127425}\n",
      "{'loss': 0.6812, 'grad_norm': 22.0, 'learning_rate': 8.120088253391345e-06, 'epoch': 6.695464362850972}\n",
      "{'loss': 0.692, 'grad_norm': 28.5, 'learning_rate': 7.985671960502895e-06, 'epoch': 6.717062634989201}\n",
      "{'loss': 0.6507, 'grad_norm': 34.75, 'learning_rate': 7.851255667614443e-06, 'epoch': 6.7386609071274295}\n",
      "{'loss': 0.6801, 'grad_norm': 14.875, 'learning_rate': 7.71683937472599e-06, 'epoch': 6.760259179265659}\n",
      "{'loss': 0.667, 'grad_norm': 34.75, 'learning_rate': 7.58242308183754e-06, 'epoch': 6.781857451403888}\n",
      "{'loss': 0.6008, 'grad_norm': 19.25, 'learning_rate': 7.448006788949088e-06, 'epoch': 6.8034557235421165}\n",
      "{'loss': 0.7012, 'grad_norm': 25.75, 'learning_rate': 7.313590496060637e-06, 'epoch': 6.825053995680346}\n",
      "{'loss': 0.6928, 'grad_norm': 27.375, 'learning_rate': 7.179174203172186e-06, 'epoch': 6.846652267818574}\n",
      "{'loss': 0.6286, 'grad_norm': 21.0, 'learning_rate': 7.044757910283734e-06, 'epoch': 6.8682505399568035}\n",
      "{'loss': 0.6475, 'grad_norm': 35.0, 'learning_rate': 6.910341617395283e-06, 'epoch': 6.889848812095033}\n",
      "{'loss': 0.6834, 'grad_norm': 50.5, 'learning_rate': 6.775925324506832e-06, 'epoch': 6.911447084233261}\n",
      "{'loss': 0.6406, 'grad_norm': 22.5, 'learning_rate': 6.64150903161838e-06, 'epoch': 6.93304535637149}\n",
      "{'loss': 0.6021, 'grad_norm': 63.25, 'learning_rate': 6.507092738729929e-06, 'epoch': 6.954643628509719}\n",
      "{'loss': 0.631, 'grad_norm': 55.75, 'learning_rate': 6.372676445841478e-06, 'epoch': 6.976241900647948}\n",
      "{'loss': 0.689, 'grad_norm': 12.0625, 'learning_rate': 6.238260152953026e-06, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 0.7115731239318848, 'eval_accuracy': 0.7359086491739553, 'eval_precision': 0.7450013098032694, 'eval_recall': 0.7538623294511984, 'eval_f1': 0.7445035494457448, 'eval_runtime': 7.8348, 'eval_samples_per_second': 525.35, 'eval_steps_per_second': 65.733, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.7359 f1=0.7445 p=0.7450 r=0.7539\n",
      "{'loss': 0.7301, 'grad_norm': 46.25, 'learning_rate': 6.1038438600645754e-06, 'epoch': 7.019438444924406}\n",
      "{'loss': 0.6255, 'grad_norm': 42.75, 'learning_rate': 5.969427567176123e-06, 'epoch': 7.041036717062635}\n",
      "{'loss': 0.6942, 'grad_norm': 55.25, 'learning_rate': 5.835011274287673e-06, 'epoch': 7.0626349892008635}\n",
      "{'loss': 0.6764, 'grad_norm': 27.75, 'learning_rate': 5.7005949813992215e-06, 'epoch': 7.084233261339093}\n",
      "{'loss': 0.6644, 'grad_norm': 27.5, 'learning_rate': 5.56617868851077e-06, 'epoch': 7.105831533477322}\n",
      "{'loss': 0.7307, 'grad_norm': 57.5, 'learning_rate': 5.431762395622318e-06, 'epoch': 7.1274298056155505}\n",
      "{'loss': 0.6115, 'grad_norm': 16.0, 'learning_rate': 5.2973461027338675e-06, 'epoch': 7.14902807775378}\n",
      "{'loss': 0.6627, 'grad_norm': 29.875, 'learning_rate': 5.162929809845416e-06, 'epoch': 7.170626349892009}\n",
      "{'loss': 0.6481, 'grad_norm': 44.75, 'learning_rate': 5.028513516956965e-06, 'epoch': 7.1922246220302375}\n",
      "{'loss': 0.6836, 'grad_norm': 30.0, 'learning_rate': 4.894097224068513e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 0.6376, 'grad_norm': 26.375, 'learning_rate': 4.759680931180062e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 0.6418, 'grad_norm': 73.0, 'learning_rate': 4.625264638291611e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 0.658, 'grad_norm': 28.75, 'learning_rate': 4.490848345403159e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 0.6153, 'grad_norm': 21.25, 'learning_rate': 4.356432052514707e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 0.65, 'grad_norm': 17.25, 'learning_rate': 4.222015759626257e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 0.6279, 'grad_norm': 21.125, 'learning_rate': 4.0875994667378055e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 0.6845, 'grad_norm': 28.625, 'learning_rate': 3.953183173849353e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 0.6342, 'grad_norm': 76.0, 'learning_rate': 3.818766880960902e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 0.5881, 'grad_norm': 30.625, 'learning_rate': 3.684350588072451e-06, 'epoch': 7.408207343412527}\n",
      "{'loss': 0.6363, 'grad_norm': 15.5, 'learning_rate': 3.5499342951840002e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 0.6407, 'grad_norm': 62.75, 'learning_rate': 3.4155180022955485e-06, 'epoch': 7.4514038876889845}\n",
      "{'loss': 0.6206, 'grad_norm': 31.875, 'learning_rate': 3.281101709407097e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 0.6128, 'grad_norm': 31.0, 'learning_rate': 3.146685416518646e-06, 'epoch': 7.494600431965443}\n",
      "{'loss': 0.6871, 'grad_norm': 41.0, 'learning_rate': 3.012269123630194e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 0.6427, 'grad_norm': 52.75, 'learning_rate': 2.877852830741743e-06, 'epoch': 7.537796976241901}\n",
      "{'loss': 0.6482, 'grad_norm': 39.75, 'learning_rate': 2.743436537853292e-06, 'epoch': 7.559395248380129}\n",
      "{'loss': 0.6326, 'grad_norm': 41.0, 'learning_rate': 2.6090202449648405e-06, 'epoch': 7.5809935205183585}\n",
      "{'loss': 0.6888, 'grad_norm': 12.625, 'learning_rate': 2.474603952076389e-06, 'epoch': 7.602591792656588}\n",
      "{'loss': 0.643, 'grad_norm': 15.75, 'learning_rate': 2.340187659187938e-06, 'epoch': 7.624190064794816}\n",
      "{'loss': 0.6293, 'grad_norm': 38.0, 'learning_rate': 2.2057713662994865e-06, 'epoch': 7.6457883369330455}\n",
      "{'loss': 0.6609, 'grad_norm': 79.5, 'learning_rate': 2.071355073411035e-06, 'epoch': 7.667386609071274}\n",
      "{'loss': 0.7303, 'grad_norm': 29.25, 'learning_rate': 1.936938780522584e-06, 'epoch': 7.688984881209503}\n",
      "{'loss': 0.6404, 'grad_norm': 43.0, 'learning_rate': 1.8025224876341325e-06, 'epoch': 7.7105831533477325}\n",
      "{'loss': 0.6696, 'grad_norm': 33.5, 'learning_rate': 1.668106194745681e-06, 'epoch': 7.732181425485961}\n",
      "{'loss': 0.6359, 'grad_norm': 71.0, 'learning_rate': 1.5336899018572299e-06, 'epoch': 7.75377969762419}\n",
      "{'loss': 0.665, 'grad_norm': 36.0, 'learning_rate': 1.3992736089687785e-06, 'epoch': 7.775377969762419}\n",
      "{'loss': 0.6625, 'grad_norm': 39.0, 'learning_rate': 1.2648573160803272e-06, 'epoch': 7.796976241900648}\n",
      "{'loss': 0.6536, 'grad_norm': 30.875, 'learning_rate': 1.1304410231918757e-06, 'epoch': 7.818574514038877}\n",
      "{'loss': 0.665, 'grad_norm': 12.25, 'learning_rate': 9.960247303034244e-07, 'epoch': 7.840172786177106}\n",
      "{'loss': 0.6476, 'grad_norm': 74.5, 'learning_rate': 8.61608437414973e-07, 'epoch': 7.861771058315335}\n",
      "{'loss': 0.6694, 'grad_norm': 53.5, 'learning_rate': 7.271921445265217e-07, 'epoch': 7.883369330453563}\n",
      "{'loss': 0.6432, 'grad_norm': 34.5, 'learning_rate': 5.927758516380704e-07, 'epoch': 7.9049676025917925}\n",
      "{'loss': 0.6391, 'grad_norm': 48.0, 'learning_rate': 4.5835955874961904e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 0.6133, 'grad_norm': 43.75, 'learning_rate': 3.239432658611677e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 0.7094, 'grad_norm': 64.5, 'learning_rate': 1.8952697297271636e-07, 'epoch': 7.9697624190064795}\n",
      "{'loss': 0.6579, 'grad_norm': 36.0, 'learning_rate': 5.511068008426505e-08, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 0.7098037600517273, 'eval_accuracy': 0.7346938775510204, 'eval_precision': 0.7434776716696587, 'eval_recall': 0.7533813436870493, 'eval_f1': 0.7434669208162482, 'eval_runtime': 7.9422, 'eval_samples_per_second': 518.242, 'eval_steps_per_second': 64.843, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.7347 f1=0.7435 p=0.7435 r=0.7534\n",
      "{'train_runtime': 1557.6284, 'train_samples_per_second': 190.233, 'train_steps_per_second': 23.78, 'train_loss': 0.7598385260892996, 'epoch': 8.0}\n",
      "{'eval_loss': 0.7221248745918274, 'eval_accuracy': 0.7359086491739553, 'eval_precision': 0.7465534664797963, 'eval_recall': 0.7528394489649252, 'eval_f1': 0.7445851369147556, 'eval_runtime': 8.2033, 'eval_samples_per_second': 501.752, 'eval_steps_per_second': 62.78, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.7359 f1=0.7446 p=0.7466 r=0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñÉ‚ñÖ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñà‚ñÜ‚ñÑ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÅ‚ñÑ‚ñà‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÑ</td></tr><tr><td>train/learning_rate</td><td>‚ñÉ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.74459</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.73591</td></tr><tr><td>eval/f1</td><td>0.74459</td></tr><tr><td>eval/loss</td><td>0.72212</td></tr><tr><td>eval/precision</td><td>0.74655</td></tr><tr><td>eval/recall</td><td>0.75284</td></tr><tr><td>eval/runtime</td><td>8.2033</td></tr><tr><td>eval/samples_per_second</td><td>501.752</td></tr><tr><td>eval/steps_per_second</td><td>62.78</td></tr><tr><td>total_flos</td><td>1.1916090232884816e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>37040</td></tr><tr><td>train/grad_norm</td><td>36</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6579</td></tr><tr><td>train_loss</td><td>0.75984</td></tr><tr><td>train_runtime</td><td>1557.6284</td></tr><tr><td>train_samples_per_second</td><td>190.233</td></tr><tr><td>train_steps_per_second</td><td>23.78</td></tr><tr><td>trial/accuracy</td><td>0.73591</td></tr><tr><td>trial/f1</td><td>0.74459</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t4</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/h3tjjl2t' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/h3tjjl2t</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_230842-h3tjjl2t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [1:53:41<1:53:02, 1356.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=4 f1=0.7446\n",
      "[I 2025-08-16 23:34:52,535] Trial 4 finished with value: 0.7445851369147556 and parameters: {'lr': 4.67997206949721e-05, 'weight_decay': 1.253691919878875e-05, 'unfreeze_last_k': 10, 'batch_size': 8}. Best is trial 2 with value: 0.8523413656247504.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_233452-vnar69s2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/vnar69s2' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t5</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/vnar69s2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/vnar69s2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=5 | epochs=8 bs=16 lr=9.51e-05 wd=2.0e-05 k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=16 lr=9.51e-05 wd=2.0e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/2315] loss=1.6455 lr=8.47e-06\n",
      "{'loss': 1.6455, 'grad_norm': 2.359375, 'learning_rate': 8.46672601852361e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b200/2315] loss=1.6320 lr=1.70e-05\n",
      "{'loss': 1.632, 'grad_norm': 1.5390625, 'learning_rate': 1.7018974522082814e-05, 'epoch': 0.08639308855291576}\n",
      "[e0 b300/2315] loss=1.5935 lr=2.56e-05\n",
      "{'loss': 1.5935, 'grad_norm': 4.40625, 'learning_rate': 2.5571223025642015e-05, 'epoch': 0.12958963282937366}\n",
      "[e0 b400/2315] loss=1.5452 lr=3.41e-05\n",
      "{'loss': 1.5452, 'grad_norm': 6.0625, 'learning_rate': 3.4123471529201215e-05, 'epoch': 0.17278617710583152}\n",
      "[e0 b500/2315] loss=1.5109 lr=4.27e-05\n",
      "{'loss': 1.5109, 'grad_norm': 19.25, 'learning_rate': 4.267572003276042e-05, 'epoch': 0.2159827213822894}\n",
      "[e0 b600/2315] loss=1.4298 lr=5.12e-05\n",
      "{'loss': 1.4298, 'grad_norm': 13.9375, 'learning_rate': 5.122796853631962e-05, 'epoch': 0.2591792656587473}\n",
      "[e0 b700/2315] loss=1.3100 lr=5.98e-05\n",
      "{'loss': 1.31, 'grad_norm': 14.9375, 'learning_rate': 5.978021703987883e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b800/2315] loss=1.2611 lr=6.83e-05\n",
      "{'loss': 1.2611, 'grad_norm': 12.8125, 'learning_rate': 6.833246554343802e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b900/2315] loss=1.1484 lr=7.69e-05\n",
      "{'loss': 1.1484, 'grad_norm': 13.8125, 'learning_rate': 7.688471404699722e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1000/2315] loss=1.1197 lr=8.54e-05\n",
      "{'loss': 1.1197, 'grad_norm': 14.1875, 'learning_rate': 8.543696255055643e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b1100/2315] loss=1.0147 lr=9.40e-05\n",
      "{'loss': 1.0147, 'grad_norm': 15.8125, 'learning_rate': 9.398921105411563e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b1200/2315] loss=0.9479 lr=9.46e-05\n",
      "{'loss': 0.9479, 'grad_norm': 8.9375, 'learning_rate': 9.462571686530655e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b1300/2315] loss=0.9353 lr=9.41e-05\n",
      "{'loss': 0.9353, 'grad_norm': 7.40625, 'learning_rate': 9.407941055005162e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b1400/2315] loss=0.8868 lr=9.35e-05\n",
      "{'loss': 0.8868, 'grad_norm': 13.0625, 'learning_rate': 9.353310423479668e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b1500/2315] loss=0.8914 lr=9.30e-05\n",
      "{'loss': 0.8914, 'grad_norm': 9.9375, 'learning_rate': 9.298679791954175e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b1600/2315] loss=0.8580 lr=9.24e-05\n",
      "{'loss': 0.858, 'grad_norm': 26.875, 'learning_rate': 9.244049160428682e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b1700/2315] loss=0.8476 lr=9.19e-05\n",
      "{'loss': 0.8476, 'grad_norm': 10.375, 'learning_rate': 9.189418528903189e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b1800/2315] loss=0.8184 lr=9.13e-05\n",
      "{'loss': 0.8184, 'grad_norm': 10.8125, 'learning_rate': 9.134787897377696e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b1900/2315] loss=0.7341 lr=9.08e-05\n",
      "{'loss': 0.7341, 'grad_norm': 8.9375, 'learning_rate': 9.080157265852203e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b2000/2315] loss=0.7861 lr=9.03e-05\n",
      "{'loss': 0.7861, 'grad_norm': 13.25, 'learning_rate': 9.02552663432671e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b2100/2315] loss=0.7376 lr=8.97e-05\n",
      "{'loss': 0.7376, 'grad_norm': 16.625, 'learning_rate': 8.970896002801217e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b2200/2315] loss=0.7467 lr=8.92e-05\n",
      "{'loss': 0.7467, 'grad_norm': 12.8125, 'learning_rate': 8.916265371275724e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b2300/2315] loss=0.7023 lr=8.86e-05\n",
      "{'loss': 0.7023, 'grad_norm': 15.125, 'learning_rate': 8.86163473975023e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 0.6783214211463928, 'eval_accuracy': 0.750485908649174, 'eval_precision': 0.7704521617545247, 'eval_recall': 0.7525145656213216, 'eval_f1': 0.7591296073885192, 'eval_runtime': 4.1575, 'eval_samples_per_second': 990.022, 'eval_steps_per_second': 62.057, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.7505 f1=0.7591 p=0.7705 r=0.7525\n",
      "{'loss': 0.6374, 'grad_norm': 6.40625, 'learning_rate': 8.807004108224737e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 0.6182, 'grad_norm': 8.1875, 'learning_rate': 8.752373476699244e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 0.6251, 'grad_norm': 35.25, 'learning_rate': 8.697742845173751e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 0.5973, 'grad_norm': 13.0625, 'learning_rate': 8.643112213648258e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 0.6185, 'grad_norm': 16.625, 'learning_rate': 8.588481582122765e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 0.5592, 'grad_norm': 13.125, 'learning_rate': 8.533850950597272e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 0.6268, 'grad_norm': 14.875, 'learning_rate': 8.479220319071779e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 0.5922, 'grad_norm': 13.4375, 'learning_rate': 8.424589687546286e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 0.5877, 'grad_norm': 7.71875, 'learning_rate': 8.369959056020793e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 0.6043, 'grad_norm': 25.5, 'learning_rate': 8.3153284244953e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 0.5652, 'grad_norm': 13.8125, 'learning_rate': 8.260697792969806e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 0.507, 'grad_norm': 31.375, 'learning_rate': 8.206067161444313e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 0.5313, 'grad_norm': 17.125, 'learning_rate': 8.151436529918822e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 0.5398, 'grad_norm': 13.375, 'learning_rate': 8.096805898393329e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 0.5772, 'grad_norm': 10.8125, 'learning_rate': 8.042175266867835e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 0.5163, 'grad_norm': 9.3125, 'learning_rate': 7.987544635342341e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 0.5401, 'grad_norm': 11.5625, 'learning_rate': 7.932914003816848e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 0.5347, 'grad_norm': 6.53125, 'learning_rate': 7.878283372291355e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 0.536, 'grad_norm': 8.8125, 'learning_rate': 7.823652740765862e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 0.5106, 'grad_norm': 5.375, 'learning_rate': 7.769022109240369e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 0.5154, 'grad_norm': 14.5, 'learning_rate': 7.714391477714875e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 0.4905, 'grad_norm': 18.25, 'learning_rate': 7.659760846189382e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 0.4311, 'grad_norm': 17.625, 'learning_rate': 7.605130214663889e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 0.5070944428443909, 'eval_accuracy': 0.8228862973760933, 'eval_precision': 0.8359863173467585, 'eval_recall': 0.8248408455088239, 'eval_f1': 0.8293269871382674, 'eval_runtime': 4.0956, 'eval_samples_per_second': 1004.991, 'eval_steps_per_second': 62.995, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.8229 f1=0.8293 p=0.8360 r=0.8248\n",
      "{'loss': 0.469, 'grad_norm': 11.1875, 'learning_rate': 7.550499583138396e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 0.4349, 'grad_norm': 17.875, 'learning_rate': 7.495868951612904e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 0.4682, 'grad_norm': 12.1875, 'learning_rate': 7.441238320087411e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 0.4056, 'grad_norm': 6.78125, 'learning_rate': 7.386607688561918e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 0.4134, 'grad_norm': 7.875, 'learning_rate': 7.331977057036425e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 0.4276, 'grad_norm': 10.0, 'learning_rate': 7.277346425510932e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 0.425, 'grad_norm': 7.5, 'learning_rate': 7.222715793985439e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 0.4105, 'grad_norm': 15.75, 'learning_rate': 7.168085162459946e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 0.4149, 'grad_norm': 8.6875, 'learning_rate': 7.113454530934453e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 0.4234, 'grad_norm': 8.8125, 'learning_rate': 7.058823899408958e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 0.4464, 'grad_norm': 18.5, 'learning_rate': 7.004193267883465e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 0.3883, 'grad_norm': 13.6875, 'learning_rate': 6.949562636357972e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 0.4475, 'grad_norm': 8.625, 'learning_rate': 6.89493200483248e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 0.4285, 'grad_norm': 8.75, 'learning_rate': 6.840301373306987e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 0.3819, 'grad_norm': 11.5, 'learning_rate': 6.785670741781494e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 0.4191, 'grad_norm': 9.9375, 'learning_rate': 6.731040110256001e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 0.4049, 'grad_norm': 9.375, 'learning_rate': 6.676409478730508e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 0.3926, 'grad_norm': 23.125, 'learning_rate': 6.621778847205015e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 0.3881, 'grad_norm': 28.125, 'learning_rate': 6.567148215679522e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 0.4234, 'grad_norm': 5.03125, 'learning_rate': 6.512517584154029e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 0.4418, 'grad_norm': 10.8125, 'learning_rate': 6.457886952628536e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 0.4102, 'grad_norm': 22.25, 'learning_rate': 6.403256321103043e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 0.4263, 'grad_norm': 10.4375, 'learning_rate': 6.34862568957755e-05, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 0.528827428817749, 'eval_accuracy': 0.8231292517006803, 'eval_precision': 0.8245127256967159, 'eval_recall': 0.8390280730942571, 'eval_f1': 0.8275850280198098, 'eval_runtime': 7.2037, 'eval_samples_per_second': 571.374, 'eval_steps_per_second': 35.815, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.8231 f1=0.8276 p=0.8245 r=0.8390\n",
      "{'loss': 0.3897, 'grad_norm': 18.0, 'learning_rate': 6.293995058052056e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 0.3519, 'grad_norm': 15.125, 'learning_rate': 6.239364426526563e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 0.3305, 'grad_norm': 8.5, 'learning_rate': 6.18473379500107e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 0.3431, 'grad_norm': 14.4375, 'learning_rate': 6.130103163475577e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 0.3909, 'grad_norm': 14.5625, 'learning_rate': 6.075472531950084e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 0.34, 'grad_norm': 5.15625, 'learning_rate': 6.020841900424591e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 0.3401, 'grad_norm': 18.25, 'learning_rate': 5.966211268899098e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 0.383, 'grad_norm': 15.5625, 'learning_rate': 5.9115806373736046e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 0.3723, 'grad_norm': 23.625, 'learning_rate': 5.8569500058481115e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 0.3394, 'grad_norm': 21.0, 'learning_rate': 5.8023193743226184e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 0.3353, 'grad_norm': 17.375, 'learning_rate': 5.747688742797125e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 0.3302, 'grad_norm': 4.46875, 'learning_rate': 5.693058111271632e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 0.3625, 'grad_norm': 31.875, 'learning_rate': 5.63842747974614e-05, 'epoch': 3.5421166306695464}\n",
      "{'loss': 0.3513, 'grad_norm': 12.125, 'learning_rate': 5.583796848220647e-05, 'epoch': 3.5853131749460045}\n",
      "{'loss': 0.3287, 'grad_norm': 10.3125, 'learning_rate': 5.5291662166951536e-05, 'epoch': 3.6285097192224622}\n",
      "{'loss': 0.352, 'grad_norm': 14.625, 'learning_rate': 5.4745355851696605e-05, 'epoch': 3.67170626349892}\n",
      "{'loss': 0.3574, 'grad_norm': 11.5625, 'learning_rate': 5.4199049536441674e-05, 'epoch': 3.714902807775378}\n",
      "{'loss': 0.3413, 'grad_norm': 10.875, 'learning_rate': 5.365274322118674e-05, 'epoch': 3.7580993520518358}\n",
      "{'loss': 0.343, 'grad_norm': 10.9375, 'learning_rate': 5.310643690593181e-05, 'epoch': 3.801295896328294}\n",
      "{'loss': 0.3791, 'grad_norm': 11.1875, 'learning_rate': 5.256013059067689e-05, 'epoch': 3.8444924406047516}\n",
      "{'loss': 0.3583, 'grad_norm': 13.8125, 'learning_rate': 5.2013824275421944e-05, 'epoch': 3.8876889848812093}\n",
      "{'loss': 0.3583, 'grad_norm': 9.75, 'learning_rate': 5.146751796016701e-05, 'epoch': 3.9308855291576674}\n",
      "{'loss': 0.3502, 'grad_norm': 19.125, 'learning_rate': 5.092121164491208e-05, 'epoch': 3.974082073434125}\n",
      "{'eval_loss': 0.49958497285842896, 'eval_accuracy': 0.848153547133139, 'eval_precision': 0.8480254765618345, 'eval_recall': 0.8590187331041298, 'eval_f1': 0.8521296468590419, 'eval_runtime': 4.1036, 'eval_samples_per_second': 1003.012, 'eval_steps_per_second': 62.871, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.8482 f1=0.8521 p=0.8480 r=0.8590\n",
      "{'loss': 0.3173, 'grad_norm': 3.6875, 'learning_rate': 5.037490532965716e-05, 'epoch': 4.017278617710583}\n",
      "{'loss': 0.2988, 'grad_norm': 6.9375, 'learning_rate': 4.9828599014402226e-05, 'epoch': 4.060475161987041}\n",
      "{'loss': 0.3462, 'grad_norm': 19.375, 'learning_rate': 4.9282292699147295e-05, 'epoch': 4.103671706263499}\n",
      "{'loss': 0.3108, 'grad_norm': 11.875, 'learning_rate': 4.8735986383892364e-05, 'epoch': 4.146868250539957}\n",
      "{'loss': 0.2971, 'grad_norm': 9.625, 'learning_rate': 4.8189680068637434e-05, 'epoch': 4.190064794816415}\n",
      "{'loss': 0.364, 'grad_norm': 9.125, 'learning_rate': 4.76433737533825e-05, 'epoch': 4.233261339092873}\n",
      "{'loss': 0.3187, 'grad_norm': 15.125, 'learning_rate': 4.709706743812757e-05, 'epoch': 4.27645788336933}\n",
      "{'loss': 0.2887, 'grad_norm': 27.0, 'learning_rate': 4.655076112287265e-05, 'epoch': 4.319654427645788}\n",
      "{'loss': 0.338, 'grad_norm': 9.5, 'learning_rate': 4.6004454807617716e-05, 'epoch': 4.362850971922247}\n",
      "{'loss': 0.3245, 'grad_norm': 12.625, 'learning_rate': 4.5458148492362785e-05, 'epoch': 4.406047516198704}\n",
      "{'loss': 0.3356, 'grad_norm': 22.25, 'learning_rate': 4.491184217710785e-05, 'epoch': 4.449244060475162}\n",
      "{'loss': 0.3166, 'grad_norm': 10.9375, 'learning_rate': 4.436553586185292e-05, 'epoch': 4.49244060475162}\n",
      "{'loss': 0.3177, 'grad_norm': 11.3125, 'learning_rate': 4.3819229546597986e-05, 'epoch': 4.535637149028078}\n",
      "{'loss': 0.2745, 'grad_norm': 2.234375, 'learning_rate': 4.327292323134306e-05, 'epoch': 4.578833693304536}\n",
      "{'loss': 0.3228, 'grad_norm': 18.875, 'learning_rate': 4.272661691608813e-05, 'epoch': 4.622030237580994}\n",
      "{'loss': 0.3156, 'grad_norm': 6.0, 'learning_rate': 4.21803106008332e-05, 'epoch': 4.665226781857451}\n",
      "{'loss': 0.3466, 'grad_norm': 15.1875, 'learning_rate': 4.163400428557827e-05, 'epoch': 4.708423326133909}\n",
      "{'loss': 0.2685, 'grad_norm': 18.75, 'learning_rate': 4.108769797032334e-05, 'epoch': 4.751619870410368}\n",
      "{'loss': 0.3199, 'grad_norm': 22.125, 'learning_rate': 4.05413916550684e-05, 'epoch': 4.794816414686825}\n",
      "{'loss': 0.3235, 'grad_norm': 9.1875, 'learning_rate': 3.9995085339813476e-05, 'epoch': 4.838012958963283}\n",
      "{'loss': 0.3018, 'grad_norm': 7.25, 'learning_rate': 3.9448779024558545e-05, 'epoch': 4.881209503239741}\n",
      "{'loss': 0.3207, 'grad_norm': 15.6875, 'learning_rate': 3.8902472709303614e-05, 'epoch': 4.924406047516198}\n",
      "{'loss': 0.3206, 'grad_norm': 9.125, 'learning_rate': 3.835616639404868e-05, 'epoch': 4.967602591792657}\n",
      "{'eval_loss': 0.49252408742904663, 'eval_accuracy': 0.8464528668610302, 'eval_precision': 0.8481715795973356, 'eval_recall': 0.8561361357081501, 'eval_f1': 0.8507414830832136, 'eval_runtime': 4.1316, 'eval_samples_per_second': 996.23, 'eval_steps_per_second': 62.446, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.8465 f1=0.8507 p=0.8482 r=0.8561\n",
      "{'loss': 0.3195, 'grad_norm': 3.921875, 'learning_rate': 3.780986007879375e-05, 'epoch': 5.010799136069115}\n",
      "{'loss': 0.2963, 'grad_norm': 6.75, 'learning_rate': 3.726355376353882e-05, 'epoch': 5.053995680345572}\n",
      "{'loss': 0.3051, 'grad_norm': 22.25, 'learning_rate': 3.671724744828389e-05, 'epoch': 5.09719222462203}\n",
      "{'loss': 0.2891, 'grad_norm': 14.0, 'learning_rate': 3.6170941133028966e-05, 'epoch': 5.140388768898488}\n",
      "{'loss': 0.2913, 'grad_norm': 7.0625, 'learning_rate': 3.562463481777403e-05, 'epoch': 5.183585313174946}\n",
      "{'loss': 0.2779, 'grad_norm': 2.15625, 'learning_rate': 3.50783285025191e-05, 'epoch': 5.226781857451404}\n",
      "{'loss': 0.307, 'grad_norm': 11.75, 'learning_rate': 3.4532022187264166e-05, 'epoch': 5.269978401727862}\n",
      "{'loss': 0.2948, 'grad_norm': 9.3125, 'learning_rate': 3.3985715872009235e-05, 'epoch': 5.313174946004319}\n",
      "{'loss': 0.2935, 'grad_norm': 12.375, 'learning_rate': 3.3439409556754304e-05, 'epoch': 5.356371490280777}\n",
      "{'loss': 0.2906, 'grad_norm': 9.375, 'learning_rate': 3.289310324149938e-05, 'epoch': 5.399568034557236}\n",
      "{'loss': 0.2946, 'grad_norm': 0.9453125, 'learning_rate': 3.234679692624445e-05, 'epoch': 5.442764578833693}\n",
      "{'loss': 0.2946, 'grad_norm': 24.5, 'learning_rate': 3.180049061098952e-05, 'epoch': 5.485961123110151}\n",
      "{'loss': 0.3033, 'grad_norm': 14.125, 'learning_rate': 3.125418429573458e-05, 'epoch': 5.529157667386609}\n",
      "{'loss': 0.325, 'grad_norm': 2.453125, 'learning_rate': 3.070787798047965e-05, 'epoch': 5.572354211663067}\n",
      "{'loss': 0.3055, 'grad_norm': 16.625, 'learning_rate': 3.016157166522472e-05, 'epoch': 5.615550755939525}\n",
      "{'loss': 0.3082, 'grad_norm': 13.8125, 'learning_rate': 2.961526534996979e-05, 'epoch': 5.658747300215983}\n",
      "{'loss': 0.3068, 'grad_norm': 29.0, 'learning_rate': 2.906895903471486e-05, 'epoch': 5.70194384449244}\n",
      "{'loss': 0.3111, 'grad_norm': 7.09375, 'learning_rate': 2.8522652719459932e-05, 'epoch': 5.745140388768899}\n",
      "{'loss': 0.3084, 'grad_norm': 16.125, 'learning_rate': 2.7976346404205e-05, 'epoch': 5.788336933045357}\n",
      "{'loss': 0.3332, 'grad_norm': 8.625, 'learning_rate': 2.743004008895007e-05, 'epoch': 5.831533477321814}\n",
      "{'loss': 0.2835, 'grad_norm': 14.5625, 'learning_rate': 2.6883733773695142e-05, 'epoch': 5.874730021598272}\n",
      "{'loss': 0.3022, 'grad_norm': 12.75, 'learning_rate': 2.6337427458440205e-05, 'epoch': 5.91792656587473}\n",
      "{'loss': 0.3025, 'grad_norm': 14.0, 'learning_rate': 2.5791121143185277e-05, 'epoch': 5.961123110151188}\n",
      "{'eval_loss': 0.5142173767089844, 'eval_accuracy': 0.8454810495626822, 'eval_precision': 0.8455696823520057, 'eval_recall': 0.8575472461653343, 'eval_f1': 0.8497406596934818, 'eval_runtime': 4.1398, 'eval_samples_per_second': 994.249, 'eval_steps_per_second': 62.322, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.8455 f1=0.8497 p=0.8456 r=0.8575\n",
      "{'loss': 0.2812, 'grad_norm': 13.0625, 'learning_rate': 2.5244814827930346e-05, 'epoch': 6.004319654427646}\n",
      "{'loss': 0.3087, 'grad_norm': 1.3046875, 'learning_rate': 2.4698508512675415e-05, 'epoch': 6.047516198704104}\n",
      "{'loss': 0.2837, 'grad_norm': 5.40625, 'learning_rate': 2.4152202197420484e-05, 'epoch': 6.090712742980561}\n",
      "{'loss': 0.309, 'grad_norm': 18.875, 'learning_rate': 2.3605895882165557e-05, 'epoch': 6.133909287257019}\n",
      "{'loss': 0.2694, 'grad_norm': 13.9375, 'learning_rate': 2.3059589566910622e-05, 'epoch': 6.177105831533478}\n",
      "{'loss': 0.2855, 'grad_norm': 48.0, 'learning_rate': 2.251328325165569e-05, 'epoch': 6.220302375809935}\n",
      "{'loss': 0.2972, 'grad_norm': 9.3125, 'learning_rate': 2.1966976936400764e-05, 'epoch': 6.263498920086393}\n",
      "{'loss': 0.3102, 'grad_norm': 5.53125, 'learning_rate': 2.1420670621145833e-05, 'epoch': 6.306695464362851}\n",
      "{'loss': 0.2773, 'grad_norm': 6.875, 'learning_rate': 2.0874364305890898e-05, 'epoch': 6.3498920086393085}\n",
      "{'loss': 0.3153, 'grad_norm': 7.96875, 'learning_rate': 2.032805799063597e-05, 'epoch': 6.393088552915767}\n",
      "{'loss': 0.258, 'grad_norm': 12.875, 'learning_rate': 1.978175167538104e-05, 'epoch': 6.436285097192225}\n",
      "{'loss': 0.2897, 'grad_norm': 20.375, 'learning_rate': 1.923544536012611e-05, 'epoch': 6.479481641468682}\n",
      "{'loss': 0.2954, 'grad_norm': 11.125, 'learning_rate': 1.868913904487118e-05, 'epoch': 6.52267818574514}\n",
      "{'loss': 0.2737, 'grad_norm': 8.6875, 'learning_rate': 1.8142832729616247e-05, 'epoch': 6.565874730021598}\n",
      "{'loss': 0.3067, 'grad_norm': 22.625, 'learning_rate': 1.7596526414361316e-05, 'epoch': 6.609071274298056}\n",
      "{'loss': 0.2826, 'grad_norm': 29.75, 'learning_rate': 1.7050220099106388e-05, 'epoch': 6.652267818574514}\n",
      "{'loss': 0.283, 'grad_norm': 17.0, 'learning_rate': 1.6503913783851457e-05, 'epoch': 6.695464362850972}\n",
      "{'loss': 0.2787, 'grad_norm': 32.0, 'learning_rate': 1.5957607468596523e-05, 'epoch': 6.7386609071274295}\n",
      "{'loss': 0.2541, 'grad_norm': 28.375, 'learning_rate': 1.5411301153341595e-05, 'epoch': 6.781857451403888}\n",
      "{'loss': 0.2982, 'grad_norm': 9.125, 'learning_rate': 1.4864994838086664e-05, 'epoch': 6.825053995680346}\n",
      "{'loss': 0.3329, 'grad_norm': 12.0, 'learning_rate': 1.4318688522831733e-05, 'epoch': 6.8682505399568035}\n",
      "{'loss': 0.2985, 'grad_norm': 5.09375, 'learning_rate': 1.37723822075768e-05, 'epoch': 6.911447084233261}\n",
      "{'loss': 0.2983, 'grad_norm': 11.3125, 'learning_rate': 1.3226075892321871e-05, 'epoch': 6.954643628509719}\n",
      "{'loss': 0.2995, 'grad_norm': 9.1875, 'learning_rate': 1.267976957706694e-05, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 0.514508068561554, 'eval_accuracy': 0.8462099125364432, 'eval_precision': 0.8461240694044113, 'eval_recall': 0.8583887606605269, 'eval_f1': 0.8504822476082377, 'eval_runtime': 4.1469, 'eval_samples_per_second': 992.556, 'eval_steps_per_second': 62.216, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.8462 f1=0.8505 p=0.8461 r=0.8584\n",
      "{'loss': 0.3103, 'grad_norm': 5.34375, 'learning_rate': 1.2133463261812011e-05, 'epoch': 7.041036717062635}\n",
      "{'loss': 0.319, 'grad_norm': 40.0, 'learning_rate': 1.158715694655708e-05, 'epoch': 7.084233261339093}\n",
      "{'loss': 0.3091, 'grad_norm': 17.625, 'learning_rate': 1.1040850631302149e-05, 'epoch': 7.1274298056155505}\n",
      "{'loss': 0.2564, 'grad_norm': 18.125, 'learning_rate': 1.0494544316047218e-05, 'epoch': 7.170626349892009}\n",
      "{'loss': 0.2932, 'grad_norm': 24.25, 'learning_rate': 9.948238000792289e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 0.2832, 'grad_norm': 20.875, 'learning_rate': 9.401931685537356e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 0.2761, 'grad_norm': 16.25, 'learning_rate': 8.855625370282427e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 0.2685, 'grad_norm': 12.25, 'learning_rate': 8.309319055027496e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 0.2833, 'grad_norm': 13.25, 'learning_rate': 7.763012739772565e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 0.2675, 'grad_norm': 6.78125, 'learning_rate': 7.216706424517634e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 0.2809, 'grad_norm': 13.8125, 'learning_rate': 6.670400109262704e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 0.3045, 'grad_norm': 10.6875, 'learning_rate': 6.124093794007773e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 0.2911, 'grad_norm': 22.25, 'learning_rate': 5.577787478752843e-06, 'epoch': 7.559395248380129}\n",
      "{'loss': 0.2821, 'grad_norm': 20.75, 'learning_rate': 5.031481163497912e-06, 'epoch': 7.602591792656588}\n",
      "{'loss': 0.2576, 'grad_norm': 13.625, 'learning_rate': 4.485174848242981e-06, 'epoch': 7.6457883369330455}\n",
      "{'loss': 0.3273, 'grad_norm': 9.5625, 'learning_rate': 3.938868532988051e-06, 'epoch': 7.688984881209503}\n",
      "{'loss': 0.2711, 'grad_norm': 17.25, 'learning_rate': 3.3925622177331197e-06, 'epoch': 7.732181425485961}\n",
      "{'loss': 0.2852, 'grad_norm': 12.3125, 'learning_rate': 2.8462559024781887e-06, 'epoch': 7.775377969762419}\n",
      "{'loss': 0.2704, 'grad_norm': 13.625, 'learning_rate': 2.299949587223258e-06, 'epoch': 7.818574514038877}\n",
      "{'loss': 0.2744, 'grad_norm': 2.375, 'learning_rate': 1.7536432719683274e-06, 'epoch': 7.861771058315335}\n",
      "{'loss': 0.2813, 'grad_norm': 18.875, 'learning_rate': 1.2073369567133969e-06, 'epoch': 7.9049676025917925}\n",
      "{'loss': 0.2673, 'grad_norm': 8.6875, 'learning_rate': 6.610306414584661e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 0.2873, 'grad_norm': 9.375, 'learning_rate': 1.1472432620353545e-07, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 0.5123448371887207, 'eval_accuracy': 0.8464528668610302, 'eval_precision': 0.8459082041717411, 'eval_recall': 0.858437603714518, 'eval_f1': 0.8504789771342344, 'eval_runtime': 3.9996, 'eval_samples_per_second': 1029.095, 'eval_steps_per_second': 64.506, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8465 f1=0.8505 p=0.8459 r=0.8584\n",
      "{'train_runtime': 867.2995, 'train_samples_per_second': 341.649, 'train_steps_per_second': 21.354, 'train_loss': 0.451524994594747, 'epoch': 8.0}\n",
      "{'eval_loss': 0.49958497285842896, 'eval_accuracy': 0.848153547133139, 'eval_precision': 0.8480254765618345, 'eval_recall': 0.8590187331041298, 'eval_f1': 0.8521296468590419, 'eval_runtime': 4.2163, 'eval_samples_per_second': 976.211, 'eval_steps_per_second': 61.191, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8482 f1=0.8521 p=0.8480 r=0.8590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñà‚ñÅ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñà‚ñÅ‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÇ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.85213</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.84815</td></tr><tr><td>eval/f1</td><td>0.85213</td></tr><tr><td>eval/loss</td><td>0.49958</td></tr><tr><td>eval/precision</td><td>0.84803</td></tr><tr><td>eval/recall</td><td>0.85902</td></tr><tr><td>eval/runtime</td><td>4.2163</td></tr><tr><td>eval/samples_per_second</td><td>976.211</td></tr><tr><td>eval/steps_per_second</td><td>61.191</td></tr><tr><td>total_flos</td><td>1.2584829318142752e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>18520</td></tr><tr><td>train/grad_norm</td><td>9.375</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2873</td></tr><tr><td>train_loss</td><td>0.45152</td></tr><tr><td>train_runtime</td><td>867.2995</td></tr><tr><td>train_samples_per_second</td><td>341.649</td></tr><tr><td>train_steps_per_second</td><td>21.354</td></tr><tr><td>trial/accuracy</td><td>0.84815</td></tr><tr><td>trial/f1</td><td>0.85213</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t5</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/vnar69s2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/vnar69s2</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_233452-vnar69s2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [2:08:17<1:19:32, 1193.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=5 f1=0.8521\n",
      "[I 2025-08-16 23:49:28,362] Trial 5 finished with value: 0.8521296468590419 and parameters: {'lr': 9.510100335957833e-05, 'weight_decay': 2.0408663465951878e-05, 'unfreeze_last_k': 11, 'batch_size': 16}. Best is trial 2 with value: 0.8523413656247504.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250816_234928-bygl5eo1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/bygl5eo1' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t6</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/bygl5eo1' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/bygl5eo1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=6 | epochs=8 bs=4 lr=1.44e-05 wd=6.3e-05 k=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[Run] epochs=8 bs=4 lr=1.44e-05 wd=6.3e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/9260] loss=1.6413 lr=3.21e-07\n",
      "{'loss': 1.6413, 'grad_norm': 6.03125, 'learning_rate': 3.207353172932416e-07, 'epoch': 0.01079913606911447}\n",
      "[e0 b200/9260] loss=1.6504 lr=6.45e-07\n",
      "{'loss': 1.6504, 'grad_norm': 2.84375, 'learning_rate': 6.44710385266213e-07, 'epoch': 0.02159827213822894}\n",
      "[e0 b300/9260] loss=1.6477 lr=9.69e-07\n",
      "{'loss': 1.6477, 'grad_norm': 7.1875, 'learning_rate': 9.68685453239184e-07, 'epoch': 0.032397408207343416}\n",
      "[e0 b400/9260] loss=1.6408 lr=1.29e-06\n",
      "{'loss': 1.6408, 'grad_norm': 4.8125, 'learning_rate': 1.2926605212121555e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b500/9260] loss=1.6511 lr=1.62e-06\n",
      "{'loss': 1.6511, 'grad_norm': 6.125, 'learning_rate': 1.6166355891851268e-06, 'epoch': 0.05399568034557235}\n",
      "[e0 b600/9260] loss=1.6474 lr=1.94e-06\n",
      "{'loss': 1.6474, 'grad_norm': 4.875, 'learning_rate': 1.940610657158098e-06, 'epoch': 0.06479481641468683}\n",
      "[e0 b700/9260] loss=1.6452 lr=2.26e-06\n",
      "{'loss': 1.6452, 'grad_norm': 4.59375, 'learning_rate': 2.264585725131069e-06, 'epoch': 0.0755939524838013}\n",
      "[e0 b800/9260] loss=1.6339 lr=2.59e-06\n",
      "{'loss': 1.6339, 'grad_norm': 3.4375, 'learning_rate': 2.5885607931040407e-06, 'epoch': 0.08639308855291576}\n",
      "[e0 b900/9260] loss=1.6376 lr=2.91e-06\n",
      "{'loss': 1.6376, 'grad_norm': 4.5625, 'learning_rate': 2.9125358610770123e-06, 'epoch': 0.09719222462203024}\n",
      "[e0 b1000/9260] loss=1.6448 lr=3.24e-06\n",
      "{'loss': 1.6448, 'grad_norm': 5.09375, 'learning_rate': 3.2365109290499834e-06, 'epoch': 0.1079913606911447}\n",
      "[e0 b1100/9260] loss=1.6480 lr=3.56e-06\n",
      "{'loss': 1.648, 'grad_norm': 4.375, 'learning_rate': 3.560485997022955e-06, 'epoch': 0.11879049676025918}\n",
      "[e0 b1200/9260] loss=1.6405 lr=3.88e-06\n",
      "{'loss': 1.6405, 'grad_norm': 5.03125, 'learning_rate': 3.884461064995925e-06, 'epoch': 0.12958963282937366}\n",
      "[e0 b1300/9260] loss=1.6432 lr=4.21e-06\n",
      "{'loss': 1.6432, 'grad_norm': 4.6875, 'learning_rate': 4.208436132968898e-06, 'epoch': 0.14038876889848811}\n",
      "[e0 b1400/9260] loss=1.6299 lr=4.53e-06\n",
      "{'loss': 1.6299, 'grad_norm': 4.59375, 'learning_rate': 4.5324112009418684e-06, 'epoch': 0.1511879049676026}\n",
      "[e0 b1500/9260] loss=1.6396 lr=4.86e-06\n",
      "{'loss': 1.6396, 'grad_norm': 4.75, 'learning_rate': 4.85638626891484e-06, 'epoch': 0.16198704103671707}\n",
      "[e0 b1600/9260] loss=1.6338 lr=5.18e-06\n",
      "{'loss': 1.6338, 'grad_norm': 5.0, 'learning_rate': 5.1803613368878116e-06, 'epoch': 0.17278617710583152}\n",
      "[e0 b1700/9260] loss=1.6402 lr=5.50e-06\n",
      "{'loss': 1.6402, 'grad_norm': 6.9375, 'learning_rate': 5.504336404860782e-06, 'epoch': 0.183585313174946}\n",
      "[e0 b1800/9260] loss=1.6275 lr=5.83e-06\n",
      "{'loss': 1.6275, 'grad_norm': 3.3125, 'learning_rate': 5.828311472833754e-06, 'epoch': 0.19438444924406048}\n",
      "[e0 b1900/9260] loss=1.6296 lr=6.15e-06\n",
      "{'loss': 1.6296, 'grad_norm': 4.9375, 'learning_rate': 6.152286540806725e-06, 'epoch': 0.20518358531317496}\n",
      "[e0 b2000/9260] loss=1.6316 lr=6.48e-06\n",
      "{'loss': 1.6316, 'grad_norm': 6.625, 'learning_rate': 6.476261608779697e-06, 'epoch': 0.2159827213822894}\n",
      "[e0 b2100/9260] loss=1.6153 lr=6.80e-06\n",
      "{'loss': 1.6153, 'grad_norm': 3.171875, 'learning_rate': 6.800236676752668e-06, 'epoch': 0.2267818574514039}\n",
      "[e0 b2200/9260] loss=1.6295 lr=7.12e-06\n",
      "{'loss': 1.6295, 'grad_norm': 2.828125, 'learning_rate': 7.124211744725639e-06, 'epoch': 0.23758099352051837}\n",
      "[e0 b2300/9260] loss=1.6275 lr=7.45e-06\n",
      "{'loss': 1.6275, 'grad_norm': 7.09375, 'learning_rate': 7.44818681269861e-06, 'epoch': 0.24838012958963282}\n",
      "[e0 b2400/9260] loss=1.6296 lr=7.77e-06\n",
      "{'loss': 1.6296, 'grad_norm': 6.0, 'learning_rate': 7.772161880671582e-06, 'epoch': 0.2591792656587473}\n",
      "[e0 b2500/9260] loss=1.6191 lr=8.10e-06\n",
      "{'loss': 1.6191, 'grad_norm': 4.65625, 'learning_rate': 8.096136948644553e-06, 'epoch': 0.26997840172786175}\n",
      "[e0 b2600/9260] loss=1.6211 lr=8.42e-06\n",
      "{'loss': 1.6211, 'grad_norm': 3.140625, 'learning_rate': 8.420112016617523e-06, 'epoch': 0.28077753779697623}\n",
      "[e0 b2700/9260] loss=1.6187 lr=8.74e-06\n",
      "{'loss': 1.6187, 'grad_norm': 4.71875, 'learning_rate': 8.744087084590495e-06, 'epoch': 0.2915766738660907}\n",
      "[e0 b2800/9260] loss=1.6163 lr=9.07e-06\n",
      "{'loss': 1.6163, 'grad_norm': 5.25, 'learning_rate': 9.068062152563466e-06, 'epoch': 0.3023758099352052}\n",
      "[e0 b2900/9260] loss=1.6071 lr=9.39e-06\n",
      "{'loss': 1.6071, 'grad_norm': 5.75, 'learning_rate': 9.392037220536438e-06, 'epoch': 0.31317494600431967}\n",
      "[e0 b3000/9260] loss=1.5952 lr=9.72e-06\n",
      "{'loss': 1.5952, 'grad_norm': 7.625, 'learning_rate': 9.716012288509411e-06, 'epoch': 0.32397408207343414}\n",
      "[e0 b3100/9260] loss=1.5727 lr=1.00e-05\n",
      "{'loss': 1.5727, 'grad_norm': 9.0625, 'learning_rate': 1.0039987356482381e-05, 'epoch': 0.3347732181425486}\n",
      "[e0 b3200/9260] loss=1.5657 lr=1.04e-05\n",
      "{'loss': 1.5657, 'grad_norm': 14.4375, 'learning_rate': 1.0363962424455352e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b3300/9260] loss=1.5555 lr=1.07e-05\n",
      "{'loss': 1.5555, 'grad_norm': 7.8125, 'learning_rate': 1.0687937492428324e-05, 'epoch': 0.3563714902807775}\n",
      "[e0 b3400/9260] loss=1.5502 lr=1.10e-05\n",
      "{'loss': 1.5502, 'grad_norm': 12.3125, 'learning_rate': 1.1011912560401296e-05, 'epoch': 0.367170626349892}\n",
      "[e0 b3500/9260] loss=1.5626 lr=1.13e-05\n",
      "{'loss': 1.5626, 'grad_norm': 12.0, 'learning_rate': 1.1335887628374266e-05, 'epoch': 0.3779697624190065}\n",
      "[e0 b3600/9260] loss=1.5557 lr=1.17e-05\n",
      "{'loss': 1.5557, 'grad_norm': 10.3125, 'learning_rate': 1.1659862696347237e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b3700/9260] loss=1.5777 lr=1.20e-05\n",
      "{'loss': 1.5777, 'grad_norm': 12.0, 'learning_rate': 1.1983837764320209e-05, 'epoch': 0.39956803455723544}\n",
      "[e0 b3800/9260] loss=1.5742 lr=1.23e-05\n",
      "{'loss': 1.5742, 'grad_norm': 13.1875, 'learning_rate': 1.230781283229318e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b3900/9260] loss=1.5654 lr=1.26e-05\n",
      "{'loss': 1.5654, 'grad_norm': 10.6875, 'learning_rate': 1.2631787900266152e-05, 'epoch': 0.42116630669546434}\n",
      "[e0 b4000/9260] loss=1.5791 lr=1.30e-05\n",
      "{'loss': 1.5791, 'grad_norm': 12.9375, 'learning_rate': 1.2955762968239122e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b4100/9260] loss=1.5650 lr=1.33e-05\n",
      "{'loss': 1.565, 'grad_norm': 11.375, 'learning_rate': 1.3279738036212093e-05, 'epoch': 0.4427645788336933}\n",
      "[e0 b4200/9260] loss=1.5497 lr=1.36e-05\n",
      "{'loss': 1.5497, 'grad_norm': 11.6875, 'learning_rate': 1.3603713104185065e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b4300/9260] loss=1.5635 lr=1.39e-05\n",
      "{'loss': 1.5635, 'grad_norm': 9.625, 'learning_rate': 1.3927688172158038e-05, 'epoch': 0.46436285097192226}\n",
      "[e0 b4400/9260] loss=1.5515 lr=1.43e-05\n",
      "{'loss': 1.5515, 'grad_norm': 12.4375, 'learning_rate': 1.4251663240131008e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b4500/9260] loss=1.5490 lr=1.44e-05\n",
      "{'loss': 1.549, 'grad_norm': 12.75, 'learning_rate': 1.4389524436643703e-05, 'epoch': 0.48596112311015116}\n",
      "[e0 b4600/9260] loss=1.5496 lr=1.44e-05\n",
      "{'loss': 1.5496, 'grad_norm': 10.75, 'learning_rate': 1.4368844187097642e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b4700/9260] loss=1.5603 lr=1.43e-05\n",
      "{'loss': 1.5603, 'grad_norm': 8.6875, 'learning_rate': 1.4348163937551584e-05, 'epoch': 0.5075593952483801}\n",
      "[e0 b4800/9260] loss=1.5532 lr=1.43e-05\n",
      "{'loss': 1.5532, 'grad_norm': 10.0625, 'learning_rate': 1.4327483688005523e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b4900/9260] loss=1.5333 lr=1.43e-05\n",
      "{'loss': 1.5333, 'grad_norm': 11.875, 'learning_rate': 1.4306803438459464e-05, 'epoch': 0.5291576673866091}\n",
      "[e0 b5000/9260] loss=1.5305 lr=1.43e-05\n",
      "{'loss': 1.5305, 'grad_norm': 16.25, 'learning_rate': 1.4286123188913404e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b5100/9260] loss=1.5134 lr=1.43e-05\n",
      "{'loss': 1.5134, 'grad_norm': 11.6875, 'learning_rate': 1.4265442939367345e-05, 'epoch': 0.550755939524838}\n",
      "[e0 b5200/9260] loss=1.5622 lr=1.42e-05\n",
      "{'loss': 1.5622, 'grad_norm': 9.875, 'learning_rate': 1.4244762689821285e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b5300/9260] loss=1.5694 lr=1.42e-05\n",
      "{'loss': 1.5694, 'grad_norm': 11.4375, 'learning_rate': 1.4224082440275226e-05, 'epoch': 0.572354211663067}\n",
      "[e0 b5400/9260] loss=1.5083 lr=1.42e-05\n",
      "{'loss': 1.5083, 'grad_norm': 14.875, 'learning_rate': 1.4203402190729166e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b5500/9260] loss=1.5204 lr=1.42e-05\n",
      "{'loss': 1.5204, 'grad_norm': 13.75, 'learning_rate': 1.4182721941183107e-05, 'epoch': 0.593952483801296}\n",
      "[e0 b5600/9260] loss=1.5367 lr=1.42e-05\n",
      "{'loss': 1.5367, 'grad_norm': 17.5, 'learning_rate': 1.4162041691637047e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b5700/9260] loss=1.5255 lr=1.41e-05\n",
      "{'loss': 1.5255, 'grad_norm': 11.625, 'learning_rate': 1.4141361442090988e-05, 'epoch': 0.6155507559395248}\n",
      "[e0 b5800/9260] loss=1.5210 lr=1.41e-05\n",
      "{'loss': 1.521, 'grad_norm': 14.5, 'learning_rate': 1.4120681192544927e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b5900/9260] loss=1.5678 lr=1.41e-05\n",
      "{'loss': 1.5678, 'grad_norm': 15.875, 'learning_rate': 1.4100000942998869e-05, 'epoch': 0.6371490280777538}\n",
      "[e0 b6000/9260] loss=1.5136 lr=1.41e-05\n",
      "{'loss': 1.5136, 'grad_norm': 23.625, 'learning_rate': 1.4079320693452808e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b6100/9260] loss=1.5606 lr=1.41e-05\n",
      "{'loss': 1.5606, 'grad_norm': 14.125, 'learning_rate': 1.405864044390675e-05, 'epoch': 0.6587473002159827}\n",
      "[e0 b6200/9260] loss=1.5081 lr=1.40e-05\n",
      "{'loss': 1.5081, 'grad_norm': 13.5625, 'learning_rate': 1.4037960194360689e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b6300/9260] loss=1.5056 lr=1.40e-05\n",
      "{'loss': 1.5056, 'grad_norm': 14.9375, 'learning_rate': 1.401727994481463e-05, 'epoch': 0.6803455723542117}\n",
      "[e0 b6400/9260] loss=1.4956 lr=1.40e-05\n",
      "{'loss': 1.4956, 'grad_norm': 15.75, 'learning_rate': 1.399659969526857e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b6500/9260] loss=1.5092 lr=1.40e-05\n",
      "{'loss': 1.5092, 'grad_norm': 15.625, 'learning_rate': 1.3975919445722511e-05, 'epoch': 0.7019438444924406}\n",
      "[e0 b6600/9260] loss=1.5452 lr=1.40e-05\n",
      "{'loss': 1.5452, 'grad_norm': 11.0625, 'learning_rate': 1.395523919617645e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b6700/9260] loss=1.5211 lr=1.39e-05\n",
      "{'loss': 1.5211, 'grad_norm': 15.125, 'learning_rate': 1.3934558946630392e-05, 'epoch': 0.7235421166306696}\n",
      "[e0 b6800/9260] loss=1.5264 lr=1.39e-05\n",
      "{'loss': 1.5264, 'grad_norm': 16.75, 'learning_rate': 1.3913878697084331e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b6900/9260] loss=1.5239 lr=1.39e-05\n",
      "{'loss': 1.5239, 'grad_norm': 12.5, 'learning_rate': 1.3893198447538273e-05, 'epoch': 0.7451403887688985}\n",
      "[e0 b7000/9260] loss=1.4964 lr=1.39e-05\n",
      "{'loss': 1.4964, 'grad_norm': 21.0, 'learning_rate': 1.3872518197992212e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b7100/9260] loss=1.5428 lr=1.39e-05\n",
      "{'loss': 1.5428, 'grad_norm': 13.375, 'learning_rate': 1.3851837948446154e-05, 'epoch': 0.7667386609071274}\n",
      "[e0 b7200/9260] loss=1.4971 lr=1.38e-05\n",
      "{'loss': 1.4971, 'grad_norm': 49.5, 'learning_rate': 1.3831157698900093e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b7300/9260] loss=1.5455 lr=1.38e-05\n",
      "{'loss': 1.5455, 'grad_norm': 14.3125, 'learning_rate': 1.3810477449354034e-05, 'epoch': 0.7883369330453563}\n",
      "[e0 b7400/9260] loss=1.5404 lr=1.38e-05\n",
      "{'loss': 1.5404, 'grad_norm': 35.0, 'learning_rate': 1.3789797199807974e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b7500/9260] loss=1.5367 lr=1.38e-05\n",
      "{'loss': 1.5367, 'grad_norm': 18.0, 'learning_rate': 1.3769116950261915e-05, 'epoch': 0.8099352051835853}\n",
      "[e0 b7600/9260] loss=1.5204 lr=1.37e-05\n",
      "{'loss': 1.5204, 'grad_norm': 19.25, 'learning_rate': 1.3748436700715855e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b7700/9260] loss=1.5125 lr=1.37e-05\n",
      "{'loss': 1.5125, 'grad_norm': 12.75, 'learning_rate': 1.3727756451169796e-05, 'epoch': 0.8315334773218143}\n",
      "[e0 b7800/9260] loss=1.4931 lr=1.37e-05\n",
      "{'loss': 1.4931, 'grad_norm': 19.0, 'learning_rate': 1.3707076201623737e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b7900/9260] loss=1.5015 lr=1.37e-05\n",
      "{'loss': 1.5015, 'grad_norm': 13.5, 'learning_rate': 1.3686395952077679e-05, 'epoch': 0.8531317494600432}\n",
      "[e0 b8000/9260] loss=1.5262 lr=1.37e-05\n",
      "{'loss': 1.5262, 'grad_norm': 13.0, 'learning_rate': 1.3665715702531618e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b8100/9260] loss=1.5037 lr=1.36e-05\n",
      "{'loss': 1.5037, 'grad_norm': 16.25, 'learning_rate': 1.364503545298556e-05, 'epoch': 0.8747300215982722}\n",
      "[e0 b8200/9260] loss=1.5238 lr=1.36e-05\n",
      "{'loss': 1.5238, 'grad_norm': 14.75, 'learning_rate': 1.3624355203439499e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b8300/9260] loss=1.4887 lr=1.36e-05\n",
      "{'loss': 1.4887, 'grad_norm': 11.0625, 'learning_rate': 1.360367495389344e-05, 'epoch': 0.896328293736501}\n",
      "[e0 b8400/9260] loss=1.5044 lr=1.36e-05\n",
      "{'loss': 1.5044, 'grad_norm': 17.375, 'learning_rate': 1.358299470434738e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b8500/9260] loss=1.5150 lr=1.36e-05\n",
      "{'loss': 1.515, 'grad_norm': 15.8125, 'learning_rate': 1.3562314454801321e-05, 'epoch': 0.91792656587473}\n",
      "[e0 b8600/9260] loss=1.5413 lr=1.35e-05\n",
      "{'loss': 1.5413, 'grad_norm': 23.75, 'learning_rate': 1.354163420525526e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b8700/9260] loss=1.5278 lr=1.35e-05\n",
      "{'loss': 1.5278, 'grad_norm': 22.75, 'learning_rate': 1.3520953955709202e-05, 'epoch': 0.9395248380129589}\n",
      "[e0 b8800/9260] loss=1.4992 lr=1.35e-05\n",
      "{'loss': 1.4992, 'grad_norm': 23.0, 'learning_rate': 1.3500273706163141e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b8900/9260] loss=1.5281 lr=1.35e-05\n",
      "{'loss': 1.5281, 'grad_norm': 13.8125, 'learning_rate': 1.3479593456617083e-05, 'epoch': 0.9611231101511879}\n",
      "[e0 b9000/9260] loss=1.4896 lr=1.35e-05\n",
      "{'loss': 1.4896, 'grad_norm': 16.25, 'learning_rate': 1.3458913207071022e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b9100/9260] loss=1.5223 lr=1.34e-05\n",
      "{'loss': 1.5223, 'grad_norm': 32.25, 'learning_rate': 1.3438232957524963e-05, 'epoch': 0.9827213822894169}\n",
      "[e0 b9200/9260] loss=1.5067 lr=1.34e-05\n",
      "{'loss': 1.5067, 'grad_norm': 15.1875, 'learning_rate': 1.3417552707978903e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.5148460865020752, 'eval_accuracy': 0.32142857142857145, 'eval_precision': 0.3392621178604524, 'eval_recall': 0.2535474337402895, 'eval_f1': 0.20395102514311186, 'eval_runtime': 15.64, 'eval_samples_per_second': 263.171, 'eval_steps_per_second': 65.793, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.3214 f1=0.2040 p=0.3393 r=0.2535\n",
      "{'loss': 1.4932, 'grad_norm': 18.75, 'learning_rate': 1.3396872458432844e-05, 'epoch': 1.0043196544276458}\n",
      "{'loss': 1.5023, 'grad_norm': 11.0, 'learning_rate': 1.3376192208886784e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 1.5005, 'grad_norm': 12.3125, 'learning_rate': 1.3355511959340725e-05, 'epoch': 1.0259179265658747}\n",
      "{'loss': 1.5069, 'grad_norm': 14.75, 'learning_rate': 1.3334831709794665e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.5024, 'grad_norm': 20.375, 'learning_rate': 1.3314151460248606e-05, 'epoch': 1.0475161987041037}\n",
      "{'loss': 1.5172, 'grad_norm': 12.9375, 'learning_rate': 1.3293471210702545e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 1.5087, 'grad_norm': 19.25, 'learning_rate': 1.3272790961156487e-05, 'epoch': 1.0691144708423326}\n",
      "{'loss': 1.4822, 'grad_norm': 39.75, 'learning_rate': 1.3252110711610426e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.5179, 'grad_norm': 18.625, 'learning_rate': 1.3231430462064368e-05, 'epoch': 1.0907127429805616}\n",
      "{'loss': 1.4698, 'grad_norm': 23.375, 'learning_rate': 1.3210750212518309e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 1.4991, 'grad_norm': 15.4375, 'learning_rate': 1.3190069962972248e-05, 'epoch': 1.1123110151187905}\n",
      "{'loss': 1.5141, 'grad_norm': 17.75, 'learning_rate': 1.316938971342619e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.4814, 'grad_norm': 22.5, 'learning_rate': 1.314870946388013e-05, 'epoch': 1.1339092872570196}\n",
      "{'loss': 1.476, 'grad_norm': 15.3125, 'learning_rate': 1.312802921433407e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 1.5344, 'grad_norm': 15.3125, 'learning_rate': 1.310734896478801e-05, 'epoch': 1.1555075593952484}\n",
      "{'loss': 1.4712, 'grad_norm': 15.5625, 'learning_rate': 1.3086668715241951e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.4821, 'grad_norm': 25.375, 'learning_rate': 1.306598846569589e-05, 'epoch': 1.1771058315334773}\n",
      "{'loss': 1.4916, 'grad_norm': 17.625, 'learning_rate': 1.3045308216149832e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 1.4834, 'grad_norm': 13.6875, 'learning_rate': 1.3024627966603772e-05, 'epoch': 1.1987041036717063}\n",
      "{'loss': 1.4753, 'grad_norm': 10.875, 'learning_rate': 1.3003947717057713e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.4985, 'grad_norm': 10.25, 'learning_rate': 1.2983267467511652e-05, 'epoch': 1.2203023758099352}\n",
      "{'loss': 1.4914, 'grad_norm': 14.375, 'learning_rate': 1.2962587217965594e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 1.4815, 'grad_norm': 33.0, 'learning_rate': 1.2941906968419533e-05, 'epoch': 1.2419006479481642}\n",
      "{'loss': 1.4871, 'grad_norm': 30.125, 'learning_rate': 1.2921226718873475e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.5257, 'grad_norm': 12.8125, 'learning_rate': 1.2900546469327414e-05, 'epoch': 1.263498920086393}\n",
      "{'loss': 1.5147, 'grad_norm': 24.75, 'learning_rate': 1.2879866219781355e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 1.4751, 'grad_norm': 14.8125, 'learning_rate': 1.2859185970235295e-05, 'epoch': 1.285097192224622}\n",
      "{'loss': 1.5149, 'grad_norm': 33.75, 'learning_rate': 1.2838505720689236e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.4995, 'grad_norm': 15.0625, 'learning_rate': 1.2817825471143176e-05, 'epoch': 1.306695464362851}\n",
      "{'loss': 1.4714, 'grad_norm': 37.5, 'learning_rate': 1.2797145221597117e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 1.4966, 'grad_norm': 19.0, 'learning_rate': 1.2776464972051057e-05, 'epoch': 1.3282937365010798}\n",
      "{'loss': 1.4981, 'grad_norm': 33.0, 'learning_rate': 1.2755784722504998e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.4851, 'grad_norm': 17.0, 'learning_rate': 1.2735104472958937e-05, 'epoch': 1.349892008639309}\n",
      "{'loss': 1.499, 'grad_norm': 12.5, 'learning_rate': 1.2714424223412879e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 1.5078, 'grad_norm': 17.5, 'learning_rate': 1.2693743973866818e-05, 'epoch': 1.3714902807775378}\n",
      "{'loss': 1.4886, 'grad_norm': 25.75, 'learning_rate': 1.267306372432076e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.5091, 'grad_norm': 24.75, 'learning_rate': 1.2652383474774699e-05, 'epoch': 1.3930885529157666}\n",
      "{'loss': 1.4834, 'grad_norm': 12.5625, 'learning_rate': 1.263170322522864e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 1.4821, 'grad_norm': 20.5, 'learning_rate': 1.261102297568258e-05, 'epoch': 1.4146868250539957}\n",
      "{'loss': 1.4985, 'grad_norm': 16.375, 'learning_rate': 1.2590342726136521e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.4879, 'grad_norm': 14.1875, 'learning_rate': 1.256966247659046e-05, 'epoch': 1.4362850971922247}\n",
      "{'loss': 1.4863, 'grad_norm': 15.0, 'learning_rate': 1.2548982227044402e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 1.459, 'grad_norm': 23.875, 'learning_rate': 1.2528301977498342e-05, 'epoch': 1.4578833693304536}\n",
      "{'loss': 1.4529, 'grad_norm': 22.125, 'learning_rate': 1.2507621727952283e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.4961, 'grad_norm': 15.0625, 'learning_rate': 1.2486941478406222e-05, 'epoch': 1.4794816414686824}\n",
      "{'loss': 1.5192, 'grad_norm': 19.125, 'learning_rate': 1.2466261228860164e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 1.5057, 'grad_norm': 13.0625, 'learning_rate': 1.2445580979314103e-05, 'epoch': 1.5010799136069113}\n",
      "{'loss': 1.4962, 'grad_norm': 20.25, 'learning_rate': 1.2424900729768044e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.5126, 'grad_norm': 27.75, 'learning_rate': 1.2404220480221984e-05, 'epoch': 1.5226781857451404}\n",
      "{'loss': 1.4998, 'grad_norm': 13.9375, 'learning_rate': 1.2383540230675925e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 1.4777, 'grad_norm': 16.625, 'learning_rate': 1.2362859981129865e-05, 'epoch': 1.5442764578833694}\n",
      "{'loss': 1.4999, 'grad_norm': 17.5, 'learning_rate': 1.2342179731583806e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.52, 'grad_norm': 14.125, 'learning_rate': 1.2321499482037746e-05, 'epoch': 1.5658747300215983}\n",
      "{'loss': 1.4967, 'grad_norm': 17.0, 'learning_rate': 1.2300819232491687e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 1.4685, 'grad_norm': 15.75, 'learning_rate': 1.2280138982945626e-05, 'epoch': 1.5874730021598271}\n",
      "{'loss': 1.4787, 'grad_norm': 13.125, 'learning_rate': 1.2259458733399568e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.4852, 'grad_norm': 25.75, 'learning_rate': 1.2238778483853507e-05, 'epoch': 1.6090712742980562}\n",
      "{'loss': 1.4548, 'grad_norm': 14.75, 'learning_rate': 1.2218098234307449e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 1.5192, 'grad_norm': 39.5, 'learning_rate': 1.2197417984761388e-05, 'epoch': 1.6306695464362853}\n",
      "{'loss': 1.5025, 'grad_norm': 24.625, 'learning_rate': 1.217673773521533e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.4769, 'grad_norm': 25.75, 'learning_rate': 1.2156057485669269e-05, 'epoch': 1.652267818574514}\n",
      "{'loss': 1.4476, 'grad_norm': 14.5, 'learning_rate': 1.213537723612321e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 1.4677, 'grad_norm': 49.25, 'learning_rate': 1.211469698657715e-05, 'epoch': 1.673866090712743}\n",
      "{'loss': 1.4821, 'grad_norm': 16.125, 'learning_rate': 1.2094016737031091e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.4944, 'grad_norm': 21.625, 'learning_rate': 1.207333648748503e-05, 'epoch': 1.6954643628509718}\n",
      "{'loss': 1.5008, 'grad_norm': 20.75, 'learning_rate': 1.2052656237938972e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 1.4688, 'grad_norm': 27.375, 'learning_rate': 1.2031975988392911e-05, 'epoch': 1.7170626349892009}\n",
      "{'loss': 1.4967, 'grad_norm': 13.4375, 'learning_rate': 1.2011295738846853e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.4977, 'grad_norm': 25.75, 'learning_rate': 1.1990615489300792e-05, 'epoch': 1.73866090712743}\n",
      "{'loss': 1.5002, 'grad_norm': 29.375, 'learning_rate': 1.1969935239754733e-05, 'epoch': 1.7494600431965441}\n",
      "{'loss': 1.4928, 'grad_norm': 14.6875, 'learning_rate': 1.1949254990208675e-05, 'epoch': 1.7602591792656588}\n",
      "{'loss': 1.4815, 'grad_norm': 16.25, 'learning_rate': 1.1928574740662616e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.5157, 'grad_norm': 19.875, 'learning_rate': 1.1907894491116556e-05, 'epoch': 1.7818574514038876}\n",
      "{'loss': 1.4842, 'grad_norm': 11.1875, 'learning_rate': 1.1887214241570497e-05, 'epoch': 1.7926565874730023}\n",
      "{'loss': 1.5164, 'grad_norm': 23.125, 'learning_rate': 1.1866533992024436e-05, 'epoch': 1.8034557235421165}\n",
      "{'loss': 1.4985, 'grad_norm': 13.375, 'learning_rate': 1.1845853742478378e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.4709, 'grad_norm': 15.3125, 'learning_rate': 1.1825173492932317e-05, 'epoch': 1.8250539956803455}\n",
      "{'loss': 1.4902, 'grad_norm': 16.75, 'learning_rate': 1.1804493243386258e-05, 'epoch': 1.83585313174946}\n",
      "{'loss': 1.4517, 'grad_norm': 14.8125, 'learning_rate': 1.1783812993840198e-05, 'epoch': 1.8466522678185746}\n",
      "{'loss': 1.4995, 'grad_norm': 32.75, 'learning_rate': 1.176313274429414e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 1.4657, 'grad_norm': 16.625, 'learning_rate': 1.1742452494748079e-05, 'epoch': 1.8682505399568035}\n",
      "{'loss': 1.4894, 'grad_norm': 17.0, 'learning_rate': 1.172177224520202e-05, 'epoch': 1.8790496760259179}\n",
      "{'loss': 1.4776, 'grad_norm': 13.5, 'learning_rate': 1.170109199565596e-05, 'epoch': 1.8898488120950323}\n",
      "{'loss': 1.471, 'grad_norm': 24.0, 'learning_rate': 1.1680411746109901e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.4974, 'grad_norm': 17.25, 'learning_rate': 1.165973149656384e-05, 'epoch': 1.9114470842332614}\n",
      "{'loss': 1.5127, 'grad_norm': 15.8125, 'learning_rate': 1.1639051247017782e-05, 'epoch': 1.9222462203023758}\n",
      "{'loss': 1.5141, 'grad_norm': 15.875, 'learning_rate': 1.1618370997471721e-05, 'epoch': 1.9330453563714904}\n",
      "{'loss': 1.498, 'grad_norm': 30.0, 'learning_rate': 1.1597690747925663e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.5044, 'grad_norm': 14.6875, 'learning_rate': 1.1577010498379602e-05, 'epoch': 1.9546436285097193}\n",
      "{'loss': 1.4685, 'grad_norm': 12.5625, 'learning_rate': 1.1556330248833543e-05, 'epoch': 1.9654427645788337}\n",
      "{'loss': 1.4808, 'grad_norm': 13.25, 'learning_rate': 1.1535649999287483e-05, 'epoch': 1.9762419006479481}\n",
      "{'loss': 1.4604, 'grad_norm': 18.125, 'learning_rate': 1.1514969749741424e-05, 'epoch': 1.9870410367170628}\n",
      "{'loss': 1.5111, 'grad_norm': 19.125, 'learning_rate': 1.1494289500195364e-05, 'epoch': 1.997840172786177}\n",
      "{'eval_loss': 1.490527868270874, 'eval_accuracy': 0.3435374149659864, 'eval_precision': 0.3393487411609243, 'eval_recall': 0.2789569187492753, 'eval_f1': 0.25034281459004876, 'eval_runtime': 15.5933, 'eval_samples_per_second': 263.959, 'eval_steps_per_second': 65.99, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.3435 f1=0.2503 p=0.3393 r=0.2790\n",
      "{'loss': 1.5209, 'grad_norm': 18.0, 'learning_rate': 1.1473609250649305e-05, 'epoch': 2.0086393088552916}\n",
      "{'loss': 1.4795, 'grad_norm': 19.125, 'learning_rate': 1.1452929001103245e-05, 'epoch': 2.019438444924406}\n",
      "{'loss': 1.4649, 'grad_norm': 17.375, 'learning_rate': 1.1432248751557186e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.507, 'grad_norm': 22.75, 'learning_rate': 1.1411568502011125e-05, 'epoch': 2.041036717062635}\n",
      "{'loss': 1.4733, 'grad_norm': 21.875, 'learning_rate': 1.1390888252465067e-05, 'epoch': 2.0518358531317493}\n",
      "{'loss': 1.4808, 'grad_norm': 22.625, 'learning_rate': 1.1370208002919006e-05, 'epoch': 2.062634989200864}\n",
      "{'loss': 1.4963, 'grad_norm': 24.75, 'learning_rate': 1.1349527753372947e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.4661, 'grad_norm': 15.375, 'learning_rate': 1.1328847503826887e-05, 'epoch': 2.084233261339093}\n",
      "{'loss': 1.4663, 'grad_norm': 17.75, 'learning_rate': 1.1308167254280828e-05, 'epoch': 2.0950323974082075}\n",
      "{'loss': 1.4777, 'grad_norm': 25.375, 'learning_rate': 1.128748700473477e-05, 'epoch': 2.1058315334773217}\n",
      "{'loss': 1.4913, 'grad_norm': 17.125, 'learning_rate': 1.1266806755188709e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.474, 'grad_norm': 32.75, 'learning_rate': 1.124612650564265e-05, 'epoch': 2.127429805615551}\n",
      "{'loss': 1.4789, 'grad_norm': 23.875, 'learning_rate': 1.122544625609659e-05, 'epoch': 2.138228941684665}\n",
      "{'loss': 1.49, 'grad_norm': 42.25, 'learning_rate': 1.1204766006550531e-05, 'epoch': 2.14902807775378}\n",
      "{'loss': 1.5089, 'grad_norm': 27.0, 'learning_rate': 1.118408575700447e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.4722, 'grad_norm': 17.0, 'learning_rate': 1.1163405507458412e-05, 'epoch': 2.1706263498920086}\n",
      "{'loss': 1.4576, 'grad_norm': 14.6875, 'learning_rate': 1.1142725257912352e-05, 'epoch': 2.1814254859611233}\n",
      "{'loss': 1.4755, 'grad_norm': 12.5625, 'learning_rate': 1.1122045008366293e-05, 'epoch': 2.1922246220302375}\n",
      "{'loss': 1.4856, 'grad_norm': 35.25, 'learning_rate': 1.1101364758820232e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.4807, 'grad_norm': 21.75, 'learning_rate': 1.1080684509274174e-05, 'epoch': 2.2138228941684663}\n",
      "{'loss': 1.4699, 'grad_norm': 24.25, 'learning_rate': 1.1060004259728113e-05, 'epoch': 2.224622030237581}\n",
      "{'loss': 1.4785, 'grad_norm': 21.5, 'learning_rate': 1.1039324010182054e-05, 'epoch': 2.2354211663066956}\n",
      "{'loss': 1.4945, 'grad_norm': 18.875, 'learning_rate': 1.1018643760635994e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.4547, 'grad_norm': 15.75, 'learning_rate': 1.0997963511089935e-05, 'epoch': 2.2570194384449245}\n",
      "{'loss': 1.4734, 'grad_norm': 22.75, 'learning_rate': 1.0977283261543875e-05, 'epoch': 2.267818574514039}\n",
      "{'loss': 1.4614, 'grad_norm': 21.875, 'learning_rate': 1.0956603011997816e-05, 'epoch': 2.2786177105831533}\n",
      "{'loss': 1.4942, 'grad_norm': 39.0, 'learning_rate': 1.0935922762451756e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.4862, 'grad_norm': 34.75, 'learning_rate': 1.0915242512905697e-05, 'epoch': 2.300215982721382}\n",
      "{'loss': 1.4879, 'grad_norm': 43.5, 'learning_rate': 1.0894562263359636e-05, 'epoch': 2.311015118790497}\n",
      "{'loss': 1.4577, 'grad_norm': 33.0, 'learning_rate': 1.0873882013813578e-05, 'epoch': 2.3218142548596115}\n",
      "{'loss': 1.4916, 'grad_norm': 26.0, 'learning_rate': 1.0853201764267517e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 1.5143, 'grad_norm': 18.125, 'learning_rate': 1.0832521514721459e-05, 'epoch': 2.3434125269978403}\n",
      "{'loss': 1.4558, 'grad_norm': 13.75, 'learning_rate': 1.0811841265175398e-05, 'epoch': 2.3542116630669545}\n",
      "{'loss': 1.4878, 'grad_norm': 19.125, 'learning_rate': 1.079116101562934e-05, 'epoch': 2.365010799136069}\n",
      "{'loss': 1.4788, 'grad_norm': 23.125, 'learning_rate': 1.0770480766083279e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.4992, 'grad_norm': 20.625, 'learning_rate': 1.074980051653722e-05, 'epoch': 2.386609071274298}\n",
      "{'loss': 1.52, 'grad_norm': 21.5, 'learning_rate': 1.072912026699116e-05, 'epoch': 2.3974082073434126}\n",
      "{'loss': 1.4812, 'grad_norm': 31.625, 'learning_rate': 1.0708440017445101e-05, 'epoch': 2.408207343412527}\n",
      "{'loss': 1.5017, 'grad_norm': 13.0, 'learning_rate': 1.068775976789904e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.4917, 'grad_norm': 39.0, 'learning_rate': 1.0667079518352982e-05, 'epoch': 2.429805615550756}\n",
      "{'loss': 1.4781, 'grad_norm': 18.0, 'learning_rate': 1.0646399268806921e-05, 'epoch': 2.4406047516198703}\n",
      "{'loss': 1.4818, 'grad_norm': 12.375, 'learning_rate': 1.0625719019260863e-05, 'epoch': 2.451403887688985}\n",
      "{'loss': 1.4662, 'grad_norm': 20.75, 'learning_rate': 1.0605038769714802e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 1.4752, 'grad_norm': 20.375, 'learning_rate': 1.0584358520168743e-05, 'epoch': 2.473002159827214}\n",
      "{'loss': 1.4756, 'grad_norm': 19.25, 'learning_rate': 1.0563678270622683e-05, 'epoch': 2.4838012958963285}\n",
      "{'loss': 1.4624, 'grad_norm': 15.25, 'learning_rate': 1.0542998021076624e-05, 'epoch': 2.4946004319654427}\n",
      "{'loss': 1.4802, 'grad_norm': 16.25, 'learning_rate': 1.0522317771530564e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 1.4693, 'grad_norm': 38.75, 'learning_rate': 1.0501637521984505e-05, 'epoch': 2.5161987041036715}\n",
      "{'loss': 1.4614, 'grad_norm': 23.125, 'learning_rate': 1.0480957272438445e-05, 'epoch': 2.526997840172786}\n",
      "{'loss': 1.4493, 'grad_norm': 24.875, 'learning_rate': 1.0460277022892386e-05, 'epoch': 2.537796976241901}\n",
      "{'loss': 1.4899, 'grad_norm': 17.625, 'learning_rate': 1.0439596773346326e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.4723, 'grad_norm': 16.75, 'learning_rate': 1.0418916523800267e-05, 'epoch': 2.5593952483801297}\n",
      "{'loss': 1.4867, 'grad_norm': 15.75, 'learning_rate': 1.0398236274254206e-05, 'epoch': 2.570194384449244}\n",
      "{'loss': 1.4678, 'grad_norm': 28.625, 'learning_rate': 1.0377556024708148e-05, 'epoch': 2.5809935205183585}\n",
      "{'loss': 1.4876, 'grad_norm': 39.0, 'learning_rate': 1.0356875775162087e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.5044, 'grad_norm': 24.625, 'learning_rate': 1.0336195525616028e-05, 'epoch': 2.6025917926565874}\n",
      "{'loss': 1.4834, 'grad_norm': 20.5, 'learning_rate': 1.0315515276069968e-05, 'epoch': 2.613390928725702}\n",
      "{'loss': 1.4538, 'grad_norm': 16.125, 'learning_rate': 1.029483502652391e-05, 'epoch': 2.624190064794816}\n",
      "{'loss': 1.4997, 'grad_norm': 14.875, 'learning_rate': 1.0274154776977849e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 1.4724, 'grad_norm': 16.75, 'learning_rate': 1.025347452743179e-05, 'epoch': 2.6457883369330455}\n",
      "{'loss': 1.5033, 'grad_norm': 24.5, 'learning_rate': 1.0232794277885731e-05, 'epoch': 2.6565874730021597}\n",
      "{'loss': 1.4965, 'grad_norm': 22.375, 'learning_rate': 1.0212114028339671e-05, 'epoch': 2.6673866090712743}\n",
      "{'loss': 1.4659, 'grad_norm': 23.0, 'learning_rate': 1.0191433778793612e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 1.4882, 'grad_norm': 16.5, 'learning_rate': 1.0170753529247553e-05, 'epoch': 2.688984881209503}\n",
      "{'loss': 1.4836, 'grad_norm': 17.875, 'learning_rate': 1.0150073279701493e-05, 'epoch': 2.699784017278618}\n",
      "{'loss': 1.4569, 'grad_norm': 20.625, 'learning_rate': 1.0129393030155434e-05, 'epoch': 2.710583153347732}\n",
      "{'loss': 1.4788, 'grad_norm': 16.5, 'learning_rate': 1.0108712780609374e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 1.4994, 'grad_norm': 21.25, 'learning_rate': 1.0088032531063315e-05, 'epoch': 2.732181425485961}\n",
      "{'loss': 1.4748, 'grad_norm': 14.6875, 'learning_rate': 1.0067352281517255e-05, 'epoch': 2.7429805615550755}\n",
      "{'loss': 1.4816, 'grad_norm': 18.5, 'learning_rate': 1.0046672031971196e-05, 'epoch': 2.75377969762419}\n",
      "{'loss': 1.4513, 'grad_norm': 16.75, 'learning_rate': 1.0025991782425135e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 1.4931, 'grad_norm': 13.6875, 'learning_rate': 1.0005311532879077e-05, 'epoch': 2.775377969762419}\n",
      "{'loss': 1.491, 'grad_norm': 19.0, 'learning_rate': 9.984631283333016e-06, 'epoch': 2.786177105831533}\n",
      "{'loss': 1.4515, 'grad_norm': 37.0, 'learning_rate': 9.963951033786958e-06, 'epoch': 2.796976241900648}\n",
      "{'loss': 1.4787, 'grad_norm': 19.375, 'learning_rate': 9.943270784240897e-06, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.4714, 'grad_norm': 12.8125, 'learning_rate': 9.922590534694838e-06, 'epoch': 2.818574514038877}\n",
      "{'loss': 1.4916, 'grad_norm': 16.25, 'learning_rate': 9.901910285148778e-06, 'epoch': 2.8293736501079914}\n",
      "{'loss': 1.4657, 'grad_norm': 15.625, 'learning_rate': 9.881230035602719e-06, 'epoch': 2.8401727861771056}\n",
      "{'loss': 1.4829, 'grad_norm': 27.375, 'learning_rate': 9.860549786056659e-06, 'epoch': 2.85097192224622}\n",
      "{'loss': 1.4918, 'grad_norm': 17.625, 'learning_rate': 9.8398695365106e-06, 'epoch': 2.861771058315335}\n",
      "{'loss': 1.4871, 'grad_norm': 39.0, 'learning_rate': 9.81918928696454e-06, 'epoch': 2.8725701943844495}\n",
      "{'loss': 1.4648, 'grad_norm': 20.625, 'learning_rate': 9.79850903741848e-06, 'epoch': 2.8833693304535637}\n",
      "{'loss': 1.4544, 'grad_norm': 36.5, 'learning_rate': 9.77782878787242e-06, 'epoch': 2.8941684665226783}\n",
      "{'loss': 1.4334, 'grad_norm': 23.5, 'learning_rate': 9.757148538326362e-06, 'epoch': 2.9049676025917925}\n",
      "{'loss': 1.4825, 'grad_norm': 20.0, 'learning_rate': 9.736468288780301e-06, 'epoch': 2.915766738660907}\n",
      "{'loss': 1.5005, 'grad_norm': 21.25, 'learning_rate': 9.715788039234242e-06, 'epoch': 2.926565874730022}\n",
      "{'loss': 1.4746, 'grad_norm': 25.0, 'learning_rate': 9.695107789688182e-06, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.4633, 'grad_norm': 14.6875, 'learning_rate': 9.674427540142123e-06, 'epoch': 2.9481641468682507}\n",
      "{'loss': 1.4841, 'grad_norm': 21.875, 'learning_rate': 9.653747290596063e-06, 'epoch': 2.958963282937365}\n",
      "{'loss': 1.4835, 'grad_norm': 23.125, 'learning_rate': 9.633067041050004e-06, 'epoch': 2.9697624190064795}\n",
      "{'loss': 1.4806, 'grad_norm': 17.25, 'learning_rate': 9.612386791503944e-06, 'epoch': 2.980561555075594}\n",
      "{'loss': 1.4576, 'grad_norm': 17.0, 'learning_rate': 9.591706541957885e-06, 'epoch': 2.9913606911447084}\n",
      "{'eval_loss': 1.4860925674438477, 'eval_accuracy': 0.33794946550048593, 'eval_precision': 0.32296427972180164, 'eval_recall': 0.27759978781340455, 'eval_f1': 0.25089685265931383, 'eval_runtime': 15.5864, 'eval_samples_per_second': 264.077, 'eval_steps_per_second': 66.019, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.3379 f1=0.2509 p=0.3230 r=0.2776\n",
      "{'loss': 1.4531, 'grad_norm': 15.3125, 'learning_rate': 9.571026292411824e-06, 'epoch': 3.002159827213823}\n",
      "{'loss': 1.447, 'grad_norm': 28.625, 'learning_rate': 9.550346042865766e-06, 'epoch': 3.012958963282937}\n",
      "{'loss': 1.5135, 'grad_norm': 18.75, 'learning_rate': 9.529665793319705e-06, 'epoch': 3.023758099352052}\n",
      "{'loss': 1.4609, 'grad_norm': 22.125, 'learning_rate': 9.508985543773647e-06, 'epoch': 3.0345572354211665}\n",
      "{'loss': 1.4905, 'grad_norm': 17.5, 'learning_rate': 9.488305294227586e-06, 'epoch': 3.0453563714902807}\n",
      "{'loss': 1.486, 'grad_norm': 21.625, 'learning_rate': 9.467625044681527e-06, 'epoch': 3.0561555075593954}\n",
      "{'loss': 1.4739, 'grad_norm': 17.125, 'learning_rate': 9.446944795135467e-06, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.4369, 'grad_norm': 19.0, 'learning_rate': 9.426264545589408e-06, 'epoch': 3.077753779697624}\n",
      "{'loss': 1.476, 'grad_norm': 17.875, 'learning_rate': 9.405584296043348e-06, 'epoch': 3.088552915766739}\n",
      "{'loss': 1.4704, 'grad_norm': 12.5625, 'learning_rate': 9.384904046497289e-06, 'epoch': 3.099352051835853}\n",
      "{'loss': 1.4619, 'grad_norm': 21.625, 'learning_rate': 9.36422379695123e-06, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.4808, 'grad_norm': 19.625, 'learning_rate': 9.34354354740517e-06, 'epoch': 3.120950323974082}\n",
      "{'loss': 1.5111, 'grad_norm': 16.75, 'learning_rate': 9.322863297859111e-06, 'epoch': 3.1317494600431965}\n",
      "{'loss': 1.4482, 'grad_norm': 16.875, 'learning_rate': 9.30218304831305e-06, 'epoch': 3.142548596112311}\n",
      "{'loss': 1.4566, 'grad_norm': 18.5, 'learning_rate': 9.281502798766992e-06, 'epoch': 3.1533477321814254}\n",
      "{'loss': 1.5142, 'grad_norm': 16.375, 'learning_rate': 9.260822549220931e-06, 'epoch': 3.16414686825054}\n",
      "{'loss': 1.4789, 'grad_norm': 17.25, 'learning_rate': 9.240142299674873e-06, 'epoch': 3.1749460043196542}\n",
      "{'loss': 1.5098, 'grad_norm': 27.25, 'learning_rate': 9.219462050128812e-06, 'epoch': 3.185745140388769}\n",
      "{'loss': 1.4923, 'grad_norm': 16.125, 'learning_rate': 9.198781800582754e-06, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.4395, 'grad_norm': 20.5, 'learning_rate': 9.178101551036693e-06, 'epoch': 3.2073434125269977}\n",
      "{'loss': 1.5041, 'grad_norm': 17.75, 'learning_rate': 9.157421301490634e-06, 'epoch': 3.2181425485961124}\n",
      "{'loss': 1.479, 'grad_norm': 16.25, 'learning_rate': 9.136741051944574e-06, 'epoch': 3.2289416846652266}\n",
      "{'loss': 1.4644, 'grad_norm': 27.875, 'learning_rate': 9.116060802398515e-06, 'epoch': 3.239740820734341}\n",
      "{'loss': 1.4431, 'grad_norm': 17.75, 'learning_rate': 9.095380552852455e-06, 'epoch': 3.250539956803456}\n",
      "{'loss': 1.4645, 'grad_norm': 11.3125, 'learning_rate': 9.074700303306396e-06, 'epoch': 3.26133909287257}\n",
      "{'loss': 1.4664, 'grad_norm': 15.0625, 'learning_rate': 9.054020053760336e-06, 'epoch': 3.2721382289416847}\n",
      "{'loss': 1.4738, 'grad_norm': 20.5, 'learning_rate': 9.033339804214277e-06, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.4815, 'grad_norm': 16.625, 'learning_rate': 9.012659554668216e-06, 'epoch': 3.2937365010799136}\n",
      "{'loss': 1.5049, 'grad_norm': 17.75, 'learning_rate': 8.991979305122158e-06, 'epoch': 3.304535637149028}\n",
      "{'loss': 1.4853, 'grad_norm': 18.5, 'learning_rate': 8.971299055576097e-06, 'epoch': 3.3153347732181424}\n",
      "{'loss': 1.4734, 'grad_norm': 17.625, 'learning_rate': 8.950618806030038e-06, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.4672, 'grad_norm': 20.0, 'learning_rate': 8.929938556483978e-06, 'epoch': 3.3369330453563713}\n",
      "{'loss': 1.4739, 'grad_norm': 18.5, 'learning_rate': 8.90925830693792e-06, 'epoch': 3.347732181425486}\n",
      "{'loss': 1.4903, 'grad_norm': 18.875, 'learning_rate': 8.888578057391859e-06, 'epoch': 3.3585313174946005}\n",
      "{'loss': 1.476, 'grad_norm': 18.75, 'learning_rate': 8.8678978078458e-06, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.4606, 'grad_norm': 17.375, 'learning_rate': 8.84721755829974e-06, 'epoch': 3.3801295896328294}\n",
      "{'loss': 1.4756, 'grad_norm': 22.625, 'learning_rate': 8.826537308753681e-06, 'epoch': 3.390928725701944}\n",
      "{'loss': 1.4753, 'grad_norm': 14.4375, 'learning_rate': 8.80585705920762e-06, 'epoch': 3.4017278617710582}\n",
      "{'loss': 1.5207, 'grad_norm': 20.625, 'learning_rate': 8.785176809661562e-06, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.4978, 'grad_norm': 20.875, 'learning_rate': 8.764496560115501e-06, 'epoch': 3.423326133909287}\n",
      "{'loss': 1.4384, 'grad_norm': 20.875, 'learning_rate': 8.743816310569443e-06, 'epoch': 3.4341252699784017}\n",
      "{'loss': 1.4967, 'grad_norm': 26.0, 'learning_rate': 8.723136061023382e-06, 'epoch': 3.4449244060475164}\n",
      "{'loss': 1.4752, 'grad_norm': 18.0, 'learning_rate': 8.702455811477323e-06, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.4612, 'grad_norm': 31.625, 'learning_rate': 8.681775561931263e-06, 'epoch': 3.466522678185745}\n",
      "{'loss': 1.4573, 'grad_norm': 15.125, 'learning_rate': 8.661095312385204e-06, 'epoch': 3.4773218142548594}\n",
      "{'loss': 1.4893, 'grad_norm': 20.125, 'learning_rate': 8.640415062839144e-06, 'epoch': 3.488120950323974}\n",
      "{'loss': 1.45, 'grad_norm': 18.75, 'learning_rate': 8.619734813293085e-06, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.4955, 'grad_norm': 23.0, 'learning_rate': 8.599054563747025e-06, 'epoch': 3.509719222462203}\n",
      "{'loss': 1.4926, 'grad_norm': 14.6875, 'learning_rate': 8.578374314200966e-06, 'epoch': 3.5205183585313176}\n",
      "{'loss': 1.4541, 'grad_norm': 15.3125, 'learning_rate': 8.557694064654905e-06, 'epoch': 3.531317494600432}\n",
      "{'loss': 1.5133, 'grad_norm': 20.625, 'learning_rate': 8.537013815108847e-06, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.4667, 'grad_norm': 34.0, 'learning_rate': 8.516333565562786e-06, 'epoch': 3.552915766738661}\n",
      "{'loss': 1.468, 'grad_norm': 15.5, 'learning_rate': 8.495653316016728e-06, 'epoch': 3.5637149028077753}\n",
      "{'loss': 1.4592, 'grad_norm': 19.375, 'learning_rate': 8.474973066470669e-06, 'epoch': 3.57451403887689}\n",
      "{'loss': 1.4756, 'grad_norm': 17.125, 'learning_rate': 8.454292816924608e-06, 'epoch': 3.5853131749460045}\n",
      "{'loss': 1.4554, 'grad_norm': 25.125, 'learning_rate': 8.43361256737855e-06, 'epoch': 3.5961123110151187}\n",
      "{'loss': 1.5016, 'grad_norm': 40.75, 'learning_rate': 8.412932317832491e-06, 'epoch': 3.6069114470842334}\n",
      "{'loss': 1.4875, 'grad_norm': 16.75, 'learning_rate': 8.39225206828643e-06, 'epoch': 3.6177105831533476}\n",
      "{'loss': 1.4966, 'grad_norm': 16.875, 'learning_rate': 8.371571818740372e-06, 'epoch': 3.6285097192224622}\n",
      "{'loss': 1.4578, 'grad_norm': 24.875, 'learning_rate': 8.350891569194311e-06, 'epoch': 3.639308855291577}\n",
      "{'loss': 1.4802, 'grad_norm': 19.375, 'learning_rate': 8.330211319648252e-06, 'epoch': 3.650107991360691}\n",
      "{'loss': 1.4657, 'grad_norm': 36.25, 'learning_rate': 8.309531070102192e-06, 'epoch': 3.6609071274298057}\n",
      "{'loss': 1.4806, 'grad_norm': 32.0, 'learning_rate': 8.288850820556133e-06, 'epoch': 3.67170626349892}\n",
      "{'loss': 1.4967, 'grad_norm': 32.75, 'learning_rate': 8.268170571010073e-06, 'epoch': 3.6825053995680346}\n",
      "{'loss': 1.4738, 'grad_norm': 13.75, 'learning_rate': 8.247490321464014e-06, 'epoch': 3.693304535637149}\n",
      "{'loss': 1.4608, 'grad_norm': 15.625, 'learning_rate': 8.226810071917954e-06, 'epoch': 3.7041036717062634}\n",
      "{'loss': 1.4359, 'grad_norm': 19.25, 'learning_rate': 8.206129822371895e-06, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.4525, 'grad_norm': 17.125, 'learning_rate': 8.185449572825835e-06, 'epoch': 3.7257019438444923}\n",
      "{'loss': 1.4762, 'grad_norm': 16.25, 'learning_rate': 8.164769323279776e-06, 'epoch': 3.736501079913607}\n",
      "{'loss': 1.4719, 'grad_norm': 20.875, 'learning_rate': 8.144089073733715e-06, 'epoch': 3.7473002159827216}\n",
      "{'loss': 1.4782, 'grad_norm': 24.75, 'learning_rate': 8.123408824187657e-06, 'epoch': 3.7580993520518358}\n",
      "{'loss': 1.4542, 'grad_norm': 21.125, 'learning_rate': 8.102728574641596e-06, 'epoch': 3.7688984881209504}\n",
      "{'loss': 1.4787, 'grad_norm': 17.25, 'learning_rate': 8.082048325095537e-06, 'epoch': 3.7796976241900646}\n",
      "{'loss': 1.4769, 'grad_norm': 15.3125, 'learning_rate': 8.061368075549477e-06, 'epoch': 3.7904967602591793}\n",
      "{'loss': 1.4944, 'grad_norm': 24.5, 'learning_rate': 8.040687826003418e-06, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.5269, 'grad_norm': 19.875, 'learning_rate': 8.020007576457358e-06, 'epoch': 3.812095032397408}\n",
      "{'loss': 1.4668, 'grad_norm': 18.75, 'learning_rate': 7.999327326911299e-06, 'epoch': 3.8228941684665227}\n",
      "{'loss': 1.512, 'grad_norm': 21.875, 'learning_rate': 7.978647077365239e-06, 'epoch': 3.833693304535637}\n",
      "{'loss': 1.4554, 'grad_norm': 20.75, 'learning_rate': 7.95796682781918e-06, 'epoch': 3.8444924406047516}\n",
      "{'loss': 1.4318, 'grad_norm': 18.25, 'learning_rate': 7.93728657827312e-06, 'epoch': 3.8552915766738662}\n",
      "{'loss': 1.4848, 'grad_norm': 18.125, 'learning_rate': 7.91660632872706e-06, 'epoch': 3.8660907127429804}\n",
      "{'loss': 1.4657, 'grad_norm': 18.625, 'learning_rate': 7.895926079181e-06, 'epoch': 3.876889848812095}\n",
      "{'loss': 1.5016, 'grad_norm': 17.25, 'learning_rate': 7.875245829634942e-06, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.4668, 'grad_norm': 62.5, 'learning_rate': 7.854565580088881e-06, 'epoch': 3.898488120950324}\n",
      "{'loss': 1.4614, 'grad_norm': 12.5, 'learning_rate': 7.833885330542822e-06, 'epoch': 3.9092872570194386}\n",
      "{'loss': 1.4647, 'grad_norm': 20.75, 'learning_rate': 7.813205080996762e-06, 'epoch': 3.920086393088553}\n",
      "{'loss': 1.5061, 'grad_norm': 38.75, 'learning_rate': 7.792524831450703e-06, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.4996, 'grad_norm': 26.75, 'learning_rate': 7.771844581904643e-06, 'epoch': 3.9416846652267816}\n",
      "{'loss': 1.4947, 'grad_norm': 18.75, 'learning_rate': 7.751164332358584e-06, 'epoch': 3.9524838012958963}\n",
      "{'loss': 1.4901, 'grad_norm': 18.5, 'learning_rate': 7.730484082812524e-06, 'epoch': 3.963282937365011}\n",
      "{'loss': 1.4736, 'grad_norm': 27.5, 'learning_rate': 7.709803833266465e-06, 'epoch': 3.974082073434125}\n",
      "{'loss': 1.4676, 'grad_norm': 15.6875, 'learning_rate': 7.689123583720404e-06, 'epoch': 3.9848812095032398}\n",
      "{'loss': 1.4673, 'grad_norm': 17.25, 'learning_rate': 7.668443334174346e-06, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 1.4847536087036133, 'eval_accuracy': 0.3396501457725947, 'eval_precision': 0.3253504148428178, 'eval_recall': 0.2806045077249791, 'eval_f1': 0.2564158769054323, 'eval_runtime': 15.8803, 'eval_samples_per_second': 259.188, 'eval_steps_per_second': 64.797, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.3397 f1=0.2564 p=0.3254 r=0.2806\n",
      "{'loss': 1.4482, 'grad_norm': 15.5, 'learning_rate': 7.647763084628285e-06, 'epoch': 4.006479481641469}\n",
      "{'loss': 1.4631, 'grad_norm': 26.75, 'learning_rate': 7.6270828350822265e-06, 'epoch': 4.017278617710583}\n",
      "{'loss': 1.4633, 'grad_norm': 13.4375, 'learning_rate': 7.606402585536166e-06, 'epoch': 4.028077753779698}\n",
      "{'loss': 1.4426, 'grad_norm': 13.375, 'learning_rate': 7.585722335990107e-06, 'epoch': 4.038876889848812}\n",
      "{'loss': 1.5084, 'grad_norm': 18.375, 'learning_rate': 7.565042086444047e-06, 'epoch': 4.049676025917926}\n",
      "{'loss': 1.4607, 'grad_norm': 22.5, 'learning_rate': 7.544361836897988e-06, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.4858, 'grad_norm': 28.125, 'learning_rate': 7.523681587351928e-06, 'epoch': 4.071274298056156}\n",
      "{'loss': 1.4764, 'grad_norm': 20.875, 'learning_rate': 7.503001337805869e-06, 'epoch': 4.08207343412527}\n",
      "{'loss': 1.4886, 'grad_norm': 35.75, 'learning_rate': 7.48232108825981e-06, 'epoch': 4.092872570194385}\n",
      "{'loss': 1.4976, 'grad_norm': 16.5, 'learning_rate': 7.46164083871375e-06, 'epoch': 4.103671706263499}\n",
      "{'loss': 1.4515, 'grad_norm': 26.75, 'learning_rate': 7.440960589167691e-06, 'epoch': 4.114470842332613}\n",
      "{'loss': 1.456, 'grad_norm': 14.9375, 'learning_rate': 7.4202803396216306e-06, 'epoch': 4.125269978401728}\n",
      "{'loss': 1.5035, 'grad_norm': 18.75, 'learning_rate': 7.399600090075572e-06, 'epoch': 4.136069114470843}\n",
      "{'loss': 1.4637, 'grad_norm': 33.0, 'learning_rate': 7.378919840529511e-06, 'epoch': 4.146868250539957}\n",
      "{'loss': 1.4671, 'grad_norm': 40.25, 'learning_rate': 7.358239590983453e-06, 'epoch': 4.157667386609071}\n",
      "{'loss': 1.4994, 'grad_norm': 21.5, 'learning_rate': 7.337559341437392e-06, 'epoch': 4.168466522678186}\n",
      "{'loss': 1.4987, 'grad_norm': 15.6875, 'learning_rate': 7.3168790918913335e-06, 'epoch': 4.1792656587473}\n",
      "{'loss': 1.4668, 'grad_norm': 24.0, 'learning_rate': 7.296198842345273e-06, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.4815, 'grad_norm': 16.75, 'learning_rate': 7.275518592799214e-06, 'epoch': 4.20086393088553}\n",
      "{'loss': 1.5042, 'grad_norm': 22.75, 'learning_rate': 7.254838343253154e-06, 'epoch': 4.211663066954643}\n",
      "{'loss': 1.4527, 'grad_norm': 25.0, 'learning_rate': 7.234158093707095e-06, 'epoch': 4.222462203023758}\n",
      "{'loss': 1.4891, 'grad_norm': 27.875, 'learning_rate': 7.213477844161035e-06, 'epoch': 4.233261339092873}\n",
      "{'loss': 1.4513, 'grad_norm': 15.0625, 'learning_rate': 7.192797594614975e-06, 'epoch': 4.244060475161987}\n",
      "{'loss': 1.471, 'grad_norm': 14.3125, 'learning_rate': 7.1721173450689155e-06, 'epoch': 4.254859611231102}\n",
      "{'loss': 1.4664, 'grad_norm': 14.625, 'learning_rate': 7.151437095522857e-06, 'epoch': 4.265658747300216}\n",
      "{'loss': 1.4813, 'grad_norm': 18.625, 'learning_rate': 7.130756845976797e-06, 'epoch': 4.27645788336933}\n",
      "{'loss': 1.4637, 'grad_norm': 14.375, 'learning_rate': 7.1100765964307376e-06, 'epoch': 4.287257019438445}\n",
      "{'loss': 1.4786, 'grad_norm': 19.125, 'learning_rate': 7.089396346884678e-06, 'epoch': 4.29805615550756}\n",
      "{'loss': 1.4745, 'grad_norm': 13.5, 'learning_rate': 7.068716097338618e-06, 'epoch': 4.308855291576674}\n",
      "{'loss': 1.4682, 'grad_norm': 18.125, 'learning_rate': 7.048035847792559e-06, 'epoch': 4.319654427645788}\n",
      "{'loss': 1.4706, 'grad_norm': 22.75, 'learning_rate': 7.027355598246499e-06, 'epoch': 4.330453563714903}\n",
      "{'loss': 1.476, 'grad_norm': 26.0, 'learning_rate': 7.00667534870044e-06, 'epoch': 4.341252699784017}\n",
      "{'loss': 1.4707, 'grad_norm': 25.375, 'learning_rate': 6.98599509915438e-06, 'epoch': 4.352051835853132}\n",
      "{'loss': 1.467, 'grad_norm': 18.375, 'learning_rate': 6.9653148496083204e-06, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.5202, 'grad_norm': 14.75, 'learning_rate': 6.944634600062261e-06, 'epoch': 4.37365010799136}\n",
      "{'loss': 1.4615, 'grad_norm': 15.4375, 'learning_rate': 6.923954350516201e-06, 'epoch': 4.384449244060475}\n",
      "{'loss': 1.4877, 'grad_norm': 15.8125, 'learning_rate': 6.903274100970142e-06, 'epoch': 4.39524838012959}\n",
      "{'loss': 1.4501, 'grad_norm': 26.25, 'learning_rate': 6.882593851424082e-06, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.5133, 'grad_norm': 18.75, 'learning_rate': 6.8619136018780225e-06, 'epoch': 4.416846652267819}\n",
      "{'loss': 1.4903, 'grad_norm': 23.375, 'learning_rate': 6.841233352331963e-06, 'epoch': 4.427645788336933}\n",
      "{'loss': 1.4534, 'grad_norm': 23.5, 'learning_rate': 6.820553102785903e-06, 'epoch': 4.438444924406047}\n",
      "{'loss': 1.459, 'grad_norm': 18.5, 'learning_rate': 6.799872853239844e-06, 'epoch': 4.449244060475162}\n",
      "{'loss': 1.4676, 'grad_norm': 24.625, 'learning_rate': 6.779192603693784e-06, 'epoch': 4.460043196544277}\n",
      "{'loss': 1.4923, 'grad_norm': 20.0, 'learning_rate': 6.7585123541477245e-06, 'epoch': 4.470842332613391}\n",
      "{'loss': 1.4485, 'grad_norm': 18.75, 'learning_rate': 6.737832104601665e-06, 'epoch': 4.481641468682505}\n",
      "{'loss': 1.4885, 'grad_norm': 17.875, 'learning_rate': 6.717151855055605e-06, 'epoch': 4.49244060475162}\n",
      "{'loss': 1.4778, 'grad_norm': 23.0, 'learning_rate': 6.696471605509546e-06, 'epoch': 4.503239740820734}\n",
      "{'loss': 1.4493, 'grad_norm': 32.5, 'learning_rate': 6.675791355963486e-06, 'epoch': 4.514038876889849}\n",
      "{'loss': 1.5048, 'grad_norm': 24.125, 'learning_rate': 6.655111106417427e-06, 'epoch': 4.524838012958964}\n",
      "{'loss': 1.4816, 'grad_norm': 39.0, 'learning_rate': 6.634430856871367e-06, 'epoch': 4.535637149028078}\n",
      "{'loss': 1.4749, 'grad_norm': 17.0, 'learning_rate': 6.613750607325307e-06, 'epoch': 4.546436285097192}\n",
      "{'loss': 1.4599, 'grad_norm': 16.125, 'learning_rate': 6.593070357779248e-06, 'epoch': 4.557235421166307}\n",
      "{'loss': 1.4806, 'grad_norm': 27.25, 'learning_rate': 6.572390108233188e-06, 'epoch': 4.568034557235421}\n",
      "{'loss': 1.4836, 'grad_norm': 21.875, 'learning_rate': 6.551709858687129e-06, 'epoch': 4.578833693304536}\n",
      "{'loss': 1.4757, 'grad_norm': 28.375, 'learning_rate': 6.53102960914107e-06, 'epoch': 4.589632829373651}\n",
      "{'loss': 1.487, 'grad_norm': 20.375, 'learning_rate': 6.51034935959501e-06, 'epoch': 4.600431965442764}\n",
      "{'loss': 1.4567, 'grad_norm': 18.25, 'learning_rate': 6.489669110048951e-06, 'epoch': 4.611231101511879}\n",
      "{'loss': 1.4616, 'grad_norm': 14.375, 'learning_rate': 6.468988860502891e-06, 'epoch': 4.622030237580994}\n",
      "{'loss': 1.482, 'grad_norm': 27.75, 'learning_rate': 6.4483086109568316e-06, 'epoch': 4.632829373650108}\n",
      "{'loss': 1.4949, 'grad_norm': 13.4375, 'learning_rate': 6.427628361410772e-06, 'epoch': 4.643628509719223}\n",
      "{'loss': 1.4438, 'grad_norm': 21.75, 'learning_rate': 6.406948111864712e-06, 'epoch': 4.654427645788337}\n",
      "{'loss': 1.5103, 'grad_norm': 41.25, 'learning_rate': 6.386267862318653e-06, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.4767, 'grad_norm': 20.875, 'learning_rate': 6.365587612772593e-06, 'epoch': 4.676025917926566}\n",
      "{'loss': 1.5014, 'grad_norm': 10.875, 'learning_rate': 6.344907363226534e-06, 'epoch': 4.686825053995681}\n",
      "{'loss': 1.4501, 'grad_norm': 11.5625, 'learning_rate': 6.324227113680474e-06, 'epoch': 4.697624190064795}\n",
      "{'loss': 1.4541, 'grad_norm': 16.25, 'learning_rate': 6.3035468641344144e-06, 'epoch': 4.708423326133909}\n",
      "{'loss': 1.4575, 'grad_norm': 18.25, 'learning_rate': 6.282866614588355e-06, 'epoch': 4.719222462203024}\n",
      "{'loss': 1.4523, 'grad_norm': 18.5, 'learning_rate': 6.262186365042296e-06, 'epoch': 4.730021598272138}\n",
      "{'loss': 1.4678, 'grad_norm': 21.125, 'learning_rate': 6.2415061154962365e-06, 'epoch': 4.740820734341253}\n",
      "{'loss': 1.4807, 'grad_norm': 24.25, 'learning_rate': 6.220825865950177e-06, 'epoch': 4.751619870410368}\n",
      "{'loss': 1.5095, 'grad_norm': 13.625, 'learning_rate': 6.200145616404117e-06, 'epoch': 4.762419006479481}\n",
      "{'loss': 1.4621, 'grad_norm': 16.125, 'learning_rate': 6.179465366858058e-06, 'epoch': 4.773218142548596}\n",
      "{'loss': 1.4902, 'grad_norm': 34.0, 'learning_rate': 6.158785117311998e-06, 'epoch': 4.784017278617711}\n",
      "{'loss': 1.4711, 'grad_norm': 33.75, 'learning_rate': 6.1381048677659386e-06, 'epoch': 4.794816414686825}\n",
      "{'loss': 1.4515, 'grad_norm': 20.125, 'learning_rate': 6.117424618219879e-06, 'epoch': 4.80561555075594}\n",
      "{'loss': 1.4883, 'grad_norm': 21.125, 'learning_rate': 6.096744368673819e-06, 'epoch': 4.816414686825054}\n",
      "{'loss': 1.4559, 'grad_norm': 17.0, 'learning_rate': 6.07606411912776e-06, 'epoch': 4.827213822894168}\n",
      "{'loss': 1.468, 'grad_norm': 27.625, 'learning_rate': 6.0553838695817e-06, 'epoch': 4.838012958963283}\n",
      "{'loss': 1.4852, 'grad_norm': 14.8125, 'learning_rate': 6.034703620035641e-06, 'epoch': 4.848812095032398}\n",
      "{'loss': 1.4814, 'grad_norm': 16.375, 'learning_rate': 6.014023370489581e-06, 'epoch': 4.859611231101512}\n",
      "{'loss': 1.5125, 'grad_norm': 19.875, 'learning_rate': 5.9933431209435214e-06, 'epoch': 4.870410367170626}\n",
      "{'loss': 1.5317, 'grad_norm': 18.5, 'learning_rate': 5.972662871397462e-06, 'epoch': 4.881209503239741}\n",
      "{'loss': 1.4551, 'grad_norm': 12.5, 'learning_rate': 5.951982621851402e-06, 'epoch': 4.892008639308855}\n",
      "{'loss': 1.4573, 'grad_norm': 30.5, 'learning_rate': 5.931302372305343e-06, 'epoch': 4.90280777537797}\n",
      "{'loss': 1.4627, 'grad_norm': 37.0, 'learning_rate': 5.910622122759283e-06, 'epoch': 4.913606911447085}\n",
      "{'loss': 1.4666, 'grad_norm': 16.75, 'learning_rate': 5.8899418732132235e-06, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.4704, 'grad_norm': 18.0, 'learning_rate': 5.869261623667164e-06, 'epoch': 4.935205183585313}\n",
      "{'loss': 1.5149, 'grad_norm': 37.25, 'learning_rate': 5.848581374121104e-06, 'epoch': 4.946004319654428}\n",
      "{'loss': 1.4853, 'grad_norm': 13.125, 'learning_rate': 5.827901124575045e-06, 'epoch': 4.956803455723542}\n",
      "{'loss': 1.4864, 'grad_norm': 47.0, 'learning_rate': 5.807220875028985e-06, 'epoch': 4.967602591792657}\n",
      "{'loss': 1.473, 'grad_norm': 24.375, 'learning_rate': 5.7865406254829255e-06, 'epoch': 4.978401727861771}\n",
      "{'loss': 1.5063, 'grad_norm': 23.25, 'learning_rate': 5.765860375936866e-06, 'epoch': 4.989200863930885}\n",
      "{'loss': 1.4718, 'grad_norm': 25.75, 'learning_rate': 5.745180126390806e-06, 'epoch': 5.0}\n",
      "{'eval_loss': 1.4838404655456543, 'eval_accuracy': 0.34086491739552965, 'eval_precision': 0.3263433604573619, 'eval_recall': 0.27887556530243074, 'eval_f1': 0.25135936788999186, 'eval_runtime': 16.2526, 'eval_samples_per_second': 253.252, 'eval_steps_per_second': 63.313, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.3409 f1=0.2514 p=0.3263 r=0.2789\n",
      "[e5 b100/9260] loss=1.4817 lr=5.72e-06\n",
      "{'loss': 1.4817, 'grad_norm': 25.0, 'learning_rate': 5.724499876844747e-06, 'epoch': 5.010799136069115}\n",
      "[e5 b200/9260] loss=1.4584 lr=5.70e-06\n",
      "{'loss': 1.4584, 'grad_norm': 20.125, 'learning_rate': 5.703819627298687e-06, 'epoch': 5.021598272138229}\n",
      "[e5 b300/9260] loss=1.4688 lr=5.68e-06\n",
      "{'loss': 1.4688, 'grad_norm': 15.25, 'learning_rate': 5.683139377752628e-06, 'epoch': 5.032397408207343}\n",
      "[e5 b400/9260] loss=1.4718 lr=5.66e-06\n",
      "{'loss': 1.4718, 'grad_norm': 15.625, 'learning_rate': 5.662459128206568e-06, 'epoch': 5.043196544276458}\n",
      "[e5 b500/9260] loss=1.4648 lr=5.64e-06\n",
      "{'loss': 1.4648, 'grad_norm': 18.0, 'learning_rate': 5.641778878660508e-06, 'epoch': 5.053995680345572}\n",
      "[e5 b600/9260] loss=1.4614 lr=5.62e-06\n",
      "{'loss': 1.4614, 'grad_norm': 14.875, 'learning_rate': 5.621098629114449e-06, 'epoch': 5.064794816414687}\n",
      "[e5 b700/9260] loss=1.4671 lr=5.60e-06\n",
      "{'loss': 1.4671, 'grad_norm': 15.25, 'learning_rate': 5.600418379568389e-06, 'epoch': 5.075593952483802}\n",
      "[e5 b800/9260] loss=1.4949 lr=5.58e-06\n",
      "{'loss': 1.4949, 'grad_norm': 31.75, 'learning_rate': 5.57973813002233e-06, 'epoch': 5.086393088552915}\n",
      "[e5 b900/9260] loss=1.4804 lr=5.56e-06\n",
      "{'loss': 1.4804, 'grad_norm': 14.75, 'learning_rate': 5.55905788047627e-06, 'epoch': 5.09719222462203}\n",
      "[e5 b1000/9260] loss=1.5066 lr=5.54e-06\n",
      "{'loss': 1.5066, 'grad_norm': 15.375, 'learning_rate': 5.5383776309302105e-06, 'epoch': 5.107991360691145}\n",
      "[e5 b1100/9260] loss=1.5203 lr=5.52e-06\n",
      "{'loss': 1.5203, 'grad_norm': 15.0625, 'learning_rate': 5.517697381384151e-06, 'epoch': 5.118790496760259}\n",
      "[e5 b1200/9260] loss=1.4921 lr=5.50e-06\n",
      "{'loss': 1.4921, 'grad_norm': 15.8125, 'learning_rate': 5.497017131838091e-06, 'epoch': 5.129589632829374}\n",
      "[e5 b1300/9260] loss=1.5105 lr=5.48e-06\n",
      "{'loss': 1.5105, 'grad_norm': 25.5, 'learning_rate': 5.476336882292032e-06, 'epoch': 5.140388768898488}\n",
      "[e5 b1400/9260] loss=1.4919 lr=5.46e-06\n",
      "{'loss': 1.4919, 'grad_norm': 11.9375, 'learning_rate': 5.455656632745972e-06, 'epoch': 5.151187904967602}\n",
      "[e5 b1500/9260] loss=1.4820 lr=5.43e-06\n",
      "{'loss': 1.482, 'grad_norm': 20.0, 'learning_rate': 5.4349763831999125e-06, 'epoch': 5.161987041036717}\n",
      "[e5 b1600/9260] loss=1.5073 lr=5.41e-06\n",
      "{'loss': 1.5073, 'grad_norm': 19.5, 'learning_rate': 5.414296133653854e-06, 'epoch': 5.172786177105832}\n",
      "[e5 b1700/9260] loss=1.4227 lr=5.39e-06\n",
      "{'loss': 1.4227, 'grad_norm': 17.625, 'learning_rate': 5.393615884107794e-06, 'epoch': 5.183585313174946}\n",
      "[e5 b1800/9260] loss=1.4800 lr=5.37e-06\n",
      "{'loss': 1.48, 'grad_norm': 24.75, 'learning_rate': 5.372935634561735e-06, 'epoch': 5.19438444924406}\n",
      "[e5 b1900/9260] loss=1.4861 lr=5.35e-06\n",
      "{'loss': 1.4861, 'grad_norm': 14.125, 'learning_rate': 5.352255385015675e-06, 'epoch': 5.205183585313175}\n",
      "[e5 b2000/9260] loss=1.4816 lr=5.33e-06\n",
      "{'loss': 1.4816, 'grad_norm': 14.5625, 'learning_rate': 5.331575135469615e-06, 'epoch': 5.215982721382289}\n",
      "[e5 b2100/9260] loss=1.4679 lr=5.31e-06\n",
      "{'loss': 1.4679, 'grad_norm': 39.75, 'learning_rate': 5.310894885923556e-06, 'epoch': 5.226781857451404}\n",
      "[e5 b2200/9260] loss=1.4522 lr=5.29e-06\n",
      "{'loss': 1.4522, 'grad_norm': 21.5, 'learning_rate': 5.290214636377496e-06, 'epoch': 5.237580993520519}\n",
      "[e5 b2300/9260] loss=1.4640 lr=5.27e-06\n",
      "{'loss': 1.464, 'grad_norm': 16.5, 'learning_rate': 5.269534386831437e-06, 'epoch': 5.248380129589632}\n",
      "[e5 b2400/9260] loss=1.4336 lr=5.25e-06\n",
      "{'loss': 1.4336, 'grad_norm': 14.125, 'learning_rate': 5.248854137285377e-06, 'epoch': 5.259179265658747}\n",
      "[e5 b2500/9260] loss=1.4761 lr=5.23e-06\n",
      "{'loss': 1.4761, 'grad_norm': 19.875, 'learning_rate': 5.2281738877393175e-06, 'epoch': 5.269978401727862}\n",
      "[e5 b2600/9260] loss=1.4788 lr=5.21e-06\n",
      "{'loss': 1.4788, 'grad_norm': 19.375, 'learning_rate': 5.207493638193258e-06, 'epoch': 5.280777537796976}\n",
      "[e5 b2700/9260] loss=1.4758 lr=5.19e-06\n",
      "{'loss': 1.4758, 'grad_norm': 29.0, 'learning_rate': 5.186813388647198e-06, 'epoch': 5.291576673866091}\n",
      "[e5 b2800/9260] loss=1.4669 lr=5.17e-06\n",
      "{'loss': 1.4669, 'grad_norm': 17.375, 'learning_rate': 5.166133139101139e-06, 'epoch': 5.302375809935205}\n",
      "[e5 b2900/9260] loss=1.4767 lr=5.15e-06\n",
      "{'loss': 1.4767, 'grad_norm': 39.5, 'learning_rate': 5.145452889555079e-06, 'epoch': 5.313174946004319}\n",
      "[e5 b3000/9260] loss=1.5220 lr=5.12e-06\n",
      "{'loss': 1.522, 'grad_norm': 35.0, 'learning_rate': 5.1247726400090195e-06, 'epoch': 5.323974082073434}\n",
      "[e5 b3100/9260] loss=1.4587 lr=5.10e-06\n",
      "{'loss': 1.4587, 'grad_norm': 13.375, 'learning_rate': 5.10409239046296e-06, 'epoch': 5.334773218142549}\n",
      "[e5 b3200/9260] loss=1.4642 lr=5.08e-06\n",
      "{'loss': 1.4642, 'grad_norm': 32.5, 'learning_rate': 5.0834121409169e-06, 'epoch': 5.345572354211663}\n",
      "[e5 b3300/9260] loss=1.5033 lr=5.06e-06\n",
      "{'loss': 1.5033, 'grad_norm': 20.625, 'learning_rate': 5.062731891370841e-06, 'epoch': 5.356371490280777}\n",
      "[e5 b3400/9260] loss=1.4540 lr=5.04e-06\n",
      "{'loss': 1.454, 'grad_norm': 21.25, 'learning_rate': 5.042051641824781e-06, 'epoch': 5.367170626349892}\n",
      "[e5 b3500/9260] loss=1.4602 lr=5.02e-06\n",
      "{'loss': 1.4602, 'grad_norm': 16.75, 'learning_rate': 5.021371392278722e-06, 'epoch': 5.377969762419006}\n",
      "[e5 b3600/9260] loss=1.4624 lr=5.00e-06\n",
      "{'loss': 1.4624, 'grad_norm': 28.375, 'learning_rate': 5.000691142732662e-06, 'epoch': 5.388768898488121}\n",
      "[e5 b3700/9260] loss=1.4749 lr=4.98e-06\n",
      "{'loss': 1.4749, 'grad_norm': 34.0, 'learning_rate': 4.980010893186602e-06, 'epoch': 5.399568034557236}\n",
      "[e5 b3800/9260] loss=1.4780 lr=4.96e-06\n",
      "{'loss': 1.478, 'grad_norm': 23.25, 'learning_rate': 4.959330643640543e-06, 'epoch': 5.41036717062635}\n",
      "[e5 b3900/9260] loss=1.4702 lr=4.94e-06\n",
      "{'loss': 1.4702, 'grad_norm': 17.375, 'learning_rate': 4.938650394094483e-06, 'epoch': 5.421166306695464}\n",
      "[e5 b4000/9260] loss=1.4472 lr=4.92e-06\n",
      "{'loss': 1.4472, 'grad_norm': 18.75, 'learning_rate': 4.917970144548424e-06, 'epoch': 5.431965442764579}\n",
      "[e5 b4100/9260] loss=1.4699 lr=4.90e-06\n",
      "{'loss': 1.4699, 'grad_norm': 23.875, 'learning_rate': 4.897289895002364e-06, 'epoch': 5.442764578833693}\n",
      "[e5 b4200/9260] loss=1.5164 lr=4.88e-06\n",
      "{'loss': 1.5164, 'grad_norm': 25.125, 'learning_rate': 4.8766096454563045e-06, 'epoch': 5.453563714902808}\n",
      "[e5 b4300/9260] loss=1.4668 lr=4.86e-06\n",
      "{'loss': 1.4668, 'grad_norm': 26.0, 'learning_rate': 4.855929395910245e-06, 'epoch': 5.464362850971923}\n",
      "[e5 b4400/9260] loss=1.4757 lr=4.84e-06\n",
      "{'loss': 1.4757, 'grad_norm': 19.875, 'learning_rate': 4.835249146364185e-06, 'epoch': 5.475161987041036}\n",
      "[e5 b4500/9260] loss=1.4564 lr=4.81e-06\n",
      "{'loss': 1.4564, 'grad_norm': 35.5, 'learning_rate': 4.814568896818126e-06, 'epoch': 5.485961123110151}\n",
      "[e5 b4600/9260] loss=1.4880 lr=4.79e-06\n",
      "{'loss': 1.488, 'grad_norm': 20.375, 'learning_rate': 4.793888647272066e-06, 'epoch': 5.496760259179266}\n",
      "[e5 b4700/9260] loss=1.4707 lr=4.77e-06\n",
      "{'loss': 1.4707, 'grad_norm': 43.75, 'learning_rate': 4.7732083977260065e-06, 'epoch': 5.50755939524838}\n",
      "[e5 b4800/9260] loss=1.4651 lr=4.75e-06\n",
      "{'loss': 1.4651, 'grad_norm': 24.625, 'learning_rate': 4.752528148179947e-06, 'epoch': 5.518358531317495}\n",
      "[e5 b4900/9260] loss=1.4755 lr=4.73e-06\n",
      "{'loss': 1.4755, 'grad_norm': 21.75, 'learning_rate': 4.731847898633887e-06, 'epoch': 5.529157667386609}\n",
      "[e5 b5000/9260] loss=1.4526 lr=4.71e-06\n",
      "{'loss': 1.4526, 'grad_norm': 13.25, 'learning_rate': 4.711167649087828e-06, 'epoch': 5.539956803455723}\n",
      "[e5 b5100/9260] loss=1.4753 lr=4.69e-06\n",
      "{'loss': 1.4753, 'grad_norm': 18.5, 'learning_rate': 4.690487399541768e-06, 'epoch': 5.550755939524838}\n",
      "[e5 b5200/9260] loss=1.4943 lr=4.67e-06\n",
      "{'loss': 1.4943, 'grad_norm': 31.25, 'learning_rate': 4.6698071499957086e-06, 'epoch': 5.561555075593953}\n",
      "[e5 b5300/9260] loss=1.4846 lr=4.65e-06\n",
      "{'loss': 1.4846, 'grad_norm': 15.0625, 'learning_rate': 4.649126900449649e-06, 'epoch': 5.572354211663067}\n",
      "[e5 b5400/9260] loss=1.4773 lr=4.63e-06\n",
      "{'loss': 1.4773, 'grad_norm': 25.375, 'learning_rate': 4.62844665090359e-06, 'epoch': 5.583153347732181}\n",
      "[e5 b5500/9260] loss=1.4700 lr=4.61e-06\n",
      "{'loss': 1.47, 'grad_norm': 30.0, 'learning_rate': 4.607766401357531e-06, 'epoch': 5.593952483801296}\n",
      "[e5 b5600/9260] loss=1.4918 lr=4.59e-06\n",
      "{'loss': 1.4918, 'grad_norm': 20.0, 'learning_rate': 4.587086151811471e-06, 'epoch': 5.60475161987041}\n",
      "[e5 b5700/9260] loss=1.4760 lr=4.57e-06\n",
      "{'loss': 1.476, 'grad_norm': 16.875, 'learning_rate': 4.5664059022654115e-06, 'epoch': 5.615550755939525}\n",
      "[e5 b5800/9260] loss=1.4554 lr=4.55e-06\n",
      "{'loss': 1.4554, 'grad_norm': 16.75, 'learning_rate': 4.545725652719352e-06, 'epoch': 5.62634989200864}\n",
      "[e5 b5900/9260] loss=1.4872 lr=4.53e-06\n",
      "{'loss': 1.4872, 'grad_norm': 19.375, 'learning_rate': 4.525045403173292e-06, 'epoch': 5.637149028077753}\n",
      "[e5 b6000/9260] loss=1.4659 lr=4.50e-06\n",
      "{'loss': 1.4659, 'grad_norm': 26.75, 'learning_rate': 4.5043651536272335e-06, 'epoch': 5.647948164146868}\n",
      "[e5 b6100/9260] loss=1.4743 lr=4.48e-06\n",
      "{'loss': 1.4743, 'grad_norm': 12.875, 'learning_rate': 4.483684904081174e-06, 'epoch': 5.658747300215983}\n",
      "[e5 b6200/9260] loss=1.4876 lr=4.46e-06\n",
      "{'loss': 1.4876, 'grad_norm': 19.625, 'learning_rate': 4.463004654535114e-06, 'epoch': 5.669546436285097}\n",
      "[e5 b6300/9260] loss=1.4809 lr=4.44e-06\n",
      "{'loss': 1.4809, 'grad_norm': 14.75, 'learning_rate': 4.442324404989055e-06, 'epoch': 5.680345572354212}\n",
      "[e5 b6400/9260] loss=1.4739 lr=4.42e-06\n",
      "{'loss': 1.4739, 'grad_norm': 17.375, 'learning_rate': 4.421644155442995e-06, 'epoch': 5.691144708423326}\n",
      "[e5 b6500/9260] loss=1.4443 lr=4.40e-06\n",
      "{'loss': 1.4443, 'grad_norm': 14.625, 'learning_rate': 4.400963905896936e-06, 'epoch': 5.70194384449244}\n",
      "[e5 b6600/9260] loss=1.4790 lr=4.38e-06\n",
      "{'loss': 1.479, 'grad_norm': 24.5, 'learning_rate': 4.380283656350876e-06, 'epoch': 5.712742980561555}\n",
      "[e5 b6700/9260] loss=1.4783 lr=4.36e-06\n",
      "{'loss': 1.4783, 'grad_norm': 34.25, 'learning_rate': 4.359603406804816e-06, 'epoch': 5.72354211663067}\n",
      "[e5 b6800/9260] loss=1.4606 lr=4.34e-06\n",
      "{'loss': 1.4606, 'grad_norm': 30.5, 'learning_rate': 4.338923157258757e-06, 'epoch': 5.734341252699784}\n",
      "[e5 b6900/9260] loss=1.5165 lr=4.32e-06\n",
      "{'loss': 1.5165, 'grad_norm': 21.75, 'learning_rate': 4.318242907712697e-06, 'epoch': 5.745140388768899}\n",
      "[e5 b7000/9260] loss=1.4851 lr=4.30e-06\n",
      "{'loss': 1.4851, 'grad_norm': 32.0, 'learning_rate': 4.297562658166638e-06, 'epoch': 5.755939524838013}\n",
      "[e5 b7100/9260] loss=1.4899 lr=4.28e-06\n",
      "{'loss': 1.4899, 'grad_norm': 18.625, 'learning_rate': 4.276882408620578e-06, 'epoch': 5.766738660907127}\n",
      "[e5 b7200/9260] loss=1.4526 lr=4.26e-06\n",
      "{'loss': 1.4526, 'grad_norm': 14.375, 'learning_rate': 4.2562021590745185e-06, 'epoch': 5.777537796976242}\n",
      "[e5 b7300/9260] loss=1.4873 lr=4.24e-06\n",
      "{'loss': 1.4873, 'grad_norm': 25.5, 'learning_rate': 4.235521909528459e-06, 'epoch': 5.788336933045357}\n",
      "[e5 b7400/9260] loss=1.4601 lr=4.21e-06\n",
      "{'loss': 1.4601, 'grad_norm': 31.875, 'learning_rate': 4.214841659982399e-06, 'epoch': 5.799136069114471}\n",
      "[e5 b7500/9260] loss=1.4949 lr=4.19e-06\n",
      "{'loss': 1.4949, 'grad_norm': 18.5, 'learning_rate': 4.19416141043634e-06, 'epoch': 5.809935205183585}\n",
      "[e5 b7600/9260] loss=1.4867 lr=4.17e-06\n",
      "{'loss': 1.4867, 'grad_norm': 16.5, 'learning_rate': 4.17348116089028e-06, 'epoch': 5.8207343412527}\n",
      "[e5 b7700/9260] loss=1.4547 lr=4.15e-06\n",
      "{'loss': 1.4547, 'grad_norm': 12.8125, 'learning_rate': 4.1528009113442205e-06, 'epoch': 5.831533477321814}\n",
      "[e5 b7800/9260] loss=1.4652 lr=4.13e-06\n",
      "{'loss': 1.4652, 'grad_norm': 21.375, 'learning_rate': 4.132120661798161e-06, 'epoch': 5.842332613390929}\n",
      "[e5 b7900/9260] loss=1.4587 lr=4.11e-06\n",
      "{'loss': 1.4587, 'grad_norm': 31.375, 'learning_rate': 4.111440412252101e-06, 'epoch': 5.853131749460044}\n",
      "[e5 b8000/9260] loss=1.4660 lr=4.09e-06\n",
      "{'loss': 1.466, 'grad_norm': 14.875, 'learning_rate': 4.090760162706042e-06, 'epoch': 5.863930885529157}\n",
      "[e5 b8100/9260] loss=1.5077 lr=4.07e-06\n",
      "{'loss': 1.5077, 'grad_norm': 34.0, 'learning_rate': 4.070079913159982e-06, 'epoch': 5.874730021598272}\n",
      "[e5 b8200/9260] loss=1.4872 lr=4.05e-06\n",
      "{'loss': 1.4872, 'grad_norm': 26.125, 'learning_rate': 4.049399663613923e-06, 'epoch': 5.885529157667387}\n",
      "[e5 b8300/9260] loss=1.4686 lr=4.03e-06\n",
      "{'loss': 1.4686, 'grad_norm': 22.25, 'learning_rate': 4.028719414067863e-06, 'epoch': 5.896328293736501}\n",
      "[e5 b8400/9260] loss=1.4879 lr=4.01e-06\n",
      "{'loss': 1.4879, 'grad_norm': 29.0, 'learning_rate': 4.008039164521803e-06, 'epoch': 5.907127429805616}\n",
      "[e5 b8500/9260] loss=1.4552 lr=3.99e-06\n",
      "{'loss': 1.4552, 'grad_norm': 13.125, 'learning_rate': 3.987358914975744e-06, 'epoch': 5.91792656587473}\n",
      "[e5 b8600/9260] loss=1.4786 lr=3.97e-06\n",
      "{'loss': 1.4786, 'grad_norm': 34.5, 'learning_rate': 3.966678665429684e-06, 'epoch': 5.928725701943844}\n",
      "[e5 b8700/9260] loss=1.4871 lr=3.95e-06\n",
      "{'loss': 1.4871, 'grad_norm': 17.875, 'learning_rate': 3.945998415883625e-06, 'epoch': 5.939524838012959}\n",
      "[e5 b8800/9260] loss=1.4658 lr=3.93e-06\n",
      "{'loss': 1.4658, 'grad_norm': 14.125, 'learning_rate': 3.925318166337565e-06, 'epoch': 5.950323974082074}\n",
      "[e5 b8900/9260] loss=1.4860 lr=3.90e-06\n",
      "{'loss': 1.486, 'grad_norm': 17.25, 'learning_rate': 3.9046379167915055e-06, 'epoch': 5.961123110151188}\n",
      "[e5 b9000/9260] loss=1.4803 lr=3.88e-06\n",
      "{'loss': 1.4803, 'grad_norm': 13.0625, 'learning_rate': 3.883957667245446e-06, 'epoch': 5.971922246220302}\n",
      "[e5 b9100/9260] loss=1.4985 lr=3.86e-06\n",
      "{'loss': 1.4985, 'grad_norm': 28.125, 'learning_rate': 3.863277417699386e-06, 'epoch': 5.982721382289417}\n",
      "[e5 b9200/9260] loss=1.4664 lr=3.84e-06\n",
      "{'loss': 1.4664, 'grad_norm': 20.0, 'learning_rate': 3.842597168153327e-06, 'epoch': 5.993520518358531}\n",
      "{'eval_loss': 1.4852970838546753, 'eval_accuracy': 0.3423226433430515, 'eval_precision': 0.33293875117999655, 'eval_recall': 0.2803845682325249, 'eval_f1': 0.25313585752315715, 'eval_runtime': 16.06, 'eval_samples_per_second': 256.29, 'eval_steps_per_second': 64.072, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.3423 f1=0.2531 p=0.3329 r=0.2804\n",
      "{'loss': 1.4724, 'grad_norm': 17.0, 'learning_rate': 3.821916918607267e-06, 'epoch': 6.004319654427646}\n",
      "{'loss': 1.455, 'grad_norm': 19.375, 'learning_rate': 3.801236669061208e-06, 'epoch': 6.015118790496761}\n",
      "{'loss': 1.4907, 'grad_norm': 25.125, 'learning_rate': 3.7805564195151483e-06, 'epoch': 6.025917926565874}\n",
      "{'loss': 1.472, 'grad_norm': 26.125, 'learning_rate': 3.7598761699690887e-06, 'epoch': 6.036717062634989}\n",
      "{'loss': 1.4771, 'grad_norm': 27.0, 'learning_rate': 3.739195920423029e-06, 'epoch': 6.047516198704104}\n",
      "{'loss': 1.4389, 'grad_norm': 27.375, 'learning_rate': 3.7185156708769696e-06, 'epoch': 6.058315334773218}\n",
      "{'loss': 1.4294, 'grad_norm': 16.75, 'learning_rate': 3.69783542133091e-06, 'epoch': 6.069114470842333}\n",
      "{'loss': 1.4423, 'grad_norm': 14.625, 'learning_rate': 3.6771551717848504e-06, 'epoch': 6.079913606911447}\n",
      "{'loss': 1.4691, 'grad_norm': 13.0625, 'learning_rate': 3.656474922238791e-06, 'epoch': 6.090712742980561}\n",
      "{'loss': 1.4614, 'grad_norm': 26.375, 'learning_rate': 3.6357946726927312e-06, 'epoch': 6.101511879049676}\n",
      "{'loss': 1.4552, 'grad_norm': 28.5, 'learning_rate': 3.6151144231466716e-06, 'epoch': 6.112311015118791}\n",
      "{'loss': 1.4846, 'grad_norm': 12.625, 'learning_rate': 3.594434173600612e-06, 'epoch': 6.123110151187905}\n",
      "{'loss': 1.5014, 'grad_norm': 30.375, 'learning_rate': 3.5737539240545524e-06, 'epoch': 6.133909287257019}\n",
      "{'loss': 1.4855, 'grad_norm': 16.375, 'learning_rate': 3.553073674508493e-06, 'epoch': 6.144708423326134}\n",
      "{'loss': 1.4706, 'grad_norm': 17.625, 'learning_rate': 3.5323934249624333e-06, 'epoch': 6.155507559395248}\n",
      "{'loss': 1.4756, 'grad_norm': 25.875, 'learning_rate': 3.5117131754163737e-06, 'epoch': 6.166306695464363}\n",
      "{'loss': 1.4703, 'grad_norm': 13.625, 'learning_rate': 3.491032925870314e-06, 'epoch': 6.177105831533478}\n",
      "{'loss': 1.4779, 'grad_norm': 17.25, 'learning_rate': 3.4703526763242545e-06, 'epoch': 6.1879049676025915}\n",
      "{'loss': 1.4773, 'grad_norm': 20.0, 'learning_rate': 3.4496724267781953e-06, 'epoch': 6.198704103671706}\n",
      "{'loss': 1.4859, 'grad_norm': 24.5, 'learning_rate': 3.4289921772321357e-06, 'epoch': 6.209503239740821}\n",
      "{'loss': 1.4796, 'grad_norm': 19.125, 'learning_rate': 3.408311927686076e-06, 'epoch': 6.220302375809935}\n",
      "{'loss': 1.4835, 'grad_norm': 23.0, 'learning_rate': 3.3876316781400166e-06, 'epoch': 6.23110151187905}\n",
      "{'loss': 1.4783, 'grad_norm': 16.25, 'learning_rate': 3.366951428593957e-06, 'epoch': 6.241900647948164}\n",
      "{'loss': 1.4624, 'grad_norm': 18.375, 'learning_rate': 3.3462711790478974e-06, 'epoch': 6.252699784017278}\n",
      "{'loss': 1.4891, 'grad_norm': 18.375, 'learning_rate': 3.325590929501838e-06, 'epoch': 6.263498920086393}\n",
      "{'loss': 1.4539, 'grad_norm': 30.875, 'learning_rate': 3.304910679955778e-06, 'epoch': 6.274298056155508}\n",
      "{'loss': 1.4421, 'grad_norm': 16.0, 'learning_rate': 3.2842304304097186e-06, 'epoch': 6.285097192224622}\n",
      "{'loss': 1.5012, 'grad_norm': 18.0, 'learning_rate': 3.263550180863659e-06, 'epoch': 6.295896328293736}\n",
      "{'loss': 1.4618, 'grad_norm': 26.625, 'learning_rate': 3.2428699313175994e-06, 'epoch': 6.306695464362851}\n",
      "{'loss': 1.4788, 'grad_norm': 15.75, 'learning_rate': 3.22218968177154e-06, 'epoch': 6.317494600431965}\n",
      "{'loss': 1.5174, 'grad_norm': 38.75, 'learning_rate': 3.2015094322254807e-06, 'epoch': 6.32829373650108}\n",
      "{'loss': 1.4605, 'grad_norm': 23.875, 'learning_rate': 3.180829182679421e-06, 'epoch': 6.339092872570195}\n",
      "{'loss': 1.4997, 'grad_norm': 31.5, 'learning_rate': 3.1601489331333615e-06, 'epoch': 6.3498920086393085}\n",
      "{'loss': 1.4732, 'grad_norm': 23.625, 'learning_rate': 3.139468683587302e-06, 'epoch': 6.360691144708423}\n",
      "{'loss': 1.5218, 'grad_norm': 17.875, 'learning_rate': 3.1187884340412423e-06, 'epoch': 6.371490280777538}\n",
      "{'loss': 1.4721, 'grad_norm': 25.125, 'learning_rate': 3.0981081844951827e-06, 'epoch': 6.382289416846652}\n",
      "{'loss': 1.478, 'grad_norm': 23.75, 'learning_rate': 3.077427934949123e-06, 'epoch': 6.393088552915767}\n",
      "{'loss': 1.4874, 'grad_norm': 25.0, 'learning_rate': 3.0567476854030636e-06, 'epoch': 6.403887688984881}\n",
      "{'loss': 1.4587, 'grad_norm': 19.625, 'learning_rate': 3.036067435857004e-06, 'epoch': 6.4146868250539955}\n",
      "{'loss': 1.5173, 'grad_norm': 22.5, 'learning_rate': 3.0153871863109444e-06, 'epoch': 6.42548596112311}\n",
      "{'loss': 1.4961, 'grad_norm': 22.5, 'learning_rate': 2.9947069367648852e-06, 'epoch': 6.436285097192225}\n",
      "{'loss': 1.473, 'grad_norm': 23.25, 'learning_rate': 2.9740266872188256e-06, 'epoch': 6.447084233261339}\n",
      "{'loss': 1.472, 'grad_norm': 21.125, 'learning_rate': 2.953346437672766e-06, 'epoch': 6.457883369330453}\n",
      "{'loss': 1.5312, 'grad_norm': 26.75, 'learning_rate': 2.9326661881267064e-06, 'epoch': 6.468682505399568}\n",
      "{'loss': 1.4542, 'grad_norm': 22.875, 'learning_rate': 2.911985938580647e-06, 'epoch': 6.479481641468682}\n",
      "{'loss': 1.4835, 'grad_norm': 19.125, 'learning_rate': 2.8913056890345873e-06, 'epoch': 6.490280777537797}\n",
      "{'loss': 1.4778, 'grad_norm': 15.0, 'learning_rate': 2.8706254394885277e-06, 'epoch': 6.501079913606912}\n",
      "{'loss': 1.4748, 'grad_norm': 19.75, 'learning_rate': 2.849945189942468e-06, 'epoch': 6.5118790496760255}\n",
      "{'loss': 1.4804, 'grad_norm': 19.0, 'learning_rate': 2.8292649403964085e-06, 'epoch': 6.52267818574514}\n",
      "{'loss': 1.5133, 'grad_norm': 19.125, 'learning_rate': 2.808584690850349e-06, 'epoch': 6.533477321814255}\n",
      "{'loss': 1.4773, 'grad_norm': 36.0, 'learning_rate': 2.7879044413042893e-06, 'epoch': 6.544276457883369}\n",
      "{'loss': 1.5015, 'grad_norm': 14.3125, 'learning_rate': 2.7672241917582297e-06, 'epoch': 6.555075593952484}\n",
      "{'loss': 1.4607, 'grad_norm': 33.5, 'learning_rate': 2.74654394221217e-06, 'epoch': 6.565874730021598}\n",
      "{'loss': 1.4708, 'grad_norm': 25.25, 'learning_rate': 2.7258636926661106e-06, 'epoch': 6.5766738660907125}\n",
      "{'loss': 1.4929, 'grad_norm': 20.75, 'learning_rate': 2.705183443120051e-06, 'epoch': 6.587473002159827}\n",
      "{'loss': 1.4921, 'grad_norm': 29.375, 'learning_rate': 2.6845031935739914e-06, 'epoch': 6.598272138228942}\n",
      "{'loss': 1.4876, 'grad_norm': 43.5, 'learning_rate': 2.6638229440279318e-06, 'epoch': 6.609071274298056}\n",
      "{'loss': 1.4689, 'grad_norm': 19.375, 'learning_rate': 2.643142694481872e-06, 'epoch': 6.61987041036717}\n",
      "{'loss': 1.4666, 'grad_norm': 16.625, 'learning_rate': 2.6224624449358126e-06, 'epoch': 6.630669546436285}\n",
      "{'loss': 1.4839, 'grad_norm': 13.0625, 'learning_rate': 2.601782195389753e-06, 'epoch': 6.6414686825053995}\n",
      "{'loss': 1.4618, 'grad_norm': 39.25, 'learning_rate': 2.5811019458436934e-06, 'epoch': 6.652267818574514}\n",
      "{'loss': 1.482, 'grad_norm': 14.5, 'learning_rate': 2.5604216962976343e-06, 'epoch': 6.663066954643629}\n",
      "{'loss': 1.5074, 'grad_norm': 13.5625, 'learning_rate': 2.5397414467515747e-06, 'epoch': 6.6738660907127425}\n",
      "{'loss': 1.4742, 'grad_norm': 23.5, 'learning_rate': 2.519061197205515e-06, 'epoch': 6.684665226781857}\n",
      "{'loss': 1.491, 'grad_norm': 36.75, 'learning_rate': 2.4983809476594555e-06, 'epoch': 6.695464362850972}\n",
      "{'loss': 1.5018, 'grad_norm': 14.875, 'learning_rate': 2.477700698113396e-06, 'epoch': 6.706263498920086}\n",
      "{'loss': 1.4748, 'grad_norm': 19.875, 'learning_rate': 2.4570204485673363e-06, 'epoch': 6.717062634989201}\n",
      "{'loss': 1.474, 'grad_norm': 33.5, 'learning_rate': 2.4363401990212767e-06, 'epoch': 6.727861771058315}\n",
      "{'loss': 1.4726, 'grad_norm': 38.25, 'learning_rate': 2.415659949475217e-06, 'epoch': 6.7386609071274295}\n",
      "{'loss': 1.4704, 'grad_norm': 28.25, 'learning_rate': 2.3949796999291575e-06, 'epoch': 6.749460043196544}\n",
      "{'loss': 1.4787, 'grad_norm': 25.0, 'learning_rate': 2.374299450383098e-06, 'epoch': 6.760259179265659}\n",
      "{'loss': 1.4616, 'grad_norm': 20.0, 'learning_rate': 2.3536192008370384e-06, 'epoch': 6.771058315334773}\n",
      "{'loss': 1.4709, 'grad_norm': 37.25, 'learning_rate': 2.3329389512909788e-06, 'epoch': 6.781857451403888}\n",
      "{'loss': 1.4635, 'grad_norm': 20.5, 'learning_rate': 2.312258701744919e-06, 'epoch': 6.792656587473002}\n",
      "{'loss': 1.4789, 'grad_norm': 18.5, 'learning_rate': 2.2915784521988596e-06, 'epoch': 6.8034557235421165}\n",
      "{'loss': 1.4762, 'grad_norm': 13.625, 'learning_rate': 2.2708982026528e-06, 'epoch': 6.814254859611231}\n",
      "{'loss': 1.4871, 'grad_norm': 16.875, 'learning_rate': 2.250217953106741e-06, 'epoch': 6.825053995680346}\n",
      "{'loss': 1.4616, 'grad_norm': 11.8125, 'learning_rate': 2.2295377035606813e-06, 'epoch': 6.83585313174946}\n",
      "{'loss': 1.4867, 'grad_norm': 16.75, 'learning_rate': 2.2088574540146217e-06, 'epoch': 6.846652267818574}\n",
      "{'loss': 1.4634, 'grad_norm': 18.25, 'learning_rate': 2.188177204468562e-06, 'epoch': 6.857451403887689}\n",
      "{'loss': 1.4961, 'grad_norm': 47.75, 'learning_rate': 2.1674969549225025e-06, 'epoch': 6.8682505399568035}\n",
      "{'loss': 1.4547, 'grad_norm': 34.25, 'learning_rate': 2.146816705376443e-06, 'epoch': 6.879049676025918}\n",
      "{'loss': 1.4555, 'grad_norm': 25.75, 'learning_rate': 2.1261364558303837e-06, 'epoch': 6.889848812095033}\n",
      "{'loss': 1.4589, 'grad_norm': 20.25, 'learning_rate': 2.105456206284324e-06, 'epoch': 6.9006479481641465}\n",
      "{'loss': 1.4799, 'grad_norm': 16.75, 'learning_rate': 2.0847759567382646e-06, 'epoch': 6.911447084233261}\n",
      "{'loss': 1.4806, 'grad_norm': 39.25, 'learning_rate': 2.064095707192205e-06, 'epoch': 6.922246220302376}\n",
      "{'loss': 1.4782, 'grad_norm': 25.5, 'learning_rate': 2.0434154576461454e-06, 'epoch': 6.93304535637149}\n",
      "{'loss': 1.4636, 'grad_norm': 22.625, 'learning_rate': 2.0227352081000858e-06, 'epoch': 6.943844492440605}\n",
      "{'loss': 1.4499, 'grad_norm': 14.875, 'learning_rate': 2.002054958554026e-06, 'epoch': 6.954643628509719}\n",
      "{'loss': 1.4679, 'grad_norm': 18.125, 'learning_rate': 1.9813747090079666e-06, 'epoch': 6.9654427645788335}\n",
      "{'loss': 1.455, 'grad_norm': 18.75, 'learning_rate': 1.960694459461907e-06, 'epoch': 6.976241900647948}\n",
      "{'loss': 1.4808, 'grad_norm': 23.625, 'learning_rate': 1.9400142099158474e-06, 'epoch': 6.987041036717063}\n",
      "{'loss': 1.4711, 'grad_norm': 36.25, 'learning_rate': 1.919333960369788e-06, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 1.484702706336975, 'eval_accuracy': 0.3396501457725947, 'eval_precision': 0.3256512410825408, 'eval_recall': 0.278774849418167, 'eval_f1': 0.252414742978066, 'eval_runtime': 17.1236, 'eval_samples_per_second': 240.37, 'eval_steps_per_second': 60.092, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.3397 f1=0.2524 p=0.3257 r=0.2788\n",
      "{'loss': 1.4955, 'grad_norm': 18.125, 'learning_rate': 1.8986537108237282e-06, 'epoch': 7.008639308855291}\n",
      "{'loss': 1.4765, 'grad_norm': 15.3125, 'learning_rate': 1.8779734612776687e-06, 'epoch': 7.019438444924406}\n",
      "{'loss': 1.4778, 'grad_norm': 18.375, 'learning_rate': 1.857293211731609e-06, 'epoch': 7.0302375809935205}\n",
      "{'loss': 1.4622, 'grad_norm': 19.25, 'learning_rate': 1.8366129621855495e-06, 'epoch': 7.041036717062635}\n",
      "{'loss': 1.4524, 'grad_norm': 21.25, 'learning_rate': 1.8159327126394899e-06, 'epoch': 7.05183585313175}\n",
      "{'loss': 1.506, 'grad_norm': 24.75, 'learning_rate': 1.7952524630934305e-06, 'epoch': 7.0626349892008635}\n",
      "{'loss': 1.4462, 'grad_norm': 25.875, 'learning_rate': 1.774572213547371e-06, 'epoch': 7.073434125269978}\n",
      "{'loss': 1.4493, 'grad_norm': 17.75, 'learning_rate': 1.7538919640013113e-06, 'epoch': 7.084233261339093}\n",
      "{'loss': 1.479, 'grad_norm': 21.375, 'learning_rate': 1.7332117144552517e-06, 'epoch': 7.0950323974082075}\n",
      "{'loss': 1.49, 'grad_norm': 14.4375, 'learning_rate': 1.7125314649091922e-06, 'epoch': 7.105831533477322}\n",
      "{'loss': 1.4981, 'grad_norm': 21.375, 'learning_rate': 1.6918512153631326e-06, 'epoch': 7.116630669546436}\n",
      "{'loss': 1.473, 'grad_norm': 22.875, 'learning_rate': 1.671170965817073e-06, 'epoch': 7.1274298056155505}\n",
      "{'loss': 1.4721, 'grad_norm': 19.375, 'learning_rate': 1.6504907162710134e-06, 'epoch': 7.138228941684665}\n",
      "{'loss': 1.4955, 'grad_norm': 21.375, 'learning_rate': 1.6298104667249538e-06, 'epoch': 7.14902807775378}\n",
      "{'loss': 1.4993, 'grad_norm': 34.0, 'learning_rate': 1.6091302171788942e-06, 'epoch': 7.159827213822894}\n",
      "{'loss': 1.4544, 'grad_norm': 30.25, 'learning_rate': 1.5884499676328348e-06, 'epoch': 7.170626349892009}\n",
      "{'loss': 1.4644, 'grad_norm': 23.625, 'learning_rate': 1.5677697180867752e-06, 'epoch': 7.181425485961123}\n",
      "{'loss': 1.471, 'grad_norm': 25.125, 'learning_rate': 1.5470894685407157e-06, 'epoch': 7.1922246220302375}\n",
      "{'loss': 1.4893, 'grad_norm': 18.25, 'learning_rate': 1.5264092189946563e-06, 'epoch': 7.203023758099352}\n",
      "{'loss': 1.4654, 'grad_norm': 19.75, 'learning_rate': 1.5057289694485967e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 1.4716, 'grad_norm': 23.75, 'learning_rate': 1.485048719902537e-06, 'epoch': 7.224622030237581}\n",
      "{'loss': 1.4662, 'grad_norm': 25.625, 'learning_rate': 1.4643684703564775e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 1.4886, 'grad_norm': 19.875, 'learning_rate': 1.443688220810418e-06, 'epoch': 7.24622030237581}\n",
      "{'loss': 1.4775, 'grad_norm': 15.3125, 'learning_rate': 1.4230079712643583e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 1.4948, 'grad_norm': 15.8125, 'learning_rate': 1.4023277217182987e-06, 'epoch': 7.267818574514039}\n",
      "{'loss': 1.4713, 'grad_norm': 20.5, 'learning_rate': 1.3816474721722391e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 1.5039, 'grad_norm': 25.875, 'learning_rate': 1.3609672226261798e-06, 'epoch': 7.2894168466522675}\n",
      "{'loss': 1.5145, 'grad_norm': 14.9375, 'learning_rate': 1.3402869730801202e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 1.4576, 'grad_norm': 21.375, 'learning_rate': 1.3196067235340606e-06, 'epoch': 7.311015118790497}\n",
      "{'loss': 1.4501, 'grad_norm': 38.0, 'learning_rate': 1.298926473988001e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 1.4601, 'grad_norm': 27.75, 'learning_rate': 1.2782462244419414e-06, 'epoch': 7.332613390928726}\n",
      "{'loss': 1.441, 'grad_norm': 21.0, 'learning_rate': 1.2575659748958818e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 1.4733, 'grad_norm': 21.125, 'learning_rate': 1.2368857253498222e-06, 'epoch': 7.3542116630669545}\n",
      "{'loss': 1.4849, 'grad_norm': 11.125, 'learning_rate': 1.2162054758037626e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 1.4726, 'grad_norm': 23.25, 'learning_rate': 1.195525226257703e-06, 'epoch': 7.375809935205184}\n",
      "{'loss': 1.4722, 'grad_norm': 21.625, 'learning_rate': 1.1748449767116435e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 1.5022, 'grad_norm': 16.125, 'learning_rate': 1.154164727165584e-06, 'epoch': 7.397408207343412}\n",
      "{'loss': 1.481, 'grad_norm': 10.9375, 'learning_rate': 1.1334844776195245e-06, 'epoch': 7.408207343412527}\n",
      "{'loss': 1.445, 'grad_norm': 20.0, 'learning_rate': 1.112804228073465e-06, 'epoch': 7.4190064794816415}\n",
      "{'loss': 1.4602, 'grad_norm': 13.25, 'learning_rate': 1.0921239785274053e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 1.4298, 'grad_norm': 34.0, 'learning_rate': 1.0714437289813457e-06, 'epoch': 7.440604751619871}\n",
      "{'loss': 1.4834, 'grad_norm': 16.375, 'learning_rate': 1.0507634794352864e-06, 'epoch': 7.4514038876889845}\n",
      "{'loss': 1.4773, 'grad_norm': 18.375, 'learning_rate': 1.0300832298892268e-06, 'epoch': 7.462203023758099}\n",
      "{'loss': 1.4992, 'grad_norm': 13.8125, 'learning_rate': 1.0094029803431672e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 1.461, 'grad_norm': 17.25, 'learning_rate': 9.887227307971076e-07, 'epoch': 7.4838012958963285}\n",
      "{'loss': 1.4759, 'grad_norm': 32.75, 'learning_rate': 9.68042481251048e-07, 'epoch': 7.494600431965443}\n",
      "{'loss': 1.4614, 'grad_norm': 15.4375, 'learning_rate': 9.473622317049885e-07, 'epoch': 7.505399568034557}\n",
      "{'loss': 1.4775, 'grad_norm': 22.125, 'learning_rate': 9.266819821589289e-07, 'epoch': 7.5161987041036715}\n",
      "{'loss': 1.4481, 'grad_norm': 26.125, 'learning_rate': 9.060017326128693e-07, 'epoch': 7.526997840172786}\n",
      "{'loss': 1.4885, 'grad_norm': 22.875, 'learning_rate': 8.853214830668097e-07, 'epoch': 7.537796976241901}\n",
      "{'loss': 1.4577, 'grad_norm': 34.25, 'learning_rate': 8.646412335207503e-07, 'epoch': 7.5485961123110155}\n",
      "{'loss': 1.4748, 'grad_norm': 16.0, 'learning_rate': 8.439609839746907e-07, 'epoch': 7.559395248380129}\n",
      "{'loss': 1.4798, 'grad_norm': 17.5, 'learning_rate': 8.232807344286311e-07, 'epoch': 7.570194384449244}\n",
      "{'loss': 1.4544, 'grad_norm': 28.125, 'learning_rate': 8.026004848825715e-07, 'epoch': 7.5809935205183585}\n",
      "{'loss': 1.4684, 'grad_norm': 22.75, 'learning_rate': 7.819202353365119e-07, 'epoch': 7.591792656587473}\n",
      "{'loss': 1.4825, 'grad_norm': 20.625, 'learning_rate': 7.612399857904524e-07, 'epoch': 7.602591792656588}\n",
      "{'loss': 1.4951, 'grad_norm': 27.25, 'learning_rate': 7.405597362443928e-07, 'epoch': 7.613390928725702}\n",
      "{'loss': 1.4656, 'grad_norm': 17.75, 'learning_rate': 7.198794866983332e-07, 'epoch': 7.624190064794816}\n",
      "{'loss': 1.4568, 'grad_norm': 28.125, 'learning_rate': 6.991992371522738e-07, 'epoch': 7.634989200863931}\n",
      "{'loss': 1.4739, 'grad_norm': 20.625, 'learning_rate': 6.785189876062142e-07, 'epoch': 7.6457883369330455}\n",
      "{'loss': 1.4717, 'grad_norm': 20.25, 'learning_rate': 6.578387380601547e-07, 'epoch': 7.65658747300216}\n",
      "{'loss': 1.4997, 'grad_norm': 23.5, 'learning_rate': 6.371584885140951e-07, 'epoch': 7.667386609071274}\n",
      "{'loss': 1.4647, 'grad_norm': 16.5, 'learning_rate': 6.164782389680355e-07, 'epoch': 7.6781857451403885}\n",
      "{'loss': 1.5012, 'grad_norm': 17.625, 'learning_rate': 5.957979894219759e-07, 'epoch': 7.688984881209503}\n",
      "{'loss': 1.47, 'grad_norm': 15.125, 'learning_rate': 5.751177398759163e-07, 'epoch': 7.699784017278618}\n",
      "{'loss': 1.5135, 'grad_norm': 42.0, 'learning_rate': 5.544374903298568e-07, 'epoch': 7.7105831533477325}\n",
      "{'loss': 1.4461, 'grad_norm': 19.5, 'learning_rate': 5.337572407837973e-07, 'epoch': 7.721382289416846}\n",
      "{'loss': 1.5079, 'grad_norm': 25.875, 'learning_rate': 5.130769912377377e-07, 'epoch': 7.732181425485961}\n",
      "{'loss': 1.4791, 'grad_norm': 44.0, 'learning_rate': 4.923967416916781e-07, 'epoch': 7.7429805615550755}\n",
      "{'loss': 1.4673, 'grad_norm': 16.125, 'learning_rate': 4.7171649214561854e-07, 'epoch': 7.75377969762419}\n",
      "{'loss': 1.4797, 'grad_norm': 21.625, 'learning_rate': 4.5103624259955906e-07, 'epoch': 7.764578833693305}\n",
      "{'loss': 1.508, 'grad_norm': 59.75, 'learning_rate': 4.303559930534994e-07, 'epoch': 7.775377969762419}\n",
      "{'loss': 1.4841, 'grad_norm': 17.375, 'learning_rate': 4.096757435074399e-07, 'epoch': 7.786177105831533}\n",
      "{'loss': 1.4908, 'grad_norm': 19.375, 'learning_rate': 3.8899549396138034e-07, 'epoch': 7.796976241900648}\n",
      "{'loss': 1.4787, 'grad_norm': 17.75, 'learning_rate': 3.6831524441532075e-07, 'epoch': 7.8077753779697625}\n",
      "{'loss': 1.4475, 'grad_norm': 29.75, 'learning_rate': 3.476349948692612e-07, 'epoch': 7.818574514038877}\n",
      "{'loss': 1.481, 'grad_norm': 34.75, 'learning_rate': 3.2695474532320163e-07, 'epoch': 7.829373650107991}\n",
      "{'loss': 1.465, 'grad_norm': 32.25, 'learning_rate': 3.062744957771421e-07, 'epoch': 7.840172786177106}\n",
      "{'loss': 1.4842, 'grad_norm': 14.5, 'learning_rate': 2.855942462310825e-07, 'epoch': 7.85097192224622}\n",
      "{'loss': 1.4879, 'grad_norm': 14.125, 'learning_rate': 2.6491399668502296e-07, 'epoch': 7.861771058315335}\n",
      "{'loss': 1.5077, 'grad_norm': 25.625, 'learning_rate': 2.4423374713896343e-07, 'epoch': 7.8725701943844495}\n",
      "{'loss': 1.4616, 'grad_norm': 19.875, 'learning_rate': 2.2355349759290384e-07, 'epoch': 7.883369330453563}\n",
      "{'loss': 1.4745, 'grad_norm': 19.125, 'learning_rate': 2.0287324804684428e-07, 'epoch': 7.894168466522678}\n",
      "{'loss': 1.4899, 'grad_norm': 19.75, 'learning_rate': 1.8219299850078474e-07, 'epoch': 7.9049676025917925}\n",
      "{'loss': 1.5025, 'grad_norm': 12.8125, 'learning_rate': 1.6151274895472515e-07, 'epoch': 7.915766738660907}\n",
      "{'loss': 1.4813, 'grad_norm': 17.0, 'learning_rate': 1.408324994086656e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 1.4583, 'grad_norm': 38.0, 'learning_rate': 1.2015224986260605e-07, 'epoch': 7.937365010799136}\n",
      "{'loss': 1.5095, 'grad_norm': 41.0, 'learning_rate': 9.947200031654649e-08, 'epoch': 7.94816414686825}\n",
      "{'loss': 1.4957, 'grad_norm': 18.0, 'learning_rate': 7.879175077048693e-08, 'epoch': 7.958963282937365}\n",
      "{'loss': 1.4933, 'grad_norm': 21.5, 'learning_rate': 5.811150122442737e-08, 'epoch': 7.9697624190064795}\n",
      "{'loss': 1.477, 'grad_norm': 15.25, 'learning_rate': 3.7431251678367806e-08, 'epoch': 7.980561555075594}\n",
      "{'loss': 1.502, 'grad_norm': 19.125, 'learning_rate': 1.6751002132308243e-08, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 1.483729600906372, 'eval_accuracy': 0.34135082604470357, 'eval_precision': 0.3324183323589064, 'eval_recall': 0.27928409743278537, 'eval_f1': 0.2521240444269801, 'eval_runtime': 15.6852, 'eval_samples_per_second': 262.412, 'eval_steps_per_second': 65.603, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.3414 f1=0.2521 p=0.3324 r=0.2793\n",
      "{'train_runtime': 3036.1736, 'train_samples_per_second': 97.594, 'train_steps_per_second': 24.399, 'train_loss': 1.4897674263939744, 'epoch': 8.0}\n",
      "{'eval_loss': 1.4847536087036133, 'eval_accuracy': 0.3396501457725947, 'eval_precision': 0.3253504148428178, 'eval_recall': 0.2806045077249791, 'eval_f1': 0.2564158769054323, 'eval_runtime': 16.145, 'eval_samples_per_second': 254.94, 'eval_steps_per_second': 63.735, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.3397 f1=0.2564 p=0.3254 r=0.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñà‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñà‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÖ‚ñÇ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñÖ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÜ‚ñÅ‚ñà‚ñÖ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ</td></tr><tr><td>train/learning_rate</td><td>‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.25642</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.33965</td></tr><tr><td>eval/f1</td><td>0.25642</td></tr><tr><td>eval/loss</td><td>1.48475</td></tr><tr><td>eval/precision</td><td>0.32535</td></tr><tr><td>eval/recall</td><td>0.2806</td></tr><tr><td>eval/runtime</td><td>16.145</td></tr><tr><td>eval/samples_per_second</td><td>254.94</td></tr><tr><td>eval/steps_per_second</td><td>63.735</td></tr><tr><td>total_flos</td><td>1.1024708094226464e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>74080</td></tr><tr><td>train/grad_norm</td><td>19.125</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.502</td></tr><tr><td>train_loss</td><td>1.48977</td></tr><tr><td>train_runtime</td><td>3036.1736</td></tr><tr><td>train_samples_per_second</td><td>97.594</td></tr><tr><td>train_steps_per_second</td><td>24.399</td></tr><tr><td>trial/accuracy</td><td>0.33965</td></tr><tr><td>trial/f1</td><td>0.25642</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t6</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/bygl5eo1' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/bygl5eo1</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250816_234928-bygl5eo1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [2:59:14<1:30:06, 1802.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=6 f1=0.2564\n",
      "[I 2025-08-17 00:40:24,677] Trial 6 finished with value: 0.2564158769054323 and parameters: {'lr': 1.4400691771398575e-05, 'weight_decay': 6.276275158132312e-05, 'unfreeze_last_k': 9, 'batch_size': 4}. Best is trial 2 with value: 0.8523413656247504.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_004024-xkvlxo53</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/xkvlxo53' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t7</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/xkvlxo53' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/xkvlxo53</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=7 | epochs=8 bs=16 lr=1.68e-05 wd=7.9e-05 k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=16 lr=1.68e-05 wd=7.9e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/2315] loss=1.6465 lr=1.50e-06\n",
      "{'loss': 1.6465, 'grad_norm': 2.4375, 'learning_rate': 1.4951136414167308e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b200/2315] loss=1.6467 lr=3.01e-06\n",
      "{'loss': 1.6467, 'grad_norm': 1.7109375, 'learning_rate': 3.0053294408275697e-06, 'epoch': 0.08639308855291576}\n",
      "[e0 b300/2315] loss=1.6431 lr=4.52e-06\n",
      "{'loss': 1.6431, 'grad_norm': 2.46875, 'learning_rate': 4.5155452402384084e-06, 'epoch': 0.12958963282937366}\n",
      "[e0 b400/2315] loss=1.6357 lr=6.03e-06\n",
      "{'loss': 1.6357, 'grad_norm': 3.453125, 'learning_rate': 6.025761039649248e-06, 'epoch': 0.17278617710583152}\n",
      "[e0 b500/2315] loss=1.6334 lr=7.54e-06\n",
      "{'loss': 1.6334, 'grad_norm': 3.078125, 'learning_rate': 7.535976839060087e-06, 'epoch': 0.2159827213822894}\n",
      "[e0 b600/2315] loss=1.6261 lr=9.05e-06\n",
      "{'loss': 1.6261, 'grad_norm': 3.671875, 'learning_rate': 9.046192638470926e-06, 'epoch': 0.2591792656587473}\n",
      "[e0 b700/2315] loss=1.6087 lr=1.06e-05\n",
      "{'loss': 1.6087, 'grad_norm': 4.8125, 'learning_rate': 1.0556408437881764e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b800/2315] loss=1.5695 lr=1.21e-05\n",
      "{'loss': 1.5695, 'grad_norm': 5.21875, 'learning_rate': 1.2066624237292604e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b900/2315] loss=1.5493 lr=1.36e-05\n",
      "{'loss': 1.5493, 'grad_norm': 5.375, 'learning_rate': 1.3576840036703443e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1000/2315] loss=1.5630 lr=1.51e-05\n",
      "{'loss': 1.563, 'grad_norm': 4.5625, 'learning_rate': 1.5087055836114283e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b1100/2315] loss=1.5483 lr=1.66e-05\n",
      "{'loss': 1.5483, 'grad_norm': 6.59375, 'learning_rate': 1.6597271635525122e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b1200/2315] loss=1.5394 lr=1.67e-05\n",
      "{'loss': 1.5394, 'grad_norm': 6.375, 'learning_rate': 1.6709670279235868e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b1300/2315] loss=1.5085 lr=1.66e-05\n",
      "{'loss': 1.5085, 'grad_norm': 6.34375, 'learning_rate': 1.6613199692784534e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b1400/2315] loss=1.4946 lr=1.65e-05\n",
      "{'loss': 1.4946, 'grad_norm': 7.875, 'learning_rate': 1.6516729106333196e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b1500/2315] loss=1.4850 lr=1.64e-05\n",
      "{'loss': 1.485, 'grad_norm': 11.625, 'learning_rate': 1.642025851988186e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b1600/2315] loss=1.4655 lr=1.63e-05\n",
      "{'loss': 1.4655, 'grad_norm': 13.6875, 'learning_rate': 1.6323787933430527e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b1700/2315] loss=1.4497 lr=1.62e-05\n",
      "{'loss': 1.4497, 'grad_norm': 13.4375, 'learning_rate': 1.622731734697919e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b1800/2315] loss=1.4413 lr=1.61e-05\n",
      "{'loss': 1.4413, 'grad_norm': 13.875, 'learning_rate': 1.6130846760527855e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b1900/2315] loss=1.4132 lr=1.60e-05\n",
      "{'loss': 1.4132, 'grad_norm': 8.9375, 'learning_rate': 1.603437617407652e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b2000/2315] loss=1.4134 lr=1.59e-05\n",
      "{'loss': 1.4134, 'grad_norm': 10.625, 'learning_rate': 1.5937905587625183e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b2100/2315] loss=1.3808 lr=1.58e-05\n",
      "{'loss': 1.3808, 'grad_norm': 13.6875, 'learning_rate': 1.5841435001173848e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b2200/2315] loss=1.3814 lr=1.57e-05\n",
      "{'loss': 1.3814, 'grad_norm': 11.125, 'learning_rate': 1.574496441472251e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b2300/2315] loss=1.3685 lr=1.56e-05\n",
      "{'loss': 1.3685, 'grad_norm': 10.25, 'learning_rate': 1.5648493828271173e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.3742977380752563, 'eval_accuracy': 0.3940719144800777, 'eval_precision': 0.4374462874205801, 'eval_recall': 0.3984146783187377, 'eval_f1': 0.39898658523729347, 'eval_runtime': 4.0881, 'eval_samples_per_second': 1006.817, 'eval_steps_per_second': 63.11, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.3941 f1=0.3990 p=0.4374 r=0.3984\n",
      "{'loss': 1.3457, 'grad_norm': 13.0625, 'learning_rate': 1.555202324181984e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.3314, 'grad_norm': 17.75, 'learning_rate': 1.5455552655368504e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.3342, 'grad_norm': 12.875, 'learning_rate': 1.5359082068917166e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.3196, 'grad_norm': 11.75, 'learning_rate': 1.5262611482465832e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.3181, 'grad_norm': 11.5, 'learning_rate': 1.5166140896014496e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.307, 'grad_norm': 11.875, 'learning_rate': 1.506967030956316e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.3528, 'grad_norm': 14.625, 'learning_rate': 1.4973199723111825e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.3427, 'grad_norm': 20.625, 'learning_rate': 1.487672913666049e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.3201, 'grad_norm': 10.4375, 'learning_rate': 1.4780258550209153e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.3259, 'grad_norm': 10.6875, 'learning_rate': 1.4683787963757817e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.3007, 'grad_norm': 9.125, 'learning_rate': 1.4587317377306483e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.2992, 'grad_norm': 24.0, 'learning_rate': 1.4490846790855147e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.325, 'grad_norm': 19.75, 'learning_rate': 1.439437620440381e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.2935, 'grad_norm': 15.8125, 'learning_rate': 1.4297905617952474e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.2838, 'grad_norm': 17.75, 'learning_rate': 1.420143503150114e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.2749, 'grad_norm': 14.0, 'learning_rate': 1.4104964445049802e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.2821, 'grad_norm': 10.125, 'learning_rate': 1.4008493858598466e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.2881, 'grad_norm': 20.375, 'learning_rate': 1.391202327214713e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.2951, 'grad_norm': 13.9375, 'learning_rate': 1.3815552685695796e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.2752, 'grad_norm': 12.1875, 'learning_rate': 1.371908209924446e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 1.271, 'grad_norm': 10.625, 'learning_rate': 1.3622611512793124e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.295, 'grad_norm': 16.75, 'learning_rate': 1.3526140926341788e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.2537, 'grad_norm': 12.25, 'learning_rate': 1.3429670339890453e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 1.285664677619934, 'eval_accuracy': 0.4433916423712342, 'eval_precision': 0.47378831574068503, 'eval_recall': 0.4582161966141388, 'eval_f1': 0.4545016812347555, 'eval_runtime': 4.1662, 'eval_samples_per_second': 987.951, 'eval_steps_per_second': 61.927, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.4434 f1=0.4545 p=0.4738 r=0.4582\n",
      "{'loss': 1.2687, 'grad_norm': 14.3125, 'learning_rate': 1.3333199753439117e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.2594, 'grad_norm': 13.25, 'learning_rate': 1.3236729166987781e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.256, 'grad_norm': 13.125, 'learning_rate': 1.3140258580536445e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.2799, 'grad_norm': 14.3125, 'learning_rate': 1.3043787994085109e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.2702, 'grad_norm': 14.625, 'learning_rate': 1.2947317407633774e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.2731, 'grad_norm': 9.375, 'learning_rate': 1.2850846821182438e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.2621, 'grad_norm': 17.5, 'learning_rate': 1.2754376234731102e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.2632, 'grad_norm': 22.5, 'learning_rate': 1.2657905648279766e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 1.2612, 'grad_norm': 10.75, 'learning_rate': 1.2561435061828432e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.2816, 'grad_norm': 12.125, 'learning_rate': 1.2464964475377094e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.2459, 'grad_norm': 19.625, 'learning_rate': 1.2368493888925758e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 1.2495, 'grad_norm': 22.375, 'learning_rate': 1.2272023302474422e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 1.2575, 'grad_norm': 9.625, 'learning_rate': 1.2175552716023088e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.2527, 'grad_norm': 14.1875, 'learning_rate': 1.2079082129571751e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.2531, 'grad_norm': 11.375, 'learning_rate': 1.1982611543120415e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 1.248, 'grad_norm': 12.0625, 'learning_rate': 1.188614095666908e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 1.2428, 'grad_norm': 12.1875, 'learning_rate': 1.1789670370217745e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 1.2373, 'grad_norm': 14.6875, 'learning_rate': 1.1693199783766409e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 1.2554, 'grad_norm': 14.0, 'learning_rate': 1.1596729197315073e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.2406, 'grad_norm': 14.8125, 'learning_rate': 1.1500258610863737e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 1.2509, 'grad_norm': 18.25, 'learning_rate': 1.1403788024412402e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 1.2374, 'grad_norm': 13.6875, 'learning_rate': 1.1307317437961066e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.2654, 'grad_norm': 11.9375, 'learning_rate': 1.121084685150973e-05, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 1.2701644897460938, 'eval_accuracy': 0.44946550048590866, 'eval_precision': 0.4796941988481807, 'eval_recall': 0.4661614758520747, 'eval_f1': 0.46058785673970115, 'eval_runtime': 4.1887, 'eval_samples_per_second': 982.645, 'eval_steps_per_second': 61.594, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.4495 f1=0.4606 p=0.4797 r=0.4662\n",
      "{'loss': 1.2384, 'grad_norm': 15.0, 'learning_rate': 1.1114376265058394e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 1.2512, 'grad_norm': 11.625, 'learning_rate': 1.101790567860706e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.2413, 'grad_norm': 14.6875, 'learning_rate': 1.0921435092155724e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.2414, 'grad_norm': 18.375, 'learning_rate': 1.0824964505704386e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 1.2465, 'grad_norm': 21.375, 'learning_rate': 1.072849391925305e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.2351, 'grad_norm': 16.25, 'learning_rate': 1.0632023332801714e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 1.2339, 'grad_norm': 16.625, 'learning_rate': 1.053555274635038e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.2459, 'grad_norm': 18.75, 'learning_rate': 1.0439082159899043e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.237, 'grad_norm': 11.8125, 'learning_rate': 1.0342611573447707e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.246, 'grad_norm': 14.6875, 'learning_rate': 1.0246140986996371e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.2544, 'grad_norm': 12.6875, 'learning_rate': 1.0149670400545037e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.2557, 'grad_norm': 12.0625, 'learning_rate': 1.00531998140937e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.2593, 'grad_norm': 11.9375, 'learning_rate': 9.956729227642365e-06, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.2301, 'grad_norm': 13.5, 'learning_rate': 9.860258641191028e-06, 'epoch': 3.5853131749460045}\n",
      "{'loss': 1.2758, 'grad_norm': 16.0, 'learning_rate': 9.763788054739694e-06, 'epoch': 3.6285097192224622}\n",
      "{'loss': 1.2366, 'grad_norm': 25.75, 'learning_rate': 9.667317468288358e-06, 'epoch': 3.67170626349892}\n",
      "{'loss': 1.2238, 'grad_norm': 26.25, 'learning_rate': 9.570846881837022e-06, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.2441, 'grad_norm': 11.375, 'learning_rate': 9.474376295385686e-06, 'epoch': 3.7580993520518358}\n",
      "{'loss': 1.2501, 'grad_norm': 19.375, 'learning_rate': 9.377905708934351e-06, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.254, 'grad_norm': 12.3125, 'learning_rate': 9.281435122483015e-06, 'epoch': 3.8444924406047516}\n",
      "{'loss': 1.2237, 'grad_norm': 14.0, 'learning_rate': 9.184964536031678e-06, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.226, 'grad_norm': 18.25, 'learning_rate': 9.088493949580342e-06, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.2464, 'grad_norm': 24.375, 'learning_rate': 8.992023363129007e-06, 'epoch': 3.974082073434125}\n",
      "{'eval_loss': 1.259281039237976, 'eval_accuracy': 0.45359572400388726, 'eval_precision': 0.4826846177565369, 'eval_recall': 0.4735761698915325, 'eval_f1': 0.4650426164759901, 'eval_runtime': 4.0153, 'eval_samples_per_second': 1025.073, 'eval_steps_per_second': 64.254, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.4536 f1=0.4650 p=0.4827 r=0.4736\n",
      "{'loss': 1.1846, 'grad_norm': 16.25, 'learning_rate': 8.895552776677671e-06, 'epoch': 4.017278617710583}\n",
      "{'loss': 1.2361, 'grad_norm': 12.1875, 'learning_rate': 8.799082190226335e-06, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.2877, 'grad_norm': 12.75, 'learning_rate': 8.702611603774999e-06, 'epoch': 4.103671706263499}\n",
      "{'loss': 1.2389, 'grad_norm': 14.25, 'learning_rate': 8.606141017323665e-06, 'epoch': 4.146868250539957}\n",
      "{'loss': 1.219, 'grad_norm': 16.875, 'learning_rate': 8.509670430872328e-06, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.2724, 'grad_norm': 14.0625, 'learning_rate': 8.413199844420992e-06, 'epoch': 4.233261339092873}\n",
      "{'loss': 1.2181, 'grad_norm': 14.375, 'learning_rate': 8.316729257969656e-06, 'epoch': 4.27645788336933}\n",
      "{'loss': 1.2261, 'grad_norm': 10.25, 'learning_rate': 8.220258671518322e-06, 'epoch': 4.319654427645788}\n",
      "{'loss': 1.2193, 'grad_norm': 23.125, 'learning_rate': 8.123788085066986e-06, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.2489, 'grad_norm': 26.375, 'learning_rate': 8.02731749861565e-06, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.2355, 'grad_norm': 11.4375, 'learning_rate': 7.930846912164314e-06, 'epoch': 4.449244060475162}\n",
      "{'loss': 1.2304, 'grad_norm': 15.6875, 'learning_rate': 7.834376325712978e-06, 'epoch': 4.49244060475162}\n",
      "{'loss': 1.2389, 'grad_norm': 16.75, 'learning_rate': 7.737905739261642e-06, 'epoch': 4.535637149028078}\n",
      "{'loss': 1.2285, 'grad_norm': 12.6875, 'learning_rate': 7.641435152810305e-06, 'epoch': 4.578833693304536}\n",
      "{'loss': 1.2341, 'grad_norm': 10.4375, 'learning_rate': 7.54496456635897e-06, 'epoch': 4.622030237580994}\n",
      "{'loss': 1.2447, 'grad_norm': 14.125, 'learning_rate': 7.448493979907635e-06, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.2244, 'grad_norm': 14.375, 'learning_rate': 7.352023393456299e-06, 'epoch': 4.708423326133909}\n",
      "{'loss': 1.219, 'grad_norm': 8.625, 'learning_rate': 7.255552807004964e-06, 'epoch': 4.751619870410368}\n",
      "{'loss': 1.2479, 'grad_norm': 15.4375, 'learning_rate': 7.159082220553627e-06, 'epoch': 4.794816414686825}\n",
      "{'loss': 1.2165, 'grad_norm': 19.0, 'learning_rate': 7.0626116341022915e-06, 'epoch': 4.838012958963283}\n",
      "{'loss': 1.2588, 'grad_norm': 18.375, 'learning_rate': 6.9661410476509554e-06, 'epoch': 4.881209503239741}\n",
      "{'loss': 1.2368, 'grad_norm': 17.125, 'learning_rate': 6.86967046119962e-06, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.2585, 'grad_norm': 18.75, 'learning_rate': 6.773199874748284e-06, 'epoch': 4.967602591792657}\n",
      "{'eval_loss': 1.253462553024292, 'eval_accuracy': 0.46039844509232264, 'eval_precision': 0.48971589986812275, 'eval_recall': 0.4762490396094042, 'eval_f1': 0.47199846608234874, 'eval_runtime': 4.1274, 'eval_samples_per_second': 997.241, 'eval_steps_per_second': 62.509, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.4604 f1=0.4720 p=0.4897 r=0.4762\n",
      "{'loss': 1.2465, 'grad_norm': 20.625, 'learning_rate': 6.676729288296949e-06, 'epoch': 5.010799136069115}\n",
      "{'loss': 1.246, 'grad_norm': 12.9375, 'learning_rate': 6.580258701845613e-06, 'epoch': 5.053995680345572}\n",
      "{'loss': 1.2343, 'grad_norm': 13.75, 'learning_rate': 6.483788115394278e-06, 'epoch': 5.09719222462203}\n",
      "{'loss': 1.2517, 'grad_norm': 18.0, 'learning_rate': 6.3873175289429415e-06, 'epoch': 5.140388768898488}\n",
      "{'loss': 1.2278, 'grad_norm': 9.125, 'learning_rate': 6.2908469424916054e-06, 'epoch': 5.183585313174946}\n",
      "{'loss': 1.2451, 'grad_norm': 13.375, 'learning_rate': 6.194376356040269e-06, 'epoch': 5.226781857451404}\n",
      "{'loss': 1.2264, 'grad_norm': 18.5, 'learning_rate': 6.097905769588934e-06, 'epoch': 5.269978401727862}\n",
      "{'loss': 1.2174, 'grad_norm': 8.125, 'learning_rate': 6.001435183137598e-06, 'epoch': 5.313174946004319}\n",
      "{'loss': 1.2594, 'grad_norm': 13.25, 'learning_rate': 5.904964596686263e-06, 'epoch': 5.356371490280777}\n",
      "{'loss': 1.2102, 'grad_norm': 11.1875, 'learning_rate': 5.808494010234927e-06, 'epoch': 5.399568034557236}\n",
      "{'loss': 1.2401, 'grad_norm': 12.5, 'learning_rate': 5.7120234237835915e-06, 'epoch': 5.442764578833693}\n",
      "{'loss': 1.2434, 'grad_norm': 15.5625, 'learning_rate': 5.6155528373322554e-06, 'epoch': 5.485961123110151}\n",
      "{'loss': 1.2348, 'grad_norm': 22.25, 'learning_rate': 5.5190822508809185e-06, 'epoch': 5.529157667386609}\n",
      "{'loss': 1.245, 'grad_norm': 18.375, 'learning_rate': 5.422611664429583e-06, 'epoch': 5.572354211663067}\n",
      "{'loss': 1.2463, 'grad_norm': 11.4375, 'learning_rate': 5.326141077978247e-06, 'epoch': 5.615550755939525}\n",
      "{'loss': 1.2283, 'grad_norm': 25.875, 'learning_rate': 5.229670491526912e-06, 'epoch': 5.658747300215983}\n",
      "{'loss': 1.2234, 'grad_norm': 17.75, 'learning_rate': 5.133199905075576e-06, 'epoch': 5.70194384449244}\n",
      "{'loss': 1.2455, 'grad_norm': 21.75, 'learning_rate': 5.036729318624241e-06, 'epoch': 5.745140388768899}\n",
      "{'loss': 1.2245, 'grad_norm': 10.0, 'learning_rate': 4.940258732172905e-06, 'epoch': 5.788336933045357}\n",
      "{'loss': 1.2425, 'grad_norm': 8.6875, 'learning_rate': 4.843788145721569e-06, 'epoch': 5.831533477321814}\n",
      "{'loss': 1.2276, 'grad_norm': 15.3125, 'learning_rate': 4.747317559270233e-06, 'epoch': 5.874730021598272}\n",
      "{'loss': 1.2327, 'grad_norm': 11.4375, 'learning_rate': 4.650846972818897e-06, 'epoch': 5.91792656587473}\n",
      "{'loss': 1.2481, 'grad_norm': 21.75, 'learning_rate': 4.554376386367561e-06, 'epoch': 5.961123110151188}\n",
      "{'eval_loss': 1.254932165145874, 'eval_accuracy': 0.4586977648202138, 'eval_precision': 0.4871283375089945, 'eval_recall': 0.4759456055336943, 'eval_f1': 0.47077431243038503, 'eval_runtime': 4.2314, 'eval_samples_per_second': 972.73, 'eval_steps_per_second': 60.973, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.4587 f1=0.4708 p=0.4871 r=0.4759\n",
      "{'loss': 1.2353, 'grad_norm': 14.125, 'learning_rate': 4.457905799916226e-06, 'epoch': 6.004319654427646}\n",
      "{'loss': 1.2153, 'grad_norm': 11.4375, 'learning_rate': 4.36143521346489e-06, 'epoch': 6.047516198704104}\n",
      "{'loss': 1.2271, 'grad_norm': 12.1875, 'learning_rate': 4.264964627013555e-06, 'epoch': 6.090712742980561}\n",
      "{'loss': 1.211, 'grad_norm': 19.5, 'learning_rate': 4.1684940405622185e-06, 'epoch': 6.133909287257019}\n",
      "{'loss': 1.2322, 'grad_norm': 11.75, 'learning_rate': 4.0720234541108824e-06, 'epoch': 6.177105831533478}\n",
      "{'loss': 1.2417, 'grad_norm': 18.125, 'learning_rate': 3.975552867659547e-06, 'epoch': 6.220302375809935}\n",
      "{'loss': 1.2339, 'grad_norm': 19.125, 'learning_rate': 3.879082281208211e-06, 'epoch': 6.263498920086393}\n",
      "{'loss': 1.236, 'grad_norm': 16.5, 'learning_rate': 3.7826116947568755e-06, 'epoch': 6.306695464362851}\n",
      "{'loss': 1.2373, 'grad_norm': 14.4375, 'learning_rate': 3.6861411083055394e-06, 'epoch': 6.3498920086393085}\n",
      "{'loss': 1.2413, 'grad_norm': 13.0, 'learning_rate': 3.5896705218542037e-06, 'epoch': 6.393088552915767}\n",
      "{'loss': 1.2192, 'grad_norm': 22.375, 'learning_rate': 3.493199935402868e-06, 'epoch': 6.436285097192225}\n",
      "{'loss': 1.251, 'grad_norm': 13.375, 'learning_rate': 3.3967293489515324e-06, 'epoch': 6.479481641468682}\n",
      "{'loss': 1.2542, 'grad_norm': 15.5, 'learning_rate': 3.3002587625001968e-06, 'epoch': 6.52267818574514}\n",
      "{'loss': 1.2463, 'grad_norm': 11.6875, 'learning_rate': 3.2037881760488607e-06, 'epoch': 6.565874730021598}\n",
      "{'loss': 1.2484, 'grad_norm': 24.125, 'learning_rate': 3.107317589597525e-06, 'epoch': 6.609071274298056}\n",
      "{'loss': 1.2348, 'grad_norm': 22.75, 'learning_rate': 3.0108470031461894e-06, 'epoch': 6.652267818574514}\n",
      "{'loss': 1.2618, 'grad_norm': 21.875, 'learning_rate': 2.9143764166948537e-06, 'epoch': 6.695464362850972}\n",
      "{'loss': 1.237, 'grad_norm': 12.75, 'learning_rate': 2.8179058302435177e-06, 'epoch': 6.7386609071274295}\n",
      "{'loss': 1.2452, 'grad_norm': 14.125, 'learning_rate': 2.721435243792182e-06, 'epoch': 6.781857451403888}\n",
      "{'loss': 1.2302, 'grad_norm': 19.625, 'learning_rate': 2.6249646573408463e-06, 'epoch': 6.825053995680346}\n",
      "{'loss': 1.233, 'grad_norm': 21.25, 'learning_rate': 2.5284940708895107e-06, 'epoch': 6.8682505399568035}\n",
      "{'loss': 1.2298, 'grad_norm': 15.8125, 'learning_rate': 2.4320234844381746e-06, 'epoch': 6.911447084233261}\n",
      "{'loss': 1.2381, 'grad_norm': 10.6875, 'learning_rate': 2.335552897986839e-06, 'epoch': 6.954643628509719}\n",
      "{'loss': 1.2352, 'grad_norm': 14.8125, 'learning_rate': 2.2390823115355033e-06, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 1.2535409927368164, 'eval_accuracy': 0.4611273080660836, 'eval_precision': 0.4881328944393366, 'eval_recall': 0.4776448363163401, 'eval_f1': 0.4722073216059421, 'eval_runtime': 4.0651, 'eval_samples_per_second': 1012.516, 'eval_steps_per_second': 63.467, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.4611 f1=0.4722 p=0.4881 r=0.4776\n",
      "{'loss': 1.2335, 'grad_norm': 21.25, 'learning_rate': 2.1426117250841677e-06, 'epoch': 7.041036717062635}\n",
      "{'loss': 1.2423, 'grad_norm': 16.375, 'learning_rate': 2.0461411386328316e-06, 'epoch': 7.084233261339093}\n",
      "{'loss': 1.2596, 'grad_norm': 15.25, 'learning_rate': 1.949670552181496e-06, 'epoch': 7.1274298056155505}\n",
      "{'loss': 1.2295, 'grad_norm': 15.375, 'learning_rate': 1.85319996573016e-06, 'epoch': 7.170626349892009}\n",
      "{'loss': 1.2378, 'grad_norm': 15.0625, 'learning_rate': 1.7567293792788244e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 1.2012, 'grad_norm': 17.125, 'learning_rate': 1.6602587928274885e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 1.2487, 'grad_norm': 12.3125, 'learning_rate': 1.5637882063761529e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 1.2107, 'grad_norm': 14.4375, 'learning_rate': 1.467317619924817e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 1.2584, 'grad_norm': 20.875, 'learning_rate': 1.3708470334734814e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 1.2182, 'grad_norm': 10.375, 'learning_rate': 1.2743764470221455e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 1.2276, 'grad_norm': 15.9375, 'learning_rate': 1.1779058605708098e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 1.2312, 'grad_norm': 12.875, 'learning_rate': 1.081435274119474e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 1.2383, 'grad_norm': 14.875, 'learning_rate': 9.849646876681383e-07, 'epoch': 7.559395248380129}\n",
      "{'loss': 1.2291, 'grad_norm': 19.625, 'learning_rate': 8.884941012168025e-07, 'epoch': 7.602591792656588}\n",
      "{'loss': 1.2259, 'grad_norm': 9.8125, 'learning_rate': 7.920235147654667e-07, 'epoch': 7.6457883369330455}\n",
      "{'loss': 1.2666, 'grad_norm': 15.9375, 'learning_rate': 6.955529283141309e-07, 'epoch': 7.688984881209503}\n",
      "{'loss': 1.2485, 'grad_norm': 22.125, 'learning_rate': 5.990823418627952e-07, 'epoch': 7.732181425485961}\n",
      "{'loss': 1.2559, 'grad_norm': 12.8125, 'learning_rate': 5.026117554114594e-07, 'epoch': 7.775377969762419}\n",
      "{'loss': 1.2251, 'grad_norm': 11.25, 'learning_rate': 4.061411689601236e-07, 'epoch': 7.818574514038877}\n",
      "{'loss': 1.2401, 'grad_norm': 22.375, 'learning_rate': 3.0967058250878785e-07, 'epoch': 7.861771058315335}\n",
      "{'loss': 1.2397, 'grad_norm': 15.0625, 'learning_rate': 2.1319999605745204e-07, 'epoch': 7.9049676025917925}\n",
      "{'loss': 1.2374, 'grad_norm': 16.875, 'learning_rate': 1.1672940960611628e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 1.2526, 'grad_norm': 14.375, 'learning_rate': 2.0258823154780514e-08, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 1.254742980003357, 'eval_accuracy': 0.4630709426627794, 'eval_precision': 0.49131260696182943, 'eval_recall': 0.47965253712584943, 'eval_f1': 0.4744787085543057, 'eval_runtime': 4.3295, 'eval_samples_per_second': 950.695, 'eval_steps_per_second': 59.592, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.4631 f1=0.4745 p=0.4913 r=0.4797\n",
      "{'train_runtime': 824.787, 'train_samples_per_second': 359.259, 'train_steps_per_second': 22.454, 'train_loss': 1.2841173302792577, 'epoch': 8.0}\n",
      "{'eval_loss': 1.254742980003357, 'eval_accuracy': 0.4630709426627794, 'eval_precision': 0.49131260696182943, 'eval_recall': 0.47965253712584943, 'eval_f1': 0.4744787085543057, 'eval_runtime': 4.3958, 'eval_samples_per_second': 936.348, 'eval_steps_per_second': 58.692, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.4631 f1=0.4745 p=0.4913 r=0.4797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñá‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñá‚ñÇ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñá‚ñÇ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñà‚ñÉ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñà‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ</td></tr><tr><td>train/learning_rate</td><td>‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.47448</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.46307</td></tr><tr><td>eval/f1</td><td>0.47448</td></tr><tr><td>eval/loss</td><td>1.25474</td></tr><tr><td>eval/precision</td><td>0.49131</td></tr><tr><td>eval/recall</td><td>0.47965</td></tr><tr><td>eval/runtime</td><td>4.3958</td></tr><tr><td>eval/samples_per_second</td><td>936.348</td></tr><tr><td>eval/steps_per_second</td><td>58.692</td></tr><tr><td>total_flos</td><td>1.2584829318142752e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>18520</td></tr><tr><td>train/grad_norm</td><td>14.375</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2526</td></tr><tr><td>train_loss</td><td>1.28412</td></tr><tr><td>train_runtime</td><td>824.787</td></tr><tr><td>train_samples_per_second</td><td>359.259</td></tr><tr><td>train_steps_per_second</td><td>22.454</td></tr><tr><td>trial/accuracy</td><td>0.46307</td></tr><tr><td>trial/f1</td><td>0.47448</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t7</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/xkvlxo53' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/xkvlxo53</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_004024-xkvlxo53\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [3:13:07<49:47, 1493.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=7 f1=0.4745\n",
      "[I 2025-08-17 00:54:18,511] Trial 7 finished with value: 0.4744787085543057 and parameters: {'lr': 1.679359968944853e-05, 'weight_decay': 7.888518722036515e-05, 'unfreeze_last_k': 11, 'batch_size': 16}. Best is trial 2 with value: 0.8523413656247504.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_005418-4g96tcri</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/4g96tcri' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t8</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/4g96tcri' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/4g96tcri</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=8 | epochs=8 bs=32 lr=2.58e-05 wd=5.9e-05 k=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n",
      "[Run] epochs=8 bs=32 lr=2.58e-05 wd=5.9e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/1158] loss=1.6438 lr=4.59e-06\n",
      "{'loss': 1.6438, 'grad_norm': 1.640625, 'learning_rate': 4.5874185501851445e-06, 'epoch': 0.08635578583765112}\n",
      "[e0 b200/1158] loss=1.6380 lr=9.22e-06\n",
      "{'loss': 1.638, 'grad_norm': 2.125, 'learning_rate': 9.22117466148327e-06, 'epoch': 0.17271157167530224}\n",
      "[e0 b300/1158] loss=1.6145 lr=1.39e-05\n",
      "{'loss': 1.6145, 'grad_norm': 4.90625, 'learning_rate': 1.3854930772781394e-05, 'epoch': 0.25906735751295334}\n",
      "[e0 b400/1158] loss=1.5627 lr=1.85e-05\n",
      "{'loss': 1.5627, 'grad_norm': 5.84375, 'learning_rate': 1.848868688407952e-05, 'epoch': 0.3454231433506045}\n",
      "[e0 b500/1158] loss=1.5340 lr=2.31e-05\n",
      "{'loss': 1.534, 'grad_norm': 5.125, 'learning_rate': 2.3122442995377645e-05, 'epoch': 0.4317789291882556}\n",
      "[e0 b600/1158] loss=1.5147 lr=2.56e-05\n",
      "{'loss': 1.5147, 'grad_norm': 5.9375, 'learning_rate': 2.563646321502691e-05, 'epoch': 0.5181347150259067}\n",
      "[e0 b700/1158] loss=1.4642 lr=2.53e-05\n",
      "{'loss': 1.4642, 'grad_norm': 8.125, 'learning_rate': 2.5340600973653256e-05, 'epoch': 0.6044905008635578}\n",
      "[e0 b800/1158] loss=1.3955 lr=2.50e-05\n",
      "{'loss': 1.3955, 'grad_norm': 7.71875, 'learning_rate': 2.5044738732279605e-05, 'epoch': 0.690846286701209}\n",
      "[e0 b900/1158] loss=1.3359 lr=2.47e-05\n",
      "{'loss': 1.3359, 'grad_norm': 10.0, 'learning_rate': 2.4748876490905955e-05, 'epoch': 0.7772020725388601}\n",
      "[e0 b1000/1158] loss=1.2988 lr=2.45e-05\n",
      "{'loss': 1.2988, 'grad_norm': 11.625, 'learning_rate': 2.4453014249532304e-05, 'epoch': 0.8635578583765112}\n",
      "[e0 b1100/1158] loss=1.2548 lr=2.42e-05\n",
      "{'loss': 1.2548, 'grad_norm': 10.9375, 'learning_rate': 2.415715200815865e-05, 'epoch': 0.9499136442141624}\n",
      "{'eval_loss': 1.233073353767395, 'eval_accuracy': 0.4647716229348882, 'eval_precision': 0.481838212155572, 'eval_recall': 0.49461139452899416, 'eval_f1': 0.47772882219062174, 'eval_runtime': 2.136, 'eval_samples_per_second': 1926.951, 'eval_steps_per_second': 60.393, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.4648 f1=0.4777 p=0.4818 r=0.4946\n",
      "{'loss': 1.2087, 'grad_norm': 7.875, 'learning_rate': 2.3861289766785e-05, 'epoch': 1.0362694300518134}\n",
      "{'loss': 1.1897, 'grad_norm': 16.125, 'learning_rate': 2.356542752541135e-05, 'epoch': 1.1226252158894645}\n",
      "{'loss': 1.1762, 'grad_norm': 10.125, 'learning_rate': 2.3269565284037695e-05, 'epoch': 1.2089810017271156}\n",
      "{'loss': 1.1679, 'grad_norm': 9.8125, 'learning_rate': 2.2973703042664045e-05, 'epoch': 1.2953367875647668}\n",
      "{'loss': 1.1597, 'grad_norm': 14.5, 'learning_rate': 2.2677840801290394e-05, 'epoch': 1.381692573402418}\n",
      "{'loss': 1.1472, 'grad_norm': 14.0625, 'learning_rate': 2.238197855991674e-05, 'epoch': 1.468048359240069}\n",
      "{'loss': 1.1353, 'grad_norm': 8.5, 'learning_rate': 2.208611631854309e-05, 'epoch': 1.5544041450777202}\n",
      "{'loss': 1.1139, 'grad_norm': 10.3125, 'learning_rate': 2.179025407716944e-05, 'epoch': 1.6407599309153713}\n",
      "{'loss': 1.1039, 'grad_norm': 23.0, 'learning_rate': 2.1494391835795785e-05, 'epoch': 1.7271157167530224}\n",
      "{'loss': 1.1024, 'grad_norm': 13.6875, 'learning_rate': 2.1198529594422135e-05, 'epoch': 1.8134715025906736}\n",
      "{'loss': 1.0743, 'grad_norm': 14.25, 'learning_rate': 2.0902667353048484e-05, 'epoch': 1.8998272884283247}\n",
      "{'loss': 1.0748, 'grad_norm': 9.4375, 'learning_rate': 2.060680511167483e-05, 'epoch': 1.9861830742659758}\n",
      "{'eval_loss': 1.113579273223877, 'eval_accuracy': 0.5259961127308066, 'eval_precision': 0.5482656629066847, 'eval_recall': 0.5599974653532704, 'eval_f1': 0.5392908907360773, 'eval_runtime': 2.0601, 'eval_samples_per_second': 1997.994, 'eval_steps_per_second': 62.619, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.5260 f1=0.5393 p=0.5483 r=0.5600\n",
      "{'loss': 1.0845, 'grad_norm': 12.4375, 'learning_rate': 2.031094287030118e-05, 'epoch': 2.0725388601036268}\n",
      "{'loss': 1.0622, 'grad_norm': 11.625, 'learning_rate': 2.001508062892753e-05, 'epoch': 2.158894645941278}\n",
      "{'loss': 1.0531, 'grad_norm': 13.6875, 'learning_rate': 1.971921838755388e-05, 'epoch': 2.245250431778929}\n",
      "{'loss': 1.0584, 'grad_norm': 14.625, 'learning_rate': 1.9423356146180225e-05, 'epoch': 2.33160621761658}\n",
      "{'loss': 1.0451, 'grad_norm': 10.375, 'learning_rate': 1.9127493904806575e-05, 'epoch': 2.4179620034542313}\n",
      "{'loss': 1.0377, 'grad_norm': 11.5, 'learning_rate': 1.8831631663432924e-05, 'epoch': 2.5043177892918824}\n",
      "{'loss': 1.0551, 'grad_norm': 12.6875, 'learning_rate': 1.853576942205927e-05, 'epoch': 2.5906735751295336}\n",
      "{'loss': 1.0314, 'grad_norm': 13.0, 'learning_rate': 1.823990718068562e-05, 'epoch': 2.6770293609671847}\n",
      "{'loss': 1.0251, 'grad_norm': 11.5, 'learning_rate': 1.794404493931197e-05, 'epoch': 2.763385146804836}\n",
      "{'loss': 1.0319, 'grad_norm': 12.5, 'learning_rate': 1.7648182697938315e-05, 'epoch': 2.849740932642487}\n",
      "{'loss': 1.0294, 'grad_norm': 9.1875, 'learning_rate': 1.7352320456564665e-05, 'epoch': 2.936096718480138}\n",
      "{'eval_loss': 1.056532621383667, 'eval_accuracy': 0.5561224489795918, 'eval_precision': 0.5740543331837064, 'eval_recall': 0.5862628137040119, 'eval_f1': 0.5704376703579526, 'eval_runtime': 2.1555, 'eval_samples_per_second': 1909.538, 'eval_steps_per_second': 59.847, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.5561 f1=0.5704 p=0.5741 r=0.5863\n",
      "{'loss': 1.0228, 'grad_norm': 17.25, 'learning_rate': 1.7056458215191014e-05, 'epoch': 3.0224525043177892}\n",
      "{'loss': 1.0225, 'grad_norm': 9.8125, 'learning_rate': 1.676059597381736e-05, 'epoch': 3.1088082901554404}\n",
      "{'loss': 1.0238, 'grad_norm': 9.125, 'learning_rate': 1.646473373244371e-05, 'epoch': 3.1951640759930915}\n",
      "{'loss': 0.9972, 'grad_norm': 13.5625, 'learning_rate': 1.616887149107006e-05, 'epoch': 3.2815198618307426}\n",
      "{'loss': 1.0088, 'grad_norm': 17.75, 'learning_rate': 1.5873009249696405e-05, 'epoch': 3.3678756476683938}\n",
      "{'loss': 1.0315, 'grad_norm': 14.5625, 'learning_rate': 1.5577147008322755e-05, 'epoch': 3.454231433506045}\n",
      "{'loss': 1.0244, 'grad_norm': 12.125, 'learning_rate': 1.5281284766949104e-05, 'epoch': 3.540587219343696}\n",
      "{'loss': 1.0074, 'grad_norm': 10.5625, 'learning_rate': 1.498542252557545e-05, 'epoch': 3.626943005181347}\n",
      "{'loss': 1.0157, 'grad_norm': 10.6875, 'learning_rate': 1.46895602842018e-05, 'epoch': 3.7132987910189983}\n",
      "{'loss': 1.0044, 'grad_norm': 15.375, 'learning_rate': 1.439369804282815e-05, 'epoch': 3.7996545768566494}\n",
      "{'loss': 1.0041, 'grad_norm': 11.1875, 'learning_rate': 1.4097835801454496e-05, 'epoch': 3.8860103626943006}\n",
      "{'loss': 1.0173, 'grad_norm': 11.4375, 'learning_rate': 1.3801973560080845e-05, 'epoch': 3.9723661485319517}\n",
      "{'eval_loss': 1.0362321138381958, 'eval_accuracy': 0.5687560738581147, 'eval_precision': 0.5850598061755622, 'eval_recall': 0.5983767407294411, 'eval_f1': 0.582648914310982, 'eval_runtime': 2.0295, 'eval_samples_per_second': 2028.052, 'eval_steps_per_second': 63.561, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.5688 f1=0.5826 p=0.5851 r=0.5984\n",
      "{'loss': 0.9825, 'grad_norm': 9.625, 'learning_rate': 1.3506111318707195e-05, 'epoch': 4.058721934369602}\n",
      "{'loss': 1.0223, 'grad_norm': 12.25, 'learning_rate': 1.3210249077333544e-05, 'epoch': 4.1450777202072535}\n",
      "{'loss': 1.0081, 'grad_norm': 13.625, 'learning_rate': 1.291438683595989e-05, 'epoch': 4.231433506044905}\n",
      "{'loss': 0.9872, 'grad_norm': 15.4375, 'learning_rate': 1.261852459458624e-05, 'epoch': 4.317789291882556}\n",
      "{'loss': 0.9933, 'grad_norm': 13.9375, 'learning_rate': 1.2322662353212587e-05, 'epoch': 4.404145077720207}\n",
      "{'loss': 0.9987, 'grad_norm': 14.625, 'learning_rate': 1.2026800111838935e-05, 'epoch': 4.490500863557858}\n",
      "{'loss': 0.9827, 'grad_norm': 12.875, 'learning_rate': 1.1730937870465285e-05, 'epoch': 4.576856649395509}\n",
      "{'loss': 0.9837, 'grad_norm': 12.25, 'learning_rate': 1.1435075629091632e-05, 'epoch': 4.66321243523316}\n",
      "{'loss': 0.9878, 'grad_norm': 14.25, 'learning_rate': 1.1139213387717982e-05, 'epoch': 4.7495682210708114}\n",
      "{'loss': 0.994, 'grad_norm': 13.8125, 'learning_rate': 1.084335114634433e-05, 'epoch': 4.835924006908463}\n",
      "{'loss': 1.0086, 'grad_norm': 11.4375, 'learning_rate': 1.0547488904970678e-05, 'epoch': 4.922279792746114}\n",
      "{'eval_loss': 1.02809476852417, 'eval_accuracy': 0.5738581146744413, 'eval_precision': 0.5909348716694385, 'eval_recall': 0.601208542406369, 'eval_f1': 0.5877017774174045, 'eval_runtime': 2.1164, 'eval_samples_per_second': 1944.811, 'eval_steps_per_second': 60.953, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.5739 f1=0.5877 p=0.5909 r=0.6012\n",
      "{'loss': 1.0207, 'grad_norm': 18.75, 'learning_rate': 1.0251626663597027e-05, 'epoch': 5.008635578583765}\n",
      "{'loss': 1.0061, 'grad_norm': 11.9375, 'learning_rate': 9.955764422223375e-06, 'epoch': 5.094991364421416}\n",
      "{'loss': 1.0008, 'grad_norm': 11.6875, 'learning_rate': 9.659902180849723e-06, 'epoch': 5.181347150259067}\n",
      "{'loss': 0.9808, 'grad_norm': 12.1875, 'learning_rate': 9.364039939476072e-06, 'epoch': 5.267702936096718}\n",
      "{'loss': 0.985, 'grad_norm': 14.625, 'learning_rate': 9.06817769810242e-06, 'epoch': 5.354058721934369}\n",
      "{'loss': 0.9814, 'grad_norm': 13.4375, 'learning_rate': 8.772315456728768e-06, 'epoch': 5.4404145077720205}\n",
      "{'loss': 0.9825, 'grad_norm': 17.875, 'learning_rate': 8.476453215355117e-06, 'epoch': 5.526770293609672}\n",
      "{'loss': 1.0021, 'grad_norm': 13.75, 'learning_rate': 8.180590973981465e-06, 'epoch': 5.613126079447323}\n",
      "{'loss': 0.9821, 'grad_norm': 12.0, 'learning_rate': 7.884728732607814e-06, 'epoch': 5.699481865284974}\n",
      "{'loss': 0.9843, 'grad_norm': 19.125, 'learning_rate': 7.588866491234163e-06, 'epoch': 5.785837651122625}\n",
      "{'loss': 1.0033, 'grad_norm': 19.0, 'learning_rate': 7.293004249860511e-06, 'epoch': 5.872193436960276}\n",
      "{'loss': 0.9967, 'grad_norm': 16.5, 'learning_rate': 6.99714200848686e-06, 'epoch': 5.958549222797927}\n",
      "{'eval_loss': 1.027535080909729, 'eval_accuracy': 0.5706997084548106, 'eval_precision': 0.5890915080746457, 'eval_recall': 0.6006112378327362, 'eval_f1': 0.5850976940057463, 'eval_runtime': 2.1237, 'eval_samples_per_second': 1938.169, 'eval_steps_per_second': 60.744, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.5707 f1=0.5851 p=0.5891 r=0.6006\n",
      "{'loss': 0.985, 'grad_norm': 12.125, 'learning_rate': 6.701279767113208e-06, 'epoch': 6.0449050086355784}\n",
      "{'loss': 0.9701, 'grad_norm': 12.5625, 'learning_rate': 6.405417525739557e-06, 'epoch': 6.13126079447323}\n",
      "{'loss': 0.9861, 'grad_norm': 16.875, 'learning_rate': 6.109555284365905e-06, 'epoch': 6.217616580310881}\n",
      "{'loss': 0.9786, 'grad_norm': 16.75, 'learning_rate': 5.813693042992253e-06, 'epoch': 6.303972366148532}\n",
      "{'loss': 0.9942, 'grad_norm': 18.375, 'learning_rate': 5.517830801618602e-06, 'epoch': 6.390328151986183}\n",
      "{'loss': 0.9852, 'grad_norm': 15.5, 'learning_rate': 5.2219685602449505e-06, 'epoch': 6.476683937823834}\n",
      "{'loss': 0.9888, 'grad_norm': 17.0, 'learning_rate': 4.926106318871298e-06, 'epoch': 6.563039723661485}\n",
      "{'loss': 0.9937, 'grad_norm': 14.0625, 'learning_rate': 4.630244077497647e-06, 'epoch': 6.649395509499136}\n",
      "{'loss': 1.0054, 'grad_norm': 14.6875, 'learning_rate': 4.334381836123996e-06, 'epoch': 6.7357512953367875}\n",
      "{'loss': 0.9887, 'grad_norm': 10.0, 'learning_rate': 4.038519594750343e-06, 'epoch': 6.822107081174439}\n",
      "{'loss': 0.9994, 'grad_norm': 18.5, 'learning_rate': 3.742657353376692e-06, 'epoch': 6.90846286701209}\n",
      "{'loss': 0.9789, 'grad_norm': 15.375, 'learning_rate': 3.4467951120030407e-06, 'epoch': 6.994818652849741}\n",
      "{'eval_loss': 1.023023009300232, 'eval_accuracy': 0.5733722060252673, 'eval_precision': 0.5910655774319771, 'eval_recall': 0.6012315359747857, 'eval_f1': 0.5874407078277082, 'eval_runtime': 2.2696, 'eval_samples_per_second': 1813.515, 'eval_steps_per_second': 56.838, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.5734 f1=0.5874 p=0.5911 r=0.6012\n",
      "{'loss': 1.0004, 'grad_norm': 16.375, 'learning_rate': 3.150932870629389e-06, 'epoch': 7.081174438687392}\n",
      "{'loss': 0.9907, 'grad_norm': 12.9375, 'learning_rate': 2.8550706292557375e-06, 'epoch': 7.167530224525043}\n",
      "{'loss': 0.9851, 'grad_norm': 15.1875, 'learning_rate': 2.559208387882086e-06, 'epoch': 7.253886010362694}\n",
      "{'loss': 0.9738, 'grad_norm': 12.75, 'learning_rate': 2.263346146508435e-06, 'epoch': 7.3402417962003454}\n",
      "{'loss': 0.9913, 'grad_norm': 9.8125, 'learning_rate': 1.967483905134783e-06, 'epoch': 7.426597582037997}\n",
      "{'loss': 0.9736, 'grad_norm': 17.125, 'learning_rate': 1.6716216637611312e-06, 'epoch': 7.512953367875648}\n",
      "{'loss': 0.9726, 'grad_norm': 10.625, 'learning_rate': 1.3757594223874799e-06, 'epoch': 7.599309153713299}\n",
      "{'loss': 1.0073, 'grad_norm': 17.375, 'learning_rate': 1.0798971810138283e-06, 'epoch': 7.68566493955095}\n",
      "{'loss': 0.9967, 'grad_norm': 10.5, 'learning_rate': 7.840349396401766e-07, 'epoch': 7.772020725388601}\n",
      "{'loss': 0.9798, 'grad_norm': 13.8125, 'learning_rate': 4.881726982665251e-07, 'epoch': 7.858376511226252}\n",
      "{'loss': 0.9731, 'grad_norm': 18.0, 'learning_rate': 1.9231045689287352e-07, 'epoch': 7.944732297063903}\n",
      "{'eval_loss': 1.0234626531600952, 'eval_accuracy': 0.5784742468415938, 'eval_precision': 0.5962668834431903, 'eval_recall': 0.6055430667944615, 'eval_f1': 0.5922805669465226, 'eval_runtime': 2.205, 'eval_samples_per_second': 1866.665, 'eval_steps_per_second': 58.503, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.5785 f1=0.5923 p=0.5963 r=0.6055\n",
      "{'train_runtime': 421.9283, 'train_samples_per_second': 702.28, 'train_steps_per_second': 21.956, 'train_loss': 1.0775914134551001, 'epoch': 8.0}\n",
      "{'eval_loss': 1.0234626531600952, 'eval_accuracy': 0.5784742468415938, 'eval_precision': 0.5962668834431903, 'eval_recall': 0.6055430667944615, 'eval_f1': 0.5922805669465226, 'eval_runtime': 2.1365, 'eval_samples_per_second': 1926.553, 'eval_steps_per_second': 60.38, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.5785 f1=0.5923 p=0.5963 r=0.6055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñà‚ñÜ‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÖ‚ñá‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÖ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÖ‚ñá‚ñÑ‚ñà‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÖ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñÑ‚ñÖ‚ñá‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÑ‚ñÜ</td></tr><tr><td>train/learning_rate</td><td>‚ñÇ‚ñÖ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.59228</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.57847</td></tr><tr><td>eval/f1</td><td>0.59228</td></tr><tr><td>eval/loss</td><td>1.02346</td></tr><tr><td>eval/precision</td><td>0.59627</td></tr><tr><td>eval/recall</td><td>0.60554</td></tr><tr><td>eval/runtime</td><td>2.1365</td></tr><tr><td>eval/samples_per_second</td><td>1926.553</td></tr><tr><td>eval/steps_per_second</td><td>60.38</td></tr><tr><td>total_flos</td><td>1.3154490312328992e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>9264</td></tr><tr><td>train/grad_norm</td><td>18</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.9731</td></tr><tr><td>train_loss</td><td>1.07759</td></tr><tr><td>train_runtime</td><td>421.9283</td></tr><tr><td>train_samples_per_second</td><td>702.28</td></tr><tr><td>train_steps_per_second</td><td>21.956</td></tr><tr><td>trial/accuracy</td><td>0.57847</td></tr><tr><td>trial/f1</td><td>0.59228</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t8</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/4g96tcri' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/4g96tcri</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_005418-4g96tcri\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 9/10 [3:20:16<19:20, 1160.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=8 f1=0.5923\n",
      "[I 2025-08-17 01:01:26,944] Trial 8 finished with value: 0.5922805669465226 and parameters: {'lr': 2.5763683978817578e-05, 'weight_decay': 5.946546881683547e-05, 'unfreeze_last_k': 10, 'batch_size': 32}. Best is trial 2 with value: 0.8523413656247504.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_010126-8nco8b3f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/8nco8b3f' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t9</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/8nco8b3f' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/8nco8b3f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=9 | epochs=8 bs=4 lr=4.21e-05 wd=1.9e-06 k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29416\\694089861.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[Run] epochs=8 bs=4 lr=4.21e-05 wd=1.9e-06 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/9260] loss=1.6410 lr=9.37e-07\n",
      "{'loss': 1.641, 'grad_norm': 6.0625, 'learning_rate': 9.368652170568546e-07, 'epoch': 0.01079913606911447}\n",
      "[e0 b200/9260] loss=1.6500 lr=1.88e-06\n",
      "{'loss': 1.65, 'grad_norm': 2.859375, 'learning_rate': 1.8831937191344855e-06, 'epoch': 0.02159827213822894}\n",
      "[e0 b300/9260] loss=1.6472 lr=2.83e-06\n",
      "{'loss': 1.6472, 'grad_norm': 7.15625, 'learning_rate': 2.8295222212121163e-06, 'epoch': 0.032397408207343416}\n",
      "[e0 b400/9260] loss=1.6394 lr=3.78e-06\n",
      "{'loss': 1.6394, 'grad_norm': 4.84375, 'learning_rate': 3.7758507232897473e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b500/9260] loss=1.6493 lr=4.72e-06\n",
      "{'loss': 1.6493, 'grad_norm': 6.15625, 'learning_rate': 4.722179225367378e-06, 'epoch': 0.05399568034557235}\n",
      "[e0 b600/9260] loss=1.6426 lr=5.67e-06\n",
      "{'loss': 1.6426, 'grad_norm': 4.8125, 'learning_rate': 5.668507727445009e-06, 'epoch': 0.06479481641468683}\n",
      "[e0 b700/9260] loss=1.6392 lr=6.61e-06\n",
      "{'loss': 1.6392, 'grad_norm': 4.59375, 'learning_rate': 6.6148362295226394e-06, 'epoch': 0.0755939524838013}\n",
      "[e0 b800/9260] loss=1.6260 lr=7.56e-06\n",
      "{'loss': 1.626, 'grad_norm': 3.671875, 'learning_rate': 7.561164731600272e-06, 'epoch': 0.08639308855291576}\n",
      "[e0 b900/9260] loss=1.6263 lr=8.51e-06\n",
      "{'loss': 1.6263, 'grad_norm': 4.625, 'learning_rate': 8.507493233677901e-06, 'epoch': 0.09719222462203024}\n",
      "[e0 b1000/9260] loss=1.6321 lr=9.45e-06\n",
      "{'loss': 1.6321, 'grad_norm': 5.03125, 'learning_rate': 9.453821735755532e-06, 'epoch': 0.1079913606911447}\n",
      "[e0 b1100/9260] loss=1.6295 lr=1.04e-05\n",
      "{'loss': 1.6295, 'grad_norm': 4.4375, 'learning_rate': 1.0400150237833164e-05, 'epoch': 0.11879049676025918}\n",
      "[e0 b1200/9260] loss=1.6188 lr=1.13e-05\n",
      "{'loss': 1.6188, 'grad_norm': 4.84375, 'learning_rate': 1.1346478739910793e-05, 'epoch': 0.12958963282937366}\n",
      "[e0 b1300/9260] loss=1.6130 lr=1.23e-05\n",
      "{'loss': 1.613, 'grad_norm': 4.59375, 'learning_rate': 1.2292807241988427e-05, 'epoch': 0.14038876889848811}\n",
      "[e0 b1400/9260] loss=1.5967 lr=1.32e-05\n",
      "{'loss': 1.5967, 'grad_norm': 9.8125, 'learning_rate': 1.3239135744066058e-05, 'epoch': 0.1511879049676026}\n",
      "[e0 b1500/9260] loss=1.5854 lr=1.42e-05\n",
      "{'loss': 1.5854, 'grad_norm': 8.1875, 'learning_rate': 1.4185464246143688e-05, 'epoch': 0.16198704103671707}\n",
      "[e0 b1600/9260] loss=1.5615 lr=1.51e-05\n",
      "{'loss': 1.5615, 'grad_norm': 10.9375, 'learning_rate': 1.5131792748221319e-05, 'epoch': 0.17278617710583152}\n",
      "[e0 b1700/9260] loss=1.5492 lr=1.61e-05\n",
      "{'loss': 1.5492, 'grad_norm': 10.125, 'learning_rate': 1.607812125029895e-05, 'epoch': 0.183585313174946}\n",
      "[e0 b1800/9260] loss=1.5530 lr=1.70e-05\n",
      "{'loss': 1.553, 'grad_norm': 7.5625, 'learning_rate': 1.7024449752376578e-05, 'epoch': 0.19438444924406048}\n",
      "[e0 b1900/9260] loss=1.5656 lr=1.80e-05\n",
      "{'loss': 1.5656, 'grad_norm': 11.625, 'learning_rate': 1.797077825445421e-05, 'epoch': 0.20518358531317496}\n",
      "[e0 b2000/9260] loss=1.5394 lr=1.89e-05\n",
      "{'loss': 1.5394, 'grad_norm': 10.8125, 'learning_rate': 1.8917106756531843e-05, 'epoch': 0.2159827213822894}\n",
      "[e0 b2100/9260] loss=1.5419 lr=1.99e-05\n",
      "{'loss': 1.5419, 'grad_norm': 14.125, 'learning_rate': 1.9863435258609475e-05, 'epoch': 0.2267818574514039}\n",
      "[e0 b2200/9260] loss=1.5299 lr=2.08e-05\n",
      "{'loss': 1.5299, 'grad_norm': 27.125, 'learning_rate': 2.0809763760687104e-05, 'epoch': 0.23758099352051837}\n",
      "[e0 b2300/9260] loss=1.5339 lr=2.18e-05\n",
      "{'loss': 1.5339, 'grad_norm': 10.9375, 'learning_rate': 2.1756092262764736e-05, 'epoch': 0.24838012958963282}\n",
      "[e0 b2400/9260] loss=1.4719 lr=2.27e-05\n",
      "{'loss': 1.4719, 'grad_norm': 13.0, 'learning_rate': 2.2702420764842365e-05, 'epoch': 0.2591792656587473}\n",
      "[e0 b2500/9260] loss=1.4894 lr=2.36e-05\n",
      "{'loss': 1.4894, 'grad_norm': 20.75, 'learning_rate': 2.3648749266919997e-05, 'epoch': 0.26997840172786175}\n",
      "[e0 b2600/9260] loss=1.4977 lr=2.46e-05\n",
      "{'loss': 1.4977, 'grad_norm': 17.625, 'learning_rate': 2.4595077768997626e-05, 'epoch': 0.28077753779697623}\n",
      "[e0 b2700/9260] loss=1.4788 lr=2.55e-05\n",
      "{'loss': 1.4788, 'grad_norm': 14.3125, 'learning_rate': 2.554140627107526e-05, 'epoch': 0.2915766738660907}\n",
      "[e0 b2800/9260] loss=1.4842 lr=2.65e-05\n",
      "{'loss': 1.4842, 'grad_norm': 25.375, 'learning_rate': 2.6487734773152887e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b2900/9260] loss=1.4495 lr=2.74e-05\n",
      "{'loss': 1.4495, 'grad_norm': 16.5, 'learning_rate': 2.743406327523052e-05, 'epoch': 0.31317494600431967}\n",
      "[e0 b3000/9260] loss=1.4880 lr=2.84e-05\n",
      "{'loss': 1.488, 'grad_norm': 23.375, 'learning_rate': 2.8380391777308152e-05, 'epoch': 0.32397408207343414}\n",
      "[e0 b3100/9260] loss=1.4182 lr=2.93e-05\n",
      "{'loss': 1.4182, 'grad_norm': 12.375, 'learning_rate': 2.9326720279385784e-05, 'epoch': 0.3347732181425486}\n",
      "[e0 b3200/9260] loss=1.4036 lr=3.03e-05\n",
      "{'loss': 1.4036, 'grad_norm': 16.375, 'learning_rate': 3.0273048781463413e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b3300/9260] loss=1.3001 lr=3.12e-05\n",
      "{'loss': 1.3001, 'grad_norm': 13.125, 'learning_rate': 3.1219377283541045e-05, 'epoch': 0.3563714902807775}\n",
      "[e0 b3400/9260] loss=1.3803 lr=3.22e-05\n",
      "{'loss': 1.3803, 'grad_norm': 31.875, 'learning_rate': 3.216570578561868e-05, 'epoch': 0.367170626349892}\n",
      "[e0 b3500/9260] loss=1.3054 lr=3.31e-05\n",
      "{'loss': 1.3054, 'grad_norm': 24.875, 'learning_rate': 3.31120342876963e-05, 'epoch': 0.3779697624190065}\n",
      "[e0 b3600/9260] loss=1.3141 lr=3.41e-05\n",
      "{'loss': 1.3141, 'grad_norm': 44.75, 'learning_rate': 3.4058362789773935e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b3700/9260] loss=1.3070 lr=3.50e-05\n",
      "{'loss': 1.307, 'grad_norm': 21.625, 'learning_rate': 3.500469129185157e-05, 'epoch': 0.39956803455723544}\n",
      "[e0 b3800/9260] loss=1.2949 lr=3.60e-05\n",
      "{'loss': 1.2949, 'grad_norm': 30.875, 'learning_rate': 3.59510197939292e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b3900/9260] loss=1.2783 lr=3.69e-05\n",
      "{'loss': 1.2783, 'grad_norm': 14.5625, 'learning_rate': 3.6897348296006825e-05, 'epoch': 0.42116630669546434}\n",
      "[e0 b4000/9260] loss=1.3566 lr=3.78e-05\n",
      "{'loss': 1.3566, 'grad_norm': 26.0, 'learning_rate': 3.784367679808446e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b4100/9260] loss=1.2796 lr=3.88e-05\n",
      "{'loss': 1.2796, 'grad_norm': 16.75, 'learning_rate': 3.879000530016209e-05, 'epoch': 0.4427645788336933}\n",
      "[e0 b4200/9260] loss=1.2059 lr=3.97e-05\n",
      "{'loss': 1.2059, 'grad_norm': 39.0, 'learning_rate': 3.973633380223972e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b4300/9260] loss=1.2433 lr=4.07e-05\n",
      "{'loss': 1.2433, 'grad_norm': 15.0625, 'learning_rate': 4.0682662304317354e-05, 'epoch': 0.46436285097192226}\n",
      "[e0 b4400/9260] loss=1.1859 lr=4.16e-05\n",
      "{'loss': 1.1859, 'grad_norm': 39.75, 'learning_rate': 4.1628990806394986e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b4500/9260] loss=1.2047 lr=4.20e-05\n",
      "{'loss': 1.2047, 'grad_norm': 51.0, 'learning_rate': 4.203168222461663e-05, 'epoch': 0.48596112311015116}\n",
      "[e0 b4600/9260] loss=1.2189 lr=4.20e-05\n",
      "{'loss': 1.2189, 'grad_norm': 25.125, 'learning_rate': 4.19712753862202e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b4700/9260] loss=1.1957 lr=4.19e-05\n",
      "{'loss': 1.1957, 'grad_norm': 36.0, 'learning_rate': 4.1910868547823774e-05, 'epoch': 0.5075593952483801}\n",
      "[e0 b4800/9260] loss=1.1669 lr=4.19e-05\n",
      "{'loss': 1.1669, 'grad_norm': 47.5, 'learning_rate': 4.185046170942735e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b4900/9260] loss=1.1665 lr=4.18e-05\n",
      "{'loss': 1.1665, 'grad_norm': 26.125, 'learning_rate': 4.179005487103093e-05, 'epoch': 0.5291576673866091}\n",
      "[e0 b5000/9260] loss=1.1275 lr=4.17e-05\n",
      "{'loss': 1.1275, 'grad_norm': 47.5, 'learning_rate': 4.17296480326345e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b5100/9260] loss=1.1317 lr=4.17e-05\n",
      "{'loss': 1.1317, 'grad_norm': 20.25, 'learning_rate': 4.1669241194238075e-05, 'epoch': 0.550755939524838}\n",
      "[e0 b5200/9260] loss=1.1042 lr=4.16e-05\n",
      "{'loss': 1.1042, 'grad_norm': 31.625, 'learning_rate': 4.1608834355841645e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b5300/9260] loss=1.1558 lr=4.15e-05\n",
      "{'loss': 1.1558, 'grad_norm': 36.0, 'learning_rate': 4.154842751744522e-05, 'epoch': 0.572354211663067}\n",
      "[e0 b5400/9260] loss=1.0590 lr=4.15e-05\n",
      "{'loss': 1.059, 'grad_norm': 29.375, 'learning_rate': 4.14880206790488e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b5500/9260] loss=1.1232 lr=4.14e-05\n",
      "{'loss': 1.1232, 'grad_norm': 29.125, 'learning_rate': 4.1427613840652375e-05, 'epoch': 0.593952483801296}\n",
      "[e0 b5600/9260] loss=1.1082 lr=4.14e-05\n",
      "{'loss': 1.1082, 'grad_norm': 23.875, 'learning_rate': 4.1367207002255945e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b5700/9260] loss=1.1147 lr=4.13e-05\n",
      "{'loss': 1.1147, 'grad_norm': 18.0, 'learning_rate': 4.130680016385952e-05, 'epoch': 0.6155507559395248}\n",
      "[e0 b5800/9260] loss=1.0975 lr=4.12e-05\n",
      "{'loss': 1.0975, 'grad_norm': 18.875, 'learning_rate': 4.12463933254631e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b5900/9260] loss=1.0833 lr=4.12e-05\n",
      "{'loss': 1.0833, 'grad_norm': 22.125, 'learning_rate': 4.1185986487066676e-05, 'epoch': 0.6371490280777538}\n",
      "[e0 b6000/9260] loss=1.0151 lr=4.11e-05\n",
      "{'loss': 1.0151, 'grad_norm': 39.5, 'learning_rate': 4.1125579648670246e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b6100/9260] loss=0.9857 lr=4.11e-05\n",
      "{'loss': 0.9857, 'grad_norm': 21.0, 'learning_rate': 4.106517281027382e-05, 'epoch': 0.6587473002159827}\n",
      "[e0 b6200/9260] loss=1.0343 lr=4.10e-05\n",
      "{'loss': 1.0343, 'grad_norm': 24.375, 'learning_rate': 4.100476597187739e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b6300/9260] loss=1.0039 lr=4.09e-05\n",
      "{'loss': 1.0039, 'grad_norm': 97.0, 'learning_rate': 4.094435913348097e-05, 'epoch': 0.6803455723542117}\n",
      "[e0 b6400/9260] loss=1.0672 lr=4.09e-05\n",
      "{'loss': 1.0672, 'grad_norm': 20.875, 'learning_rate': 4.0883952295084546e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b6500/9260] loss=1.0529 lr=4.08e-05\n",
      "{'loss': 1.0529, 'grad_norm': 27.625, 'learning_rate': 4.082354545668812e-05, 'epoch': 0.7019438444924406}\n",
      "[e0 b6600/9260] loss=1.0742 lr=4.08e-05\n",
      "{'loss': 1.0742, 'grad_norm': 54.0, 'learning_rate': 4.076313861829169e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b6700/9260] loss=0.9226 lr=4.07e-05\n",
      "{'loss': 0.9226, 'grad_norm': 52.25, 'learning_rate': 4.070273177989527e-05, 'epoch': 0.7235421166306696}\n",
      "[e0 b6800/9260] loss=0.9813 lr=4.06e-05\n",
      "{'loss': 0.9813, 'grad_norm': 28.125, 'learning_rate': 4.064232494149885e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b6900/9260] loss=1.0428 lr=4.06e-05\n",
      "{'loss': 1.0428, 'grad_norm': 14.125, 'learning_rate': 4.0581918103102424e-05, 'epoch': 0.7451403887688985}\n",
      "[e0 b7000/9260] loss=1.0040 lr=4.05e-05\n",
      "{'loss': 1.004, 'grad_norm': 46.25, 'learning_rate': 4.0521511264705994e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b7100/9260] loss=1.0516 lr=4.05e-05\n",
      "{'loss': 1.0516, 'grad_norm': 58.75, 'learning_rate': 4.046110442630957e-05, 'epoch': 0.7667386609071274}\n",
      "[e0 b7200/9260] loss=0.9942 lr=4.04e-05\n",
      "{'loss': 0.9942, 'grad_norm': 36.75, 'learning_rate': 4.040069758791314e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b7300/9260] loss=0.9972 lr=4.03e-05\n",
      "{'loss': 0.9972, 'grad_norm': 23.5, 'learning_rate': 4.034029074951672e-05, 'epoch': 0.7883369330453563}\n",
      "[e0 b7400/9260] loss=0.9784 lr=4.03e-05\n",
      "{'loss': 0.9784, 'grad_norm': 26.375, 'learning_rate': 4.0279883911120294e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b7500/9260] loss=0.9214 lr=4.02e-05\n",
      "{'loss': 0.9214, 'grad_norm': 41.0, 'learning_rate': 4.021947707272387e-05, 'epoch': 0.8099352051835853}\n",
      "[e0 b7600/9260] loss=0.8860 lr=4.02e-05\n",
      "{'loss': 0.886, 'grad_norm': 29.375, 'learning_rate': 4.015907023432744e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b7700/9260] loss=1.0333 lr=4.01e-05\n",
      "{'loss': 1.0333, 'grad_norm': 42.5, 'learning_rate': 4.009866339593102e-05, 'epoch': 0.8315334773218143}\n",
      "[e0 b7800/9260] loss=1.0216 lr=4.00e-05\n",
      "{'loss': 1.0216, 'grad_norm': 40.75, 'learning_rate': 4.003825655753459e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b7900/9260] loss=1.0008 lr=4.00e-05\n",
      "{'loss': 1.0008, 'grad_norm': 43.75, 'learning_rate': 3.997784971913817e-05, 'epoch': 0.8531317494600432}\n",
      "[e0 b8000/9260] loss=0.9465 lr=3.99e-05\n",
      "{'loss': 0.9465, 'grad_norm': 32.5, 'learning_rate': 3.991744288074174e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b8100/9260] loss=0.9717 lr=3.99e-05\n",
      "{'loss': 0.9717, 'grad_norm': 10.625, 'learning_rate': 3.985703604234532e-05, 'epoch': 0.8747300215982722}\n",
      "[e0 b8200/9260] loss=1.0035 lr=3.98e-05\n",
      "{'loss': 1.0035, 'grad_norm': 49.5, 'learning_rate': 3.979662920394889e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b8300/9260] loss=0.8455 lr=3.97e-05\n",
      "{'loss': 0.8455, 'grad_norm': 28.625, 'learning_rate': 3.9736222365552465e-05, 'epoch': 0.896328293736501}\n",
      "[e0 b8400/9260] loss=0.9242 lr=3.97e-05\n",
      "{'loss': 0.9242, 'grad_norm': 30.375, 'learning_rate': 3.967581552715604e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b8500/9260] loss=0.9577 lr=3.96e-05\n",
      "{'loss': 0.9577, 'grad_norm': 24.25, 'learning_rate': 3.961540868875962e-05, 'epoch': 0.91792656587473}\n",
      "[e0 b8600/9260] loss=1.0425 lr=3.96e-05\n",
      "{'loss': 1.0425, 'grad_norm': 14.0, 'learning_rate': 3.955500185036319e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b8700/9260] loss=0.8795 lr=3.95e-05\n",
      "{'loss': 0.8795, 'grad_norm': 22.25, 'learning_rate': 3.9494595011966766e-05, 'epoch': 0.9395248380129589}\n",
      "[e0 b8800/9260] loss=0.9246 lr=3.94e-05\n",
      "{'loss': 0.9246, 'grad_norm': 8.4375, 'learning_rate': 3.9434188173570336e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b8900/9260] loss=1.0013 lr=3.94e-05\n",
      "{'loss': 1.0013, 'grad_norm': 26.125, 'learning_rate': 3.937378133517392e-05, 'epoch': 0.9611231101511879}\n",
      "[e0 b9000/9260] loss=0.9228 lr=3.93e-05\n",
      "{'loss': 0.9228, 'grad_norm': 22.375, 'learning_rate': 3.931337449677749e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b9100/9260] loss=0.8249 lr=3.93e-05\n",
      "{'loss': 0.8249, 'grad_norm': 25.375, 'learning_rate': 3.9252967658381066e-05, 'epoch': 0.9827213822894169}\n",
      "[e0 b9200/9260] loss=0.8268 lr=3.92e-05\n",
      "{'loss': 0.8268, 'grad_norm': 31.0, 'learning_rate': 3.9192560819984636e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 0.879344642162323, 'eval_accuracy': 0.6574344023323615, 'eval_precision': 0.676479553104007, 'eval_recall': 0.6689662156951238, 'eval_f1': 0.6688719547502491, 'eval_runtime': 15.6919, 'eval_samples_per_second': 262.301, 'eval_steps_per_second': 65.575, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.6574 f1=0.6689 p=0.6765 r=0.6690\n",
      "{'loss': 0.8406, 'grad_norm': 42.75, 'learning_rate': 3.913215398158821e-05, 'epoch': 1.0043196544276458}\n",
      "{'loss': 0.8382, 'grad_norm': 8.9375, 'learning_rate': 3.907174714319179e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 0.8607, 'grad_norm': 30.0, 'learning_rate': 3.9011340304795366e-05, 'epoch': 1.0259179265658747}\n",
      "{'loss': 0.8905, 'grad_norm': 22.625, 'learning_rate': 3.8950933466398937e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 0.9587, 'grad_norm': 65.0, 'learning_rate': 3.889052662800251e-05, 'epoch': 1.0475161987041037}\n",
      "{'loss': 0.8519, 'grad_norm': 39.0, 'learning_rate': 3.8830119789606083e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 0.89, 'grad_norm': 35.5, 'learning_rate': 3.876971295120966e-05, 'epoch': 1.0691144708423326}\n",
      "{'loss': 0.8756, 'grad_norm': 44.5, 'learning_rate': 3.870930611281324e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 0.8654, 'grad_norm': 19.625, 'learning_rate': 3.8648899274416814e-05, 'epoch': 1.0907127429805616}\n",
      "{'loss': 0.8573, 'grad_norm': 58.5, 'learning_rate': 3.858849243602039e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 0.8286, 'grad_norm': 30.25, 'learning_rate': 3.852808559762396e-05, 'epoch': 1.1123110151187905}\n",
      "{'loss': 0.9974, 'grad_norm': 44.75, 'learning_rate': 3.846767875922754e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 0.8554, 'grad_norm': 53.75, 'learning_rate': 3.8407271920831114e-05, 'epoch': 1.1339092872570196}\n",
      "{'loss': 0.8157, 'grad_norm': 33.5, 'learning_rate': 3.834686508243469e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 0.8479, 'grad_norm': 66.5, 'learning_rate': 3.828645824403826e-05, 'epoch': 1.1555075593952484}\n",
      "{'loss': 0.8058, 'grad_norm': 38.0, 'learning_rate': 3.822605140564184e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 0.8764, 'grad_norm': 57.75, 'learning_rate': 3.816564456724541e-05, 'epoch': 1.1771058315334773}\n",
      "{'loss': 0.8533, 'grad_norm': 22.25, 'learning_rate': 3.810523772884899e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 0.8394, 'grad_norm': 37.75, 'learning_rate': 3.804483089045256e-05, 'epoch': 1.1987041036717063}\n",
      "{'loss': 0.8165, 'grad_norm': 28.125, 'learning_rate': 3.798442405205614e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 0.8249, 'grad_norm': 63.0, 'learning_rate': 3.792401721365971e-05, 'epoch': 1.2203023758099352}\n",
      "{'loss': 0.7851, 'grad_norm': 17.75, 'learning_rate': 3.7863610375263285e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 0.778, 'grad_norm': 31.625, 'learning_rate': 3.780320353686686e-05, 'epoch': 1.2419006479481642}\n",
      "{'loss': 0.8361, 'grad_norm': 20.375, 'learning_rate': 3.774279669847044e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 0.9091, 'grad_norm': 60.25, 'learning_rate': 3.768238986007401e-05, 'epoch': 1.263498920086393}\n",
      "{'loss': 0.8978, 'grad_norm': 15.25, 'learning_rate': 3.7621983021677586e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 0.8478, 'grad_norm': 24.75, 'learning_rate': 3.7561576183281156e-05, 'epoch': 1.285097192224622}\n",
      "{'loss': 0.8534, 'grad_norm': 27.75, 'learning_rate': 3.750116934488473e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 0.9471, 'grad_norm': 53.0, 'learning_rate': 3.744076250648831e-05, 'epoch': 1.306695464362851}\n",
      "{'loss': 0.8987, 'grad_norm': 42.25, 'learning_rate': 3.7380355668091886e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 0.8058, 'grad_norm': 19.25, 'learning_rate': 3.7319948829695456e-05, 'epoch': 1.3282937365010798}\n",
      "{'loss': 0.8322, 'grad_norm': 62.75, 'learning_rate': 3.725954199129903e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 0.8553, 'grad_norm': 41.0, 'learning_rate': 3.719913515290261e-05, 'epoch': 1.349892008639309}\n",
      "{'loss': 0.877, 'grad_norm': 23.25, 'learning_rate': 3.713872831450619e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 0.7768, 'grad_norm': 10.6875, 'learning_rate': 3.707832147610976e-05, 'epoch': 1.3714902807775378}\n",
      "{'loss': 0.8438, 'grad_norm': 88.5, 'learning_rate': 3.7017914637713334e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 0.8621, 'grad_norm': 39.25, 'learning_rate': 3.6957507799316904e-05, 'epoch': 1.3930885529157666}\n",
      "{'loss': 0.8673, 'grad_norm': 51.0, 'learning_rate': 3.689710096092048e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 0.9161, 'grad_norm': 25.875, 'learning_rate': 3.683669412252406e-05, 'epoch': 1.4146868250539957}\n",
      "{'loss': 0.7933, 'grad_norm': 37.5, 'learning_rate': 3.6776287284127634e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 0.78, 'grad_norm': 47.0, 'learning_rate': 3.6715880445731204e-05, 'epoch': 1.4362850971922247}\n",
      "{'loss': 0.8218, 'grad_norm': 53.25, 'learning_rate': 3.665547360733478e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 0.7514, 'grad_norm': 71.0, 'learning_rate': 3.659506676893836e-05, 'epoch': 1.4578833693304536}\n",
      "{'loss': 0.828, 'grad_norm': 26.875, 'learning_rate': 3.6534659930541935e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 0.879, 'grad_norm': 23.5, 'learning_rate': 3.6474253092145505e-05, 'epoch': 1.4794816414686824}\n",
      "{'loss': 0.8633, 'grad_norm': 53.25, 'learning_rate': 3.641384625374908e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 0.8049, 'grad_norm': 35.75, 'learning_rate': 3.635343941535265e-05, 'epoch': 1.5010799136069113}\n",
      "{'loss': 0.7728, 'grad_norm': 30.625, 'learning_rate': 3.629303257695623e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 0.8515, 'grad_norm': 36.25, 'learning_rate': 3.6232625738559805e-05, 'epoch': 1.5226781857451404}\n",
      "{'loss': 0.8058, 'grad_norm': 41.5, 'learning_rate': 3.617221890016338e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 0.8556, 'grad_norm': 16.375, 'learning_rate': 3.611181206176695e-05, 'epoch': 1.5442764578833694}\n",
      "{'loss': 0.8404, 'grad_norm': 68.5, 'learning_rate': 3.605140522337053e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 0.9145, 'grad_norm': 15.125, 'learning_rate': 3.5990998384974106e-05, 'epoch': 1.5658747300215983}\n",
      "{'loss': 0.7706, 'grad_norm': 30.25, 'learning_rate': 3.593059154657768e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 0.7867, 'grad_norm': 29.875, 'learning_rate': 3.587018470818125e-05, 'epoch': 1.5874730021598271}\n",
      "{'loss': 0.751, 'grad_norm': 8.8125, 'learning_rate': 3.580977786978483e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 0.7618, 'grad_norm': 26.0, 'learning_rate': 3.57493710313884e-05, 'epoch': 1.6090712742980562}\n",
      "{'loss': 0.8591, 'grad_norm': 56.5, 'learning_rate': 3.5688964192991976e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 0.864, 'grad_norm': 105.0, 'learning_rate': 3.562855735459555e-05, 'epoch': 1.6306695464362853}\n",
      "{'loss': 0.8746, 'grad_norm': 29.625, 'learning_rate': 3.556815051619913e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 0.8051, 'grad_norm': 48.5, 'learning_rate': 3.55077436778027e-05, 'epoch': 1.652267818574514}\n",
      "{'loss': 0.7947, 'grad_norm': 27.875, 'learning_rate': 3.5447336839406277e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 0.7999, 'grad_norm': 3.875, 'learning_rate': 3.538693000100985e-05, 'epoch': 1.673866090712743}\n",
      "{'loss': 0.7509, 'grad_norm': 22.875, 'learning_rate': 3.532652316261343e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 0.8625, 'grad_norm': 95.5, 'learning_rate': 3.5266116324217e-05, 'epoch': 1.6954643628509718}\n",
      "{'loss': 0.8353, 'grad_norm': 23.25, 'learning_rate': 3.520570948582058e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 0.8837, 'grad_norm': 30.875, 'learning_rate': 3.514530264742415e-05, 'epoch': 1.7170626349892009}\n",
      "{'loss': 0.8013, 'grad_norm': 18.875, 'learning_rate': 3.5084895809027724e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 0.8288, 'grad_norm': 88.0, 'learning_rate': 3.50244889706313e-05, 'epoch': 1.73866090712743}\n",
      "{'loss': 0.8362, 'grad_norm': 33.5, 'learning_rate': 3.496408213223488e-05, 'epoch': 1.7494600431965441}\n",
      "{'loss': 0.7606, 'grad_norm': 65.5, 'learning_rate': 3.490367529383845e-05, 'epoch': 1.7602591792656588}\n",
      "{'loss': 0.693, 'grad_norm': 7.125, 'learning_rate': 3.4843268455442024e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 0.9062, 'grad_norm': 28.25, 'learning_rate': 3.47828616170456e-05, 'epoch': 1.7818574514038876}\n",
      "{'loss': 0.8019, 'grad_norm': 38.0, 'learning_rate': 3.472245477864918e-05, 'epoch': 1.7926565874730023}\n",
      "{'loss': 0.8446, 'grad_norm': 45.25, 'learning_rate': 3.466204794025275e-05, 'epoch': 1.8034557235421165}\n",
      "{'loss': 0.7697, 'grad_norm': 76.0, 'learning_rate': 3.4601641101856325e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 0.7552, 'grad_norm': 12.25, 'learning_rate': 3.4541234263459895e-05, 'epoch': 1.8250539956803455}\n",
      "{'loss': 0.7031, 'grad_norm': 43.75, 'learning_rate': 3.448082742506347e-05, 'epoch': 1.83585313174946}\n",
      "{'loss': 0.7993, 'grad_norm': 43.25, 'learning_rate': 3.442042058666705e-05, 'epoch': 1.8466522678185746}\n",
      "{'loss': 0.7721, 'grad_norm': 41.0, 'learning_rate': 3.4360013748270625e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 0.8169, 'grad_norm': 49.25, 'learning_rate': 3.4299606909874195e-05, 'epoch': 1.8682505399568035}\n",
      "{'loss': 0.7774, 'grad_norm': 42.75, 'learning_rate': 3.423920007147777e-05, 'epoch': 1.8790496760259179}\n",
      "{'loss': 0.8899, 'grad_norm': 81.5, 'learning_rate': 3.417879323308135e-05, 'epoch': 1.8898488120950323}\n",
      "{'loss': 0.7666, 'grad_norm': 39.0, 'learning_rate': 3.4118386394684926e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 0.8051, 'grad_norm': 26.25, 'learning_rate': 3.4057979556288496e-05, 'epoch': 1.9114470842332614}\n",
      "{'loss': 0.801, 'grad_norm': 36.25, 'learning_rate': 3.399757271789207e-05, 'epoch': 1.9222462203023758}\n",
      "{'loss': 0.7993, 'grad_norm': 59.25, 'learning_rate': 3.393716587949564e-05, 'epoch': 1.9330453563714904}\n",
      "{'loss': 0.8336, 'grad_norm': 55.5, 'learning_rate': 3.387675904109922e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 0.7433, 'grad_norm': 198.0, 'learning_rate': 3.3816352202702796e-05, 'epoch': 1.9546436285097193}\n",
      "{'loss': 0.6899, 'grad_norm': 56.5, 'learning_rate': 3.375594536430637e-05, 'epoch': 1.9654427645788337}\n",
      "{'loss': 0.6963, 'grad_norm': 9.4375, 'learning_rate': 3.369553852590994e-05, 'epoch': 1.9762419006479481}\n",
      "{'loss': 0.7617, 'grad_norm': 18.125, 'learning_rate': 3.363513168751352e-05, 'epoch': 1.9870410367170628}\n",
      "{'loss': 0.7987, 'grad_norm': 25.375, 'learning_rate': 3.357472484911709e-05, 'epoch': 1.997840172786177}\n",
      "{'eval_loss': 0.8316420316696167, 'eval_accuracy': 0.6984936831875608, 'eval_precision': 0.7168539383369219, 'eval_recall': 0.7150040910834048, 'eval_f1': 0.7087611381990372, 'eval_runtime': 15.637, 'eval_samples_per_second': 263.223, 'eval_steps_per_second': 65.806, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.6985 f1=0.7088 p=0.7169 r=0.7150\n",
      "{'loss': 0.8063, 'grad_norm': 24.625, 'learning_rate': 3.3514318010720674e-05, 'epoch': 2.0086393088552916}\n",
      "{'loss': 0.8914, 'grad_norm': 45.5, 'learning_rate': 3.3453911172324244e-05, 'epoch': 2.019438444924406}\n",
      "{'loss': 0.6578, 'grad_norm': 5.0625, 'learning_rate': 3.339350433392782e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 0.7914, 'grad_norm': 25.625, 'learning_rate': 3.333309749553139e-05, 'epoch': 2.041036717062635}\n",
      "{'loss': 0.7545, 'grad_norm': 42.75, 'learning_rate': 3.327269065713497e-05, 'epoch': 2.0518358531317493}\n",
      "{'loss': 0.6993, 'grad_norm': 32.5, 'learning_rate': 3.3212283818738544e-05, 'epoch': 2.062634989200864}\n",
      "{'loss': 0.7736, 'grad_norm': 8.6875, 'learning_rate': 3.315187698034212e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 0.6914, 'grad_norm': 26.25, 'learning_rate': 3.309147014194569e-05, 'epoch': 2.084233261339093}\n",
      "{'loss': 0.8255, 'grad_norm': 5.15625, 'learning_rate': 3.303106330354927e-05, 'epoch': 2.0950323974082075}\n",
      "{'loss': 0.7952, 'grad_norm': 28.125, 'learning_rate': 3.2970656465152845e-05, 'epoch': 2.1058315334773217}\n",
      "{'loss': 0.8092, 'grad_norm': 60.5, 'learning_rate': 3.291024962675642e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 0.7909, 'grad_norm': 26.25, 'learning_rate': 3.284984278836e-05, 'epoch': 2.127429805615551}\n",
      "{'loss': 0.769, 'grad_norm': 39.75, 'learning_rate': 3.278943594996357e-05, 'epoch': 2.138228941684665}\n",
      "{'loss': 0.6663, 'grad_norm': 32.0, 'learning_rate': 3.2729029111567145e-05, 'epoch': 2.14902807775378}\n",
      "{'loss': 0.8777, 'grad_norm': 76.5, 'learning_rate': 3.2668622273170715e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 0.7218, 'grad_norm': 28.75, 'learning_rate': 3.260821543477429e-05, 'epoch': 2.1706263498920086}\n",
      "{'loss': 0.763, 'grad_norm': 49.0, 'learning_rate': 3.254780859637787e-05, 'epoch': 2.1814254859611233}\n",
      "{'loss': 0.7633, 'grad_norm': 27.375, 'learning_rate': 3.2487401757981446e-05, 'epoch': 2.1922246220302375}\n",
      "{'loss': 0.7948, 'grad_norm': 55.5, 'learning_rate': 3.2426994919585016e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 0.8352, 'grad_norm': 61.25, 'learning_rate': 3.236658808118859e-05, 'epoch': 2.2138228941684663}\n",
      "{'loss': 0.7749, 'grad_norm': 37.0, 'learning_rate': 3.230618124279216e-05, 'epoch': 2.224622030237581}\n",
      "{'loss': 0.7434, 'grad_norm': 86.5, 'learning_rate': 3.2245774404395746e-05, 'epoch': 2.2354211663066956}\n",
      "{'loss': 0.7688, 'grad_norm': 31.375, 'learning_rate': 3.2185367565999316e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 0.7685, 'grad_norm': 26.5, 'learning_rate': 3.212496072760289e-05, 'epoch': 2.2570194384449245}\n",
      "{'loss': 0.8056, 'grad_norm': 37.0, 'learning_rate': 3.206455388920646e-05, 'epoch': 2.267818574514039}\n",
      "{'loss': 0.7495, 'grad_norm': 36.25, 'learning_rate': 3.200414705081004e-05, 'epoch': 2.2786177105831533}\n",
      "{'loss': 0.6939, 'grad_norm': 39.0, 'learning_rate': 3.1943740212413617e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 0.7418, 'grad_norm': 30.625, 'learning_rate': 3.188333337401719e-05, 'epoch': 2.300215982721382}\n",
      "{'loss': 0.716, 'grad_norm': 36.5, 'learning_rate': 3.1822926535620763e-05, 'epoch': 2.311015118790497}\n",
      "{'loss': 0.7102, 'grad_norm': 29.125, 'learning_rate': 3.176251969722434e-05, 'epoch': 2.3218142548596115}\n",
      "{'loss': 0.7796, 'grad_norm': 72.5, 'learning_rate': 3.170211285882791e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 0.7545, 'grad_norm': 9.5625, 'learning_rate': 3.1641706020431494e-05, 'epoch': 2.3434125269978403}\n",
      "{'loss': 0.6674, 'grad_norm': 22.375, 'learning_rate': 3.1581299182035064e-05, 'epoch': 2.3542116630669545}\n",
      "{'loss': 0.7061, 'grad_norm': 10.75, 'learning_rate': 3.152089234363864e-05, 'epoch': 2.365010799136069}\n",
      "{'loss': 0.7747, 'grad_norm': 66.0, 'learning_rate': 3.146048550524221e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 0.7913, 'grad_norm': 25.875, 'learning_rate': 3.140007866684579e-05, 'epoch': 2.386609071274298}\n",
      "{'loss': 0.7686, 'grad_norm': 21.0, 'learning_rate': 3.1339671828449364e-05, 'epoch': 2.3974082073434126}\n",
      "{'loss': 0.7993, 'grad_norm': 59.0, 'learning_rate': 3.127926499005294e-05, 'epoch': 2.408207343412527}\n",
      "{'loss': 0.8063, 'grad_norm': 74.5, 'learning_rate': 3.121885815165651e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 0.7034, 'grad_norm': 26.0, 'learning_rate': 3.115845131326009e-05, 'epoch': 2.429805615550756}\n",
      "{'loss': 0.7778, 'grad_norm': 42.0, 'learning_rate': 3.109804447486366e-05, 'epoch': 2.4406047516198703}\n",
      "{'loss': 0.8013, 'grad_norm': 38.5, 'learning_rate': 3.1037637636467235e-05, 'epoch': 2.451403887688985}\n",
      "{'loss': 0.7503, 'grad_norm': 83.0, 'learning_rate': 3.097723079807081e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 0.7775, 'grad_norm': 71.0, 'learning_rate': 3.091682395967439e-05, 'epoch': 2.473002159827214}\n",
      "{'loss': 0.7167, 'grad_norm': 95.5, 'learning_rate': 3.085641712127796e-05, 'epoch': 2.4838012958963285}\n",
      "{'loss': 0.757, 'grad_norm': 43.5, 'learning_rate': 3.0796010282881535e-05, 'epoch': 2.4946004319654427}\n",
      "{'loss': 0.746, 'grad_norm': 52.0, 'learning_rate': 3.073560344448511e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 0.7453, 'grad_norm': 17.375, 'learning_rate': 3.067519660608869e-05, 'epoch': 2.5161987041036715}\n",
      "{'loss': 0.7402, 'grad_norm': 53.75, 'learning_rate': 3.061478976769226e-05, 'epoch': 2.526997840172786}\n",
      "{'loss': 0.7396, 'grad_norm': 61.5, 'learning_rate': 3.0554382929295836e-05, 'epoch': 2.537796976241901}\n",
      "{'loss': 0.7879, 'grad_norm': 49.0, 'learning_rate': 3.049397609089941e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 0.7477, 'grad_norm': 102.5, 'learning_rate': 3.0433569252502986e-05, 'epoch': 2.5593952483801297}\n",
      "{'loss': 0.758, 'grad_norm': 38.75, 'learning_rate': 3.037316241410656e-05, 'epoch': 2.570194384449244}\n",
      "{'loss': 0.7197, 'grad_norm': 26.375, 'learning_rate': 3.0312755575710136e-05, 'epoch': 2.5809935205183585}\n",
      "{'loss': 0.8463, 'grad_norm': 59.5, 'learning_rate': 3.0252348737313706e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 0.7553, 'grad_norm': 52.0, 'learning_rate': 3.0191941898917287e-05, 'epoch': 2.6025917926565874}\n",
      "{'loss': 0.7544, 'grad_norm': 12.625, 'learning_rate': 3.0131535060520857e-05, 'epoch': 2.613390928725702}\n",
      "{'loss': 0.7396, 'grad_norm': 53.25, 'learning_rate': 3.0071128222124433e-05, 'epoch': 2.624190064794816}\n",
      "{'loss': 0.7047, 'grad_norm': 16.875, 'learning_rate': 3.0010721383728007e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 0.6836, 'grad_norm': 67.5, 'learning_rate': 2.9950314545331584e-05, 'epoch': 2.6457883369330455}\n",
      "{'loss': 0.7443, 'grad_norm': 67.5, 'learning_rate': 2.9889907706935157e-05, 'epoch': 2.6565874730021597}\n",
      "{'loss': 0.7691, 'grad_norm': 67.0, 'learning_rate': 2.9829500868538734e-05, 'epoch': 2.6673866090712743}\n",
      "{'loss': 0.7443, 'grad_norm': 48.25, 'learning_rate': 2.9769094030142307e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 0.7227, 'grad_norm': 33.0, 'learning_rate': 2.9708687191745884e-05, 'epoch': 2.688984881209503}\n",
      "{'loss': 0.7123, 'grad_norm': 44.5, 'learning_rate': 2.9648280353349454e-05, 'epoch': 2.699784017278618}\n",
      "{'loss': 0.7067, 'grad_norm': 19.375, 'learning_rate': 2.958787351495303e-05, 'epoch': 2.710583153347732}\n",
      "{'loss': 0.7471, 'grad_norm': 61.25, 'learning_rate': 2.9527466676556604e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 0.7578, 'grad_norm': 16.125, 'learning_rate': 2.946705983816018e-05, 'epoch': 2.732181425485961}\n",
      "{'loss': 0.7072, 'grad_norm': 23.75, 'learning_rate': 2.9406652999763755e-05, 'epoch': 2.7429805615550755}\n",
      "{'loss': 0.7465, 'grad_norm': 25.5, 'learning_rate': 2.934624616136733e-05, 'epoch': 2.75377969762419}\n",
      "{'loss': 0.7235, 'grad_norm': 52.25, 'learning_rate': 2.9285839322970905e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 0.7529, 'grad_norm': 34.5, 'learning_rate': 2.9225432484574482e-05, 'epoch': 2.775377969762419}\n",
      "{'loss': 0.7856, 'grad_norm': 83.5, 'learning_rate': 2.9165025646178055e-05, 'epoch': 2.786177105831533}\n",
      "{'loss': 0.6949, 'grad_norm': 16.0, 'learning_rate': 2.9104618807781632e-05, 'epoch': 2.796976241900648}\n",
      "{'loss': 0.8018, 'grad_norm': 61.75, 'learning_rate': 2.9044211969385202e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 0.8269, 'grad_norm': 8.5625, 'learning_rate': 2.898380513098878e-05, 'epoch': 2.818574514038877}\n",
      "{'loss': 0.7975, 'grad_norm': 57.25, 'learning_rate': 2.8923398292592352e-05, 'epoch': 2.8293736501079914}\n",
      "{'loss': 0.72, 'grad_norm': 50.0, 'learning_rate': 2.886299145419593e-05, 'epoch': 2.8401727861771056}\n",
      "{'loss': 0.7776, 'grad_norm': 68.5, 'learning_rate': 2.8802584615799502e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 0.7409, 'grad_norm': 47.5, 'learning_rate': 2.874217777740308e-05, 'epoch': 2.861771058315335}\n",
      "{'loss': 0.6748, 'grad_norm': 42.75, 'learning_rate': 2.8681770939006653e-05, 'epoch': 2.8725701943844495}\n",
      "{'loss': 0.8552, 'grad_norm': 37.5, 'learning_rate': 2.862136410061023e-05, 'epoch': 2.8833693304535637}\n",
      "{'loss': 0.7041, 'grad_norm': 30.25, 'learning_rate': 2.85609572622138e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 0.6589, 'grad_norm': 67.5, 'learning_rate': 2.850055042381738e-05, 'epoch': 2.9049676025917925}\n",
      "{'loss': 0.8107, 'grad_norm': 18.25, 'learning_rate': 2.844014358542095e-05, 'epoch': 2.915766738660907}\n",
      "{'loss': 0.7213, 'grad_norm': 44.5, 'learning_rate': 2.8379736747024527e-05, 'epoch': 2.926565874730022}\n",
      "{'loss': 0.7308, 'grad_norm': 54.5, 'learning_rate': 2.83193299086281e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 0.8192, 'grad_norm': 38.5, 'learning_rate': 2.8258923070231677e-05, 'epoch': 2.9481641468682507}\n",
      "{'loss': 0.7415, 'grad_norm': 47.25, 'learning_rate': 2.819851623183525e-05, 'epoch': 2.958963282937365}\n",
      "{'loss': 0.6734, 'grad_norm': 25.5, 'learning_rate': 2.8138109393438827e-05, 'epoch': 2.9697624190064795}\n",
      "{'loss': 0.7537, 'grad_norm': 17.875, 'learning_rate': 2.80777025550424e-05, 'epoch': 2.980561555075594}\n",
      "{'loss': 0.6732, 'grad_norm': 44.5, 'learning_rate': 2.8017295716645977e-05, 'epoch': 2.9913606911447084}\n",
      "{'eval_loss': 0.7967474460601807, 'eval_accuracy': 0.7137998056365403, 'eval_precision': 0.7271842755150175, 'eval_recall': 0.731199817819659, 'eval_f1': 0.7245806248910756, 'eval_runtime': 15.9858, 'eval_samples_per_second': 257.478, 'eval_steps_per_second': 64.369, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.7138 f1=0.7246 p=0.7272 r=0.7312\n",
      "{'loss': 0.7911, 'grad_norm': 60.0, 'learning_rate': 2.7956888878249547e-05, 'epoch': 3.002159827213823}\n",
      "{'loss': 0.7057, 'grad_norm': 38.5, 'learning_rate': 2.7896482039853128e-05, 'epoch': 3.012958963282937}\n",
      "{'loss': 0.7482, 'grad_norm': 60.5, 'learning_rate': 2.7836075201456698e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 0.7443, 'grad_norm': 60.5, 'learning_rate': 2.7775668363060274e-05, 'epoch': 3.0345572354211665}\n",
      "{'loss': 0.7154, 'grad_norm': 99.5, 'learning_rate': 2.7715261524663848e-05, 'epoch': 3.0453563714902807}\n",
      "{'loss': 0.6707, 'grad_norm': 41.25, 'learning_rate': 2.7654854686267425e-05, 'epoch': 3.0561555075593954}\n",
      "{'loss': 0.7415, 'grad_norm': 29.0, 'learning_rate': 2.7594447847870998e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 0.6769, 'grad_norm': 72.5, 'learning_rate': 2.7534041009474575e-05, 'epoch': 3.077753779697624}\n",
      "{'loss': 0.7369, 'grad_norm': 66.5, 'learning_rate': 2.747363417107815e-05, 'epoch': 3.088552915766739}\n",
      "{'loss': 0.7352, 'grad_norm': 65.5, 'learning_rate': 2.7413227332681725e-05, 'epoch': 3.099352051835853}\n",
      "{'loss': 0.7111, 'grad_norm': 21.5, 'learning_rate': 2.7352820494285302e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 0.7145, 'grad_norm': 38.5, 'learning_rate': 2.7292413655888872e-05, 'epoch': 3.120950323974082}\n",
      "{'loss': 0.7079, 'grad_norm': 65.5, 'learning_rate': 2.7232006817492452e-05, 'epoch': 3.1317494600431965}\n",
      "{'loss': 0.7588, 'grad_norm': 64.5, 'learning_rate': 2.7171599979096022e-05, 'epoch': 3.142548596112311}\n",
      "{'loss': 0.7555, 'grad_norm': 7.71875, 'learning_rate': 2.71111931406996e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 0.6957, 'grad_norm': 71.0, 'learning_rate': 2.7050786302303172e-05, 'epoch': 3.16414686825054}\n",
      "{'loss': 0.6773, 'grad_norm': 36.5, 'learning_rate': 2.699037946390675e-05, 'epoch': 3.1749460043196542}\n",
      "{'loss': 0.7824, 'grad_norm': 26.25, 'learning_rate': 2.6929972625510323e-05, 'epoch': 3.185745140388769}\n",
      "{'loss': 0.7242, 'grad_norm': 25.625, 'learning_rate': 2.68695657871139e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 0.7442, 'grad_norm': 18.75, 'learning_rate': 2.6809158948717473e-05, 'epoch': 3.2073434125269977}\n",
      "{'loss': 0.7527, 'grad_norm': 39.25, 'learning_rate': 2.674875211032105e-05, 'epoch': 3.2181425485961124}\n",
      "{'loss': 0.7682, 'grad_norm': 30.25, 'learning_rate': 2.668834527192462e-05, 'epoch': 3.2289416846652266}\n",
      "{'loss': 0.6667, 'grad_norm': 39.0, 'learning_rate': 2.66279384335282e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 0.7553, 'grad_norm': 37.75, 'learning_rate': 2.656753159513177e-05, 'epoch': 3.250539956803456}\n",
      "{'loss': 0.6846, 'grad_norm': 15.375, 'learning_rate': 2.6507124756735347e-05, 'epoch': 3.26133909287257}\n",
      "{'loss': 0.7058, 'grad_norm': 60.0, 'learning_rate': 2.644671791833892e-05, 'epoch': 3.2721382289416847}\n",
      "{'loss': 0.7906, 'grad_norm': 61.5, 'learning_rate': 2.6386311079942497e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 0.7569, 'grad_norm': 6.4375, 'learning_rate': 2.632590424154607e-05, 'epoch': 3.2937365010799136}\n",
      "{'loss': 0.7681, 'grad_norm': 19.0, 'learning_rate': 2.6265497403149647e-05, 'epoch': 3.304535637149028}\n",
      "{'loss': 0.7302, 'grad_norm': 33.25, 'learning_rate': 2.620509056475322e-05, 'epoch': 3.3153347732181424}\n",
      "{'loss': 0.7163, 'grad_norm': 18.625, 'learning_rate': 2.6144683726356798e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 0.7533, 'grad_norm': 35.0, 'learning_rate': 2.6084276887960368e-05, 'epoch': 3.3369330453563713}\n",
      "{'loss': 0.7581, 'grad_norm': 74.0, 'learning_rate': 2.6023870049563944e-05, 'epoch': 3.347732181425486}\n",
      "{'loss': 0.7377, 'grad_norm': 5.1875, 'learning_rate': 2.5963463211167518e-05, 'epoch': 3.3585313174946005}\n",
      "{'loss': 0.7351, 'grad_norm': 34.75, 'learning_rate': 2.5903056372771095e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 0.699, 'grad_norm': 57.75, 'learning_rate': 2.5842649534374668e-05, 'epoch': 3.3801295896328294}\n",
      "{'loss': 0.673, 'grad_norm': 33.5, 'learning_rate': 2.5782242695978245e-05, 'epoch': 3.390928725701944}\n",
      "{'loss': 0.745, 'grad_norm': 40.0, 'learning_rate': 2.572183585758182e-05, 'epoch': 3.4017278617710582}\n",
      "{'loss': 0.8041, 'grad_norm': 108.5, 'learning_rate': 2.5661429019185395e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 0.6717, 'grad_norm': 48.75, 'learning_rate': 2.560102218078897e-05, 'epoch': 3.423326133909287}\n",
      "{'loss': 0.6677, 'grad_norm': 20.625, 'learning_rate': 2.5540615342392545e-05, 'epoch': 3.4341252699784017}\n",
      "{'loss': 0.8402, 'grad_norm': 55.0, 'learning_rate': 2.5480208503996115e-05, 'epoch': 3.4449244060475164}\n",
      "{'loss': 0.7364, 'grad_norm': 67.0, 'learning_rate': 2.5419801665599692e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 0.7835, 'grad_norm': 25.75, 'learning_rate': 2.5359394827203266e-05, 'epoch': 3.466522678185745}\n",
      "{'loss': 0.7318, 'grad_norm': 36.0, 'learning_rate': 2.5298987988806842e-05, 'epoch': 3.4773218142548594}\n",
      "{'loss': 0.7342, 'grad_norm': 100.5, 'learning_rate': 2.5238581150410416e-05, 'epoch': 3.488120950323974}\n",
      "{'loss': 0.7416, 'grad_norm': 34.75, 'learning_rate': 2.5178174312013993e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 0.7734, 'grad_norm': 20.875, 'learning_rate': 2.5117767473617566e-05, 'epoch': 3.509719222462203}\n",
      "{'loss': 0.7936, 'grad_norm': 34.0, 'learning_rate': 2.5057360635221143e-05, 'epoch': 3.5205183585313176}\n",
      "{'loss': 0.632, 'grad_norm': 49.5, 'learning_rate': 2.4996953796824716e-05, 'epoch': 3.531317494600432}\n",
      "{'loss': 0.7569, 'grad_norm': 59.0, 'learning_rate': 2.4936546958428293e-05, 'epoch': 3.5421166306695464}\n",
      "{'loss': 0.6931, 'grad_norm': 52.75, 'learning_rate': 2.4876140120031863e-05, 'epoch': 3.552915766738661}\n",
      "{'loss': 0.8177, 'grad_norm': 52.25, 'learning_rate': 2.481573328163544e-05, 'epoch': 3.5637149028077753}\n",
      "{'loss': 0.6589, 'grad_norm': 45.5, 'learning_rate': 2.4755326443239013e-05, 'epoch': 3.57451403887689}\n",
      "{'loss': 0.7437, 'grad_norm': 59.0, 'learning_rate': 2.469491960484259e-05, 'epoch': 3.5853131749460045}\n",
      "{'loss': 0.699, 'grad_norm': 52.0, 'learning_rate': 2.4634512766446164e-05, 'epoch': 3.5961123110151187}\n",
      "{'loss': 0.7087, 'grad_norm': 23.625, 'learning_rate': 2.457410592804974e-05, 'epoch': 3.6069114470842334}\n",
      "{'loss': 0.7303, 'grad_norm': 35.25, 'learning_rate': 2.4513699089653314e-05, 'epoch': 3.6177105831533476}\n",
      "{'loss': 0.7429, 'grad_norm': 26.875, 'learning_rate': 2.445329225125689e-05, 'epoch': 3.6285097192224622}\n",
      "{'loss': 0.7764, 'grad_norm': 91.0, 'learning_rate': 2.439288541286046e-05, 'epoch': 3.639308855291577}\n",
      "{'loss': 0.6831, 'grad_norm': 10.125, 'learning_rate': 2.433247857446404e-05, 'epoch': 3.650107991360691}\n",
      "{'loss': 0.6803, 'grad_norm': 34.0, 'learning_rate': 2.427207173606761e-05, 'epoch': 3.6609071274298057}\n",
      "{'loss': 0.7504, 'grad_norm': 71.5, 'learning_rate': 2.4211664897671188e-05, 'epoch': 3.67170626349892}\n",
      "{'loss': 0.7694, 'grad_norm': 52.0, 'learning_rate': 2.415125805927476e-05, 'epoch': 3.6825053995680346}\n",
      "{'loss': 0.6862, 'grad_norm': 48.75, 'learning_rate': 2.4090851220878338e-05, 'epoch': 3.693304535637149}\n",
      "{'loss': 0.7175, 'grad_norm': 38.0, 'learning_rate': 2.403044438248191e-05, 'epoch': 3.7041036717062634}\n",
      "{'loss': 0.684, 'grad_norm': 85.5, 'learning_rate': 2.397003754408549e-05, 'epoch': 3.714902807775378}\n",
      "{'loss': 0.8103, 'grad_norm': 12.75, 'learning_rate': 2.3909630705689062e-05, 'epoch': 3.7257019438444923}\n",
      "{'loss': 0.706, 'grad_norm': 85.5, 'learning_rate': 2.384922386729264e-05, 'epoch': 3.736501079913607}\n",
      "{'loss': 0.7657, 'grad_norm': 64.5, 'learning_rate': 2.378881702889621e-05, 'epoch': 3.7473002159827216}\n",
      "{'loss': 0.7245, 'grad_norm': 109.5, 'learning_rate': 2.3728410190499785e-05, 'epoch': 3.7580993520518358}\n",
      "{'loss': 0.6691, 'grad_norm': 33.75, 'learning_rate': 2.366800335210336e-05, 'epoch': 3.7688984881209504}\n",
      "{'loss': 0.7116, 'grad_norm': 88.5, 'learning_rate': 2.3607596513706936e-05, 'epoch': 3.7796976241900646}\n",
      "{'loss': 0.7912, 'grad_norm': 47.0, 'learning_rate': 2.354718967531051e-05, 'epoch': 3.7904967602591793}\n",
      "{'loss': 0.7296, 'grad_norm': 14.4375, 'learning_rate': 2.3486782836914086e-05, 'epoch': 3.801295896328294}\n",
      "{'loss': 0.7715, 'grad_norm': 109.5, 'learning_rate': 2.342637599851766e-05, 'epoch': 3.812095032397408}\n",
      "{'loss': 0.756, 'grad_norm': 68.5, 'learning_rate': 2.3365969160121236e-05, 'epoch': 3.8228941684665227}\n",
      "{'loss': 0.7061, 'grad_norm': 89.5, 'learning_rate': 2.330556232172481e-05, 'epoch': 3.833693304535637}\n",
      "{'loss': 0.732, 'grad_norm': 40.0, 'learning_rate': 2.3245155483328386e-05, 'epoch': 3.8444924406047516}\n",
      "{'loss': 0.6915, 'grad_norm': 85.5, 'learning_rate': 2.3184748644931956e-05, 'epoch': 3.8552915766738662}\n",
      "{'loss': 0.6459, 'grad_norm': 49.25, 'learning_rate': 2.3124341806535533e-05, 'epoch': 3.8660907127429804}\n",
      "{'loss': 0.6657, 'grad_norm': 76.0, 'learning_rate': 2.3063934968139107e-05, 'epoch': 3.876889848812095}\n",
      "{'loss': 0.6966, 'grad_norm': 6.75, 'learning_rate': 2.3003528129742684e-05, 'epoch': 3.8876889848812093}\n",
      "{'loss': 0.8151, 'grad_norm': 86.5, 'learning_rate': 2.2943121291346257e-05, 'epoch': 3.898488120950324}\n",
      "{'loss': 0.725, 'grad_norm': 47.25, 'learning_rate': 2.2882714452949834e-05, 'epoch': 3.9092872570194386}\n",
      "{'loss': 0.7422, 'grad_norm': 97.5, 'learning_rate': 2.2822307614553407e-05, 'epoch': 3.920086393088553}\n",
      "{'loss': 0.7062, 'grad_norm': 43.25, 'learning_rate': 2.2761900776156984e-05, 'epoch': 3.9308855291576674}\n",
      "{'loss': 0.6891, 'grad_norm': 36.0, 'learning_rate': 2.2701493937760557e-05, 'epoch': 3.9416846652267816}\n",
      "{'loss': 0.7314, 'grad_norm': 85.0, 'learning_rate': 2.2641087099364134e-05, 'epoch': 3.9524838012958963}\n",
      "{'loss': 0.7265, 'grad_norm': 41.75, 'learning_rate': 2.2580680260967704e-05, 'epoch': 3.963282937365011}\n",
      "{'loss': 0.6368, 'grad_norm': 18.0, 'learning_rate': 2.252027342257128e-05, 'epoch': 3.974082073434125}\n",
      "{'loss': 0.6631, 'grad_norm': 54.0, 'learning_rate': 2.2459866584174855e-05, 'epoch': 3.9848812095032398}\n",
      "{'loss': 0.6822, 'grad_norm': 43.75, 'learning_rate': 2.239945974577843e-05, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 0.8075549602508545, 'eval_accuracy': 0.7152575315840622, 'eval_precision': 0.7254159361118734, 'eval_recall': 0.7373200477867922, 'eval_f1': 0.7251415930131341, 'eval_runtime': 15.6989, 'eval_samples_per_second': 262.184, 'eval_steps_per_second': 65.546, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.7153 f1=0.7251 p=0.7254 r=0.7373\n",
      "{'loss': 0.7603, 'grad_norm': 37.0, 'learning_rate': 2.2339052907382005e-05, 'epoch': 4.006479481641469}\n",
      "{'loss': 0.6138, 'grad_norm': 21.75, 'learning_rate': 2.227864606898558e-05, 'epoch': 4.017278617710583}\n",
      "{'loss': 0.7437, 'grad_norm': 5.71875, 'learning_rate': 2.2218239230589155e-05, 'epoch': 4.028077753779698}\n",
      "{'loss': 0.6894, 'grad_norm': 109.5, 'learning_rate': 2.2157832392192732e-05, 'epoch': 4.038876889848812}\n",
      "{'loss': 0.7546, 'grad_norm': 35.5, 'learning_rate': 2.2097425553796302e-05, 'epoch': 4.049676025917926}\n",
      "{'loss': 0.7777, 'grad_norm': 78.5, 'learning_rate': 2.2037018715399882e-05, 'epoch': 4.060475161987041}\n",
      "{'loss': 0.8037, 'grad_norm': 60.5, 'learning_rate': 2.1976611877003452e-05, 'epoch': 4.071274298056156}\n",
      "{'loss': 0.8185, 'grad_norm': 47.0, 'learning_rate': 2.191620503860703e-05, 'epoch': 4.08207343412527}\n",
      "{'loss': 0.7713, 'grad_norm': 6.78125, 'learning_rate': 2.1855798200210606e-05, 'epoch': 4.092872570194385}\n",
      "{'loss': 0.7366, 'grad_norm': 69.0, 'learning_rate': 2.179539136181418e-05, 'epoch': 4.103671706263499}\n",
      "{'loss': 0.7969, 'grad_norm': 29.25, 'learning_rate': 2.1734984523417756e-05, 'epoch': 4.114470842332613}\n",
      "{'loss': 0.7202, 'grad_norm': 58.25, 'learning_rate': 2.167457768502133e-05, 'epoch': 4.125269978401728}\n",
      "{'loss': 0.7187, 'grad_norm': 10.0, 'learning_rate': 2.1614170846624906e-05, 'epoch': 4.136069114470843}\n",
      "{'loss': 0.6452, 'grad_norm': 29.125, 'learning_rate': 2.155376400822848e-05, 'epoch': 4.146868250539957}\n",
      "{'loss': 0.6906, 'grad_norm': 47.75, 'learning_rate': 2.1493357169832056e-05, 'epoch': 4.157667386609071}\n",
      "{'loss': 0.6754, 'grad_norm': 80.5, 'learning_rate': 2.143295033143563e-05, 'epoch': 4.168466522678186}\n",
      "{'loss': 0.7284, 'grad_norm': 86.5, 'learning_rate': 2.1372543493039207e-05, 'epoch': 4.1792656587473}\n",
      "{'loss': 0.7345, 'grad_norm': 48.5, 'learning_rate': 2.1312136654642777e-05, 'epoch': 4.190064794816415}\n",
      "{'loss': 0.8074, 'grad_norm': 41.5, 'learning_rate': 2.1251729816246354e-05, 'epoch': 4.20086393088553}\n",
      "{'loss': 0.7302, 'grad_norm': 10.375, 'learning_rate': 2.1191322977849927e-05, 'epoch': 4.211663066954643}\n",
      "{'loss': 0.6885, 'grad_norm': 39.5, 'learning_rate': 2.1130916139453504e-05, 'epoch': 4.222462203023758}\n",
      "{'loss': 0.7805, 'grad_norm': 77.0, 'learning_rate': 2.1070509301057077e-05, 'epoch': 4.233261339092873}\n",
      "{'loss': 0.6824, 'grad_norm': 67.5, 'learning_rate': 2.101010246266065e-05, 'epoch': 4.244060475161987}\n",
      "{'loss': 0.7308, 'grad_norm': 25.5, 'learning_rate': 2.0949695624264227e-05, 'epoch': 4.254859611231102}\n",
      "{'loss': 0.6003, 'grad_norm': 43.75, 'learning_rate': 2.08892887858678e-05, 'epoch': 4.265658747300216}\n",
      "{'loss': 0.7073, 'grad_norm': 33.75, 'learning_rate': 2.0828881947471374e-05, 'epoch': 4.27645788336933}\n",
      "{'loss': 0.6753, 'grad_norm': 18.625, 'learning_rate': 2.076847510907495e-05, 'epoch': 4.287257019438445}\n",
      "{'loss': 0.6956, 'grad_norm': 64.5, 'learning_rate': 2.0708068270678525e-05, 'epoch': 4.29805615550756}\n",
      "{'loss': 0.7412, 'grad_norm': 60.75, 'learning_rate': 2.06476614322821e-05, 'epoch': 4.308855291576674}\n",
      "{'loss': 0.6699, 'grad_norm': 8.0, 'learning_rate': 2.0587254593885675e-05, 'epoch': 4.319654427645788}\n",
      "{'loss': 0.7159, 'grad_norm': 33.5, 'learning_rate': 2.0526847755489248e-05, 'epoch': 4.330453563714903}\n",
      "{'loss': 0.679, 'grad_norm': 5.21875, 'learning_rate': 2.0466440917092825e-05, 'epoch': 4.341252699784017}\n",
      "{'loss': 0.6441, 'grad_norm': 81.5, 'learning_rate': 2.04060340786964e-05, 'epoch': 4.352051835853132}\n",
      "{'loss': 0.764, 'grad_norm': 46.75, 'learning_rate': 2.0345627240299975e-05, 'epoch': 4.362850971922247}\n",
      "{'loss': 0.7817, 'grad_norm': 51.25, 'learning_rate': 2.028522040190355e-05, 'epoch': 4.37365010799136}\n",
      "{'loss': 0.7927, 'grad_norm': 27.75, 'learning_rate': 2.0224813563507122e-05, 'epoch': 4.384449244060475}\n",
      "{'loss': 0.7275, 'grad_norm': 20.375, 'learning_rate': 2.01644067251107e-05, 'epoch': 4.39524838012959}\n",
      "{'loss': 0.7462, 'grad_norm': 21.625, 'learning_rate': 2.0103999886714272e-05, 'epoch': 4.406047516198704}\n",
      "{'loss': 0.7429, 'grad_norm': 37.25, 'learning_rate': 2.004359304831785e-05, 'epoch': 4.416846652267819}\n",
      "{'loss': 0.7583, 'grad_norm': 4.0625, 'learning_rate': 1.9983186209921423e-05, 'epoch': 4.427645788336933}\n",
      "{'loss': 0.7454, 'grad_norm': 88.0, 'learning_rate': 1.9922779371524996e-05, 'epoch': 4.438444924406047}\n",
      "{'loss': 0.6528, 'grad_norm': 74.5, 'learning_rate': 1.9862372533128573e-05, 'epoch': 4.449244060475162}\n",
      "{'loss': 0.7047, 'grad_norm': 19.375, 'learning_rate': 1.9801965694732146e-05, 'epoch': 4.460043196544277}\n",
      "{'loss': 0.6988, 'grad_norm': 58.0, 'learning_rate': 1.9741558856335723e-05, 'epoch': 4.470842332613391}\n",
      "{'loss': 0.6502, 'grad_norm': 47.25, 'learning_rate': 1.9681152017939296e-05, 'epoch': 4.481641468682505}\n",
      "{'loss': 0.7258, 'grad_norm': 54.25, 'learning_rate': 1.962074517954287e-05, 'epoch': 4.49244060475162}\n",
      "{'loss': 0.6858, 'grad_norm': 42.75, 'learning_rate': 1.9560338341146447e-05, 'epoch': 4.503239740820734}\n",
      "{'loss': 0.7512, 'grad_norm': 3.125, 'learning_rate': 1.949993150275002e-05, 'epoch': 4.514038876889849}\n",
      "{'loss': 0.7069, 'grad_norm': 13.375, 'learning_rate': 1.9439524664353597e-05, 'epoch': 4.524838012958964}\n",
      "{'loss': 0.6311, 'grad_norm': 76.5, 'learning_rate': 1.937911782595717e-05, 'epoch': 4.535637149028078}\n",
      "{'loss': 0.5684, 'grad_norm': 45.75, 'learning_rate': 1.9318710987560744e-05, 'epoch': 4.546436285097192}\n",
      "{'loss': 0.706, 'grad_norm': 50.5, 'learning_rate': 1.925830414916432e-05, 'epoch': 4.557235421166307}\n",
      "{'loss': 0.7351, 'grad_norm': 37.25, 'learning_rate': 1.9197897310767894e-05, 'epoch': 4.568034557235421}\n",
      "{'loss': 0.6642, 'grad_norm': 24.625, 'learning_rate': 1.913749047237147e-05, 'epoch': 4.578833693304536}\n",
      "{'loss': 0.6822, 'grad_norm': 18.5, 'learning_rate': 1.9077083633975048e-05, 'epoch': 4.589632829373651}\n",
      "{'loss': 0.7463, 'grad_norm': 39.5, 'learning_rate': 1.901667679557862e-05, 'epoch': 4.600431965442764}\n",
      "{'loss': 0.6633, 'grad_norm': 34.0, 'learning_rate': 1.8956269957182195e-05, 'epoch': 4.611231101511879}\n",
      "{'loss': 0.732, 'grad_norm': 119.5, 'learning_rate': 1.889586311878577e-05, 'epoch': 4.622030237580994}\n",
      "{'loss': 0.6685, 'grad_norm': 13.75, 'learning_rate': 1.8835456280389345e-05, 'epoch': 4.632829373650108}\n",
      "{'loss': 0.7188, 'grad_norm': 29.125, 'learning_rate': 1.877504944199292e-05, 'epoch': 4.643628509719223}\n",
      "{'loss': 0.7906, 'grad_norm': 44.75, 'learning_rate': 1.8714642603596495e-05, 'epoch': 4.654427645788337}\n",
      "{'loss': 0.6398, 'grad_norm': 83.0, 'learning_rate': 1.865423576520007e-05, 'epoch': 4.665226781857451}\n",
      "{'loss': 0.7055, 'grad_norm': 62.5, 'learning_rate': 1.8593828926803645e-05, 'epoch': 4.676025917926566}\n",
      "{'loss': 0.7332, 'grad_norm': 40.25, 'learning_rate': 1.853342208840722e-05, 'epoch': 4.686825053995681}\n",
      "{'loss': 0.7152, 'grad_norm': 24.0, 'learning_rate': 1.8473015250010795e-05, 'epoch': 4.697624190064795}\n",
      "{'loss': 0.7751, 'grad_norm': 49.5, 'learning_rate': 1.841260841161437e-05, 'epoch': 4.708423326133909}\n",
      "{'loss': 0.7693, 'grad_norm': 73.5, 'learning_rate': 1.8352201573217942e-05, 'epoch': 4.719222462203024}\n",
      "{'loss': 0.6681, 'grad_norm': 3.90625, 'learning_rate': 1.829179473482152e-05, 'epoch': 4.730021598272138}\n",
      "{'loss': 0.6818, 'grad_norm': 53.75, 'learning_rate': 1.8231387896425093e-05, 'epoch': 4.740820734341253}\n",
      "{'loss': 0.7541, 'grad_norm': 18.5, 'learning_rate': 1.817098105802867e-05, 'epoch': 4.751619870410368}\n",
      "{'loss': 0.7464, 'grad_norm': 48.0, 'learning_rate': 1.8110574219632243e-05, 'epoch': 4.762419006479481}\n",
      "{'loss': 0.7655, 'grad_norm': 52.0, 'learning_rate': 1.8050167381235816e-05, 'epoch': 4.773218142548596}\n",
      "{'loss': 0.7252, 'grad_norm': 9.0, 'learning_rate': 1.7989760542839393e-05, 'epoch': 4.784017278617711}\n",
      "{'loss': 0.6983, 'grad_norm': 32.5, 'learning_rate': 1.7929353704442966e-05, 'epoch': 4.794816414686825}\n",
      "{'loss': 0.7563, 'grad_norm': 13.1875, 'learning_rate': 1.7868946866046543e-05, 'epoch': 4.80561555075594}\n",
      "{'loss': 0.74, 'grad_norm': 42.5, 'learning_rate': 1.7808540027650117e-05, 'epoch': 4.816414686825054}\n",
      "{'loss': 0.6862, 'grad_norm': 41.0, 'learning_rate': 1.774813318925369e-05, 'epoch': 4.827213822894168}\n",
      "{'loss': 0.7639, 'grad_norm': 14.375, 'learning_rate': 1.7687726350857267e-05, 'epoch': 4.838012958963283}\n",
      "{'loss': 0.6721, 'grad_norm': 78.0, 'learning_rate': 1.762731951246084e-05, 'epoch': 4.848812095032398}\n",
      "{'loss': 0.7911, 'grad_norm': 25.0, 'learning_rate': 1.7566912674064417e-05, 'epoch': 4.859611231101512}\n",
      "{'loss': 0.6222, 'grad_norm': 18.625, 'learning_rate': 1.750650583566799e-05, 'epoch': 4.870410367170626}\n",
      "{'loss': 0.7331, 'grad_norm': 5.75, 'learning_rate': 1.7446098997271564e-05, 'epoch': 4.881209503239741}\n",
      "{'loss': 0.6661, 'grad_norm': 34.0, 'learning_rate': 1.738569215887514e-05, 'epoch': 4.892008639308855}\n",
      "{'loss': 0.7489, 'grad_norm': 23.375, 'learning_rate': 1.7325285320478714e-05, 'epoch': 4.90280777537797}\n",
      "{'loss': 0.7319, 'grad_norm': 59.0, 'learning_rate': 1.7264878482082288e-05, 'epoch': 4.913606911447085}\n",
      "{'loss': 0.6747, 'grad_norm': 51.25, 'learning_rate': 1.7204471643685865e-05, 'epoch': 4.924406047516198}\n",
      "{'loss': 0.7016, 'grad_norm': 53.0, 'learning_rate': 1.7144064805289438e-05, 'epoch': 4.935205183585313}\n",
      "{'loss': 0.7126, 'grad_norm': 12.5, 'learning_rate': 1.7083657966893015e-05, 'epoch': 4.946004319654428}\n",
      "{'loss': 0.6946, 'grad_norm': 56.0, 'learning_rate': 1.7023251128496588e-05, 'epoch': 4.956803455723542}\n",
      "{'loss': 0.8063, 'grad_norm': 14.6875, 'learning_rate': 1.696284429010016e-05, 'epoch': 4.967602591792657}\n",
      "{'loss': 0.7749, 'grad_norm': 18.0, 'learning_rate': 1.690243745170374e-05, 'epoch': 4.978401727861771}\n",
      "{'loss': 0.7377, 'grad_norm': 102.5, 'learning_rate': 1.6842030613307312e-05, 'epoch': 4.989200863930885}\n",
      "{'loss': 0.7439, 'grad_norm': 70.5, 'learning_rate': 1.678162377491089e-05, 'epoch': 5.0}\n",
      "{'eval_loss': 0.7875571846961975, 'eval_accuracy': 0.7235179786200194, 'eval_precision': 0.7340967031965364, 'eval_recall': 0.7423102014188776, 'eval_f1': 0.7333747370018296, 'eval_runtime': 15.9245, 'eval_samples_per_second': 258.47, 'eval_steps_per_second': 64.617, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.7235 f1=0.7334 p=0.7341 r=0.7423\n",
      "[e5 b100/9260] loss=0.6743 lr=1.67e-05\n",
      "{'loss': 0.6743, 'grad_norm': 57.5, 'learning_rate': 1.6721216936514462e-05, 'epoch': 5.010799136069115}\n",
      "[e5 b200/9260] loss=0.7703 lr=1.67e-05\n",
      "{'loss': 0.7703, 'grad_norm': 61.25, 'learning_rate': 1.6660810098118036e-05, 'epoch': 5.021598272138229}\n",
      "[e5 b300/9260] loss=0.7095 lr=1.66e-05\n",
      "{'loss': 0.7095, 'grad_norm': 53.5, 'learning_rate': 1.6600403259721612e-05, 'epoch': 5.032397408207343}\n",
      "[e5 b400/9260] loss=0.7469 lr=1.65e-05\n",
      "{'loss': 0.7469, 'grad_norm': 111.5, 'learning_rate': 1.6539996421325186e-05, 'epoch': 5.043196544276458}\n",
      "[e5 b500/9260] loss=0.7120 lr=1.65e-05\n",
      "{'loss': 0.712, 'grad_norm': 13.125, 'learning_rate': 1.6479589582928763e-05, 'epoch': 5.053995680345572}\n",
      "[e5 b600/9260] loss=0.7915 lr=1.64e-05\n",
      "{'loss': 0.7915, 'grad_norm': 33.75, 'learning_rate': 1.6419182744532336e-05, 'epoch': 5.064794816414687}\n",
      "[e5 b700/9260] loss=0.6649 lr=1.64e-05\n",
      "{'loss': 0.6649, 'grad_norm': 46.0, 'learning_rate': 1.635877590613591e-05, 'epoch': 5.075593952483802}\n",
      "[e5 b800/9260] loss=0.7226 lr=1.63e-05\n",
      "{'loss': 0.7226, 'grad_norm': 48.75, 'learning_rate': 1.6298369067739486e-05, 'epoch': 5.086393088552915}\n",
      "[e5 b900/9260] loss=0.6733 lr=1.62e-05\n",
      "{'loss': 0.6733, 'grad_norm': 85.5, 'learning_rate': 1.623796222934306e-05, 'epoch': 5.09719222462203}\n",
      "[e5 b1000/9260] loss=0.7758 lr=1.62e-05\n",
      "{'loss': 0.7758, 'grad_norm': 54.5, 'learning_rate': 1.6177555390946636e-05, 'epoch': 5.107991360691145}\n",
      "[e5 b1100/9260] loss=0.6831 lr=1.61e-05\n",
      "{'loss': 0.6831, 'grad_norm': 74.5, 'learning_rate': 1.611714855255021e-05, 'epoch': 5.118790496760259}\n",
      "[e5 b1200/9260] loss=0.6697 lr=1.61e-05\n",
      "{'loss': 0.6697, 'grad_norm': 66.5, 'learning_rate': 1.6056741714153783e-05, 'epoch': 5.129589632829374}\n",
      "[e5 b1300/9260] loss=0.7250 lr=1.60e-05\n",
      "{'loss': 0.725, 'grad_norm': 19.625, 'learning_rate': 1.599633487575736e-05, 'epoch': 5.140388768898488}\n",
      "[e5 b1400/9260] loss=0.7316 lr=1.59e-05\n",
      "{'loss': 0.7316, 'grad_norm': 23.875, 'learning_rate': 1.5935928037360934e-05, 'epoch': 5.151187904967602}\n",
      "[e5 b1500/9260] loss=0.7044 lr=1.59e-05\n",
      "{'loss': 0.7044, 'grad_norm': 58.75, 'learning_rate': 1.587552119896451e-05, 'epoch': 5.161987041036717}\n",
      "[e5 b1600/9260] loss=0.8317 lr=1.58e-05\n",
      "{'loss': 0.8317, 'grad_norm': 109.5, 'learning_rate': 1.5815114360568084e-05, 'epoch': 5.172786177105832}\n",
      "[e5 b1700/9260] loss=0.6833 lr=1.58e-05\n",
      "{'loss': 0.6833, 'grad_norm': 3.015625, 'learning_rate': 1.5754707522171657e-05, 'epoch': 5.183585313174946}\n",
      "[e5 b1800/9260] loss=0.7051 lr=1.57e-05\n",
      "{'loss': 0.7051, 'grad_norm': 36.25, 'learning_rate': 1.5694300683775234e-05, 'epoch': 5.19438444924406}\n",
      "[e5 b1900/9260] loss=0.6746 lr=1.56e-05\n",
      "{'loss': 0.6746, 'grad_norm': 44.5, 'learning_rate': 1.5633893845378807e-05, 'epoch': 5.205183585313175}\n",
      "[e5 b2000/9260] loss=0.7826 lr=1.56e-05\n",
      "{'loss': 0.7826, 'grad_norm': 28.0, 'learning_rate': 1.5573487006982384e-05, 'epoch': 5.215982721382289}\n",
      "[e5 b2100/9260] loss=0.6076 lr=1.55e-05\n",
      "{'loss': 0.6076, 'grad_norm': 14.0, 'learning_rate': 1.5513080168585958e-05, 'epoch': 5.226781857451404}\n",
      "[e5 b2200/9260] loss=0.6971 lr=1.55e-05\n",
      "{'loss': 0.6971, 'grad_norm': 33.5, 'learning_rate': 1.545267333018953e-05, 'epoch': 5.237580993520519}\n",
      "[e5 b2300/9260] loss=0.7216 lr=1.54e-05\n",
      "{'loss': 0.7216, 'grad_norm': 13.1875, 'learning_rate': 1.5392266491793108e-05, 'epoch': 5.248380129589632}\n",
      "[e5 b2400/9260] loss=0.6636 lr=1.53e-05\n",
      "{'loss': 0.6636, 'grad_norm': 6.3125, 'learning_rate': 1.533185965339668e-05, 'epoch': 5.259179265658747}\n",
      "[e5 b2500/9260] loss=0.7441 lr=1.53e-05\n",
      "{'loss': 0.7441, 'grad_norm': 88.5, 'learning_rate': 1.5271452815000258e-05, 'epoch': 5.269978401727862}\n",
      "[e5 b2600/9260] loss=0.7852 lr=1.52e-05\n",
      "{'loss': 0.7852, 'grad_norm': 99.5, 'learning_rate': 1.5211045976603832e-05, 'epoch': 5.280777537796976}\n",
      "[e5 b2700/9260] loss=0.6230 lr=1.52e-05\n",
      "{'loss': 0.623, 'grad_norm': 234.0, 'learning_rate': 1.5150639138207407e-05, 'epoch': 5.291576673866091}\n",
      "[e5 b2800/9260] loss=0.6068 lr=1.51e-05\n",
      "{'loss': 0.6068, 'grad_norm': 97.5, 'learning_rate': 1.509023229981098e-05, 'epoch': 5.302375809935205}\n",
      "[e5 b2900/9260] loss=0.6971 lr=1.50e-05\n",
      "{'loss': 0.6971, 'grad_norm': 26.625, 'learning_rate': 1.5029825461414555e-05, 'epoch': 5.313174946004319}\n",
      "[e5 b3000/9260] loss=0.7646 lr=1.50e-05\n",
      "{'loss': 0.7646, 'grad_norm': 20.875, 'learning_rate': 1.496941862301813e-05, 'epoch': 5.323974082073434}\n",
      "[e5 b3100/9260] loss=0.6933 lr=1.49e-05\n",
      "{'loss': 0.6933, 'grad_norm': 80.0, 'learning_rate': 1.4909011784621706e-05, 'epoch': 5.334773218142549}\n",
      "[e5 b3200/9260] loss=0.7144 lr=1.48e-05\n",
      "{'loss': 0.7144, 'grad_norm': 38.0, 'learning_rate': 1.484860494622528e-05, 'epoch': 5.345572354211663}\n",
      "[e5 b3300/9260] loss=0.7720 lr=1.48e-05\n",
      "{'loss': 0.772, 'grad_norm': 85.5, 'learning_rate': 1.4788198107828854e-05, 'epoch': 5.356371490280777}\n",
      "[e5 b3400/9260] loss=0.7264 lr=1.47e-05\n",
      "{'loss': 0.7264, 'grad_norm': 82.0, 'learning_rate': 1.472779126943243e-05, 'epoch': 5.367170626349892}\n",
      "[e5 b3500/9260] loss=0.7027 lr=1.47e-05\n",
      "{'loss': 0.7027, 'grad_norm': 54.0, 'learning_rate': 1.4667384431036004e-05, 'epoch': 5.377969762419006}\n",
      "[e5 b3600/9260] loss=0.6451 lr=1.46e-05\n",
      "{'loss': 0.6451, 'grad_norm': 4.65625, 'learning_rate': 1.460697759263958e-05, 'epoch': 5.388768898488121}\n",
      "[e5 b3700/9260] loss=0.7030 lr=1.45e-05\n",
      "{'loss': 0.703, 'grad_norm': 51.5, 'learning_rate': 1.4546570754243155e-05, 'epoch': 5.399568034557236}\n",
      "[e5 b3800/9260] loss=0.8061 lr=1.45e-05\n",
      "{'loss': 0.8061, 'grad_norm': 92.5, 'learning_rate': 1.4486163915846728e-05, 'epoch': 5.41036717062635}\n",
      "[e5 b3900/9260] loss=0.7089 lr=1.44e-05\n",
      "{'loss': 0.7089, 'grad_norm': 50.25, 'learning_rate': 1.4425757077450303e-05, 'epoch': 5.421166306695464}\n",
      "[e5 b4000/9260] loss=0.7125 lr=1.44e-05\n",
      "{'loss': 0.7125, 'grad_norm': 54.5, 'learning_rate': 1.4365350239053878e-05, 'epoch': 5.431965442764579}\n",
      "[e5 b4100/9260] loss=0.6343 lr=1.43e-05\n",
      "{'loss': 0.6343, 'grad_norm': 6.15625, 'learning_rate': 1.4304943400657453e-05, 'epoch': 5.442764578833693}\n",
      "[e5 b4200/9260] loss=0.7394 lr=1.42e-05\n",
      "{'loss': 0.7394, 'grad_norm': 25.75, 'learning_rate': 1.4244536562261028e-05, 'epoch': 5.453563714902808}\n",
      "[e5 b4300/9260] loss=0.7079 lr=1.42e-05\n",
      "{'loss': 0.7079, 'grad_norm': 94.5, 'learning_rate': 1.4184129723864602e-05, 'epoch': 5.464362850971923}\n",
      "[e5 b4400/9260] loss=0.6256 lr=1.41e-05\n",
      "{'loss': 0.6256, 'grad_norm': 33.25, 'learning_rate': 1.4123722885468177e-05, 'epoch': 5.475161987041036}\n",
      "[e5 b4500/9260] loss=0.7652 lr=1.41e-05\n",
      "{'loss': 0.7652, 'grad_norm': 8.5625, 'learning_rate': 1.4063316047071752e-05, 'epoch': 5.485961123110151}\n",
      "[e5 b4600/9260] loss=0.6601 lr=1.40e-05\n",
      "{'loss': 0.6601, 'grad_norm': 73.0, 'learning_rate': 1.4002909208675327e-05, 'epoch': 5.496760259179266}\n",
      "[e5 b4700/9260] loss=0.7259 lr=1.39e-05\n",
      "{'loss': 0.7259, 'grad_norm': 35.25, 'learning_rate': 1.39425023702789e-05, 'epoch': 5.50755939524838}\n",
      "[e5 b4800/9260] loss=0.7057 lr=1.39e-05\n",
      "{'loss': 0.7057, 'grad_norm': 23.75, 'learning_rate': 1.3882095531882476e-05, 'epoch': 5.518358531317495}\n",
      "[e5 b4900/9260] loss=0.7459 lr=1.38e-05\n",
      "{'loss': 0.7459, 'grad_norm': 25.5, 'learning_rate': 1.3821688693486051e-05, 'epoch': 5.529157667386609}\n",
      "[e5 b5000/9260] loss=0.7071 lr=1.38e-05\n",
      "{'loss': 0.7071, 'grad_norm': 4.1875, 'learning_rate': 1.3761281855089626e-05, 'epoch': 5.539956803455723}\n",
      "[e5 b5100/9260] loss=0.7817 lr=1.37e-05\n",
      "{'loss': 0.7817, 'grad_norm': 22.25, 'learning_rate': 1.3700875016693201e-05, 'epoch': 5.550755939524838}\n",
      "[e5 b5200/9260] loss=0.7272 lr=1.36e-05\n",
      "{'loss': 0.7272, 'grad_norm': 45.5, 'learning_rate': 1.3640468178296775e-05, 'epoch': 5.561555075593953}\n",
      "[e5 b5300/9260] loss=0.7996 lr=1.36e-05\n",
      "{'loss': 0.7996, 'grad_norm': 22.25, 'learning_rate': 1.358006133990035e-05, 'epoch': 5.572354211663067}\n",
      "[e5 b5400/9260] loss=0.7394 lr=1.35e-05\n",
      "{'loss': 0.7394, 'grad_norm': 3.515625, 'learning_rate': 1.3519654501503927e-05, 'epoch': 5.583153347732181}\n",
      "[e5 b5500/9260] loss=0.6642 lr=1.35e-05\n",
      "{'loss': 0.6642, 'grad_norm': 60.0, 'learning_rate': 1.3459247663107502e-05, 'epoch': 5.593952483801296}\n",
      "[e5 b5600/9260] loss=0.7667 lr=1.34e-05\n",
      "{'loss': 0.7667, 'grad_norm': 30.375, 'learning_rate': 1.3398840824711077e-05, 'epoch': 5.60475161987041}\n",
      "[e5 b5700/9260] loss=0.7285 lr=1.33e-05\n",
      "{'loss': 0.7285, 'grad_norm': 25.25, 'learning_rate': 1.3338433986314652e-05, 'epoch': 5.615550755939525}\n",
      "[e5 b5800/9260] loss=0.7225 lr=1.33e-05\n",
      "{'loss': 0.7225, 'grad_norm': 47.25, 'learning_rate': 1.3278027147918227e-05, 'epoch': 5.62634989200864}\n",
      "[e5 b5900/9260] loss=0.7148 lr=1.32e-05\n",
      "{'loss': 0.7148, 'grad_norm': 35.5, 'learning_rate': 1.32176203095218e-05, 'epoch': 5.637149028077753}\n",
      "[e5 b6000/9260] loss=0.6153 lr=1.32e-05\n",
      "{'loss': 0.6153, 'grad_norm': 47.25, 'learning_rate': 1.3157213471125376e-05, 'epoch': 5.647948164146868}\n",
      "[e5 b6100/9260] loss=0.6898 lr=1.31e-05\n",
      "{'loss': 0.6898, 'grad_norm': 61.75, 'learning_rate': 1.309680663272895e-05, 'epoch': 5.658747300215983}\n",
      "[e5 b6200/9260] loss=0.7060 lr=1.30e-05\n",
      "{'loss': 0.706, 'grad_norm': 25.5, 'learning_rate': 1.3036399794332526e-05, 'epoch': 5.669546436285097}\n",
      "[e5 b6300/9260] loss=0.6605 lr=1.30e-05\n",
      "{'loss': 0.6605, 'grad_norm': 52.0, 'learning_rate': 1.2975992955936101e-05, 'epoch': 5.680345572354212}\n",
      "[e5 b6400/9260] loss=0.7256 lr=1.29e-05\n",
      "{'loss': 0.7256, 'grad_norm': 14.3125, 'learning_rate': 1.2915586117539674e-05, 'epoch': 5.691144708423326}\n",
      "[e5 b6500/9260] loss=0.6758 lr=1.29e-05\n",
      "{'loss': 0.6758, 'grad_norm': 32.0, 'learning_rate': 1.285517927914325e-05, 'epoch': 5.70194384449244}\n",
      "[e5 b6600/9260] loss=0.7427 lr=1.28e-05\n",
      "{'loss': 0.7427, 'grad_norm': 54.0, 'learning_rate': 1.2794772440746825e-05, 'epoch': 5.712742980561555}\n",
      "[e5 b6700/9260] loss=0.7346 lr=1.27e-05\n",
      "{'loss': 0.7346, 'grad_norm': 67.0, 'learning_rate': 1.27343656023504e-05, 'epoch': 5.72354211663067}\n",
      "[e5 b6800/9260] loss=0.6653 lr=1.27e-05\n",
      "{'loss': 0.6653, 'grad_norm': 51.25, 'learning_rate': 1.2673958763953973e-05, 'epoch': 5.734341252699784}\n",
      "[e5 b6900/9260] loss=0.7094 lr=1.26e-05\n",
      "{'loss': 0.7094, 'grad_norm': 60.0, 'learning_rate': 1.2613551925557548e-05, 'epoch': 5.745140388768899}\n",
      "[e5 b7000/9260] loss=0.6556 lr=1.26e-05\n",
      "{'loss': 0.6556, 'grad_norm': 6.03125, 'learning_rate': 1.2553145087161123e-05, 'epoch': 5.755939524838013}\n",
      "[e5 b7100/9260] loss=0.6917 lr=1.25e-05\n",
      "{'loss': 0.6917, 'grad_norm': 3.46875, 'learning_rate': 1.2492738248764698e-05, 'epoch': 5.766738660907127}\n",
      "[e5 b7200/9260] loss=0.6768 lr=1.24e-05\n",
      "{'loss': 0.6768, 'grad_norm': 19.0, 'learning_rate': 1.2432331410368274e-05, 'epoch': 5.777537796976242}\n",
      "[e5 b7300/9260] loss=0.7026 lr=1.24e-05\n",
      "{'loss': 0.7026, 'grad_norm': 9.75, 'learning_rate': 1.2371924571971847e-05, 'epoch': 5.788336933045357}\n",
      "[e5 b7400/9260] loss=0.7317 lr=1.23e-05\n",
      "{'loss': 0.7317, 'grad_norm': 29.75, 'learning_rate': 1.2311517733575422e-05, 'epoch': 5.799136069114471}\n",
      "[e5 b7500/9260] loss=0.7722 lr=1.23e-05\n",
      "{'loss': 0.7722, 'grad_norm': 53.0, 'learning_rate': 1.2251110895178997e-05, 'epoch': 5.809935205183585}\n",
      "[e5 b7600/9260] loss=0.7228 lr=1.22e-05\n",
      "{'loss': 0.7228, 'grad_norm': 23.25, 'learning_rate': 1.2190704056782572e-05, 'epoch': 5.8207343412527}\n",
      "[e5 b7700/9260] loss=0.7299 lr=1.21e-05\n",
      "{'loss': 0.7299, 'grad_norm': 109.5, 'learning_rate': 1.2130297218386147e-05, 'epoch': 5.831533477321814}\n",
      "[e5 b7800/9260] loss=0.6790 lr=1.21e-05\n",
      "{'loss': 0.679, 'grad_norm': 51.25, 'learning_rate': 1.2069890379989721e-05, 'epoch': 5.842332613390929}\n",
      "[e5 b7900/9260] loss=0.6891 lr=1.20e-05\n",
      "{'loss': 0.6891, 'grad_norm': 16.125, 'learning_rate': 1.2009483541593296e-05, 'epoch': 5.853131749460044}\n",
      "[e5 b8000/9260] loss=0.7350 lr=1.19e-05\n",
      "{'loss': 0.735, 'grad_norm': 8.125, 'learning_rate': 1.1949076703196871e-05, 'epoch': 5.863930885529157}\n",
      "[e5 b8100/9260] loss=0.6875 lr=1.19e-05\n",
      "{'loss': 0.6875, 'grad_norm': 9.75, 'learning_rate': 1.1888669864800446e-05, 'epoch': 5.874730021598272}\n",
      "[e5 b8200/9260] loss=0.8192 lr=1.18e-05\n",
      "{'loss': 0.8192, 'grad_norm': 39.0, 'learning_rate': 1.1828263026404021e-05, 'epoch': 5.885529157667387}\n",
      "[e5 b8300/9260] loss=0.6026 lr=1.18e-05\n",
      "{'loss': 0.6026, 'grad_norm': 35.25, 'learning_rate': 1.1767856188007595e-05, 'epoch': 5.896328293736501}\n",
      "[e5 b8400/9260] loss=0.6721 lr=1.17e-05\n",
      "{'loss': 0.6721, 'grad_norm': 40.5, 'learning_rate': 1.170744934961117e-05, 'epoch': 5.907127429805616}\n",
      "[e5 b8500/9260] loss=0.7263 lr=1.16e-05\n",
      "{'loss': 0.7263, 'grad_norm': 50.75, 'learning_rate': 1.1647042511214745e-05, 'epoch': 5.91792656587473}\n",
      "[e5 b8600/9260] loss=0.7658 lr=1.16e-05\n",
      "{'loss': 0.7658, 'grad_norm': 3.359375, 'learning_rate': 1.158663567281832e-05, 'epoch': 5.928725701943844}\n",
      "[e5 b8700/9260] loss=0.8090 lr=1.15e-05\n",
      "{'loss': 0.809, 'grad_norm': 66.5, 'learning_rate': 1.1526228834421895e-05, 'epoch': 5.939524838012959}\n",
      "[e5 b8800/9260] loss=0.6975 lr=1.15e-05\n",
      "{'loss': 0.6975, 'grad_norm': 32.5, 'learning_rate': 1.1465821996025469e-05, 'epoch': 5.950323974082074}\n",
      "[e5 b8900/9260] loss=0.7036 lr=1.14e-05\n",
      "{'loss': 0.7036, 'grad_norm': 12.1875, 'learning_rate': 1.1405415157629044e-05, 'epoch': 5.961123110151188}\n",
      "[e5 b9000/9260] loss=0.7534 lr=1.13e-05\n",
      "{'loss': 0.7534, 'grad_norm': 37.25, 'learning_rate': 1.1345008319232619e-05, 'epoch': 5.971922246220302}\n",
      "[e5 b9100/9260] loss=0.6927 lr=1.13e-05\n",
      "{'loss': 0.6927, 'grad_norm': 13.625, 'learning_rate': 1.1284601480836194e-05, 'epoch': 5.982721382289417}\n",
      "[e5 b9200/9260] loss=0.6622 lr=1.12e-05\n",
      "{'loss': 0.6622, 'grad_norm': 35.0, 'learning_rate': 1.1224194642439768e-05, 'epoch': 5.993520518358531}\n",
      "{'eval_loss': 0.797606885433197, 'eval_accuracy': 0.7227891156462585, 'eval_precision': 0.7334184343902534, 'eval_recall': 0.7421135966617914, 'eval_f1': 0.7322673563377747, 'eval_runtime': 16.1293, 'eval_samples_per_second': 255.187, 'eval_steps_per_second': 63.797, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.7228 f1=0.7323 p=0.7334 r=0.7421\n",
      "{'loss': 0.6406, 'grad_norm': 38.5, 'learning_rate': 1.1163787804043343e-05, 'epoch': 6.004319654427646}\n",
      "{'loss': 0.7225, 'grad_norm': 31.625, 'learning_rate': 1.1103380965646918e-05, 'epoch': 6.015118790496761}\n",
      "{'loss': 0.7247, 'grad_norm': 55.25, 'learning_rate': 1.1042974127250493e-05, 'epoch': 6.025917926565874}\n",
      "{'loss': 0.7205, 'grad_norm': 68.5, 'learning_rate': 1.0982567288854068e-05, 'epoch': 6.036717062634989}\n",
      "{'loss': 0.7136, 'grad_norm': 40.75, 'learning_rate': 1.0922160450457641e-05, 'epoch': 6.047516198704104}\n",
      "{'loss': 0.6887, 'grad_norm': 18.625, 'learning_rate': 1.0861753612061217e-05, 'epoch': 6.058315334773218}\n",
      "{'loss': 0.693, 'grad_norm': 56.75, 'learning_rate': 1.0801346773664792e-05, 'epoch': 6.069114470842333}\n",
      "{'loss': 0.7757, 'grad_norm': 42.75, 'learning_rate': 1.0740939935268367e-05, 'epoch': 6.079913606911447}\n",
      "{'loss': 0.7597, 'grad_norm': 41.75, 'learning_rate': 1.0680533096871942e-05, 'epoch': 6.090712742980561}\n",
      "{'loss': 0.6767, 'grad_norm': 48.75, 'learning_rate': 1.0620126258475515e-05, 'epoch': 6.101511879049676}\n",
      "{'loss': 0.6278, 'grad_norm': 60.25, 'learning_rate': 1.055971942007909e-05, 'epoch': 6.112311015118791}\n",
      "{'loss': 0.6746, 'grad_norm': 25.875, 'learning_rate': 1.0499312581682666e-05, 'epoch': 6.123110151187905}\n",
      "{'loss': 0.7764, 'grad_norm': 59.5, 'learning_rate': 1.043890574328624e-05, 'epoch': 6.133909287257019}\n",
      "{'loss': 0.6839, 'grad_norm': 27.25, 'learning_rate': 1.0378498904889816e-05, 'epoch': 6.144708423326134}\n",
      "{'loss': 0.7469, 'grad_norm': 81.5, 'learning_rate': 1.031809206649339e-05, 'epoch': 6.155507559395248}\n",
      "{'loss': 0.6711, 'grad_norm': 18.125, 'learning_rate': 1.0257685228096964e-05, 'epoch': 6.166306695464363}\n",
      "{'loss': 0.6708, 'grad_norm': 56.25, 'learning_rate': 1.019727838970054e-05, 'epoch': 6.177105831533478}\n",
      "{'loss': 0.6599, 'grad_norm': 62.75, 'learning_rate': 1.0136871551304115e-05, 'epoch': 6.1879049676025915}\n",
      "{'loss': 0.7283, 'grad_norm': 16.875, 'learning_rate': 1.0076464712907688e-05, 'epoch': 6.198704103671706}\n",
      "{'loss': 0.6888, 'grad_norm': 25.375, 'learning_rate': 1.0016057874511263e-05, 'epoch': 6.209503239740821}\n",
      "{'loss': 0.742, 'grad_norm': 27.0, 'learning_rate': 9.955651036114838e-06, 'epoch': 6.220302375809935}\n",
      "{'loss': 0.7739, 'grad_norm': 100.0, 'learning_rate': 9.895244197718413e-06, 'epoch': 6.23110151187905}\n",
      "{'loss': 0.705, 'grad_norm': 57.75, 'learning_rate': 9.834837359321989e-06, 'epoch': 6.241900647948164}\n",
      "{'loss': 0.6868, 'grad_norm': 31.75, 'learning_rate': 9.774430520925562e-06, 'epoch': 6.252699784017278}\n",
      "{'loss': 0.7453, 'grad_norm': 44.75, 'learning_rate': 9.714023682529137e-06, 'epoch': 6.263498920086393}\n",
      "{'loss': 0.6688, 'grad_norm': 42.5, 'learning_rate': 9.653616844132712e-06, 'epoch': 6.274298056155508}\n",
      "{'loss': 0.7011, 'grad_norm': 77.0, 'learning_rate': 9.593210005736287e-06, 'epoch': 6.285097192224622}\n",
      "{'loss': 0.743, 'grad_norm': 27.0, 'learning_rate': 9.532803167339862e-06, 'epoch': 6.295896328293736}\n",
      "{'loss': 0.6776, 'grad_norm': 57.5, 'learning_rate': 9.472396328943436e-06, 'epoch': 6.306695464362851}\n",
      "{'loss': 0.6871, 'grad_norm': 100.0, 'learning_rate': 9.411989490547011e-06, 'epoch': 6.317494600431965}\n",
      "{'loss': 0.7077, 'grad_norm': 10.4375, 'learning_rate': 9.351582652150588e-06, 'epoch': 6.32829373650108}\n",
      "{'loss': 0.7473, 'grad_norm': 48.25, 'learning_rate': 9.291175813754161e-06, 'epoch': 6.339092872570195}\n",
      "{'loss': 0.6276, 'grad_norm': 38.5, 'learning_rate': 9.230768975357736e-06, 'epoch': 6.3498920086393085}\n",
      "{'loss': 0.6408, 'grad_norm': 86.0, 'learning_rate': 9.170362136961311e-06, 'epoch': 6.360691144708423}\n",
      "{'loss': 0.7368, 'grad_norm': 38.25, 'learning_rate': 9.109955298564887e-06, 'epoch': 6.371490280777538}\n",
      "{'loss': 0.7266, 'grad_norm': 13.0, 'learning_rate': 9.049548460168462e-06, 'epoch': 6.382289416846652}\n",
      "{'loss': 0.7603, 'grad_norm': 60.0, 'learning_rate': 8.989141621772035e-06, 'epoch': 6.393088552915767}\n",
      "{'loss': 0.7051, 'grad_norm': 115.0, 'learning_rate': 8.92873478337561e-06, 'epoch': 6.403887688984881}\n",
      "{'loss': 0.7035, 'grad_norm': 32.0, 'learning_rate': 8.868327944979185e-06, 'epoch': 6.4146868250539955}\n",
      "{'loss': 0.6562, 'grad_norm': 67.5, 'learning_rate': 8.80792110658276e-06, 'epoch': 6.42548596112311}\n",
      "{'loss': 0.7371, 'grad_norm': 12.4375, 'learning_rate': 8.747514268186336e-06, 'epoch': 6.436285097192225}\n",
      "{'loss': 0.7031, 'grad_norm': 70.5, 'learning_rate': 8.687107429789909e-06, 'epoch': 6.447084233261339}\n",
      "{'loss': 0.7505, 'grad_norm': 80.5, 'learning_rate': 8.626700591393484e-06, 'epoch': 6.457883369330453}\n",
      "{'loss': 0.6752, 'grad_norm': 79.0, 'learning_rate': 8.56629375299706e-06, 'epoch': 6.468682505399568}\n",
      "{'loss': 0.6452, 'grad_norm': 54.75, 'learning_rate': 8.505886914600634e-06, 'epoch': 6.479481641468682}\n",
      "{'loss': 0.6903, 'grad_norm': 28.125, 'learning_rate': 8.44548007620421e-06, 'epoch': 6.490280777537797}\n",
      "{'loss': 0.6951, 'grad_norm': 6.4375, 'learning_rate': 8.385073237807783e-06, 'epoch': 6.501079913606912}\n",
      "{'loss': 0.6945, 'grad_norm': 58.75, 'learning_rate': 8.324666399411358e-06, 'epoch': 6.5118790496760255}\n",
      "{'loss': 0.7387, 'grad_norm': 9.8125, 'learning_rate': 8.264259561014933e-06, 'epoch': 6.52267818574514}\n",
      "{'loss': 0.7003, 'grad_norm': 105.5, 'learning_rate': 8.203852722618508e-06, 'epoch': 6.533477321814255}\n",
      "{'loss': 0.6938, 'grad_norm': 7.375, 'learning_rate': 8.143445884222082e-06, 'epoch': 6.544276457883369}\n",
      "{'loss': 0.6266, 'grad_norm': 60.0, 'learning_rate': 8.083039045825657e-06, 'epoch': 6.555075593952484}\n",
      "{'loss': 0.6735, 'grad_norm': 16.875, 'learning_rate': 8.022632207429232e-06, 'epoch': 6.565874730021598}\n",
      "{'loss': 0.723, 'grad_norm': 56.5, 'learning_rate': 7.962225369032807e-06, 'epoch': 6.5766738660907125}\n",
      "{'loss': 0.7817, 'grad_norm': 47.25, 'learning_rate': 7.901818530636382e-06, 'epoch': 6.587473002159827}\n",
      "{'loss': 0.7073, 'grad_norm': 47.5, 'learning_rate': 7.841411692239956e-06, 'epoch': 6.598272138228942}\n",
      "{'loss': 0.7286, 'grad_norm': 73.0, 'learning_rate': 7.78100485384353e-06, 'epoch': 6.609071274298056}\n",
      "{'loss': 0.7432, 'grad_norm': 25.75, 'learning_rate': 7.720598015447106e-06, 'epoch': 6.61987041036717}\n",
      "{'loss': 0.7394, 'grad_norm': 80.0, 'learning_rate': 7.660191177050681e-06, 'epoch': 6.630669546436285}\n",
      "{'loss': 0.6813, 'grad_norm': 32.25, 'learning_rate': 7.599784338654255e-06, 'epoch': 6.6414686825053995}\n",
      "{'loss': 0.7621, 'grad_norm': 65.0, 'learning_rate': 7.53937750025783e-06, 'epoch': 6.652267818574514}\n",
      "{'loss': 0.6693, 'grad_norm': 53.0, 'learning_rate': 7.478970661861405e-06, 'epoch': 6.663066954643629}\n",
      "{'loss': 0.7572, 'grad_norm': 117.0, 'learning_rate': 7.41856382346498e-06, 'epoch': 6.6738660907127425}\n",
      "{'loss': 0.7644, 'grad_norm': 56.25, 'learning_rate': 7.358156985068555e-06, 'epoch': 6.684665226781857}\n",
      "{'loss': 0.7441, 'grad_norm': 61.75, 'learning_rate': 7.297750146672129e-06, 'epoch': 6.695464362850972}\n",
      "{'loss': 0.7453, 'grad_norm': 20.25, 'learning_rate': 7.237343308275704e-06, 'epoch': 6.706263498920086}\n",
      "{'loss': 0.6989, 'grad_norm': 36.25, 'learning_rate': 7.1769364698792785e-06, 'epoch': 6.717062634989201}\n",
      "{'loss': 0.7037, 'grad_norm': 38.25, 'learning_rate': 7.116529631482854e-06, 'epoch': 6.727861771058315}\n",
      "{'loss': 0.7902, 'grad_norm': 51.25, 'learning_rate': 7.056122793086428e-06, 'epoch': 6.7386609071274295}\n",
      "{'loss': 0.7132, 'grad_norm': 37.0, 'learning_rate': 6.995715954690003e-06, 'epoch': 6.749460043196544}\n",
      "{'loss': 0.6504, 'grad_norm': 10.0625, 'learning_rate': 6.935309116293578e-06, 'epoch': 6.760259179265659}\n",
      "{'loss': 0.7965, 'grad_norm': 57.25, 'learning_rate': 6.8749022778971524e-06, 'epoch': 6.771058315334773}\n",
      "{'loss': 0.6903, 'grad_norm': 53.75, 'learning_rate': 6.8144954395007276e-06, 'epoch': 6.781857451403888}\n",
      "{'loss': 0.7001, 'grad_norm': 117.5, 'learning_rate': 6.754088601104302e-06, 'epoch': 6.792656587473002}\n",
      "{'loss': 0.6852, 'grad_norm': 14.5, 'learning_rate': 6.693681762707877e-06, 'epoch': 6.8034557235421165}\n",
      "{'loss': 0.8173, 'grad_norm': 19.125, 'learning_rate': 6.633274924311451e-06, 'epoch': 6.814254859611231}\n",
      "{'loss': 0.6762, 'grad_norm': 21.375, 'learning_rate': 6.572868085915027e-06, 'epoch': 6.825053995680346}\n",
      "{'loss': 0.6788, 'grad_norm': 21.5, 'learning_rate': 6.512461247518602e-06, 'epoch': 6.83585313174946}\n",
      "{'loss': 0.8279, 'grad_norm': 62.75, 'learning_rate': 6.4520544091221774e-06, 'epoch': 6.846652267818574}\n",
      "{'loss': 0.6952, 'grad_norm': 50.75, 'learning_rate': 6.391647570725752e-06, 'epoch': 6.857451403887689}\n",
      "{'loss': 0.6783, 'grad_norm': 32.25, 'learning_rate': 6.331240732329327e-06, 'epoch': 6.8682505399568035}\n",
      "{'loss': 0.6756, 'grad_norm': 115.0, 'learning_rate': 6.270833893932901e-06, 'epoch': 6.879049676025918}\n",
      "{'loss': 0.7136, 'grad_norm': 62.0, 'learning_rate': 6.210427055536476e-06, 'epoch': 6.889848812095033}\n",
      "{'loss': 0.6977, 'grad_norm': 76.0, 'learning_rate': 6.150020217140051e-06, 'epoch': 6.9006479481641465}\n",
      "{'loss': 0.7714, 'grad_norm': 86.5, 'learning_rate': 6.089613378743626e-06, 'epoch': 6.911447084233261}\n",
      "{'loss': 0.6757, 'grad_norm': 36.5, 'learning_rate': 6.029206540347201e-06, 'epoch': 6.922246220302376}\n",
      "{'loss': 0.7449, 'grad_norm': 31.875, 'learning_rate': 5.968799701950775e-06, 'epoch': 6.93304535637149}\n",
      "{'loss': 0.662, 'grad_norm': 32.75, 'learning_rate': 5.90839286355435e-06, 'epoch': 6.943844492440605}\n",
      "{'loss': 0.6623, 'grad_norm': 77.0, 'learning_rate': 5.847986025157924e-06, 'epoch': 6.954643628509719}\n",
      "{'loss': 0.6299, 'grad_norm': 14.0, 'learning_rate': 5.7875791867614995e-06, 'epoch': 6.9654427645788335}\n",
      "{'loss': 0.6904, 'grad_norm': 29.5, 'learning_rate': 5.727172348365075e-06, 'epoch': 6.976241900647948}\n",
      "{'loss': 0.7402, 'grad_norm': 66.5, 'learning_rate': 5.666765509968649e-06, 'epoch': 6.987041036717063}\n",
      "{'loss': 0.7888, 'grad_norm': 5.09375, 'learning_rate': 5.606358671572224e-06, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 0.7957254648208618, 'eval_accuracy': 0.7215743440233237, 'eval_precision': 0.7328170349039331, 'eval_recall': 0.7412374963101158, 'eval_f1': 0.731258024954141, 'eval_runtime': 15.7448, 'eval_samples_per_second': 261.42, 'eval_steps_per_second': 65.355, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.7216 f1=0.7313 p=0.7328 r=0.7412\n",
      "{'loss': 0.7347, 'grad_norm': 36.5, 'learning_rate': 5.545951833175798e-06, 'epoch': 7.008639308855291}\n",
      "{'loss': 0.8378, 'grad_norm': 67.0, 'learning_rate': 5.4855449947793734e-06, 'epoch': 7.019438444924406}\n",
      "{'loss': 0.6845, 'grad_norm': 60.5, 'learning_rate': 5.4251381563829485e-06, 'epoch': 7.0302375809935205}\n",
      "{'loss': 0.705, 'grad_norm': 100.5, 'learning_rate': 5.364731317986523e-06, 'epoch': 7.041036717062635}\n",
      "{'loss': 0.794, 'grad_norm': 73.0, 'learning_rate': 5.304324479590098e-06, 'epoch': 7.05183585313175}\n",
      "{'loss': 0.7191, 'grad_norm': 51.25, 'learning_rate': 5.243917641193672e-06, 'epoch': 7.0626349892008635}\n",
      "{'loss': 0.7362, 'grad_norm': 32.75, 'learning_rate': 5.183510802797247e-06, 'epoch': 7.073434125269978}\n",
      "{'loss': 0.7336, 'grad_norm': 25.25, 'learning_rate': 5.123103964400822e-06, 'epoch': 7.084233261339093}\n",
      "{'loss': 0.749, 'grad_norm': 41.25, 'learning_rate': 5.062697126004397e-06, 'epoch': 7.0950323974082075}\n",
      "{'loss': 0.7231, 'grad_norm': 28.125, 'learning_rate': 5.002290287607972e-06, 'epoch': 7.105831533477322}\n",
      "{'loss': 0.7758, 'grad_norm': 40.75, 'learning_rate': 4.941883449211546e-06, 'epoch': 7.116630669546436}\n",
      "{'loss': 0.7857, 'grad_norm': 24.25, 'learning_rate': 4.881476610815121e-06, 'epoch': 7.1274298056155505}\n",
      "{'loss': 0.6241, 'grad_norm': 20.625, 'learning_rate': 4.8210697724186955e-06, 'epoch': 7.138228941684665}\n",
      "{'loss': 0.7396, 'grad_norm': 36.5, 'learning_rate': 4.760662934022271e-06, 'epoch': 7.14902807775378}\n",
      "{'loss': 0.7962, 'grad_norm': 79.5, 'learning_rate': 4.700256095625845e-06, 'epoch': 7.159827213822894}\n",
      "{'loss': 0.7261, 'grad_norm': 12.1875, 'learning_rate': 4.63984925722942e-06, 'epoch': 7.170626349892009}\n",
      "{'loss': 0.649, 'grad_norm': 85.5, 'learning_rate': 4.579442418832995e-06, 'epoch': 7.181425485961123}\n",
      "{'loss': 0.7746, 'grad_norm': 84.5, 'learning_rate': 4.519035580436569e-06, 'epoch': 7.1922246220302375}\n",
      "{'loss': 0.7508, 'grad_norm': 26.0, 'learning_rate': 4.458628742040145e-06, 'epoch': 7.203023758099352}\n",
      "{'loss': 0.7571, 'grad_norm': 53.5, 'learning_rate': 4.39822190364372e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 0.7057, 'grad_norm': 53.25, 'learning_rate': 4.337815065247295e-06, 'epoch': 7.224622030237581}\n",
      "{'loss': 0.6584, 'grad_norm': 10.875, 'learning_rate': 4.277408226850869e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 0.7347, 'grad_norm': 20.375, 'learning_rate': 4.217001388454444e-06, 'epoch': 7.24622030237581}\n",
      "{'loss': 0.689, 'grad_norm': 41.5, 'learning_rate': 4.1565945500580184e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 0.706, 'grad_norm': 30.25, 'learning_rate': 4.0961877116615936e-06, 'epoch': 7.267818574514039}\n",
      "{'loss': 0.7342, 'grad_norm': 54.25, 'learning_rate': 4.035780873265169e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 0.6327, 'grad_norm': 44.75, 'learning_rate': 3.975374034868743e-06, 'epoch': 7.2894168466522675}\n",
      "{'loss': 0.7257, 'grad_norm': 96.5, 'learning_rate': 3.914967196472318e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 0.6559, 'grad_norm': 33.5, 'learning_rate': 3.854560358075892e-06, 'epoch': 7.311015118790497}\n",
      "{'loss': 0.667, 'grad_norm': 34.5, 'learning_rate': 3.7941535196794675e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 0.7314, 'grad_norm': 19.0, 'learning_rate': 3.733746681283042e-06, 'epoch': 7.332613390928726}\n",
      "{'loss': 0.7126, 'grad_norm': 6.75, 'learning_rate': 3.673339842886617e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 0.6253, 'grad_norm': 71.5, 'learning_rate': 3.6129330044901916e-06, 'epoch': 7.3542116630669545}\n",
      "{'loss': 0.7968, 'grad_norm': 14.75, 'learning_rate': 3.5525261660937663e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 0.6948, 'grad_norm': 50.25, 'learning_rate': 3.4921193276973414e-06, 'epoch': 7.375809935205184}\n",
      "{'loss': 0.6538, 'grad_norm': 30.875, 'learning_rate': 3.431712489300916e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 0.672, 'grad_norm': 26.25, 'learning_rate': 3.3713056509044908e-06, 'epoch': 7.397408207343412}\n",
      "{'loss': 0.6495, 'grad_norm': 15.8125, 'learning_rate': 3.3108988125080655e-06, 'epoch': 7.408207343412527}\n",
      "{'loss': 0.7421, 'grad_norm': 77.5, 'learning_rate': 3.25049197411164e-06, 'epoch': 7.4190064794816415}\n",
      "{'loss': 0.6747, 'grad_norm': 12.25, 'learning_rate': 3.190085135715215e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 0.6662, 'grad_norm': 27.375, 'learning_rate': 3.12967829731879e-06, 'epoch': 7.440604751619871}\n",
      "{'loss': 0.7313, 'grad_norm': 63.25, 'learning_rate': 3.069271458922365e-06, 'epoch': 7.4514038876889845}\n",
      "{'loss': 0.593, 'grad_norm': 2.015625, 'learning_rate': 3.00886462052594e-06, 'epoch': 7.462203023758099}\n",
      "{'loss': 0.6845, 'grad_norm': 45.5, 'learning_rate': 2.9484577821295145e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 0.7387, 'grad_norm': 22.75, 'learning_rate': 2.8880509437330896e-06, 'epoch': 7.4838012958963285}\n",
      "{'loss': 0.72, 'grad_norm': 55.5, 'learning_rate': 2.8276441053366643e-06, 'epoch': 7.494600431965443}\n",
      "{'loss': 0.7531, 'grad_norm': 66.0, 'learning_rate': 2.767237266940239e-06, 'epoch': 7.505399568034557}\n",
      "{'loss': 0.6968, 'grad_norm': 80.5, 'learning_rate': 2.7068304285438137e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 0.7299, 'grad_norm': 58.75, 'learning_rate': 2.6464235901473884e-06, 'epoch': 7.526997840172786}\n",
      "{'loss': 0.634, 'grad_norm': 55.0, 'learning_rate': 2.586016751750963e-06, 'epoch': 7.537796976241901}\n",
      "{'loss': 0.6463, 'grad_norm': 35.5, 'learning_rate': 2.525609913354538e-06, 'epoch': 7.5485961123110155}\n",
      "{'loss': 0.725, 'grad_norm': 48.0, 'learning_rate': 2.465203074958113e-06, 'epoch': 7.559395248380129}\n",
      "{'loss': 0.6661, 'grad_norm': 47.25, 'learning_rate': 2.4047962365616876e-06, 'epoch': 7.570194384449244}\n",
      "{'loss': 0.6739, 'grad_norm': 87.5, 'learning_rate': 2.3443893981652623e-06, 'epoch': 7.5809935205183585}\n",
      "{'loss': 0.7891, 'grad_norm': 70.5, 'learning_rate': 2.283982559768837e-06, 'epoch': 7.591792656587473}\n",
      "{'loss': 0.7036, 'grad_norm': 28.625, 'learning_rate': 2.2235757213724117e-06, 'epoch': 7.602591792656588}\n",
      "{'loss': 0.7049, 'grad_norm': 61.25, 'learning_rate': 2.163168882975987e-06, 'epoch': 7.613390928725702}\n",
      "{'loss': 0.6919, 'grad_norm': 40.0, 'learning_rate': 2.1027620445795615e-06, 'epoch': 7.624190064794816}\n",
      "{'loss': 0.6999, 'grad_norm': 44.5, 'learning_rate': 2.0423552061831366e-06, 'epoch': 7.634989200863931}\n",
      "{'loss': 0.6994, 'grad_norm': 105.0, 'learning_rate': 1.9819483677867113e-06, 'epoch': 7.6457883369330455}\n",
      "{'loss': 0.6355, 'grad_norm': 5.65625, 'learning_rate': 1.921541529390286e-06, 'epoch': 7.65658747300216}\n",
      "{'loss': 0.7452, 'grad_norm': 69.5, 'learning_rate': 1.8611346909938607e-06, 'epoch': 7.667386609071274}\n",
      "{'loss': 0.7976, 'grad_norm': 10.3125, 'learning_rate': 1.8007278525974356e-06, 'epoch': 7.6781857451403885}\n",
      "{'loss': 0.7811, 'grad_norm': 5.75, 'learning_rate': 1.7403210142010103e-06, 'epoch': 7.688984881209503}\n",
      "{'loss': 0.6976, 'grad_norm': 60.5, 'learning_rate': 1.679914175804585e-06, 'epoch': 7.699784017278618}\n",
      "{'loss': 0.7034, 'grad_norm': 8.4375, 'learning_rate': 1.6195073374081597e-06, 'epoch': 7.7105831533477325}\n",
      "{'loss': 0.688, 'grad_norm': 4.375, 'learning_rate': 1.5591004990117346e-06, 'epoch': 7.721382289416846}\n",
      "{'loss': 0.7518, 'grad_norm': 27.125, 'learning_rate': 1.4986936606153093e-06, 'epoch': 7.732181425485961}\n",
      "{'loss': 0.6667, 'grad_norm': 42.5, 'learning_rate': 1.438286822218884e-06, 'epoch': 7.7429805615550755}\n",
      "{'loss': 0.7095, 'grad_norm': 16.0, 'learning_rate': 1.377879983822459e-06, 'epoch': 7.75377969762419}\n",
      "{'loss': 0.6993, 'grad_norm': 22.0, 'learning_rate': 1.317473145426034e-06, 'epoch': 7.764578833693305}\n",
      "{'loss': 0.6907, 'grad_norm': 97.5, 'learning_rate': 1.2570663070296085e-06, 'epoch': 7.775377969762419}\n",
      "{'loss': 0.7159, 'grad_norm': 96.0, 'learning_rate': 1.1966594686331832e-06, 'epoch': 7.786177105831533}\n",
      "{'loss': 0.6653, 'grad_norm': 27.75, 'learning_rate': 1.1362526302367581e-06, 'epoch': 7.796976241900648}\n",
      "{'loss': 0.702, 'grad_norm': 12.5, 'learning_rate': 1.075845791840333e-06, 'epoch': 7.8077753779697625}\n",
      "{'loss': 0.7222, 'grad_norm': 72.0, 'learning_rate': 1.0154389534439077e-06, 'epoch': 7.818574514038877}\n",
      "{'loss': 0.7283, 'grad_norm': 51.75, 'learning_rate': 9.550321150474824e-07, 'epoch': 7.829373650107991}\n",
      "{'loss': 0.6788, 'grad_norm': 11.4375, 'learning_rate': 8.946252766510573e-07, 'epoch': 7.840172786177106}\n",
      "{'loss': 0.6622, 'grad_norm': 43.0, 'learning_rate': 8.34218438254632e-07, 'epoch': 7.85097192224622}\n",
      "{'loss': 0.721, 'grad_norm': 38.5, 'learning_rate': 7.73811599858207e-07, 'epoch': 7.861771058315335}\n",
      "{'loss': 0.7093, 'grad_norm': 23.875, 'learning_rate': 7.134047614617818e-07, 'epoch': 7.8725701943844495}\n",
      "{'loss': 0.723, 'grad_norm': 49.25, 'learning_rate': 6.529979230653566e-07, 'epoch': 7.883369330453563}\n",
      "{'loss': 0.7075, 'grad_norm': 54.5, 'learning_rate': 5.925910846689312e-07, 'epoch': 7.894168466522678}\n",
      "{'loss': 0.6452, 'grad_norm': 40.0, 'learning_rate': 5.321842462725062e-07, 'epoch': 7.9049676025917925}\n",
      "{'loss': 0.7026, 'grad_norm': 22.625, 'learning_rate': 4.7177740787608085e-07, 'epoch': 7.915766738660907}\n",
      "{'loss': 0.665, 'grad_norm': 56.75, 'learning_rate': 4.1137056947965566e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 0.6954, 'grad_norm': 6.53125, 'learning_rate': 3.5096373108323046e-07, 'epoch': 7.937365010799136}\n",
      "{'loss': 0.7333, 'grad_norm': 71.0, 'learning_rate': 2.9055689268680526e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 0.7681, 'grad_norm': 21.5, 'learning_rate': 2.3015005429038006e-07, 'epoch': 7.958963282937365}\n",
      "{'loss': 0.7651, 'grad_norm': 89.5, 'learning_rate': 1.6974321589395487e-07, 'epoch': 7.9697624190064795}\n",
      "{'loss': 0.751, 'grad_norm': 18.375, 'learning_rate': 1.0933637749752964e-07, 'epoch': 7.980561555075594}\n",
      "{'loss': 0.6406, 'grad_norm': 27.5, 'learning_rate': 4.8929539101104424e-08, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 0.7947435975074768, 'eval_accuracy': 0.7225461613216715, 'eval_precision': 0.7340715935033696, 'eval_recall': 0.7416319220611332, 'eval_f1': 0.7321607863313986, 'eval_runtime': 15.4673, 'eval_samples_per_second': 266.11, 'eval_steps_per_second': 66.527, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.7225 f1=0.7322 p=0.7341 r=0.7416\n",
      "{'train_runtime': 3461.0473, 'train_samples_per_second': 85.613, 'train_steps_per_second': 21.404, 'train_loss': 0.8000716897890325, 'epoch': 8.0}\n",
      "{'eval_loss': 0.7875571846961975, 'eval_accuracy': 0.7235179786200194, 'eval_precision': 0.7340967031965364, 'eval_recall': 0.7423102014188776, 'eval_f1': 0.7333747370018296, 'eval_runtime': 15.8965, 'eval_samples_per_second': 258.924, 'eval_steps_per_second': 64.731, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.7235 f1=0.7334 p=0.7341 r=0.7423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÜ‚ñà‚ñÑ‚ñÅ‚ñÜ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÜ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñà‚ñÉ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñà‚ñá‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñá</td></tr><tr><td>train/learning_rate</td><td>‚ñÖ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.73337</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.72352</td></tr><tr><td>eval/f1</td><td>0.73337</td></tr><tr><td>eval/loss</td><td>0.78756</td></tr><tr><td>eval/precision</td><td>0.7341</td></tr><tr><td>eval/recall</td><td>0.74231</td></tr><tr><td>eval/runtime</td><td>15.8965</td></tr><tr><td>eval/samples_per_second</td><td>258.924</td></tr><tr><td>eval/steps_per_second</td><td>64.731</td></tr><tr><td>total_flos</td><td>1.1024708094226464e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>74080</td></tr><tr><td>train/grad_norm</td><td>27.5</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.6406</td></tr><tr><td>train_loss</td><td>0.80007</td></tr><tr><td>train_runtime</td><td>3461.0473</td></tr><tr><td>train_samples_per_second</td><td>85.613</td></tr><tr><td>train_steps_per_second</td><td>21.404</td></tr><tr><td>trial/accuracy</td><td>0.72352</td></tr><tr><td>trial/f1</td><td>0.73337</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t9</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/8nco8b3f' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/8nco8b3f</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_010126-8nco8b3f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.852341: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [4:18:17<00:00, 1549.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=9 f1=0.7334\n",
      "[I 2025-08-17 01:59:27,931] Trial 9 finished with value: 0.7333747370018296 and parameters: {'lr': 4.2064301917350694e-05, 'weight_decay': 1.935643819405686e-06, 'unfreeze_last_k': 12, 'batch_size': 4}. Best is trial 2 with value: 0.8523413656247504.\n",
      "[Study best] {'lr': 9.034601256162855e-05, 'weight_decay': 2.3378864159216274e-06, 'num_unfreeze_last_layers': 12, 'batch_size': 32, 'epochs': 12}\n",
      "Saved best params ‚Üí hf_ckpts/best_params_optuna1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    # --- Ex.5: Optuna study (12 trials √ó 12 epochs) around your current best HPs ---\n",
    "\n",
    "import optuna\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "STUDY_GROUP   = BASE_RUN_NAME + \"_study\"\n",
    "REFINE_EPOCHS = 8        # <- per your request\n",
    "N_TRIALS      = 10        # <- per your request\n",
    "LOG_STEPS     = 100\n",
    "\n",
    "# Convenience: candidate batch sizes \"around\" your current BATCH_SIZE, but safe on 4090\n",
    "def bs_candidates(bs):\n",
    "    cands = {bs}\n",
    "    if bs <= 16:\n",
    "        cands.add(min(32, bs * 2))\n",
    "    else:\n",
    "        cands.add(max(8, bs // 2))\n",
    "    # keep them reasonable\n",
    "    return sorted({b for b in cands if 4 <= b <= 64})\n",
    "\n",
    "# Trainer factory for a single trial\n",
    "def build_trainer_for_trial(lr, wd, k_unfreeze, bs, run_name, epochs):\n",
    "    model = build_model(unfreeze_last_k=int(k_unfreeze))\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%) ; unfreeze_last_k={k_unfreeze}\")\n",
    "\n",
    "    # bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    safe_name = BASE_RUN_NAME\n",
    "    out_dir = os.path.join(\"hf_ckpts\", safe_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    args = TrainingArguments(\n",
    "        output_dir=os.path.join(\"hf_ckpts\", run_name),\n",
    "        run_name=run_name,\n",
    "        report_to=[\"wandb\"],\n",
    "        seed=SEED,\n",
    "        learning_rate=float(lr),\n",
    "        weight_decay=float(wd),\n",
    "        warmup_ratio=WARMUP_RATIO,\n",
    "        num_train_epochs=int(epochs),\n",
    "        per_device_train_batch_size=int(bs),\n",
    "        per_device_eval_batch_size=int(bs),\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "        dataloader_num_workers=NUM_WORKERS,\n",
    "        dataloader_pin_memory=True,\n",
    "        dataloader_prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    "        fp16=(torch.cuda.is_available() and not bf16_ok),\n",
    "        bf16=bf16_ok,\n",
    "        optim=\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",\n",
    "        save_safetensors=True,\n",
    "        disable_tqdm=True,             # we print ourselves\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=LOG_STEPS,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "        greater_is_better=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[\n",
    "            PrintAndWBCallback(print_every=LOG_STEPS),\n",
    "            EarlyStoppingCallback(early_stopping_patience=PATIENCE),\n",
    "        ],\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "# Objective: search narrowly around your current best HPs\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-4, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\",1e-6, 1e-4, log=True)\n",
    "    k  = trial.suggest_int(\"unfreeze_last_k\", 8,12)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32])\n",
    "\n",
    "    run_name = f\"{BASE_RUN_NAME}__t{trial.number}\"\n",
    "\n",
    "    # W&B run per trial (grouped under the study for easy comparison)\n",
    "    wb = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=run_name,\n",
    "        group=STUDY_GROUP,\n",
    "        job_type=\"optuna-trial\",\n",
    "        tags=[\"optuna\",\"trainer\",\"ex5\"],\n",
    "        config={\n",
    "            \"trial\": trial.number, \"model\": MODEL_NAME,\n",
    "            \"lr\": lr, \"weight_decay\": wd, \"unfreeze_last_k\": k,\n",
    "            \"batch_size\": bs, \"epochs\": REFINE_EPOCHS,\n",
    "            \"warmup_ratio\": WARMUP_RATIO, \"max_len\": MAX_LEN,\n",
    "        },\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "    )\n",
    "\n",
    "    print(f\"[TUNE] trial={trial.number} | epochs={REFINE_EPOCHS} bs={bs} lr={lr:.2e} wd={wd:.1e} k={k}\")\n",
    "\n",
    "    trainer = build_trainer_for_trial(lr, wd, k, bs, run_name, REFINE_EPOCHS)\n",
    "\n",
    "    # Train + epoch evals (printed by callback; logged to W&B automatically)\n",
    "    trainer.train()\n",
    "\n",
    "    # Final eval for the objective score\n",
    "    metrics = trainer.evaluate()\n",
    "    f1 = float(metrics.get(\"eval_f1\", 0.0))\n",
    "    acc = float(metrics.get(\"eval_accuracy\", 0.0))\n",
    "\n",
    "    # Extra trial-level logging\n",
    "    wandb.log({\"trial/f1\": f1, \"trial/accuracy\": acc})\n",
    "    wb.summary[\"best_model_ckpt\"] = trainer.state.best_model_checkpoint\n",
    "    wb.summary[\"best_eval_f1\"] = f1\n",
    "    wb.summary[\"params\"] = dict(lr=lr, weight_decay=wd, unfreeze_last_k=k, batch_size=bs, epochs=REFINE_EPOCHS)\n",
    "    wandb.finish()\n",
    "\n",
    "    print(f\"[TUNE-END] trial={trial.number} f1={f1:.4f}\")\n",
    "    return f1\n",
    "\n",
    "# Run the study\n",
    "print(f\"[Study] Starting Optuna: trials={N_TRIALS}, epochs/trial={REFINE_EPOCHS} ; centered at \"\n",
    "      f\"lr={LR:.2e}, wd={WEIGHT_DECAY:.1e}, k={UNFREEZE_LAST_K}, bs={BATCH_SIZE}\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# Persist best params\n",
    "best = {\n",
    "    \"lr\": float(study.best_trial.params[\"lr\"]),\n",
    "    \"weight_decay\": float(study.best_trial.params[\"weight_decay\"]),\n",
    "    \"num_unfreeze_last_layers\": int(study.best_trial.params[\"unfreeze_last_k\"]),\n",
    "    \"batch_size\": int(study.best_trial.params[\"batch_size\"]),\n",
    "    \"epochs\": int(EPOCHS),   # you can bump this later for final train if you want\n",
    "}\n",
    "os.makedirs(\"hf_ckpts\", exist_ok=True)\n",
    "with open(\"hf_ckpts/best_params_optuna1.json\", \"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "\n",
    "print(\"[Study best]\", best)\n",
    "print(\"Saved best params ‚Üí hf_ckpts/best_params_optuna1.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b4529-d064-4fd9-9ca8-6821ff16efa9",
   "metadata": {},
   "source": [
    "# üìä Ex.5 ‚Äî Optuna Trials: First Results  \n",
    "\n",
    "We ran **10 trials** with Optuna on top of the HuggingFace Trainer. The goal was to see if the ‚Äúsweet spot‚Äù we found earlier (low LR, deep unfreezing) would still hold.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîë Main Observations\n",
    "- **Learning rate**:  \n",
    "  The good runs are still **between `1e-5` and `9e-5`**.  \n",
    "  Too small (‚âà`1e-5`) ‚Üí model struggles, F1 < 0.6.  \n",
    "  Too large (‚â•`1e-4`) ‚Üí model basically collapses (trial 10 got F1 ‚âà 0.08 üò¨).  \n",
    "\n",
    "- **Unfreezing**:  \n",
    "  Best trials need **10‚Äì12 layers unfrozen**.  \n",
    "  When we unfreeze fewer layers (‚â§9), performance drops sharply.  \n",
    "\n",
    "- **Overall performance**:  \n",
    "  Only **two trials (2 and 5)** crossed **0.85 F1**, while most others stayed in the 0.59‚Äì0.74 range.  \n",
    "  So we‚Äôre **on the right track** but not as strong/consistent as before.  \n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ Best Trials\n",
    "- **Trial 2** ‚Üí LR ‚âà 9e-5, WD ‚âà 2e-6, batch size = 32, unfreeze = 12 ‚Üí **F1 = 0.852**  \n",
    "- **Trial 5** ‚Üí LR ‚âà 9.5e-5, WD ‚âà 2e-5, batch size = 16, unfreeze = 11 ‚Üí **F1 = 0.852**  \n",
    "\n",
    "Both show that **higher LR (but still <1e-4)** + **deep unfreezing** works best.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Takeaways\n",
    "- The **sweet spot (LR, unfreezing)** is consistent with earlier experiments.  \n",
    "- But the results are **less stable** this time ‚Üí some configs just collapse.  \n",
    "- We should:  \n",
    "  - Run **more trials (20‚Äì30)** to confirm the pattern.  \n",
    "  - Maybe tune **warmup ratio/scheduler** for stability.  \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6596d95d52307bb",
   "metadata": {},
   "source": [
    "# Another try with higher lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749990ddf8142d92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T03:24:56.026829Z",
     "start_time": "2025-08-17T01:34:57.053976Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-17 04:34:57,147] A new study created in memory with name: no-name-4f0d5460-fa46-45df-86f5-9b8bd36a09cf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Study] Starting Optuna: trials=15, epochs/trial=8 ; centered at lr=3.00e-05, wd=5.0e-02, k=10, bs=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find ex5_trainer_new.ipynb.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_043457-2gfpbaqh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/2gfpbaqh' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t10</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/2gfpbaqh' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/2gfpbaqh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=10 | epochs=8 bs=16 lr=3.88e-04 wd=9.1e-06 k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=16 lr=3.88e-04 wd=9.1e-06 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/2315] loss=1.5958 lr=3.45e-05\n",
      "{'loss': 1.5958, 'grad_norm': 1.8046875, 'learning_rate': 3.451588945865563e-05, 'epoch': 0.04319654427645788}\n",
      "[e0 b200/2315] loss=1.5764 lr=6.94e-05\n",
      "{'loss': 1.5764, 'grad_norm': 3.734375, 'learning_rate': 6.938042426537849e-05, 'epoch': 0.08639308855291576}\n",
      "[e0 b300/2315] loss=1.4845 lr=1.04e-04\n",
      "{'loss': 1.4845, 'grad_norm': 6.0, 'learning_rate': 0.00010424495907210135, 'epoch': 0.12958963282937366}\n",
      "[e0 b400/2315] loss=1.3034 lr=1.39e-04\n",
      "{'loss': 1.3034, 'grad_norm': 7.46875, 'learning_rate': 0.0001391094938788242, 'epoch': 0.17278617710583152}\n",
      "[e0 b500/2315] loss=1.2625 lr=1.74e-04\n",
      "{'loss': 1.2625, 'grad_norm': 9.5, 'learning_rate': 0.00017397402868554707, 'epoch': 0.2159827213822894}\n",
      "[e0 b600/2315] loss=1.5507 lr=2.09e-04\n",
      "{'loss': 1.5507, 'grad_norm': 3.28125, 'learning_rate': 0.00020883856349226992, 'epoch': 0.2591792656587473}\n",
      "[e0 b700/2315] loss=1.5851 lr=2.44e-04\n",
      "{'loss': 1.5851, 'grad_norm': 4.6875, 'learning_rate': 0.0002437030982989928, 'epoch': 0.3023758099352052}\n",
      "[e0 b800/2315] loss=1.5937 lr=2.79e-04\n",
      "{'loss': 1.5937, 'grad_norm': 2.875, 'learning_rate': 0.00027856763310571566, 'epoch': 0.34557235421166305}\n",
      "[e0 b900/2315] loss=1.5811 lr=3.13e-04\n",
      "{'loss': 1.5811, 'grad_norm': 1.8828125, 'learning_rate': 0.0003134321679124385, 'epoch': 0.38876889848812096}\n",
      "[e0 b1000/2315] loss=1.6047 lr=3.48e-04\n",
      "{'loss': 1.6047, 'grad_norm': 0.9453125, 'learning_rate': 0.00034829670271916134, 'epoch': 0.4319654427645788}\n",
      "[e0 b1100/2315] loss=1.5921 lr=3.83e-04\n",
      "{'loss': 1.5921, 'grad_norm': 1.2734375, 'learning_rate': 0.00038316123752588424, 'epoch': 0.47516198704103674}\n",
      "[e0 b1200/2315] loss=1.5861 lr=3.86e-04\n",
      "{'loss': 1.5861, 'grad_norm': 0.703125, 'learning_rate': 0.0003857560497556401, 'epoch': 0.5183585313174947}\n",
      "[e0 b1300/2315] loss=1.5795 lr=3.84e-04\n",
      "{'loss': 1.5795, 'grad_norm': 0.94140625, 'learning_rate': 0.0003835289494164239, 'epoch': 0.5615550755939525}\n",
      "[e0 b1400/2315] loss=1.5791 lr=3.81e-04\n",
      "{'loss': 1.5791, 'grad_norm': 0.87890625, 'learning_rate': 0.0003813018490772077, 'epoch': 0.6047516198704104}\n",
      "[e0 b1500/2315] loss=1.5782 lr=3.79e-04\n",
      "{'loss': 1.5782, 'grad_norm': 1.1953125, 'learning_rate': 0.00037907474873799144, 'epoch': 0.6479481641468683}\n",
      "[e0 b1600/2315] loss=1.5833 lr=3.77e-04\n",
      "{'loss': 1.5833, 'grad_norm': 0.9921875, 'learning_rate': 0.00037684764839877524, 'epoch': 0.6911447084233261}\n",
      "[e0 b1700/2315] loss=1.5800 lr=3.75e-04\n",
      "{'loss': 1.58, 'grad_norm': 0.7421875, 'learning_rate': 0.00037462054805955904, 'epoch': 0.734341252699784}\n",
      "[e0 b1800/2315] loss=1.5667 lr=3.72e-04\n",
      "{'loss': 1.5667, 'grad_norm': 0.87109375, 'learning_rate': 0.00037239344772034284, 'epoch': 0.7775377969762419}\n",
      "[e0 b1900/2315] loss=1.6035 lr=3.70e-04\n",
      "{'loss': 1.6035, 'grad_norm': 0.66015625, 'learning_rate': 0.00037016634738112664, 'epoch': 0.8207343412526998}\n",
      "[e0 b2000/2315] loss=1.5706 lr=3.68e-04\n",
      "{'loss': 1.5706, 'grad_norm': 1.34375, 'learning_rate': 0.00036793924704191044, 'epoch': 0.8639308855291576}\n",
      "[e0 b2100/2315] loss=1.5812 lr=3.66e-04\n",
      "{'loss': 1.5812, 'grad_norm': 0.5078125, 'learning_rate': 0.00036571214670269423, 'epoch': 0.9071274298056156}\n",
      "[e0 b2200/2315] loss=1.5818 lr=3.63e-04\n",
      "{'loss': 1.5818, 'grad_norm': 0.76171875, 'learning_rate': 0.000363485046363478, 'epoch': 0.9503239740820735}\n",
      "[e0 b2300/2315] loss=1.5769 lr=3.61e-04\n",
      "{'loss': 1.5769, 'grad_norm': 0.578125, 'learning_rate': 0.0003612579460242617, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.5764610767364502, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 4.2247, 'eval_samples_per_second': 974.26, 'eval_steps_per_second': 61.069, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5817, 'grad_norm': 0.91015625, 'learning_rate': 0.0003590308456850455, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.5842, 'grad_norm': 1.59375, 'learning_rate': 0.0003568037453458293, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.5788, 'grad_norm': 1.3984375, 'learning_rate': 0.0003545766450066131, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.5664, 'grad_norm': 0.388671875, 'learning_rate': 0.0003523495446673969, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.5661, 'grad_norm': 0.6484375, 'learning_rate': 0.0003501224443281807, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.5865, 'grad_norm': 0.78515625, 'learning_rate': 0.0003478953439889645, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.5799, 'grad_norm': 0.8671875, 'learning_rate': 0.00034566824364974826, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.5798, 'grad_norm': 0.55078125, 'learning_rate': 0.00034344114331053205, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.5824, 'grad_norm': 0.455078125, 'learning_rate': 0.00034121404297131585, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.5736, 'grad_norm': 0.330078125, 'learning_rate': 0.00033898694263209965, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.5637, 'grad_norm': 0.46875, 'learning_rate': 0.00033675984229288345, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.5967, 'grad_norm': 0.7421875, 'learning_rate': 0.00033453274195366725, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.5717, 'grad_norm': 0.53515625, 'learning_rate': 0.00033230564161445105, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.5794, 'grad_norm': 0.625, 'learning_rate': 0.0003300785412752348, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.5829, 'grad_norm': 0.62109375, 'learning_rate': 0.0003278514409360186, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.5668, 'grad_norm': 0.8671875, 'learning_rate': 0.00032562434059680233, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.5718, 'grad_norm': 0.484375, 'learning_rate': 0.00032339724025758613, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.5741, 'grad_norm': 0.365234375, 'learning_rate': 0.00032117013991836993, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.5865, 'grad_norm': 0.45703125, 'learning_rate': 0.00031894303957915373, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.5662, 'grad_norm': 0.7578125, 'learning_rate': 0.0003167159392399375, 'epoch': 1.857451403887689}\n",
      "{'loss': 1.5797, 'grad_norm': 0.75390625, 'learning_rate': 0.0003144888389007213, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.5823, 'grad_norm': 0.6953125, 'learning_rate': 0.00031226173856150507, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.571, 'grad_norm': 0.294921875, 'learning_rate': 0.00031003463822228887, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 1.5754207372665405, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.9196, 'eval_samples_per_second': 1050.113, 'eval_steps_per_second': 65.823, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5801, 'grad_norm': 0.43359375, 'learning_rate': 0.00030780753788307267, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.5838, 'grad_norm': 0.54296875, 'learning_rate': 0.00030558043754385646, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.5781, 'grad_norm': 0.2890625, 'learning_rate': 0.00030335333720464026, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.5771, 'grad_norm': 0.439453125, 'learning_rate': 0.00030112623686542406, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.5773, 'grad_norm': 0.294921875, 'learning_rate': 0.00029889913652620786, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.5665, 'grad_norm': 0.4609375, 'learning_rate': 0.0002966720361869916, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.5668, 'grad_norm': 0.50390625, 'learning_rate': 0.0002944449358477754, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.5723, 'grad_norm': 0.64453125, 'learning_rate': 0.0002922178355085592, 'epoch': 2.3326133909287257}\n",
      "{'loss': 1.581, 'grad_norm': 0.478515625, 'learning_rate': 0.000289990735169343, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.581, 'grad_norm': 0.283203125, 'learning_rate': 0.00028776363483012674, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.5711, 'grad_norm': 0.578125, 'learning_rate': 0.00028553653449091054, 'epoch': 2.462203023758099}\n",
      "{'loss': 1.5741, 'grad_norm': 0.60546875, 'learning_rate': 0.00028330943415169434, 'epoch': 2.5053995680345573}\n",
      "{'loss': 1.5757, 'grad_norm': 0.20703125, 'learning_rate': 0.00028108233381247814, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.5754, 'grad_norm': 0.470703125, 'learning_rate': 0.0002788552334732619, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.582, 'grad_norm': 0.455078125, 'learning_rate': 0.0002766281331340457, 'epoch': 2.634989200863931}\n",
      "{'loss': 1.5785, 'grad_norm': 0.5390625, 'learning_rate': 0.0002744010327948295, 'epoch': 2.6781857451403885}\n",
      "{'loss': 1.5781, 'grad_norm': 0.703125, 'learning_rate': 0.0002721739324556133, 'epoch': 2.7213822894168467}\n",
      "{'loss': 1.5781, 'grad_norm': 0.2470703125, 'learning_rate': 0.0002699468321163971, 'epoch': 2.7645788336933044}\n",
      "{'loss': 1.5745, 'grad_norm': 0.310546875, 'learning_rate': 0.0002677197317771809, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.5746, 'grad_norm': 0.34765625, 'learning_rate': 0.00026549263143796467, 'epoch': 2.85097192224622}\n",
      "{'loss': 1.5769, 'grad_norm': 0.447265625, 'learning_rate': 0.0002632655310987484, 'epoch': 2.8941684665226783}\n",
      "{'loss': 1.5759, 'grad_norm': 0.43359375, 'learning_rate': 0.0002610384307595322, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.5769, 'grad_norm': 0.498046875, 'learning_rate': 0.000258811330420316, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 1.5755220651626587, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 4.0897, 'eval_samples_per_second': 1006.429, 'eval_steps_per_second': 63.085, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.572, 'grad_norm': 0.2001953125, 'learning_rate': 0.0002565842300810998, 'epoch': 3.023758099352052}\n",
      "{'loss': 1.5761, 'grad_norm': 0.3828125, 'learning_rate': 0.0002543571297418836, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.5623, 'grad_norm': 0.53125, 'learning_rate': 0.0002521300294026674, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.5763, 'grad_norm': 0.51171875, 'learning_rate': 0.00024990292906345115, 'epoch': 3.1533477321814254}\n",
      "{'loss': 1.5854, 'grad_norm': 0.392578125, 'learning_rate': 0.00024767582872423495, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.5933, 'grad_norm': 0.78125, 'learning_rate': 0.0002454487283850187, 'epoch': 3.239740820734341}\n",
      "{'loss': 1.5673, 'grad_norm': 0.2578125, 'learning_rate': 0.00024322162804580252, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.5759, 'grad_norm': 0.283203125, 'learning_rate': 0.0002409945277065863, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.5796, 'grad_norm': 0.57421875, 'learning_rate': 0.0002387674273673701, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.5801, 'grad_norm': 0.61328125, 'learning_rate': 0.0002365403270281539, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.5766, 'grad_norm': 0.494140625, 'learning_rate': 0.00023431322668893769, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.5635, 'grad_norm': 0.68359375, 'learning_rate': 0.00023208612634972146, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.5777, 'grad_norm': 0.435546875, 'learning_rate': 0.00022985902601050526, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.5652, 'grad_norm': 0.59765625, 'learning_rate': 0.00022763192567128905, 'epoch': 3.5853131749460045}\n",
      "{'loss': 1.5712, 'grad_norm': 0.45703125, 'learning_rate': 0.00022540482533207283, 'epoch': 3.6285097192224622}\n",
      "{'loss': 1.5706, 'grad_norm': 0.47265625, 'learning_rate': 0.00022317772499285662, 'epoch': 3.67170626349892}\n",
      "{'loss': 1.5708, 'grad_norm': 0.44921875, 'learning_rate': 0.00022095062465364042, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.5752, 'grad_norm': 0.484375, 'learning_rate': 0.0002187235243144242, 'epoch': 3.7580993520518358}\n",
      "{'loss': 1.5691, 'grad_norm': 0.52734375, 'learning_rate': 0.000216496423975208, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.5786, 'grad_norm': 0.625, 'learning_rate': 0.0002142693236359918, 'epoch': 3.8444924406047516}\n",
      "{'loss': 1.5853, 'grad_norm': 0.55859375, 'learning_rate': 0.00021204222329677553, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.5772, 'grad_norm': 0.73046875, 'learning_rate': 0.00020981512295755933, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.5819, 'grad_norm': 0.59765625, 'learning_rate': 0.0002075880226183431, 'epoch': 3.974082073434125}\n",
      "{'eval_loss': 1.5753023624420166, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.9328, 'eval_samples_per_second': 1046.596, 'eval_steps_per_second': 65.603, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5759, 'grad_norm': 0.38671875, 'learning_rate': 0.0002053609222791269, 'epoch': 4.017278617710583}\n",
      "{'loss': 1.5683, 'grad_norm': 0.40234375, 'learning_rate': 0.0002031338219399107, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.567, 'grad_norm': 0.421875, 'learning_rate': 0.0002009067216006945, 'epoch': 4.103671706263499}\n",
      "{'loss': 1.5686, 'grad_norm': 0.52734375, 'learning_rate': 0.00019867962126147827, 'epoch': 4.146868250539957}\n",
      "{'loss': 1.5685, 'grad_norm': 0.439453125, 'learning_rate': 0.00019645252092226207, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.571, 'grad_norm': 0.412109375, 'learning_rate': 0.00019422542058304587, 'epoch': 4.233261339092873}\n",
      "{'loss': 1.5784, 'grad_norm': 0.6171875, 'learning_rate': 0.00019199832024382964, 'epoch': 4.27645788336933}\n",
      "{'loss': 1.5757, 'grad_norm': 0.51171875, 'learning_rate': 0.00018977121990461344, 'epoch': 4.319654427645788}\n",
      "{'loss': 1.5744, 'grad_norm': 0.470703125, 'learning_rate': 0.00018754411956539723, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.5813, 'grad_norm': 0.6171875, 'learning_rate': 0.00018531701922618103, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.5763, 'grad_norm': 0.2314453125, 'learning_rate': 0.00018308991888696478, 'epoch': 4.449244060475162}\n",
      "{'loss': 1.5704, 'grad_norm': 0.40625, 'learning_rate': 0.00018086281854774858, 'epoch': 4.49244060475162}\n",
      "{'loss': 1.5794, 'grad_norm': 0.515625, 'learning_rate': 0.00017863571820853237, 'epoch': 4.535637149028078}\n",
      "{'loss': 1.5795, 'grad_norm': 0.48046875, 'learning_rate': 0.00017640861786931617, 'epoch': 4.578833693304536}\n",
      "{'loss': 1.5802, 'grad_norm': 0.416015625, 'learning_rate': 0.00017418151753009994, 'epoch': 4.622030237580994}\n",
      "{'loss': 1.5794, 'grad_norm': 0.6328125, 'learning_rate': 0.00017195441719088374, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.5792, 'grad_norm': 0.28125, 'learning_rate': 0.00016972731685166754, 'epoch': 4.708423326133909}\n",
      "{'loss': 1.5748, 'grad_norm': 0.453125, 'learning_rate': 0.0001675002165124513, 'epoch': 4.751619870410368}\n",
      "{'loss': 1.5787, 'grad_norm': 0.54296875, 'learning_rate': 0.00016527311617323508, 'epoch': 4.794816414686825}\n",
      "{'loss': 1.5728, 'grad_norm': 0.443359375, 'learning_rate': 0.00016304601583401888, 'epoch': 4.838012958963283}\n",
      "{'loss': 1.5832, 'grad_norm': 0.57421875, 'learning_rate': 0.00016081891549480268, 'epoch': 4.881209503239741}\n",
      "{'loss': 1.5639, 'grad_norm': 0.298828125, 'learning_rate': 0.00015859181515558645, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.5751, 'grad_norm': 0.7421875, 'learning_rate': 0.00015636471481637025, 'epoch': 4.967602591792657}\n",
      "{'eval_loss': 1.575175166130066, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.8469, 'eval_samples_per_second': 1069.953, 'eval_steps_per_second': 67.067, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'train_runtime': 499.3979, 'train_samples_per_second': 593.338, 'train_steps_per_second': 37.085, 'train_loss': 1.5712170304695936, 'epoch': 5.0}\n",
      "{'eval_loss': 1.5764610767364502, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.9007, 'eval_samples_per_second': 1055.198, 'eval_steps_per_second': 66.142, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñà</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñá‚ñÉ‚ñÜ‚ñà‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñá‚ñÉ‚ñÜ‚ñà‚ñá</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÜ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.08688</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.27745</td></tr><tr><td>eval/f1</td><td>0.08688</td></tr><tr><td>eval/loss</td><td>1.57646</td></tr><tr><td>eval/precision</td><td>0.05549</td></tr><tr><td>eval/recall</td><td>0.2</td></tr><tr><td>eval/runtime</td><td>3.9007</td></tr><tr><td>eval/samples_per_second</td><td>1055.198</td></tr><tr><td>eval/steps_per_second</td><td>66.142</td></tr><tr><td>total_flos</td><td>7868409591839808.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>11575</td></tr><tr><td>train/grad_norm</td><td>0.74219</td></tr><tr><td>train/learning_rate</td><td>0.00016</td></tr><tr><td>train/loss</td><td>1.5751</td></tr><tr><td>train_loss</td><td>1.57122</td></tr><tr><td>train_runtime</td><td>499.3979</td></tr><tr><td>train_samples_per_second</td><td>593.338</td></tr><tr><td>train_steps_per_second</td><td>37.085</td></tr><tr><td>trial/accuracy</td><td>0.27745</td></tr><tr><td>trial/f1</td><td>0.08688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t10</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/2gfpbaqh' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/2gfpbaqh</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_043457-2gfpbaqh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0868771:   7%|‚ñã         | 1/15 [08:28<1:58:45, 508.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=10 f1=0.0869\n",
      "[I 2025-08-17 04:43:26,129] Trial 0 finished with value: 0.08687713959680486 and parameters: {'lr': 0.0003876936270507582, 'weight_decay': 9.064634974566982e-06, 'unfreeze_last_k': 11, 'batch_size': 16}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_044326-wuvh9m00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/wuvh9m00' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t11</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/wuvh9m00' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/wuvh9m00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=11 | epochs=8 bs=8 lr=3.22e-04 wd=4.9e-05 k=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=8 lr=3.22e-04 wd=4.9e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/4630] loss=1.6171 lr=1.44e-05\n",
      "{'loss': 1.6171, 'grad_norm': 3.546875, 'learning_rate': 1.4360509850171916e-05, 'epoch': 0.02159827213822894}\n",
      "[e0 b200/4630] loss=1.5949 lr=2.89e-05\n",
      "{'loss': 1.5949, 'grad_norm': 5.0625, 'learning_rate': 2.8866075355396078e-05, 'epoch': 0.04319654427645788}\n",
      "[e0 b300/4630] loss=1.5492 lr=4.34e-05\n",
      "{'loss': 1.5492, 'grad_norm': 9.8125, 'learning_rate': 4.3371640860620236e-05, 'epoch': 0.06479481641468683}\n",
      "[e0 b400/4630] loss=1.5356 lr=5.79e-05\n",
      "{'loss': 1.5356, 'grad_norm': 6.21875, 'learning_rate': 5.78772063658444e-05, 'epoch': 0.08639308855291576}\n",
      "[e0 b500/4630] loss=1.5311 lr=7.24e-05\n",
      "{'loss': 1.5311, 'grad_norm': 5.03125, 'learning_rate': 7.238277187106855e-05, 'epoch': 0.1079913606911447}\n",
      "[e0 b600/4630] loss=1.6071 lr=8.69e-05\n",
      "{'loss': 1.6071, 'grad_norm': 4.09375, 'learning_rate': 8.688833737629271e-05, 'epoch': 0.12958963282937366}\n",
      "[e0 b700/4630] loss=1.5662 lr=1.01e-04\n",
      "{'loss': 1.5662, 'grad_norm': 8.1875, 'learning_rate': 0.00010139390288151689, 'epoch': 0.1511879049676026}\n",
      "[e0 b800/4630] loss=1.5267 lr=1.16e-04\n",
      "{'loss': 1.5267, 'grad_norm': 4.125, 'learning_rate': 0.00011589946838674104, 'epoch': 0.17278617710583152}\n",
      "[e0 b900/4630] loss=1.4833 lr=1.30e-04\n",
      "{'loss': 1.4833, 'grad_norm': 6.34375, 'learning_rate': 0.00013040503389196518, 'epoch': 0.19438444924406048}\n",
      "[e0 b1000/4630] loss=1.4988 lr=1.45e-04\n",
      "{'loss': 1.4988, 'grad_norm': 11.125, 'learning_rate': 0.00014491059939718935, 'epoch': 0.2159827213822894}\n",
      "[e0 b1100/4630] loss=1.4566 lr=1.59e-04\n",
      "{'loss': 1.4566, 'grad_norm': 5.78125, 'learning_rate': 0.0001594161649024135, 'epoch': 0.23758099352051837}\n",
      "[e0 b1200/4630] loss=1.3889 lr=1.74e-04\n",
      "{'loss': 1.3889, 'grad_norm': 4.46875, 'learning_rate': 0.00017392173040763767, 'epoch': 0.2591792656587473}\n",
      "[e0 b1300/4630] loss=1.5100 lr=1.88e-04\n",
      "{'loss': 1.51, 'grad_norm': 8.0625, 'learning_rate': 0.00018842729591286183, 'epoch': 0.28077753779697623}\n",
      "[e0 b1400/4630] loss=1.4622 lr=2.03e-04\n",
      "{'loss': 1.4622, 'grad_norm': 3.265625, 'learning_rate': 0.00020293286141808602, 'epoch': 0.3023758099352052}\n",
      "[e0 b1500/4630] loss=1.5447 lr=2.17e-04\n",
      "{'loss': 1.5447, 'grad_norm': 2.0, 'learning_rate': 0.00021743842692331015, 'epoch': 0.32397408207343414}\n",
      "[e0 b1600/4630] loss=1.5358 lr=2.32e-04\n",
      "{'loss': 1.5358, 'grad_norm': 1.875, 'learning_rate': 0.00023194399242853432, 'epoch': 0.34557235421166305}\n",
      "[e0 b1700/4630] loss=1.5120 lr=2.46e-04\n",
      "{'loss': 1.512, 'grad_norm': 1.8046875, 'learning_rate': 0.0002464495579337585, 'epoch': 0.367170626349892}\n",
      "[e0 b1800/4630] loss=1.5514 lr=2.61e-04\n",
      "{'loss': 1.5514, 'grad_norm': 1.6796875, 'learning_rate': 0.0002609551234389826, 'epoch': 0.38876889848812096}\n",
      "[e0 b1900/4630] loss=1.5988 lr=2.75e-04\n",
      "{'loss': 1.5988, 'grad_norm': 1.9609375, 'learning_rate': 0.00027546068894420675, 'epoch': 0.4103671706263499}\n",
      "[e0 b2000/4630] loss=1.6090 lr=2.90e-04\n",
      "{'loss': 1.609, 'grad_norm': 0.8515625, 'learning_rate': 0.00028996625444943093, 'epoch': 0.4319654427645788}\n",
      "[e0 b2100/4630] loss=1.5806 lr=3.04e-04\n",
      "{'loss': 1.5806, 'grad_norm': 1.859375, 'learning_rate': 0.0003044718199546551, 'epoch': 0.4535637149028078}\n",
      "[e0 b2200/4630] loss=1.5958 lr=3.19e-04\n",
      "{'loss': 1.5958, 'grad_norm': 0.66015625, 'learning_rate': 0.00031897738545987926, 'epoch': 0.47516198704103674}\n",
      "[e0 b2300/4630] loss=1.5815 lr=3.22e-04\n",
      "{'loss': 1.5815, 'grad_norm': 1.3125, 'learning_rate': 0.0003217548448330914, 'epoch': 0.49676025917926564}\n",
      "[e0 b2400/4630] loss=1.5904 lr=3.21e-04\n",
      "{'loss': 1.5904, 'grad_norm': 1.6171875, 'learning_rate': 0.00032082869174356293, 'epoch': 0.5183585313174947}\n",
      "[e0 b2500/4630] loss=1.5764 lr=3.20e-04\n",
      "{'loss': 1.5764, 'grad_norm': 1.1796875, 'learning_rate': 0.00031990253865403446, 'epoch': 0.5399568034557235}\n",
      "[e0 b2600/4630] loss=1.5801 lr=3.19e-04\n",
      "{'loss': 1.5801, 'grad_norm': 0.94921875, 'learning_rate': 0.00031897638556450594, 'epoch': 0.5615550755939525}\n",
      "[e0 b2700/4630] loss=1.5754 lr=3.18e-04\n",
      "{'loss': 1.5754, 'grad_norm': 1.859375, 'learning_rate': 0.0003180502324749775, 'epoch': 0.5831533477321814}\n",
      "[e0 b2800/4630] loss=1.5890 lr=3.17e-04\n",
      "{'loss': 1.589, 'grad_norm': 1.8203125, 'learning_rate': 0.00031712407938544896, 'epoch': 0.6047516198704104}\n",
      "[e0 b2900/4630] loss=1.5738 lr=3.16e-04\n",
      "{'loss': 1.5738, 'grad_norm': 1.296875, 'learning_rate': 0.0003161979262959205, 'epoch': 0.6263498920086393}\n",
      "[e0 b3000/4630] loss=1.5856 lr=3.15e-04\n",
      "{'loss': 1.5856, 'grad_norm': 1.609375, 'learning_rate': 0.00031527177320639203, 'epoch': 0.6479481641468683}\n",
      "[e0 b3100/4630] loss=1.6012 lr=3.14e-04\n",
      "{'loss': 1.6012, 'grad_norm': 1.8671875, 'learning_rate': 0.0003143456201168635, 'epoch': 0.6695464362850972}\n",
      "[e0 b3200/4630] loss=1.5606 lr=3.13e-04\n",
      "{'loss': 1.5606, 'grad_norm': 1.25, 'learning_rate': 0.00031341946702733505, 'epoch': 0.6911447084233261}\n",
      "[e0 b3300/4630] loss=1.5781 lr=3.12e-04\n",
      "{'loss': 1.5781, 'grad_norm': 1.3515625, 'learning_rate': 0.0003124933139378066, 'epoch': 0.712742980561555}\n",
      "[e0 b3400/4630] loss=1.5879 lr=3.12e-04\n",
      "{'loss': 1.5879, 'grad_norm': 1.3828125, 'learning_rate': 0.00031156716084827806, 'epoch': 0.734341252699784}\n",
      "[e0 b3500/4630] loss=1.5744 lr=3.11e-04\n",
      "{'loss': 1.5744, 'grad_norm': 0.97265625, 'learning_rate': 0.00031064100775874954, 'epoch': 0.755939524838013}\n",
      "[e0 b3600/4630] loss=1.5606 lr=3.10e-04\n",
      "{'loss': 1.5606, 'grad_norm': 1.671875, 'learning_rate': 0.0003097148546692211, 'epoch': 0.7775377969762419}\n",
      "[e0 b3700/4630] loss=1.6078 lr=3.09e-04\n",
      "{'loss': 1.6078, 'grad_norm': 1.5390625, 'learning_rate': 0.0003087887015796926, 'epoch': 0.7991360691144709}\n",
      "[e0 b3800/4630] loss=1.6006 lr=3.08e-04\n",
      "{'loss': 1.6006, 'grad_norm': 1.3515625, 'learning_rate': 0.00030786254849016415, 'epoch': 0.8207343412526998}\n",
      "[e0 b3900/4630] loss=1.5621 lr=3.07e-04\n",
      "{'loss': 1.5621, 'grad_norm': 1.796875, 'learning_rate': 0.00030693639540063563, 'epoch': 0.8423326133909287}\n",
      "[e0 b4000/4630] loss=1.5840 lr=3.06e-04\n",
      "{'loss': 1.584, 'grad_norm': 2.34375, 'learning_rate': 0.00030601024231110717, 'epoch': 0.8639308855291576}\n",
      "[e0 b4100/4630] loss=1.5938 lr=3.05e-04\n",
      "{'loss': 1.5938, 'grad_norm': 1.5078125, 'learning_rate': 0.00030508408922157865, 'epoch': 0.8855291576673866}\n",
      "[e0 b4200/4630] loss=1.5706 lr=3.04e-04\n",
      "{'loss': 1.5706, 'grad_norm': 0.58203125, 'learning_rate': 0.0003041579361320502, 'epoch': 0.9071274298056156}\n",
      "[e0 b4300/4630] loss=1.5749 lr=3.03e-04\n",
      "{'loss': 1.5749, 'grad_norm': 1.6796875, 'learning_rate': 0.00030323178304252166, 'epoch': 0.9287257019438445}\n",
      "[e0 b4400/4630] loss=1.5965 lr=3.02e-04\n",
      "{'loss': 1.5965, 'grad_norm': 1.375, 'learning_rate': 0.0003023056299529932, 'epoch': 0.9503239740820735}\n",
      "[e0 b4500/4630] loss=1.5693 lr=3.01e-04\n",
      "{'loss': 1.5693, 'grad_norm': 0.92578125, 'learning_rate': 0.00030137947686346473, 'epoch': 0.9719222462203023}\n",
      "[e0 b4600/4630] loss=1.5882 lr=3.00e-04\n",
      "{'loss': 1.5882, 'grad_norm': 0.69921875, 'learning_rate': 0.00030045332377393627, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.5790148973464966, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 7.5783, 'eval_samples_per_second': 543.132, 'eval_steps_per_second': 67.957, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.576, 'grad_norm': 0.96875, 'learning_rate': 0.00029952717068440775, 'epoch': 1.0151187904967602}\n",
      "{'loss': 1.5884, 'grad_norm': 1.140625, 'learning_rate': 0.00029860101759487923, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.5915, 'grad_norm': 0.9609375, 'learning_rate': 0.00029767486450535077, 'epoch': 1.0583153347732182}\n",
      "{'loss': 1.5791, 'grad_norm': 1.2890625, 'learning_rate': 0.0002967487114158223, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.5876, 'grad_norm': 1.0390625, 'learning_rate': 0.0002958225583262938, 'epoch': 1.101511879049676}\n",
      "{'loss': 1.5724, 'grad_norm': 1.25, 'learning_rate': 0.0002948964052367653, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.5603, 'grad_norm': 1.4140625, 'learning_rate': 0.0002939702521472368, 'epoch': 1.144708423326134}\n",
      "{'loss': 1.5754, 'grad_norm': 0.9609375, 'learning_rate': 0.00029304409905770833, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.5738, 'grad_norm': 1.0390625, 'learning_rate': 0.00029211794596817987, 'epoch': 1.187904967602592}\n",
      "{'loss': 1.5633, 'grad_norm': 0.75, 'learning_rate': 0.00029119179287865135, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.5916, 'grad_norm': 1.390625, 'learning_rate': 0.0002902656397891229, 'epoch': 1.2311015118790496}\n",
      "{'loss': 1.5816, 'grad_norm': 1.046875, 'learning_rate': 0.0002893394866995944, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.5902, 'grad_norm': 1.078125, 'learning_rate': 0.0002884133336100659, 'epoch': 1.2742980561555075}\n",
      "{'loss': 1.5729, 'grad_norm': 1.1953125, 'learning_rate': 0.0002874871805205374, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.5789, 'grad_norm': 1.171875, 'learning_rate': 0.0002865610274310089, 'epoch': 1.3174946004319654}\n",
      "{'loss': 1.5837, 'grad_norm': 1.0859375, 'learning_rate': 0.00028563487434148045, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.5756, 'grad_norm': 1.0078125, 'learning_rate': 0.000284708721251952, 'epoch': 1.3606911447084233}\n",
      "{'loss': 1.5914, 'grad_norm': 0.82421875, 'learning_rate': 0.00028378256816242347, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.5808, 'grad_norm': 0.9140625, 'learning_rate': 0.000282856415072895, 'epoch': 1.4038876889848813}\n",
      "{'loss': 1.5686, 'grad_norm': 0.875, 'learning_rate': 0.0002819302619833665, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.5759, 'grad_norm': 0.55859375, 'learning_rate': 0.000281004108893838, 'epoch': 1.4470842332613392}\n",
      "{'loss': 1.5533, 'grad_norm': 0.734375, 'learning_rate': 0.00028007795580430956, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.6076, 'grad_norm': 1.703125, 'learning_rate': 0.00027915180271478104, 'epoch': 1.490280777537797}\n",
      "{'loss': 1.589, 'grad_norm': 1.078125, 'learning_rate': 0.00027822564962525257, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.5731, 'grad_norm': 1.265625, 'learning_rate': 0.0002772994965357241, 'epoch': 1.5334773218142548}\n",
      "{'loss': 1.5712, 'grad_norm': 0.7578125, 'learning_rate': 0.0002763733434461956, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.5903, 'grad_norm': 0.84375, 'learning_rate': 0.00027544719035666707, 'epoch': 1.5766738660907127}\n",
      "{'loss': 1.5692, 'grad_norm': 0.58203125, 'learning_rate': 0.0002745210372671386, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.571, 'grad_norm': 1.140625, 'learning_rate': 0.00027359488417761014, 'epoch': 1.6198704103671706}\n",
      "{'loss': 1.5961, 'grad_norm': 0.6484375, 'learning_rate': 0.0002726687310880817, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.5667, 'grad_norm': 0.9921875, 'learning_rate': 0.00027174257799855316, 'epoch': 1.6630669546436285}\n",
      "{'loss': 1.5676, 'grad_norm': 1.1953125, 'learning_rate': 0.00027081642490902464, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.5823, 'grad_norm': 0.58203125, 'learning_rate': 0.00026989027181949617, 'epoch': 1.7062634989200864}\n",
      "{'loss': 1.5643, 'grad_norm': 0.734375, 'learning_rate': 0.0002689641187299677, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.5793, 'grad_norm': 1.171875, 'learning_rate': 0.0002680379656404392, 'epoch': 1.7494600431965441}\n",
      "{'loss': 1.5677, 'grad_norm': 1.0859375, 'learning_rate': 0.0002671118125509107, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.5933, 'grad_norm': 1.2421875, 'learning_rate': 0.00026618565946138226, 'epoch': 1.7926565874730023}\n",
      "{'loss': 1.5796, 'grad_norm': 1.0390625, 'learning_rate': 0.0002652595063718538, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.5649, 'grad_norm': 0.73046875, 'learning_rate': 0.0002643333532823253, 'epoch': 1.83585313174946}\n",
      "{'loss': 1.5703, 'grad_norm': 1.1484375, 'learning_rate': 0.00026340720019279675, 'epoch': 1.857451403887689}\n",
      "{'loss': 1.5787, 'grad_norm': 0.8046875, 'learning_rate': 0.0002624810471032683, 'epoch': 1.8790496760259179}\n",
      "{'loss': 1.5786, 'grad_norm': 1.203125, 'learning_rate': 0.0002615548940137398, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.5783, 'grad_norm': 0.921875, 'learning_rate': 0.0002606287409242113, 'epoch': 1.9222462203023758}\n",
      "{'loss': 1.5877, 'grad_norm': 0.79296875, 'learning_rate': 0.00025970258783468284, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.5689, 'grad_norm': 0.9453125, 'learning_rate': 0.0002587764347451543, 'epoch': 1.9654427645788337}\n",
      "{'loss': 1.5755, 'grad_norm': 0.98046875, 'learning_rate': 0.00025785028165562586, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 1.5755865573883057, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 7.7319, 'eval_samples_per_second': 532.341, 'eval_steps_per_second': 66.607, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5964, 'grad_norm': 0.71484375, 'learning_rate': 0.0002569241285660974, 'epoch': 2.0086393088552916}\n",
      "{'loss': 1.5653, 'grad_norm': 0.484375, 'learning_rate': 0.0002559979754765689, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.58, 'grad_norm': 0.859375, 'learning_rate': 0.0002550718223870404, 'epoch': 2.0518358531317493}\n",
      "{'loss': 1.5862, 'grad_norm': 1.3984375, 'learning_rate': 0.00025414566929751194, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.5682, 'grad_norm': 0.8515625, 'learning_rate': 0.0002532195162079834, 'epoch': 2.0950323974082075}\n",
      "{'loss': 1.5881, 'grad_norm': 1.21875, 'learning_rate': 0.0002522933631184549, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.5643, 'grad_norm': 1.234375, 'learning_rate': 0.00025136721002892644, 'epoch': 2.138228941684665}\n",
      "{'loss': 1.5918, 'grad_norm': 0.515625, 'learning_rate': 0.000250441056939398, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.5752, 'grad_norm': 0.79296875, 'learning_rate': 0.0002495149038498695, 'epoch': 2.1814254859611233}\n",
      "{'loss': 1.5806, 'grad_norm': 0.73828125, 'learning_rate': 0.000248588750760341, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.5643, 'grad_norm': 0.90234375, 'learning_rate': 0.00024766259767081253, 'epoch': 2.224622030237581}\n",
      "{'loss': 1.5687, 'grad_norm': 0.78125, 'learning_rate': 0.000246736444581284, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.5675, 'grad_norm': 0.734375, 'learning_rate': 0.00024581029149175554, 'epoch': 2.267818574514039}\n",
      "{'loss': 1.5681, 'grad_norm': 1.015625, 'learning_rate': 0.0002448841384022271, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.5759, 'grad_norm': 1.296875, 'learning_rate': 0.00024395798531269856, 'epoch': 2.311015118790497}\n",
      "{'loss': 1.5679, 'grad_norm': 1.0859375, 'learning_rate': 0.0002430318322231701, 'epoch': 2.3326133909287257}\n",
      "{'loss': 1.5854, 'grad_norm': 0.87890625, 'learning_rate': 0.0002421056791336416, 'epoch': 2.3542116630669545}\n",
      "{'loss': 1.5765, 'grad_norm': 0.80078125, 'learning_rate': 0.00024117952604411308, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.5876, 'grad_norm': 0.78515625, 'learning_rate': 0.00024025337295458462, 'epoch': 2.3974082073434126}\n",
      "{'loss': 1.5764, 'grad_norm': 0.416015625, 'learning_rate': 0.00023932721986505613, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.579, 'grad_norm': 0.4375, 'learning_rate': 0.00023840106677552766, 'epoch': 2.4406047516198703}\n",
      "{'loss': 1.5628, 'grad_norm': 0.66796875, 'learning_rate': 0.00023747491368599917, 'epoch': 2.462203023758099}\n",
      "{'loss': 1.5793, 'grad_norm': 0.890625, 'learning_rate': 0.00023654876059647068, 'epoch': 2.4838012958963285}\n",
      "{'loss': 1.5692, 'grad_norm': 0.73046875, 'learning_rate': 0.0002356226075069422, 'epoch': 2.5053995680345573}\n",
      "{'loss': 1.5814, 'grad_norm': 1.0859375, 'learning_rate': 0.0002346964544174137, 'epoch': 2.526997840172786}\n",
      "{'loss': 1.5694, 'grad_norm': 0.74609375, 'learning_rate': 0.0002337703013278852, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.5714, 'grad_norm': 0.85546875, 'learning_rate': 0.00023284414823835674, 'epoch': 2.570194384449244}\n",
      "{'loss': 1.5799, 'grad_norm': 0.6640625, 'learning_rate': 0.00023191799514882825, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.5812, 'grad_norm': 0.8125, 'learning_rate': 0.00023099184205929978, 'epoch': 2.613390928725702}\n",
      "{'loss': 1.5822, 'grad_norm': 0.75, 'learning_rate': 0.00023006568896977126, 'epoch': 2.634989200863931}\n",
      "{'loss': 1.588, 'grad_norm': 1.21875, 'learning_rate': 0.00022913953588024277, 'epoch': 2.6565874730021597}\n",
      "{'loss': 1.5707, 'grad_norm': 0.90234375, 'learning_rate': 0.0002282133827907143, 'epoch': 2.6781857451403885}\n",
      "{'loss': 1.5828, 'grad_norm': 0.4140625, 'learning_rate': 0.00022728722970118581, 'epoch': 2.699784017278618}\n",
      "{'loss': 1.5752, 'grad_norm': 0.498046875, 'learning_rate': 0.00022636107661165732, 'epoch': 2.7213822894168467}\n",
      "{'loss': 1.5772, 'grad_norm': 0.70703125, 'learning_rate': 0.00022543492352212886, 'epoch': 2.7429805615550755}\n",
      "{'loss': 1.5797, 'grad_norm': 0.66796875, 'learning_rate': 0.00022450877043260037, 'epoch': 2.7645788336933044}\n",
      "{'loss': 1.5797, 'grad_norm': 1.1875, 'learning_rate': 0.00022358261734307185, 'epoch': 2.786177105831533}\n",
      "{'loss': 1.5694, 'grad_norm': 0.86328125, 'learning_rate': 0.00022265646425354338, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.5747, 'grad_norm': 0.5546875, 'learning_rate': 0.0002217303111640149, 'epoch': 2.8293736501079914}\n",
      "{'loss': 1.5737, 'grad_norm': 0.578125, 'learning_rate': 0.00022080415807448642, 'epoch': 2.85097192224622}\n",
      "{'loss': 1.5851, 'grad_norm': 0.9296875, 'learning_rate': 0.00021987800498495793, 'epoch': 2.8725701943844495}\n",
      "{'loss': 1.5688, 'grad_norm': 0.83203125, 'learning_rate': 0.00021895185189542944, 'epoch': 2.8941684665226783}\n",
      "{'loss': 1.5682, 'grad_norm': 0.61328125, 'learning_rate': 0.00021802569880590095, 'epoch': 2.915766738660907}\n",
      "{'loss': 1.5859, 'grad_norm': 0.84765625, 'learning_rate': 0.00021709954571637246, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.571, 'grad_norm': 0.546875, 'learning_rate': 0.00021617339262684397, 'epoch': 2.958963282937365}\n",
      "{'loss': 1.5833, 'grad_norm': 0.65625, 'learning_rate': 0.0002152472395373155, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 1.5757542848587036, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 7.8827, 'eval_samples_per_second': 522.155, 'eval_steps_per_second': 65.333, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5711, 'grad_norm': 0.5703125, 'learning_rate': 0.000214321086447787, 'epoch': 3.002159827213823}\n",
      "{'loss': 1.5737, 'grad_norm': 0.369140625, 'learning_rate': 0.00021339493335825854, 'epoch': 3.023758099352052}\n",
      "{'loss': 1.5724, 'grad_norm': 0.72265625, 'learning_rate': 0.00021246878026873002, 'epoch': 3.0453563714902807}\n",
      "{'loss': 1.5789, 'grad_norm': 1.0390625, 'learning_rate': 0.00021154262717920153, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.557, 'grad_norm': 1.15625, 'learning_rate': 0.00021061647408967307, 'epoch': 3.088552915766739}\n",
      "{'loss': 1.5686, 'grad_norm': 0.79296875, 'learning_rate': 0.00020969032100014458, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.584, 'grad_norm': 1.140625, 'learning_rate': 0.00020876416791061608, 'epoch': 3.1317494600431965}\n",
      "{'loss': 1.5689, 'grad_norm': 0.63671875, 'learning_rate': 0.00020783801482108762, 'epoch': 3.1533477321814254}\n",
      "{'loss': 1.582, 'grad_norm': 0.87890625, 'learning_rate': 0.00020691186173155913, 'epoch': 3.1749460043196542}\n",
      "{'loss': 1.5876, 'grad_norm': 0.87890625, 'learning_rate': 0.0002059857086420306, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.5867, 'grad_norm': 0.875, 'learning_rate': 0.00020505955555250214, 'epoch': 3.2181425485961124}\n",
      "{'loss': 1.6005, 'grad_norm': 0.9453125, 'learning_rate': 0.00020413340246297365, 'epoch': 3.239740820734341}\n",
      "{'loss': 1.5719, 'grad_norm': 0.87109375, 'learning_rate': 0.0002032072493734452, 'epoch': 3.26133909287257}\n",
      "{'loss': 1.5633, 'grad_norm': 0.8515625, 'learning_rate': 0.0002022810962839167, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.5826, 'grad_norm': 0.62109375, 'learning_rate': 0.0002013549431943882, 'epoch': 3.304535637149028}\n",
      "{'loss': 1.5725, 'grad_norm': 0.515625, 'learning_rate': 0.0002004287901048597, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.5836, 'grad_norm': 0.9609375, 'learning_rate': 0.00019950263701533122, 'epoch': 3.347732181425486}\n",
      "{'loss': 1.5749, 'grad_norm': 1.03125, 'learning_rate': 0.00019857648392580273, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.5688, 'grad_norm': 0.3515625, 'learning_rate': 0.00019765033083627426, 'epoch': 3.390928725701944}\n",
      "{'loss': 1.593, 'grad_norm': 1.171875, 'learning_rate': 0.00019672417774674577, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.576, 'grad_norm': 0.66796875, 'learning_rate': 0.0001957980246572173, 'epoch': 3.4341252699784017}\n",
      "{'loss': 1.5781, 'grad_norm': 0.59765625, 'learning_rate': 0.0001948718715676888, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.5629, 'grad_norm': 0.404296875, 'learning_rate': 0.0001939457184781603, 'epoch': 3.4773218142548594}\n",
      "{'loss': 1.566, 'grad_norm': 0.64453125, 'learning_rate': 0.00019301956538863183, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.5831, 'grad_norm': 1.171875, 'learning_rate': 0.00019209341229910334, 'epoch': 3.5205183585313176}\n",
      "{'loss': 1.5724, 'grad_norm': 0.84375, 'learning_rate': 0.00019116725920957485, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.5652, 'grad_norm': 0.734375, 'learning_rate': 0.00019024110612004638, 'epoch': 3.5637149028077753}\n",
      "{'loss': 1.5647, 'grad_norm': 1.15625, 'learning_rate': 0.00018931495303051786, 'epoch': 3.5853131749460045}\n",
      "{'loss': 1.5715, 'grad_norm': 0.58203125, 'learning_rate': 0.00018838879994098937, 'epoch': 3.6069114470842334}\n",
      "{'loss': 1.5736, 'grad_norm': 1.1171875, 'learning_rate': 0.0001874626468514609, 'epoch': 3.6285097192224622}\n",
      "{'loss': 1.5735, 'grad_norm': 0.5859375, 'learning_rate': 0.00018653649376193241, 'epoch': 3.650107991360691}\n",
      "{'loss': 1.5684, 'grad_norm': 0.59375, 'learning_rate': 0.00018561034067240395, 'epoch': 3.67170626349892}\n",
      "{'loss': 1.5739, 'grad_norm': 1.125, 'learning_rate': 0.00018468418758287546, 'epoch': 3.693304535637149}\n",
      "{'loss': 1.567, 'grad_norm': 0.8125, 'learning_rate': 0.00018375803449334697, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.5697, 'grad_norm': 1.3203125, 'learning_rate': 0.00018283188140381847, 'epoch': 3.736501079913607}\n",
      "{'loss': 1.5815, 'grad_norm': 0.6484375, 'learning_rate': 0.00018190572831428998, 'epoch': 3.7580993520518358}\n",
      "{'loss': 1.5699, 'grad_norm': 0.66796875, 'learning_rate': 0.0001809795752247615, 'epoch': 3.7796976241900646}\n",
      "{'loss': 1.5689, 'grad_norm': 0.57421875, 'learning_rate': 0.00018005342213523302, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.5783, 'grad_norm': 0.8671875, 'learning_rate': 0.00017912726904570453, 'epoch': 3.8228941684665227}\n",
      "{'loss': 1.578, 'grad_norm': 0.828125, 'learning_rate': 0.00017820111595617607, 'epoch': 3.8444924406047516}\n",
      "{'loss': 1.5796, 'grad_norm': 1.015625, 'learning_rate': 0.00017727496286664755, 'epoch': 3.8660907127429804}\n",
      "{'loss': 1.5935, 'grad_norm': 0.373046875, 'learning_rate': 0.00017634880977711906, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.577, 'grad_norm': 0.7578125, 'learning_rate': 0.0001754226566875906, 'epoch': 3.9092872570194386}\n",
      "{'loss': 1.5766, 'grad_norm': 1.21875, 'learning_rate': 0.0001744965035980621, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.5863, 'grad_norm': 0.5390625, 'learning_rate': 0.0001735703505085336, 'epoch': 3.9524838012958963}\n",
      "{'loss': 1.5782, 'grad_norm': 0.90234375, 'learning_rate': 0.00017264419741900514, 'epoch': 3.974082073434125}\n",
      "{'loss': 1.5755, 'grad_norm': 0.498046875, 'learning_rate': 0.00017171804432947662, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 1.5755267143249512, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 7.5477, 'eval_samples_per_second': 545.332, 'eval_steps_per_second': 68.233, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5773, 'grad_norm': 0.71875, 'learning_rate': 0.00017079189123994813, 'epoch': 4.017278617710583}\n",
      "{'loss': 1.5632, 'grad_norm': 0.57421875, 'learning_rate': 0.00016986573815041967, 'epoch': 4.038876889848812}\n",
      "{'loss': 1.5742, 'grad_norm': 0.47265625, 'learning_rate': 0.00016893958506089118, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.5682, 'grad_norm': 1.078125, 'learning_rate': 0.0001680134319713627, 'epoch': 4.08207343412527}\n",
      "{'loss': 1.5659, 'grad_norm': 0.453125, 'learning_rate': 0.00016708727888183422, 'epoch': 4.103671706263499}\n",
      "{'loss': 1.5604, 'grad_norm': 0.58984375, 'learning_rate': 0.00016616112579230573, 'epoch': 4.125269978401728}\n",
      "{'loss': 1.577, 'grad_norm': 1.046875, 'learning_rate': 0.0001652349727027772, 'epoch': 4.146868250539957}\n",
      "{'loss': 1.561, 'grad_norm': 0.75390625, 'learning_rate': 0.00016430881961324874, 'epoch': 4.168466522678186}\n",
      "{'loss': 1.5762, 'grad_norm': 0.80078125, 'learning_rate': 0.00016338266652372025, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.5763, 'grad_norm': 0.69921875, 'learning_rate': 0.0001624565134341918, 'epoch': 4.211663066954643}\n",
      "{'loss': 1.5667, 'grad_norm': 0.703125, 'learning_rate': 0.0001615303603446633, 'epoch': 4.233261339092873}\n",
      "{'loss': 1.5779, 'grad_norm': 1.1171875, 'learning_rate': 0.0001606042072551348, 'epoch': 4.254859611231102}\n",
      "{'loss': 1.5796, 'grad_norm': 0.69921875, 'learning_rate': 0.0001596780541656063, 'epoch': 4.27645788336933}\n",
      "{'loss': 1.5708, 'grad_norm': 0.7734375, 'learning_rate': 0.00015875190107607782, 'epoch': 4.29805615550756}\n",
      "{'loss': 1.582, 'grad_norm': 0.76171875, 'learning_rate': 0.00015782574798654935, 'epoch': 4.319654427645788}\n",
      "{'loss': 1.5662, 'grad_norm': 1.109375, 'learning_rate': 0.00015689959489702086, 'epoch': 4.341252699784017}\n",
      "{'loss': 1.584, 'grad_norm': 0.431640625, 'learning_rate': 0.00015597344180749237, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.5897, 'grad_norm': 0.5703125, 'learning_rate': 0.00015504728871796388, 'epoch': 4.384449244060475}\n",
      "{'loss': 1.573, 'grad_norm': 0.8046875, 'learning_rate': 0.00015412113562843541, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.5819, 'grad_norm': 0.8046875, 'learning_rate': 0.00015319498253890692, 'epoch': 4.427645788336933}\n",
      "{'loss': 1.5709, 'grad_norm': 0.55859375, 'learning_rate': 0.00015226882944937843, 'epoch': 4.449244060475162}\n",
      "{'loss': 1.5695, 'grad_norm': 0.80078125, 'learning_rate': 0.00015134267635984994, 'epoch': 4.470842332613391}\n",
      "{'loss': 1.5709, 'grad_norm': 0.4296875, 'learning_rate': 0.00015041652327032147, 'epoch': 4.49244060475162}\n",
      "{'loss': 1.5764, 'grad_norm': 0.52734375, 'learning_rate': 0.00014949037018079295, 'epoch': 4.514038876889849}\n",
      "{'loss': 1.584, 'grad_norm': 0.96484375, 'learning_rate': 0.0001485642170912645, 'epoch': 4.535637149028078}\n",
      "{'loss': 1.5812, 'grad_norm': 0.8046875, 'learning_rate': 0.000147638064001736, 'epoch': 4.557235421166307}\n",
      "{'loss': 1.5787, 'grad_norm': 0.875, 'learning_rate': 0.0001467119109122075, 'epoch': 4.578833693304536}\n",
      "{'loss': 1.5869, 'grad_norm': 0.66015625, 'learning_rate': 0.00014578575782267901, 'epoch': 4.600431965442764}\n",
      "{'loss': 1.5731, 'grad_norm': 1.09375, 'learning_rate': 0.00014485960473315055, 'epoch': 4.622030237580994}\n",
      "{'loss': 1.5768, 'grad_norm': 0.59765625, 'learning_rate': 0.00014393345164362203, 'epoch': 4.643628509719223}\n",
      "{'loss': 1.583, 'grad_norm': 0.99609375, 'learning_rate': 0.00014300729855409357, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.5912, 'grad_norm': 0.62109375, 'learning_rate': 0.00014208114546456507, 'epoch': 4.686825053995681}\n",
      "{'loss': 1.5686, 'grad_norm': 0.32421875, 'learning_rate': 0.00014115499237503658, 'epoch': 4.708423326133909}\n",
      "{'loss': 1.5784, 'grad_norm': 0.353515625, 'learning_rate': 0.00014022883928550812, 'epoch': 4.730021598272138}\n",
      "{'loss': 1.5712, 'grad_norm': 0.484375, 'learning_rate': 0.00013930268619597962, 'epoch': 4.751619870410368}\n",
      "{'loss': 1.582, 'grad_norm': 0.7109375, 'learning_rate': 0.00013837653310645113, 'epoch': 4.773218142548596}\n",
      "{'loss': 1.5755, 'grad_norm': 0.64453125, 'learning_rate': 0.00013745038001692264, 'epoch': 4.794816414686825}\n",
      "{'loss': 1.5747, 'grad_norm': 0.875, 'learning_rate': 0.00013652422692739418, 'epoch': 4.816414686825054}\n",
      "{'loss': 1.5708, 'grad_norm': 0.53515625, 'learning_rate': 0.00013559807383786568, 'epoch': 4.838012958963283}\n",
      "{'loss': 1.5754, 'grad_norm': 1.03125, 'learning_rate': 0.0001346719207483372, 'epoch': 4.859611231101512}\n",
      "{'loss': 1.5913, 'grad_norm': 0.87890625, 'learning_rate': 0.0001337457676588087, 'epoch': 4.881209503239741}\n",
      "{'loss': 1.5617, 'grad_norm': 0.90625, 'learning_rate': 0.00013281961456928024, 'epoch': 4.90280777537797}\n",
      "{'loss': 1.5663, 'grad_norm': 0.52734375, 'learning_rate': 0.00013189346147975172, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.5841, 'grad_norm': 0.94921875, 'learning_rate': 0.00013096730839022325, 'epoch': 4.946004319654428}\n",
      "{'loss': 1.5689, 'grad_norm': 1.0078125, 'learning_rate': 0.00013004115530069476, 'epoch': 4.967602591792657}\n",
      "{'loss': 1.584, 'grad_norm': 0.765625, 'learning_rate': 0.00012911500221116627, 'epoch': 4.989200863930885}\n",
      "{'eval_loss': 1.5754115581512451, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 7.5925, 'eval_samples_per_second': 542.111, 'eval_steps_per_second': 67.83, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'train_runtime': 897.1927, 'train_samples_per_second': 330.266, 'train_steps_per_second': 41.284, 'train_loss': 1.5733962884165555, 'epoch': 5.0}\n",
      "{'eval_loss': 1.5790148973464966, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 7.5962, 'eval_samples_per_second': 541.849, 'eval_steps_per_second': 67.797, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÖ‚ñà‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñÑ‚ñÅ‚ñà‚ñá‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñÑ‚ñÅ‚ñà‚ñá‚ñá</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ</td></tr><tr><td>train/loss</td><td>‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.08688</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.27745</td></tr><tr><td>eval/f1</td><td>0.08688</td></tr><tr><td>eval/loss</td><td>1.57901</td></tr><tr><td>eval/precision</td><td>0.05549</td></tr><tr><td>eval/recall</td><td>0.2</td></tr><tr><td>eval/runtime</td><td>7.5962</td></tr><tr><td>eval/samples_per_second</td><td>541.849</td></tr><tr><td>eval/steps_per_second</td><td>67.797</td></tr><tr><td>total_flos</td><td>7449764674720896.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>23150</td></tr><tr><td>train/grad_norm</td><td>0.76562</td></tr><tr><td>train/learning_rate</td><td>0.00013</td></tr><tr><td>train/loss</td><td>1.584</td></tr><tr><td>train_loss</td><td>1.5734</td></tr><tr><td>train_runtime</td><td>897.1927</td></tr><tr><td>train_samples_per_second</td><td>330.266</td></tr><tr><td>train_steps_per_second</td><td>41.284</td></tr><tr><td>trial/accuracy</td><td>0.27745</td></tr><tr><td>trial/f1</td><td>0.08688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t11</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/wuvh9m00' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/wuvh9m00</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_044326-wuvh9m00\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.0868771:  13%|‚ñà‚ñé        | 2/15 [23:43<2:41:54, 747.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=11 f1=0.0869\n",
      "[I 2025-08-17 04:58:40,220] Trial 1 finished with value: 0.08687713959680486 and parameters: {'lr': 0.00032245872118113306, 'weight_decay': 4.9425739199642315e-05, 'unfreeze_last_k': 9, 'batch_size': 8}. Best is trial 0 with value: 0.08687713959680486.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_045840-ou16m6fx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/ou16m6fx' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t12</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/ou16m6fx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/ou16m6fx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=12 | epochs=8 bs=16 lr=1.05e-04 wd=5.1e-05 k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=16 lr=1.05e-04 wd=5.1e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/2315] loss=1.6179 lr=9.34e-06\n",
      "{'loss': 1.6179, 'grad_norm': 1.9921875, 'learning_rate': 9.340105462722638e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b200/2315] loss=1.5963 lr=1.88e-05\n",
      "{'loss': 1.5963, 'grad_norm': 1.90625, 'learning_rate': 1.8774555425068738e-05, 'epoch': 0.08639308855291576}\n",
      "[e0 b300/2315] loss=1.5674 lr=2.82e-05\n",
      "{'loss': 1.5674, 'grad_norm': 5.5, 'learning_rate': 2.8209005387414836e-05, 'epoch': 0.12958963282937366}\n",
      "[e0 b400/2315] loss=1.5294 lr=3.76e-05\n",
      "{'loss': 1.5294, 'grad_norm': 7.1875, 'learning_rate': 3.7643455349760934e-05, 'epoch': 0.17278617710583152}\n",
      "[e0 b500/2315] loss=1.5100 lr=4.71e-05\n",
      "{'loss': 1.51, 'grad_norm': 10.6875, 'learning_rate': 4.707790531210703e-05, 'epoch': 0.2159827213822894}\n",
      "[e0 b600/2315] loss=1.4554 lr=5.65e-05\n",
      "{'loss': 1.4554, 'grad_norm': 8.8125, 'learning_rate': 5.651235527445313e-05, 'epoch': 0.2591792656587473}\n",
      "[e0 b700/2315] loss=1.3374 lr=6.59e-05\n",
      "{'loss': 1.3374, 'grad_norm': 11.625, 'learning_rate': 6.594680523679923e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b800/2315] loss=1.2506 lr=7.54e-05\n",
      "{'loss': 1.2506, 'grad_norm': 9.3125, 'learning_rate': 7.538125519914532e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b900/2315] loss=1.1343 lr=8.48e-05\n",
      "{'loss': 1.1343, 'grad_norm': 12.6875, 'learning_rate': 8.481570516149142e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1000/2315] loss=1.0971 lr=9.43e-05\n",
      "{'loss': 1.0971, 'grad_norm': 14.625, 'learning_rate': 9.425015512383752e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b1100/2315] loss=1.0272 lr=1.04e-04\n",
      "{'loss': 1.0272, 'grad_norm': 11.3125, 'learning_rate': 0.00010368460508618362, 'epoch': 0.47516198704103674}\n",
      "[e0 b1200/2315] loss=0.9483 lr=1.04e-04\n",
      "{'loss': 0.9483, 'grad_norm': 14.1875, 'learning_rate': 0.00010438676922745291, 'epoch': 0.5183585313174947}\n",
      "[e0 b1300/2315] loss=0.9559 lr=1.04e-04\n",
      "{'loss': 0.9559, 'grad_norm': 6.5625, 'learning_rate': 0.00010378410905063024, 'epoch': 0.5615550755939525}\n",
      "[e0 b1400/2315] loss=0.8890 lr=1.03e-04\n",
      "{'loss': 0.889, 'grad_norm': 9.5625, 'learning_rate': 0.00010318144887380759, 'epoch': 0.6047516198704104}\n",
      "[e0 b1500/2315] loss=0.8702 lr=1.03e-04\n",
      "{'loss': 0.8702, 'grad_norm': 14.125, 'learning_rate': 0.00010257878869698492, 'epoch': 0.6479481641468683}\n",
      "[e0 b1600/2315] loss=0.8678 lr=1.02e-04\n",
      "{'loss': 0.8678, 'grad_norm': 11.75, 'learning_rate': 0.00010197612852016227, 'epoch': 0.6911447084233261}\n",
      "[e0 b1700/2315] loss=0.7914 lr=1.01e-04\n",
      "{'loss': 0.7914, 'grad_norm': 9.1875, 'learning_rate': 0.00010137346834333961, 'epoch': 0.734341252699784}\n",
      "[e0 b1800/2315] loss=0.7842 lr=1.01e-04\n",
      "{'loss': 0.7842, 'grad_norm': 9.8125, 'learning_rate': 0.00010077080816651694, 'epoch': 0.7775377969762419}\n",
      "[e0 b1900/2315] loss=0.7078 lr=1.00e-04\n",
      "{'loss': 0.7078, 'grad_norm': 13.125, 'learning_rate': 0.00010016814798969429, 'epoch': 0.8207343412526998}\n",
      "[e0 b2000/2315] loss=0.7748 lr=9.96e-05\n",
      "{'loss': 0.7748, 'grad_norm': 10.5, 'learning_rate': 9.956548781287163e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b2100/2315] loss=0.7298 lr=9.90e-05\n",
      "{'loss': 0.7298, 'grad_norm': 9.5625, 'learning_rate': 9.896282763604897e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b2200/2315] loss=0.6859 lr=9.84e-05\n",
      "{'loss': 0.6859, 'grad_norm': 7.03125, 'learning_rate': 9.83601674592263e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b2300/2315] loss=0.6870 lr=9.78e-05\n",
      "{'loss': 0.687, 'grad_norm': 7.46875, 'learning_rate': 9.775750728240364e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 0.6350731253623962, 'eval_accuracy': 0.7708940719144801, 'eval_precision': 0.7876651750775209, 'eval_recall': 0.7710614009323561, 'eval_f1': 0.77788448231029, 'eval_runtime': 4.2641, 'eval_samples_per_second': 965.278, 'eval_steps_per_second': 60.506, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.7709 f1=0.7779 p=0.7877 r=0.7711\n",
      "{'loss': 0.6323, 'grad_norm': 13.0625, 'learning_rate': 9.715484710558098e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 0.6078, 'grad_norm': 14.125, 'learning_rate': 9.655218692875833e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 0.6417, 'grad_norm': 17.625, 'learning_rate': 9.594952675193566e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 0.6008, 'grad_norm': 8.625, 'learning_rate': 9.534686657511301e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 0.561, 'grad_norm': 20.75, 'learning_rate': 9.474420639829034e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 0.5695, 'grad_norm': 15.8125, 'learning_rate': 9.414154622146769e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 0.6106, 'grad_norm': 5.5625, 'learning_rate': 9.353888604464503e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 0.5827, 'grad_norm': 10.0, 'learning_rate': 9.293622586782236e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 0.5645, 'grad_norm': 9.8125, 'learning_rate': 9.233356569099971e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 0.592, 'grad_norm': 8.75, 'learning_rate': 9.173090551417705e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 0.5406, 'grad_norm': 55.0, 'learning_rate': 9.11282453373544e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 0.4821, 'grad_norm': 18.875, 'learning_rate': 9.052558516053173e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 0.5309, 'grad_norm': 10.625, 'learning_rate': 8.992292498370908e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 0.558, 'grad_norm': 10.75, 'learning_rate': 8.932026480688641e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 0.566, 'grad_norm': 7.65625, 'learning_rate': 8.871760463006376e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 0.5015, 'grad_norm': 4.53125, 'learning_rate': 8.811494445324108e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 0.5582, 'grad_norm': 8.9375, 'learning_rate': 8.751228427641843e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 0.5195, 'grad_norm': 4.9375, 'learning_rate': 8.690962409959576e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 0.5237, 'grad_norm': 10.875, 'learning_rate': 8.63069639227731e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 0.5033, 'grad_norm': 4.875, 'learning_rate': 8.570430374595045e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 0.4952, 'grad_norm': 12.625, 'learning_rate': 8.510164356912778e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 0.4877, 'grad_norm': 16.25, 'learning_rate': 8.449898339230513e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 0.4407, 'grad_norm': 6.28125, 'learning_rate': 8.389632321548247e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 0.5285328030586243, 'eval_accuracy': 0.8107385811467445, 'eval_precision': 0.8272724746214589, 'eval_recall': 0.8120786665985124, 'eval_f1': 0.8164072388119141, 'eval_runtime': 3.9515, 'eval_samples_per_second': 1041.636, 'eval_steps_per_second': 65.292, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.8107 f1=0.8164 p=0.8273 r=0.8121\n",
      "{'loss': 0.4447, 'grad_norm': 11.3125, 'learning_rate': 8.329366303865981e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 0.4272, 'grad_norm': 14.5, 'learning_rate': 8.269100286183715e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 0.4628, 'grad_norm': 12.875, 'learning_rate': 8.20883426850145e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 0.4032, 'grad_norm': 3.359375, 'learning_rate': 8.148568250819183e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 0.4104, 'grad_norm': 8.0, 'learning_rate': 8.088302233136918e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 0.436, 'grad_norm': 7.84375, 'learning_rate': 8.028036215454652e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 0.4258, 'grad_norm': 14.125, 'learning_rate': 7.967770197772385e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 0.4002, 'grad_norm': 12.9375, 'learning_rate': 7.90750418009012e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 0.3957, 'grad_norm': 7.71875, 'learning_rate': 7.847238162407853e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 0.4202, 'grad_norm': 7.78125, 'learning_rate': 7.786972144725587e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 0.4448, 'grad_norm': 7.09375, 'learning_rate': 7.72670612704332e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 0.3845, 'grad_norm': 20.375, 'learning_rate': 7.666440109361055e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 0.4233, 'grad_norm': 8.375, 'learning_rate': 7.606174091678789e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 0.427, 'grad_norm': 9.75, 'learning_rate': 7.545908073996523e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 0.3742, 'grad_norm': 10.4375, 'learning_rate': 7.485642056314257e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 0.395, 'grad_norm': 33.0, 'learning_rate': 7.425376038631992e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 0.3812, 'grad_norm': 8.3125, 'learning_rate': 7.365110020949725e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 0.3797, 'grad_norm': 13.5625, 'learning_rate': 7.30484400326746e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 0.3713, 'grad_norm': 35.75, 'learning_rate': 7.244577985585194e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 0.4116, 'grad_norm': 12.9375, 'learning_rate': 7.184311967902927e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 0.4155, 'grad_norm': 2.71875, 'learning_rate': 7.124045950220662e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 0.3969, 'grad_norm': 18.375, 'learning_rate': 7.063779932538395e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 0.4216, 'grad_norm': 14.25, 'learning_rate': 7.00351391485613e-05, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 0.4956532120704651, 'eval_accuracy': 0.8328474246841594, 'eval_precision': 0.8338738809801673, 'eval_recall': 0.8461256618516181, 'eval_f1': 0.8364620972105863, 'eval_runtime': 4.0479, 'eval_samples_per_second': 1016.816, 'eval_steps_per_second': 63.736, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.8328 f1=0.8365 p=0.8339 r=0.8461\n",
      "{'loss': 0.3696, 'grad_norm': 12.125, 'learning_rate': 6.943247897173864e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 0.3346, 'grad_norm': 8.4375, 'learning_rate': 6.882981879491598e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 0.3321, 'grad_norm': 6.84375, 'learning_rate': 6.822715861809332e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 0.3231, 'grad_norm': 21.375, 'learning_rate': 6.762449844127065e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 0.3774, 'grad_norm': 7.96875, 'learning_rate': 6.702183826444799e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 0.3128, 'grad_norm': 8.3125, 'learning_rate': 6.641917808762534e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 0.3315, 'grad_norm': 21.125, 'learning_rate': 6.581651791080267e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 0.3848, 'grad_norm': 48.25, 'learning_rate': 6.521385773398002e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 0.3511, 'grad_norm': 14.625, 'learning_rate': 6.461119755715736e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 0.3306, 'grad_norm': 12.125, 'learning_rate': 6.400853738033469e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 0.3307, 'grad_norm': 15.375, 'learning_rate': 6.340587720351204e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 0.3303, 'grad_norm': 1.1953125, 'learning_rate': 6.280321702668937e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 0.3628, 'grad_norm': 11.0, 'learning_rate': 6.220055684986672e-05, 'epoch': 3.5421166306695464}\n",
      "{'loss': 0.329, 'grad_norm': 3.15625, 'learning_rate': 6.159789667304406e-05, 'epoch': 3.5853131749460045}\n",
      "{'loss': 0.3389, 'grad_norm': 15.375, 'learning_rate': 6.09952364962214e-05, 'epoch': 3.6285097192224622}\n",
      "{'loss': 0.3151, 'grad_norm': 7.125, 'learning_rate': 6.039257631939874e-05, 'epoch': 3.67170626349892}\n",
      "{'loss': 0.3462, 'grad_norm': 3.84375, 'learning_rate': 5.978991614257608e-05, 'epoch': 3.714902807775378}\n",
      "{'loss': 0.318, 'grad_norm': 5.75, 'learning_rate': 5.918725596575342e-05, 'epoch': 3.7580993520518358}\n",
      "{'loss': 0.3475, 'grad_norm': 1.7265625, 'learning_rate': 5.8584595788930764e-05, 'epoch': 3.801295896328294}\n",
      "{'loss': 0.3491, 'grad_norm': 11.0, 'learning_rate': 5.7981935612108106e-05, 'epoch': 3.8444924406047516}\n",
      "{'loss': 0.335, 'grad_norm': 9.625, 'learning_rate': 5.7379275435285434e-05, 'epoch': 3.8876889848812093}\n",
      "{'loss': 0.3419, 'grad_norm': 24.125, 'learning_rate': 5.6776615258462775e-05, 'epoch': 3.9308855291576674}\n",
      "{'loss': 0.3101, 'grad_norm': 11.125, 'learning_rate': 5.617395508164012e-05, 'epoch': 3.974082073434125}\n",
      "{'eval_loss': 0.49193626642227173, 'eval_accuracy': 0.8554421768707483, 'eval_precision': 0.8547633476697915, 'eval_recall': 0.8658719409788883, 'eval_f1': 0.8588243859157035, 'eval_runtime': 3.845, 'eval_samples_per_second': 1070.482, 'eval_steps_per_second': 67.1, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.8554 f1=0.8588 p=0.8548 r=0.8659\n",
      "{'loss': 0.3163, 'grad_norm': 28.25, 'learning_rate': 5.557129490481746e-05, 'epoch': 4.017278617710583}\n",
      "{'loss': 0.282, 'grad_norm': 8.75, 'learning_rate': 5.49686347279948e-05, 'epoch': 4.060475161987041}\n",
      "{'loss': 0.3485, 'grad_norm': 22.5, 'learning_rate': 5.436597455117214e-05, 'epoch': 4.103671706263499}\n",
      "{'loss': 0.299, 'grad_norm': 4.75, 'learning_rate': 5.3763314374349476e-05, 'epoch': 4.146868250539957}\n",
      "{'loss': 0.2904, 'grad_norm': 20.75, 'learning_rate': 5.316065419752682e-05, 'epoch': 4.190064794816415}\n",
      "{'loss': 0.332, 'grad_norm': 9.1875, 'learning_rate': 5.255799402070416e-05, 'epoch': 4.233261339092873}\n",
      "{'loss': 0.3013, 'grad_norm': 19.125, 'learning_rate': 5.19553338438815e-05, 'epoch': 4.27645788336933}\n",
      "{'loss': 0.2995, 'grad_norm': 14.9375, 'learning_rate': 5.135267366705884e-05, 'epoch': 4.319654427645788}\n",
      "{'loss': 0.3187, 'grad_norm': 35.25, 'learning_rate': 5.0750013490236184e-05, 'epoch': 4.362850971922247}\n",
      "{'loss': 0.3216, 'grad_norm': 22.625, 'learning_rate': 5.0147353313413526e-05, 'epoch': 4.406047516198704}\n",
      "{'loss': 0.3213, 'grad_norm': 15.75, 'learning_rate': 4.954469313659086e-05, 'epoch': 4.449244060475162}\n",
      "{'loss': 0.3021, 'grad_norm': 11.4375, 'learning_rate': 4.89420329597682e-05, 'epoch': 4.49244060475162}\n",
      "{'loss': 0.3143, 'grad_norm': 8.5625, 'learning_rate': 4.8339372782945543e-05, 'epoch': 4.535637149028078}\n",
      "{'loss': 0.2745, 'grad_norm': 7.65625, 'learning_rate': 4.7736712606122885e-05, 'epoch': 4.578833693304536}\n",
      "{'loss': 0.3007, 'grad_norm': 51.25, 'learning_rate': 4.713405242930022e-05, 'epoch': 4.622030237580994}\n",
      "{'loss': 0.2824, 'grad_norm': 9.6875, 'learning_rate': 4.653139225247756e-05, 'epoch': 4.665226781857451}\n",
      "{'loss': 0.3089, 'grad_norm': 25.75, 'learning_rate': 4.59287320756549e-05, 'epoch': 4.708423326133909}\n",
      "{'loss': 0.2685, 'grad_norm': 7.53125, 'learning_rate': 4.5326071898832245e-05, 'epoch': 4.751619870410368}\n",
      "{'loss': 0.3033, 'grad_norm': 15.0625, 'learning_rate': 4.472341172200958e-05, 'epoch': 4.794816414686825}\n",
      "{'loss': 0.3109, 'grad_norm': 12.25, 'learning_rate': 4.412075154518692e-05, 'epoch': 4.838012958963283}\n",
      "{'loss': 0.2742, 'grad_norm': 3.71875, 'learning_rate': 4.351809136836426e-05, 'epoch': 4.881209503239741}\n",
      "{'loss': 0.3221, 'grad_norm': 15.5, 'learning_rate': 4.2915431191541604e-05, 'epoch': 4.924406047516198}\n",
      "{'loss': 0.294, 'grad_norm': 14.6875, 'learning_rate': 4.2312771014718946e-05, 'epoch': 4.967602591792657}\n",
      "{'eval_loss': 0.500215470790863, 'eval_accuracy': 0.8513119533527697, 'eval_precision': 0.8494548201029868, 'eval_recall': 0.8652079041577359, 'eval_f1': 0.8551197422121005, 'eval_runtime': 4.1044, 'eval_samples_per_second': 1002.821, 'eval_steps_per_second': 62.859, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.8513 f1=0.8551 p=0.8495 r=0.8652\n",
      "{'loss': 0.3189, 'grad_norm': 4.53125, 'learning_rate': 4.171011083789629e-05, 'epoch': 5.010799136069115}\n",
      "{'loss': 0.2884, 'grad_norm': 8.9375, 'learning_rate': 4.110745066107363e-05, 'epoch': 5.053995680345572}\n",
      "{'loss': 0.2855, 'grad_norm': 36.0, 'learning_rate': 4.050479048425097e-05, 'epoch': 5.09719222462203}\n",
      "{'loss': 0.285, 'grad_norm': 11.75, 'learning_rate': 3.9902130307428305e-05, 'epoch': 5.140388768898488}\n",
      "{'loss': 0.2771, 'grad_norm': 1.6484375, 'learning_rate': 3.929947013060564e-05, 'epoch': 5.183585313174946}\n",
      "{'loss': 0.2856, 'grad_norm': 0.93359375, 'learning_rate': 3.869680995378298e-05, 'epoch': 5.226781857451404}\n",
      "{'loss': 0.289, 'grad_norm': 17.5, 'learning_rate': 3.809414977696032e-05, 'epoch': 5.269978401727862}\n",
      "{'loss': 0.2642, 'grad_norm': 10.0625, 'learning_rate': 3.7491489600137665e-05, 'epoch': 5.313174946004319}\n",
      "{'loss': 0.3015, 'grad_norm': 9.25, 'learning_rate': 3.6888829423315006e-05, 'epoch': 5.356371490280777}\n",
      "{'loss': 0.283, 'grad_norm': 37.0, 'learning_rate': 3.628616924649235e-05, 'epoch': 5.399568034557236}\n",
      "{'loss': 0.2814, 'grad_norm': 55.25, 'learning_rate': 3.568350906966969e-05, 'epoch': 5.442764578833693}\n",
      "{'loss': 0.2819, 'grad_norm': 24.875, 'learning_rate': 3.508084889284703e-05, 'epoch': 5.485961123110151}\n",
      "{'loss': 0.26, 'grad_norm': 10.6875, 'learning_rate': 3.4478188716024366e-05, 'epoch': 5.529157667386609}\n",
      "{'loss': 0.3305, 'grad_norm': 23.75, 'learning_rate': 3.387552853920171e-05, 'epoch': 5.572354211663067}\n",
      "{'loss': 0.292, 'grad_norm': 14.6875, 'learning_rate': 3.327286836237905e-05, 'epoch': 5.615550755939525}\n",
      "{'loss': 0.2909, 'grad_norm': 15.3125, 'learning_rate': 3.267020818555639e-05, 'epoch': 5.658747300215983}\n",
      "{'loss': 0.2714, 'grad_norm': 45.25, 'learning_rate': 3.2067548008733725e-05, 'epoch': 5.70194384449244}\n",
      "{'loss': 0.3025, 'grad_norm': 13.4375, 'learning_rate': 3.1464887831911067e-05, 'epoch': 5.745140388768899}\n",
      "{'loss': 0.2713, 'grad_norm': 11.75, 'learning_rate': 3.086222765508841e-05, 'epoch': 5.788336933045357}\n",
      "{'loss': 0.294, 'grad_norm': 16.375, 'learning_rate': 3.025956747826575e-05, 'epoch': 5.831533477321814}\n",
      "{'loss': 0.27, 'grad_norm': 9.0625, 'learning_rate': 2.965690730144309e-05, 'epoch': 5.874730021598272}\n",
      "{'loss': 0.3034, 'grad_norm': 13.375, 'learning_rate': 2.9054247124620426e-05, 'epoch': 5.91792656587473}\n",
      "{'loss': 0.2839, 'grad_norm': 14.25, 'learning_rate': 2.8451586947797768e-05, 'epoch': 5.961123110151188}\n",
      "{'eval_loss': 0.53461754322052, 'eval_accuracy': 0.8483965014577259, 'eval_precision': 0.8467033986629462, 'eval_recall': 0.8625786720920493, 'eval_f1': 0.852208417824883, 'eval_runtime': 4.1046, 'eval_samples_per_second': 1002.789, 'eval_steps_per_second': 62.857, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.8484 f1=0.8522 p=0.8467 r=0.8626\n",
      "{'loss': 0.2752, 'grad_norm': 2.328125, 'learning_rate': 2.784892677097511e-05, 'epoch': 6.004319654427646}\n",
      "{'loss': 0.2901, 'grad_norm': 2.046875, 'learning_rate': 2.724626659415245e-05, 'epoch': 6.047516198704104}\n",
      "{'loss': 0.2874, 'grad_norm': 6.78125, 'learning_rate': 2.664360641732979e-05, 'epoch': 6.090712742980561}\n",
      "{'loss': 0.3265, 'grad_norm': 20.625, 'learning_rate': 2.604094624050713e-05, 'epoch': 6.133909287257019}\n",
      "{'loss': 0.247, 'grad_norm': 11.8125, 'learning_rate': 2.543828606368447e-05, 'epoch': 6.177105831533478}\n",
      "{'loss': 0.2744, 'grad_norm': 20.625, 'learning_rate': 2.483562588686181e-05, 'epoch': 6.220302375809935}\n",
      "{'loss': 0.2837, 'grad_norm': 12.8125, 'learning_rate': 2.4232965710039152e-05, 'epoch': 6.263498920086393}\n",
      "{'loss': 0.2957, 'grad_norm': 4.53125, 'learning_rate': 2.363030553321649e-05, 'epoch': 6.306695464362851}\n",
      "{'loss': 0.2768, 'grad_norm': 17.25, 'learning_rate': 2.3027645356393828e-05, 'epoch': 6.3498920086393085}\n",
      "{'loss': 0.2877, 'grad_norm': 9.625, 'learning_rate': 2.242498517957117e-05, 'epoch': 6.393088552915767}\n",
      "{'loss': 0.263, 'grad_norm': 10.25, 'learning_rate': 2.182232500274851e-05, 'epoch': 6.436285097192225}\n",
      "{'loss': 0.2783, 'grad_norm': 0.87890625, 'learning_rate': 2.1219664825925853e-05, 'epoch': 6.479481641468682}\n",
      "{'loss': 0.2706, 'grad_norm': 6.03125, 'learning_rate': 2.0617004649103194e-05, 'epoch': 6.52267818574514}\n",
      "{'loss': 0.2611, 'grad_norm': 8.625, 'learning_rate': 2.0014344472280533e-05, 'epoch': 6.565874730021598}\n",
      "{'loss': 0.3234, 'grad_norm': 47.5, 'learning_rate': 1.941168429545787e-05, 'epoch': 6.609071274298056}\n",
      "{'loss': 0.2632, 'grad_norm': 31.75, 'learning_rate': 1.8809024118635212e-05, 'epoch': 6.652267818574514}\n",
      "{'loss': 0.3002, 'grad_norm': 15.8125, 'learning_rate': 1.8206363941812554e-05, 'epoch': 6.695464362850972}\n",
      "{'loss': 0.27, 'grad_norm': 11.1875, 'learning_rate': 1.7603703764989892e-05, 'epoch': 6.7386609071274295}\n",
      "{'loss': 0.2472, 'grad_norm': 9.625, 'learning_rate': 1.7001043588167234e-05, 'epoch': 6.781857451403888}\n",
      "{'loss': 0.3016, 'grad_norm': 10.9375, 'learning_rate': 1.6398383411344572e-05, 'epoch': 6.825053995680346}\n",
      "{'loss': 0.2969, 'grad_norm': 17.75, 'learning_rate': 1.5795723234521913e-05, 'epoch': 6.8682505399568035}\n",
      "{'loss': 0.2717, 'grad_norm': 9.0625, 'learning_rate': 1.5193063057699252e-05, 'epoch': 6.911447084233261}\n",
      "{'loss': 0.2633, 'grad_norm': 5.84375, 'learning_rate': 1.4590402880876593e-05, 'epoch': 6.954643628509719}\n",
      "{'loss': 0.2809, 'grad_norm': 12.1875, 'learning_rate': 1.3987742704053935e-05, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 0.5224191546440125, 'eval_accuracy': 0.8496112730806609, 'eval_precision': 0.847068196175293, 'eval_recall': 0.8642612332604397, 'eval_f1': 0.8532772539166895, 'eval_runtime': 3.872, 'eval_samples_per_second': 1063.005, 'eval_steps_per_second': 66.632, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.8496 f1=0.8533 p=0.8471 r=0.8643\n",
      "{'loss': 0.281, 'grad_norm': 2.8125, 'learning_rate': 1.3385082527231275e-05, 'epoch': 7.041036717062635}\n",
      "{'loss': 0.3058, 'grad_norm': 3.984375, 'learning_rate': 1.2782422350408614e-05, 'epoch': 7.084233261339093}\n",
      "{'loss': 0.2614, 'grad_norm': 9.1875, 'learning_rate': 1.2179762173585954e-05, 'epoch': 7.1274298056155505}\n",
      "{'loss': 0.2523, 'grad_norm': 27.0, 'learning_rate': 1.1577101996763294e-05, 'epoch': 7.170626349892009}\n",
      "{'loss': 0.3041, 'grad_norm': 25.0, 'learning_rate': 1.0974441819940636e-05, 'epoch': 7.213822894168467}\n",
      "{'loss': 0.282, 'grad_norm': 4.0625, 'learning_rate': 1.0371781643117976e-05, 'epoch': 7.2570194384449245}\n",
      "{'loss': 0.2462, 'grad_norm': 7.65625, 'learning_rate': 9.769121466295315e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 0.27, 'grad_norm': 16.375, 'learning_rate': 9.166461289472655e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 0.2702, 'grad_norm': 10.0625, 'learning_rate': 8.563801112649997e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 0.265, 'grad_norm': 12.5625, 'learning_rate': 7.961140935827335e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 0.2699, 'grad_norm': 2.1875, 'learning_rate': 7.358480759004677e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 0.2754, 'grad_norm': 19.625, 'learning_rate': 6.755820582182016e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 0.2649, 'grad_norm': 10.25, 'learning_rate': 6.153160405359357e-06, 'epoch': 7.559395248380129}\n",
      "{'loss': 0.2554, 'grad_norm': 9.25, 'learning_rate': 5.550500228536697e-06, 'epoch': 7.602591792656588}\n",
      "{'loss': 0.2526, 'grad_norm': 11.8125, 'learning_rate': 4.947840051714037e-06, 'epoch': 7.6457883369330455}\n",
      "{'loss': 0.2987, 'grad_norm': 10.75, 'learning_rate': 4.345179874891378e-06, 'epoch': 7.688984881209503}\n",
      "{'loss': 0.268, 'grad_norm': 45.5, 'learning_rate': 3.7425196980687175e-06, 'epoch': 7.732181425485961}\n",
      "{'loss': 0.2723, 'grad_norm': 17.5, 'learning_rate': 3.139859521246058e-06, 'epoch': 7.775377969762419}\n",
      "{'loss': 0.264, 'grad_norm': 17.875, 'learning_rate': 2.5371993444233977e-06, 'epoch': 7.818574514038877}\n",
      "{'loss': 0.2572, 'grad_norm': 1.8125, 'learning_rate': 1.934539167600738e-06, 'epoch': 7.861771058315335}\n",
      "{'loss': 0.2768, 'grad_norm': 9.6875, 'learning_rate': 1.3318789907780782e-06, 'epoch': 7.9049676025917925}\n",
      "{'loss': 0.2624, 'grad_norm': 32.0, 'learning_rate': 7.292188139554183e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 0.2864, 'grad_norm': 12.1875, 'learning_rate': 1.2655863713275857e-07, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 0.5223392248153687, 'eval_accuracy': 0.8520408163265306, 'eval_precision': 0.8498626023873767, 'eval_recall': 0.8657652902088493, 'eval_f1': 0.8555955437502114, 'eval_runtime': 4.1463, 'eval_samples_per_second': 992.704, 'eval_steps_per_second': 62.225, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8520 f1=0.8556 p=0.8499 r=0.8658\n",
      "{'train_runtime': 841.4052, 'train_samples_per_second': 352.163, 'train_steps_per_second': 22.011, 'train_loss': 0.4394344683084838, 'epoch': 8.0}\n",
      "{'eval_loss': 0.49193626642227173, 'eval_accuracy': 0.8554421768707483, 'eval_precision': 0.8547633476697915, 'eval_recall': 0.8658719409788883, 'eval_f1': 0.8588243859157035, 'eval_runtime': 4.1906, 'eval_samples_per_second': 982.196, 'eval_steps_per_second': 61.566, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8554 f1=0.8588 p=0.8548 r=0.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÉ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñá</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÇ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÇ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñà‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñá‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.85882</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.85544</td></tr><tr><td>eval/f1</td><td>0.85882</td></tr><tr><td>eval/loss</td><td>0.49194</td></tr><tr><td>eval/precision</td><td>0.85476</td></tr><tr><td>eval/recall</td><td>0.86587</td></tr><tr><td>eval/runtime</td><td>4.1906</td></tr><tr><td>eval/samples_per_second</td><td>982.196</td></tr><tr><td>eval/steps_per_second</td><td>61.566</td></tr><tr><td>total_flos</td><td>1.2584829318142752e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>18520</td></tr><tr><td>train/grad_norm</td><td>12.1875</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.2864</td></tr><tr><td>train_loss</td><td>0.43943</td></tr><tr><td>train_runtime</td><td>841.4052</td></tr><tr><td>train_samples_per_second</td><td>352.163</td></tr><tr><td>train_steps_per_second</td><td>22.011</td></tr><tr><td>trial/accuracy</td><td>0.85544</td></tr><tr><td>trial/f1</td><td>0.85882</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t12</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/ou16m6fx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/ou16m6fx</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_045840-ou16m6fx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.858824:  20%|‚ñà‚ñà        | 3/15 [37:53<2:38:51, 794.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=12 f1=0.8588\n",
      "[I 2025-08-17 05:12:50,514] Trial 2 finished with value: 0.8588243859157035 and parameters: {'lr': 0.00010491108358128862, 'weight_decay': 5.114700076417737e-05, 'unfreeze_last_k': 12, 'batch_size': 16}. Best is trial 2 with value: 0.8588243859157035.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_051250-nv60aodc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nv60aodc' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t13</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nv60aodc' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nv60aodc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=13 | epochs=8 bs=8 lr=9.48e-05 wd=6.0e-05 k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[Run] epochs=8 bs=8 lr=9.48e-05 wd=6.0e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/4630] loss=1.6395 lr=4.22e-06\n",
      "{'loss': 1.6395, 'grad_norm': 4.0625, 'learning_rate': 4.223401305424146e-06, 'epoch': 0.02159827213822894}\n",
      "[e0 b200/4630] loss=1.6430 lr=8.49e-06\n",
      "{'loss': 1.643, 'grad_norm': 5.03125, 'learning_rate': 8.489463230095e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b300/4630] loss=1.6378 lr=1.28e-05\n",
      "{'loss': 1.6378, 'grad_norm': 3.453125, 'learning_rate': 1.2755525154765854e-05, 'epoch': 0.06479481641468683}\n",
      "[e0 b400/4630] loss=1.6172 lr=1.70e-05\n",
      "{'loss': 1.6172, 'grad_norm': 2.953125, 'learning_rate': 1.702158707943671e-05, 'epoch': 0.08639308855291576}\n",
      "[e0 b500/4630] loss=1.5862 lr=2.13e-05\n",
      "{'loss': 1.5862, 'grad_norm': 6.8125, 'learning_rate': 2.1287649004107564e-05, 'epoch': 0.1079913606911447}\n",
      "[e0 b600/4630] loss=1.5688 lr=2.56e-05\n",
      "{'loss': 1.5688, 'grad_norm': 7.46875, 'learning_rate': 2.5553710928778416e-05, 'epoch': 0.12958963282937366}\n",
      "[e0 b700/4630] loss=1.5302 lr=2.98e-05\n",
      "{'loss': 1.5302, 'grad_norm': 9.5625, 'learning_rate': 2.9819772853449275e-05, 'epoch': 0.1511879049676026}\n",
      "[e0 b800/4630] loss=1.4666 lr=3.41e-05\n",
      "{'loss': 1.4666, 'grad_norm': 13.4375, 'learning_rate': 3.408583477812013e-05, 'epoch': 0.17278617710583152}\n",
      "[e0 b900/4630] loss=1.4161 lr=3.84e-05\n",
      "{'loss': 1.4161, 'grad_norm': 16.875, 'learning_rate': 3.835189670279098e-05, 'epoch': 0.19438444924406048}\n",
      "[e0 b1000/4630] loss=1.3933 lr=4.26e-05\n",
      "{'loss': 1.3933, 'grad_norm': 14.1875, 'learning_rate': 4.261795862746184e-05, 'epoch': 0.2159827213822894}\n",
      "[e0 b1100/4630] loss=1.3563 lr=4.69e-05\n",
      "{'loss': 1.3563, 'grad_norm': 13.6875, 'learning_rate': 4.688402055213269e-05, 'epoch': 0.23758099352051837}\n",
      "[e0 b1200/4630] loss=1.3046 lr=5.12e-05\n",
      "{'loss': 1.3046, 'grad_norm': 21.625, 'learning_rate': 5.115008247680354e-05, 'epoch': 0.2591792656587473}\n",
      "[e0 b1300/4630] loss=1.2851 lr=5.54e-05\n",
      "{'loss': 1.2851, 'grad_norm': 18.625, 'learning_rate': 5.541614440147439e-05, 'epoch': 0.28077753779697623}\n",
      "[e0 b1400/4630] loss=1.2570 lr=5.97e-05\n",
      "{'loss': 1.257, 'grad_norm': 17.75, 'learning_rate': 5.968220632614526e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b1500/4630] loss=1.2149 lr=6.39e-05\n",
      "{'loss': 1.2149, 'grad_norm': 28.875, 'learning_rate': 6.394826825081611e-05, 'epoch': 0.32397408207343414}\n",
      "[e0 b1600/4630] loss=1.1739 lr=6.82e-05\n",
      "{'loss': 1.1739, 'grad_norm': 17.75, 'learning_rate': 6.821433017548696e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b1700/4630] loss=1.1073 lr=7.25e-05\n",
      "{'loss': 1.1073, 'grad_norm': 17.625, 'learning_rate': 7.248039210015781e-05, 'epoch': 0.367170626349892}\n",
      "[e0 b1800/4630] loss=1.0554 lr=7.67e-05\n",
      "{'loss': 1.0554, 'grad_norm': 26.375, 'learning_rate': 7.674645402482867e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1900/4630] loss=1.0768 lr=8.10e-05\n",
      "{'loss': 1.0768, 'grad_norm': 12.25, 'learning_rate': 8.101251594949952e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b2000/4630] loss=1.0872 lr=8.53e-05\n",
      "{'loss': 1.0872, 'grad_norm': 28.5, 'learning_rate': 8.527857787417038e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b2100/4630] loss=0.9838 lr=8.95e-05\n",
      "{'loss': 0.9838, 'grad_norm': 25.25, 'learning_rate': 8.954463979884123e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b2200/4630] loss=1.0369 lr=9.38e-05\n",
      "{'loss': 1.0369, 'grad_norm': 39.75, 'learning_rate': 9.381070172351209e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b2300/4630] loss=0.9771 lr=9.46e-05\n",
      "{'loss': 0.9771, 'grad_norm': 19.25, 'learning_rate': 9.462754775927079e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b2400/4630] loss=0.9932 lr=9.44e-05\n",
      "{'loss': 0.9932, 'grad_norm': 36.75, 'learning_rate': 9.435516772484671e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b2500/4630] loss=0.9464 lr=9.41e-05\n",
      "{'loss': 0.9464, 'grad_norm': 12.25, 'learning_rate': 9.408278769042262e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b2600/4630] loss=0.9189 lr=9.38e-05\n",
      "{'loss': 0.9189, 'grad_norm': 14.5625, 'learning_rate': 9.381040765599854e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b2700/4630] loss=0.8846 lr=9.35e-05\n",
      "{'loss': 0.8846, 'grad_norm': 18.5, 'learning_rate': 9.353802762157446e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b2800/4630] loss=0.8829 lr=9.33e-05\n",
      "{'loss': 0.8829, 'grad_norm': 32.75, 'learning_rate': 9.326564758715036e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b2900/4630] loss=0.9556 lr=9.30e-05\n",
      "{'loss': 0.9556, 'grad_norm': 12.875, 'learning_rate': 9.299326755272628e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b3000/4630] loss=0.8117 lr=9.27e-05\n",
      "{'loss': 0.8117, 'grad_norm': 16.0, 'learning_rate': 9.27208875183022e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b3100/4630] loss=0.8630 lr=9.24e-05\n",
      "{'loss': 0.863, 'grad_norm': 12.5625, 'learning_rate': 9.244850748387812e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b3200/4630] loss=0.8771 lr=9.22e-05\n",
      "{'loss': 0.8771, 'grad_norm': 22.875, 'learning_rate': 9.217612744945405e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b3300/4630] loss=0.8232 lr=9.19e-05\n",
      "{'loss': 0.8232, 'grad_norm': 11.625, 'learning_rate': 9.190374741502996e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b3400/4630] loss=0.7426 lr=9.16e-05\n",
      "{'loss': 0.7426, 'grad_norm': 19.125, 'learning_rate': 9.163136738060587e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b3500/4630] loss=0.7875 lr=9.14e-05\n",
      "{'loss': 0.7875, 'grad_norm': 15.875, 'learning_rate': 9.135898734618178e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b3600/4630] loss=0.7685 lr=9.11e-05\n",
      "{'loss': 0.7685, 'grad_norm': 21.5, 'learning_rate': 9.10866073117577e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b3700/4630] loss=0.7654 lr=9.08e-05\n",
      "{'loss': 0.7654, 'grad_norm': 10.3125, 'learning_rate': 9.081422727733362e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b3800/4630] loss=0.6970 lr=9.05e-05\n",
      "{'loss': 0.697, 'grad_norm': 12.0, 'learning_rate': 9.054184724290954e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b3900/4630] loss=0.7820 lr=9.03e-05\n",
      "{'loss': 0.782, 'grad_norm': 18.875, 'learning_rate': 9.026946720848545e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b4000/4630] loss=0.7466 lr=9.00e-05\n",
      "{'loss': 0.7466, 'grad_norm': 24.0, 'learning_rate': 8.999708717406137e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b4100/4630] loss=0.7263 lr=8.97e-05\n",
      "{'loss': 0.7263, 'grad_norm': 11.25, 'learning_rate': 8.972470713963729e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b4200/4630] loss=0.7216 lr=8.95e-05\n",
      "{'loss': 0.7216, 'grad_norm': 19.625, 'learning_rate': 8.94523271052132e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b4300/4630] loss=0.7592 lr=8.92e-05\n",
      "{'loss': 0.7592, 'grad_norm': 29.875, 'learning_rate': 8.917994707078912e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b4400/4630] loss=0.7149 lr=8.89e-05\n",
      "{'loss': 0.7149, 'grad_norm': 16.0, 'learning_rate': 8.890756703636504e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b4500/4630] loss=0.7490 lr=8.86e-05\n",
      "{'loss': 0.749, 'grad_norm': 35.75, 'learning_rate': 8.863518700194096e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b4600/4630] loss=0.6303 lr=8.84e-05\n",
      "{'loss': 0.6303, 'grad_norm': 11.875, 'learning_rate': 8.836280696751688e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 0.7369186878204346, 'eval_accuracy': 0.7480563654033042, 'eval_precision': 0.7737891898353793, 'eval_recall': 0.7420562321745875, 'eval_f1': 0.7533018281377599, 'eval_runtime': 7.624, 'eval_samples_per_second': 539.874, 'eval_steps_per_second': 67.55, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.7481 f1=0.7533 p=0.7738 r=0.7421\n",
      "{'loss': 0.6646, 'grad_norm': 4.90625, 'learning_rate': 8.809042693309278e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 0.6312, 'grad_norm': 8.875, 'learning_rate': 8.78180468986687e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 0.6767, 'grad_norm': 9.0625, 'learning_rate': 8.754566686424461e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 0.6468, 'grad_norm': 9.4375, 'learning_rate': 8.727328682982055e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 0.6394, 'grad_norm': 17.25, 'learning_rate': 8.700090679539646e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 0.6338, 'grad_norm': 23.625, 'learning_rate': 8.672852676097238e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 0.5753, 'grad_norm': 16.125, 'learning_rate': 8.645614672654828e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 0.6225, 'grad_norm': 33.75, 'learning_rate': 8.61837666921242e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 0.6302, 'grad_norm': 17.25, 'learning_rate': 8.591138665770012e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 0.6019, 'grad_norm': 28.875, 'learning_rate': 8.563900662327604e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 0.5996, 'grad_norm': 22.875, 'learning_rate': 8.536662658885195e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 0.622, 'grad_norm': 15.5, 'learning_rate': 8.509424655442787e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 0.6498, 'grad_norm': 16.125, 'learning_rate': 8.482186652000379e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 0.6363, 'grad_norm': 18.25, 'learning_rate': 8.45494864855797e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 0.6529, 'grad_norm': 13.5, 'learning_rate': 8.427710645115562e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 0.5749, 'grad_norm': 13.0625, 'learning_rate': 8.400472641673154e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 0.561, 'grad_norm': 16.25, 'learning_rate': 8.373234638230746e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 0.61, 'grad_norm': 12.375, 'learning_rate': 8.345996634788337e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 0.6069, 'grad_norm': 7.84375, 'learning_rate': 8.318758631345929e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 0.6299, 'grad_norm': 16.125, 'learning_rate': 8.29152062790352e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 0.593, 'grad_norm': 33.75, 'learning_rate': 8.264282624461111e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 0.5781, 'grad_norm': 24.0, 'learning_rate': 8.237044621018703e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 0.5404, 'grad_norm': 30.5, 'learning_rate': 8.209806617576296e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 0.5122, 'grad_norm': 18.25, 'learning_rate': 8.182568614133888e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 0.5334, 'grad_norm': 20.375, 'learning_rate': 8.15533061069148e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 0.5782, 'grad_norm': 31.375, 'learning_rate': 8.12809260724907e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 0.5703, 'grad_norm': 15.5625, 'learning_rate': 8.100854603806662e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 0.5393, 'grad_norm': 43.75, 'learning_rate': 8.073616600364253e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 0.5645, 'grad_norm': 21.75, 'learning_rate': 8.046378596921845e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 0.5922, 'grad_norm': 18.0, 'learning_rate': 8.019140593479437e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 0.5556, 'grad_norm': 16.0, 'learning_rate': 7.991902590037029e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 0.4871, 'grad_norm': 6.46875, 'learning_rate': 7.96466458659462e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 0.5967, 'grad_norm': 12.8125, 'learning_rate': 7.937426583152212e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 0.5899, 'grad_norm': 18.25, 'learning_rate': 7.910188579709804e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 0.5404, 'grad_norm': 37.25, 'learning_rate': 7.882950576267396e-05, 'epoch': 1.7494600431965441}\n",
      "{'loss': 0.5621, 'grad_norm': 5.40625, 'learning_rate': 7.855712572824987e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 0.593, 'grad_norm': 10.75, 'learning_rate': 7.828474569382579e-05, 'epoch': 1.7926565874730023}\n",
      "{'loss': 0.5351, 'grad_norm': 15.6875, 'learning_rate': 7.801236565940171e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 0.4922, 'grad_norm': 11.625, 'learning_rate': 7.773998562497761e-05, 'epoch': 1.83585313174946}\n",
      "{'loss': 0.5821, 'grad_norm': 37.75, 'learning_rate': 7.746760559055353e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 0.5145, 'grad_norm': 14.5625, 'learning_rate': 7.719522555612946e-05, 'epoch': 1.8790496760259179}\n",
      "{'loss': 0.5161, 'grad_norm': 17.0, 'learning_rate': 7.692284552170538e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 0.5194, 'grad_norm': 17.375, 'learning_rate': 7.66504654872813e-05, 'epoch': 1.9222462203023758}\n",
      "{'loss': 0.5381, 'grad_norm': 22.875, 'learning_rate': 7.637808545285721e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 0.5322, 'grad_norm': 18.5, 'learning_rate': 7.610570541843312e-05, 'epoch': 1.9654427645788337}\n",
      "{'loss': 0.3988, 'grad_norm': 8.5, 'learning_rate': 7.583332538400903e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 0.5918283462524414, 'eval_accuracy': 0.814868804664723, 'eval_precision': 0.8208429446627049, 'eval_recall': 0.8250431230453635, 'eval_f1': 0.820727663155018, 'eval_runtime': 7.6387, 'eval_samples_per_second': 538.836, 'eval_steps_per_second': 67.42, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.8149 f1=0.8207 p=0.8208 r=0.8250\n",
      "{'loss': 0.5404, 'grad_norm': 4.96875, 'learning_rate': 7.556094534958495e-05, 'epoch': 2.0086393088552916}\n",
      "{'loss': 0.4624, 'grad_norm': 0.765625, 'learning_rate': 7.528856531516087e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 0.4598, 'grad_norm': 87.0, 'learning_rate': 7.501618528073679e-05, 'epoch': 2.0518358531317493}\n",
      "{'loss': 0.4781, 'grad_norm': 23.375, 'learning_rate': 7.47438052463127e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 0.4723, 'grad_norm': 0.98828125, 'learning_rate': 7.447142521188862e-05, 'epoch': 2.0950323974082075}\n",
      "{'loss': 0.4899, 'grad_norm': 23.0, 'learning_rate': 7.419904517746454e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 0.4689, 'grad_norm': 7.90625, 'learning_rate': 7.392666514304046e-05, 'epoch': 2.138228941684665}\n",
      "{'loss': 0.4354, 'grad_norm': 11.25, 'learning_rate': 7.365428510861637e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 0.4271, 'grad_norm': 14.75, 'learning_rate': 7.338190507419229e-05, 'epoch': 2.1814254859611233}\n",
      "{'loss': 0.4861, 'grad_norm': 1.328125, 'learning_rate': 7.310952503976821e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 0.4506, 'grad_norm': 21.625, 'learning_rate': 7.283714500534412e-05, 'epoch': 2.224622030237581}\n",
      "{'loss': 0.4476, 'grad_norm': 9.25, 'learning_rate': 7.256476497092003e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 0.5245, 'grad_norm': 2.1875, 'learning_rate': 7.229238493649596e-05, 'epoch': 2.267818574514039}\n",
      "{'loss': 0.388, 'grad_norm': 33.25, 'learning_rate': 7.202000490207188e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 0.4589, 'grad_norm': 2.140625, 'learning_rate': 7.17476248676478e-05, 'epoch': 2.311015118790497}\n",
      "{'loss': 0.4354, 'grad_norm': 31.25, 'learning_rate': 7.147524483322371e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 0.4462, 'grad_norm': 1.609375, 'learning_rate': 7.120286479879963e-05, 'epoch': 2.3542116630669545}\n",
      "{'loss': 0.4215, 'grad_norm': 8.875, 'learning_rate': 7.093048476437553e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 0.5044, 'grad_norm': 2.453125, 'learning_rate': 7.065810472995145e-05, 'epoch': 2.3974082073434126}\n",
      "{'loss': 0.438, 'grad_norm': 22.125, 'learning_rate': 7.038572469552737e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 0.4759, 'grad_norm': 18.375, 'learning_rate': 7.011334466110328e-05, 'epoch': 2.4406047516198703}\n",
      "{'loss': 0.4252, 'grad_norm': 37.0, 'learning_rate': 6.98409646266792e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 0.409, 'grad_norm': 8.75, 'learning_rate': 6.956858459225512e-05, 'epoch': 2.4838012958963285}\n",
      "{'loss': 0.4194, 'grad_norm': 14.625, 'learning_rate': 6.929620455783104e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 0.485, 'grad_norm': 19.125, 'learning_rate': 6.902382452340695e-05, 'epoch': 2.526997840172786}\n",
      "{'loss': 0.5128, 'grad_norm': 10.6875, 'learning_rate': 6.875144448898287e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 0.4696, 'grad_norm': 4.65625, 'learning_rate': 6.847906445455879e-05, 'epoch': 2.570194384449244}\n",
      "{'loss': 0.4033, 'grad_norm': 25.875, 'learning_rate': 6.82066844201347e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 0.4148, 'grad_norm': 0.59765625, 'learning_rate': 6.793430438571062e-05, 'epoch': 2.613390928725702}\n",
      "{'loss': 0.4105, 'grad_norm': 7.0625, 'learning_rate': 6.766192435128653e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 0.4634, 'grad_norm': 24.875, 'learning_rate': 6.738954431686244e-05, 'epoch': 2.6565874730021597}\n",
      "{'loss': 0.4143, 'grad_norm': 31.125, 'learning_rate': 6.711716428243838e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 0.3899, 'grad_norm': 15.3125, 'learning_rate': 6.684478424801429e-05, 'epoch': 2.699784017278618}\n",
      "{'loss': 0.4205, 'grad_norm': 26.75, 'learning_rate': 6.657240421359021e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 0.4288, 'grad_norm': 4.8125, 'learning_rate': 6.630002417916613e-05, 'epoch': 2.7429805615550755}\n",
      "{'loss': 0.4433, 'grad_norm': 1.015625, 'learning_rate': 6.602764414474205e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 0.4513, 'grad_norm': 34.0, 'learning_rate': 6.575526411031795e-05, 'epoch': 2.786177105831533}\n",
      "{'loss': 0.4374, 'grad_norm': 4.375, 'learning_rate': 6.548288407589387e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 0.4671, 'grad_norm': 28.5, 'learning_rate': 6.521050404146978e-05, 'epoch': 2.8293736501079914}\n",
      "{'loss': 0.4791, 'grad_norm': 17.25, 'learning_rate': 6.49381240070457e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 0.4518, 'grad_norm': 21.5, 'learning_rate': 6.466574397262162e-05, 'epoch': 2.8725701943844495}\n",
      "{'loss': 0.5011, 'grad_norm': 7.625, 'learning_rate': 6.439336393819755e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 0.417, 'grad_norm': 17.5, 'learning_rate': 6.412098390377345e-05, 'epoch': 2.915766738660907}\n",
      "{'loss': 0.4617, 'grad_norm': 14.6875, 'learning_rate': 6.384860386934937e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 0.482, 'grad_norm': 22.125, 'learning_rate': 6.357622383492529e-05, 'epoch': 2.958963282937365}\n",
      "{'loss': 0.4253, 'grad_norm': 17.5, 'learning_rate': 6.33038438005012e-05, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 0.5953887701034546, 'eval_accuracy': 0.8185131195335277, 'eval_precision': 0.8228066427224234, 'eval_recall': 0.8323012593265968, 'eval_f1': 0.8248323175224307, 'eval_runtime': 7.6269, 'eval_samples_per_second': 539.668, 'eval_steps_per_second': 67.524, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.8185 f1=0.8248 p=0.8228 r=0.8323\n",
      "{'loss': 0.4509, 'grad_norm': 9.375, 'learning_rate': 6.303146376607712e-05, 'epoch': 3.002159827213823}\n",
      "{'loss': 0.384, 'grad_norm': 10.8125, 'learning_rate': 6.275908373165304e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 0.3998, 'grad_norm': 14.4375, 'learning_rate': 6.248670369722894e-05, 'epoch': 3.0453563714902807}\n",
      "{'loss': 0.4285, 'grad_norm': 34.75, 'learning_rate': 6.221432366280487e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 0.4188, 'grad_norm': 18.125, 'learning_rate': 6.194194362838079e-05, 'epoch': 3.088552915766739}\n",
      "{'loss': 0.3581, 'grad_norm': 16.125, 'learning_rate': 6.166956359395671e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 0.4369, 'grad_norm': 28.25, 'learning_rate': 6.139718355953263e-05, 'epoch': 3.1317494600431965}\n",
      "{'loss': 0.4099, 'grad_norm': 1.890625, 'learning_rate': 6.112480352510854e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 0.399, 'grad_norm': 16.5, 'learning_rate': 6.085242349068446e-05, 'epoch': 3.1749460043196542}\n",
      "{'loss': 0.4484, 'grad_norm': 14.9375, 'learning_rate': 6.0580043456260366e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 0.4362, 'grad_norm': 1.15625, 'learning_rate': 6.030766342183629e-05, 'epoch': 3.2181425485961124}\n",
      "{'loss': 0.3557, 'grad_norm': 2.21875, 'learning_rate': 6.003528338741221e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 0.372, 'grad_norm': 19.5, 'learning_rate': 5.9762903352988124e-05, 'epoch': 3.26133909287257}\n",
      "{'loss': 0.3833, 'grad_norm': 42.0, 'learning_rate': 5.949052331856404e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 0.4641, 'grad_norm': 1.984375, 'learning_rate': 5.921814328413996e-05, 'epoch': 3.304535637149028}\n",
      "{'loss': 0.4507, 'grad_norm': 26.125, 'learning_rate': 5.894576324971587e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 0.4261, 'grad_norm': 51.0, 'learning_rate': 5.867338321529179e-05, 'epoch': 3.347732181425486}\n",
      "{'loss': 0.3561, 'grad_norm': 36.25, 'learning_rate': 5.8401003180867704e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 0.4319, 'grad_norm': 4.59375, 'learning_rate': 5.812862314644362e-05, 'epoch': 3.390928725701944}\n",
      "{'loss': 0.3971, 'grad_norm': 52.25, 'learning_rate': 5.785624311201954e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 0.3422, 'grad_norm': 22.25, 'learning_rate': 5.7583863077595457e-05, 'epoch': 3.4341252699784017}\n",
      "{'loss': 0.4121, 'grad_norm': 2.09375, 'learning_rate': 5.731148304317137e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 0.3998, 'grad_norm': 136.0, 'learning_rate': 5.7039103008747284e-05, 'epoch': 3.4773218142548594}\n",
      "{'loss': 0.4746, 'grad_norm': 1.265625, 'learning_rate': 5.67667229743232e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 0.4196, 'grad_norm': 10.75, 'learning_rate': 5.649434293989912e-05, 'epoch': 3.5205183585313176}\n",
      "{'loss': 0.4256, 'grad_norm': 27.5, 'learning_rate': 5.622196290547504e-05, 'epoch': 3.5421166306695464}\n",
      "{'loss': 0.4351, 'grad_norm': 1.8359375, 'learning_rate': 5.594958287105096e-05, 'epoch': 3.5637149028077753}\n",
      "{'loss': 0.3755, 'grad_norm': 60.25, 'learning_rate': 5.5677202836626865e-05, 'epoch': 3.5853131749460045}\n",
      "{'loss': 0.3522, 'grad_norm': 18.75, 'learning_rate': 5.540482280220279e-05, 'epoch': 3.6069114470842334}\n",
      "{'loss': 0.3694, 'grad_norm': 25.625, 'learning_rate': 5.5132442767778706e-05, 'epoch': 3.6285097192224622}\n",
      "{'loss': 0.4829, 'grad_norm': 0.40625, 'learning_rate': 5.4860062733354623e-05, 'epoch': 3.650107991360691}\n",
      "{'loss': 0.3877, 'grad_norm': 2.6875, 'learning_rate': 5.458768269893054e-05, 'epoch': 3.67170626349892}\n",
      "{'loss': 0.421, 'grad_norm': 42.5, 'learning_rate': 5.431530266450646e-05, 'epoch': 3.693304535637149}\n",
      "{'loss': 0.4076, 'grad_norm': 18.125, 'learning_rate': 5.4042922630082376e-05, 'epoch': 3.714902807775378}\n",
      "{'loss': 0.3733, 'grad_norm': 3.453125, 'learning_rate': 5.3770542595658286e-05, 'epoch': 3.736501079913607}\n",
      "{'loss': 0.38, 'grad_norm': 13.625, 'learning_rate': 5.3498162561234203e-05, 'epoch': 3.7580993520518358}\n",
      "{'loss': 0.3625, 'grad_norm': 33.5, 'learning_rate': 5.322578252681012e-05, 'epoch': 3.7796976241900646}\n",
      "{'loss': 0.4248, 'grad_norm': 0.390625, 'learning_rate': 5.295340249238604e-05, 'epoch': 3.801295896328294}\n",
      "{'loss': 0.4421, 'grad_norm': 4.0, 'learning_rate': 5.2681022457961956e-05, 'epoch': 3.8228941684665227}\n",
      "{'loss': 0.378, 'grad_norm': 1.234375, 'learning_rate': 5.240864242353788e-05, 'epoch': 3.8444924406047516}\n",
      "{'loss': 0.3934, 'grad_norm': 18.25, 'learning_rate': 5.2136262389113784e-05, 'epoch': 3.8660907127429804}\n",
      "{'loss': 0.4424, 'grad_norm': 23.125, 'learning_rate': 5.18638823546897e-05, 'epoch': 3.8876889848812093}\n",
      "{'loss': 0.4113, 'grad_norm': 29.875, 'learning_rate': 5.159150232026562e-05, 'epoch': 3.9092872570194386}\n",
      "{'loss': 0.3716, 'grad_norm': 47.75, 'learning_rate': 5.131912228584154e-05, 'epoch': 3.9308855291576674}\n",
      "{'loss': 0.4402, 'grad_norm': 28.375, 'learning_rate': 5.104674225141746e-05, 'epoch': 3.9524838012958963}\n",
      "{'loss': 0.3578, 'grad_norm': 13.625, 'learning_rate': 5.077436221699338e-05, 'epoch': 3.974082073434125}\n",
      "{'loss': 0.3524, 'grad_norm': 4.75, 'learning_rate': 5.050198218256928e-05, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 0.6203027367591858, 'eval_accuracy': 0.8350340136054422, 'eval_precision': 0.8339649090296535, 'eval_recall': 0.8492474538500258, 'eval_f1': 0.8390672547361353, 'eval_runtime': 7.3537, 'eval_samples_per_second': 559.716, 'eval_steps_per_second': 70.032, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.8350 f1=0.8391 p=0.8340 r=0.8492\n",
      "{'loss': 0.3716, 'grad_norm': 20.75, 'learning_rate': 5.0229602148145205e-05, 'epoch': 4.017278617710583}\n",
      "{'loss': 0.3426, 'grad_norm': 5.3125, 'learning_rate': 4.995722211372112e-05, 'epoch': 4.038876889848812}\n",
      "{'loss': 0.3984, 'grad_norm': 1.453125, 'learning_rate': 4.968484207929704e-05, 'epoch': 4.060475161987041}\n",
      "{'loss': 0.415, 'grad_norm': 34.0, 'learning_rate': 4.941246204487296e-05, 'epoch': 4.08207343412527}\n",
      "{'loss': 0.3989, 'grad_norm': 10.625, 'learning_rate': 4.9140082010448875e-05, 'epoch': 4.103671706263499}\n",
      "{'loss': 0.3768, 'grad_norm': 21.625, 'learning_rate': 4.886770197602479e-05, 'epoch': 4.125269978401728}\n",
      "{'loss': 0.3547, 'grad_norm': 57.5, 'learning_rate': 4.85953219416007e-05, 'epoch': 4.146868250539957}\n",
      "{'loss': 0.3664, 'grad_norm': 7.125, 'learning_rate': 4.832294190717662e-05, 'epoch': 4.168466522678186}\n",
      "{'loss': 0.3514, 'grad_norm': 2.9375, 'learning_rate': 4.805056187275254e-05, 'epoch': 4.190064794816415}\n",
      "{'loss': 0.3918, 'grad_norm': 21.5, 'learning_rate': 4.7778181838328455e-05, 'epoch': 4.211663066954643}\n",
      "{'loss': 0.4433, 'grad_norm': 39.0, 'learning_rate': 4.750580180390438e-05, 'epoch': 4.233261339092873}\n",
      "{'loss': 0.4087, 'grad_norm': 8.9375, 'learning_rate': 4.723342176948029e-05, 'epoch': 4.254859611231102}\n",
      "{'loss': 0.3748, 'grad_norm': 37.75, 'learning_rate': 4.696104173505621e-05, 'epoch': 4.27645788336933}\n",
      "{'loss': 0.3411, 'grad_norm': 10.0, 'learning_rate': 4.668866170063212e-05, 'epoch': 4.29805615550756}\n",
      "{'loss': 0.3805, 'grad_norm': 31.25, 'learning_rate': 4.641628166620804e-05, 'epoch': 4.319654427645788}\n",
      "{'loss': 0.3841, 'grad_norm': 2.25, 'learning_rate': 4.614390163178396e-05, 'epoch': 4.341252699784017}\n",
      "{'loss': 0.4102, 'grad_norm': 29.875, 'learning_rate': 4.5871521597359876e-05, 'epoch': 4.362850971922247}\n",
      "{'loss': 0.4249, 'grad_norm': 19.75, 'learning_rate': 4.559914156293579e-05, 'epoch': 4.384449244060475}\n",
      "{'loss': 0.3607, 'grad_norm': 21.125, 'learning_rate': 4.5326761528511704e-05, 'epoch': 4.406047516198704}\n",
      "{'loss': 0.3664, 'grad_norm': 8.9375, 'learning_rate': 4.505438149408762e-05, 'epoch': 4.427645788336933}\n",
      "{'loss': 0.419, 'grad_norm': 1.21875, 'learning_rate': 4.478200145966354e-05, 'epoch': 4.449244060475162}\n",
      "{'loss': 0.3907, 'grad_norm': 27.25, 'learning_rate': 4.4509621425239456e-05, 'epoch': 4.470842332613391}\n",
      "{'loss': 0.359, 'grad_norm': 41.75, 'learning_rate': 4.4237241390815374e-05, 'epoch': 4.49244060475162}\n",
      "{'loss': 0.3774, 'grad_norm': 4.875, 'learning_rate': 4.396486135639129e-05, 'epoch': 4.514038876889849}\n",
      "{'loss': 0.3617, 'grad_norm': 0.78125, 'learning_rate': 4.369248132196721e-05, 'epoch': 4.535637149028078}\n",
      "{'loss': 0.3547, 'grad_norm': 6.375, 'learning_rate': 4.3420101287543126e-05, 'epoch': 4.557235421166307}\n",
      "{'loss': 0.3258, 'grad_norm': 15.0625, 'learning_rate': 4.3147721253119036e-05, 'epoch': 4.578833693304536}\n",
      "{'loss': 0.3836, 'grad_norm': 3.140625, 'learning_rate': 4.2875341218694954e-05, 'epoch': 4.600431965442764}\n",
      "{'loss': 0.372, 'grad_norm': 55.75, 'learning_rate': 4.260296118427087e-05, 'epoch': 4.622030237580994}\n",
      "{'loss': 0.3956, 'grad_norm': 23.625, 'learning_rate': 4.233058114984679e-05, 'epoch': 4.643628509719223}\n",
      "{'loss': 0.3668, 'grad_norm': 2.9375, 'learning_rate': 4.2058201115422706e-05, 'epoch': 4.665226781857451}\n",
      "{'loss': 0.418, 'grad_norm': 28.5, 'learning_rate': 4.178582108099862e-05, 'epoch': 4.686825053995681}\n",
      "{'loss': 0.3884, 'grad_norm': 78.0, 'learning_rate': 4.1513441046574534e-05, 'epoch': 4.708423326133909}\n",
      "{'loss': 0.3835, 'grad_norm': 32.0, 'learning_rate': 4.124106101215046e-05, 'epoch': 4.730021598272138}\n",
      "{'loss': 0.3426, 'grad_norm': 1.59375, 'learning_rate': 4.0968680977726375e-05, 'epoch': 4.751619870410368}\n",
      "{'loss': 0.3811, 'grad_norm': 2.46875, 'learning_rate': 4.069630094330229e-05, 'epoch': 4.773218142548596}\n",
      "{'loss': 0.3861, 'grad_norm': 25.875, 'learning_rate': 4.04239209088782e-05, 'epoch': 4.794816414686825}\n",
      "{'loss': 0.3701, 'grad_norm': 38.0, 'learning_rate': 4.015154087445412e-05, 'epoch': 4.816414686825054}\n",
      "{'loss': 0.3739, 'grad_norm': 6.4375, 'learning_rate': 3.9879160840030045e-05, 'epoch': 4.838012958963283}\n",
      "{'loss': 0.4104, 'grad_norm': 26.125, 'learning_rate': 3.9606780805605955e-05, 'epoch': 4.859611231101512}\n",
      "{'loss': 0.3222, 'grad_norm': 28.125, 'learning_rate': 3.933440077118187e-05, 'epoch': 4.881209503239741}\n",
      "{'loss': 0.4218, 'grad_norm': 20.875, 'learning_rate': 3.906202073675779e-05, 'epoch': 4.90280777537797}\n",
      "{'loss': 0.3621, 'grad_norm': 48.0, 'learning_rate': 3.878964070233371e-05, 'epoch': 4.924406047516198}\n",
      "{'loss': 0.3558, 'grad_norm': 0.94140625, 'learning_rate': 3.8517260667909625e-05, 'epoch': 4.946004319654428}\n",
      "{'loss': 0.3644, 'grad_norm': 1.5, 'learning_rate': 3.824488063348554e-05, 'epoch': 4.967602591792657}\n",
      "{'loss': 0.4255, 'grad_norm': 29.5, 'learning_rate': 3.797250059906145e-05, 'epoch': 4.989200863930885}\n",
      "{'eval_loss': 0.587822437286377, 'eval_accuracy': 0.8428085519922255, 'eval_precision': 0.8444036375137512, 'eval_recall': 0.8525357878974995, 'eval_f1': 0.8470683080135002, 'eval_runtime': 7.6245, 'eval_samples_per_second': 539.838, 'eval_steps_per_second': 67.545, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.8428 f1=0.8471 p=0.8444 r=0.8525\n",
      "{'loss': 0.3563, 'grad_norm': 32.0, 'learning_rate': 3.770012056463737e-05, 'epoch': 5.010799136069115}\n",
      "{'loss': 0.3711, 'grad_norm': 16.25, 'learning_rate': 3.7427740530213294e-05, 'epoch': 5.032397408207343}\n",
      "{'loss': 0.3692, 'grad_norm': 27.25, 'learning_rate': 3.7155360495789205e-05, 'epoch': 5.053995680345572}\n",
      "{'loss': 0.3151, 'grad_norm': 10.5, 'learning_rate': 3.688298046136512e-05, 'epoch': 5.075593952483802}\n",
      "{'loss': 0.3749, 'grad_norm': 30.375, 'learning_rate': 3.661060042694104e-05, 'epoch': 5.09719222462203}\n",
      "{'loss': 0.3379, 'grad_norm': 24.5, 'learning_rate': 3.633822039251696e-05, 'epoch': 5.118790496760259}\n",
      "{'loss': 0.3568, 'grad_norm': 0.84765625, 'learning_rate': 3.6065840358092874e-05, 'epoch': 5.140388768898488}\n",
      "{'loss': 0.3119, 'grad_norm': 7.71875, 'learning_rate': 3.579346032366879e-05, 'epoch': 5.161987041036717}\n",
      "{'loss': 0.3653, 'grad_norm': 21.75, 'learning_rate': 3.55210802892447e-05, 'epoch': 5.183585313174946}\n",
      "{'loss': 0.3653, 'grad_norm': 12.4375, 'learning_rate': 3.524870025482062e-05, 'epoch': 5.205183585313175}\n",
      "{'loss': 0.3452, 'grad_norm': 0.87890625, 'learning_rate': 3.4976320220396544e-05, 'epoch': 5.226781857451404}\n",
      "{'loss': 0.3723, 'grad_norm': 17.125, 'learning_rate': 3.470394018597246e-05, 'epoch': 5.248380129589632}\n",
      "{'loss': 0.3582, 'grad_norm': 34.0, 'learning_rate': 3.443156015154837e-05, 'epoch': 5.269978401727862}\n",
      "{'loss': 0.3489, 'grad_norm': 0.859375, 'learning_rate': 3.415918011712429e-05, 'epoch': 5.291576673866091}\n",
      "{'loss': 0.3208, 'grad_norm': 13.375, 'learning_rate': 3.3886800082700206e-05, 'epoch': 5.313174946004319}\n",
      "{'loss': 0.4106, 'grad_norm': 37.5, 'learning_rate': 3.3614420048276124e-05, 'epoch': 5.334773218142549}\n",
      "{'loss': 0.3455, 'grad_norm': 29.125, 'learning_rate': 3.334204001385204e-05, 'epoch': 5.356371490280777}\n",
      "{'loss': 0.4157, 'grad_norm': 35.75, 'learning_rate': 3.306965997942796e-05, 'epoch': 5.377969762419006}\n",
      "{'loss': 0.3229, 'grad_norm': 25.25, 'learning_rate': 3.279727994500387e-05, 'epoch': 5.399568034557236}\n",
      "{'loss': 0.4084, 'grad_norm': 62.25, 'learning_rate': 3.252489991057979e-05, 'epoch': 5.421166306695464}\n",
      "{'loss': 0.3149, 'grad_norm': 1.890625, 'learning_rate': 3.225251987615571e-05, 'epoch': 5.442764578833693}\n",
      "{'loss': 0.3401, 'grad_norm': 1.671875, 'learning_rate': 3.198013984173162e-05, 'epoch': 5.464362850971923}\n",
      "{'loss': 0.3127, 'grad_norm': 1.875, 'learning_rate': 3.170775980730754e-05, 'epoch': 5.485961123110151}\n",
      "{'loss': 0.3299, 'grad_norm': 5.78125, 'learning_rate': 3.1435379772883456e-05, 'epoch': 5.50755939524838}\n",
      "{'loss': 0.4141, 'grad_norm': 0.8515625, 'learning_rate': 3.116299973845937e-05, 'epoch': 5.529157667386609}\n",
      "{'loss': 0.4517, 'grad_norm': 0.8671875, 'learning_rate': 3.089061970403529e-05, 'epoch': 5.550755939524838}\n",
      "{'loss': 0.3656, 'grad_norm': 0.92578125, 'learning_rate': 3.061823966961121e-05, 'epoch': 5.572354211663067}\n",
      "{'loss': 0.3612, 'grad_norm': 4.71875, 'learning_rate': 3.0345859635187122e-05, 'epoch': 5.593952483801296}\n",
      "{'loss': 0.3416, 'grad_norm': 11.8125, 'learning_rate': 3.007347960076304e-05, 'epoch': 5.615550755939525}\n",
      "{'loss': 0.4125, 'grad_norm': 16.75, 'learning_rate': 2.9801099566338957e-05, 'epoch': 5.637149028077753}\n",
      "{'loss': 0.3481, 'grad_norm': 26.25, 'learning_rate': 2.952871953191487e-05, 'epoch': 5.658747300215983}\n",
      "{'loss': 0.3018, 'grad_norm': 58.75, 'learning_rate': 2.9256339497490788e-05, 'epoch': 5.680345572354212}\n",
      "{'loss': 0.3368, 'grad_norm': 21.875, 'learning_rate': 2.898395946306671e-05, 'epoch': 5.70194384449244}\n",
      "{'loss': 0.3788, 'grad_norm': 40.75, 'learning_rate': 2.8711579428642626e-05, 'epoch': 5.72354211663067}\n",
      "{'loss': 0.3959, 'grad_norm': 0.56640625, 'learning_rate': 2.843919939421854e-05, 'epoch': 5.745140388768899}\n",
      "{'loss': 0.329, 'grad_norm': 4.40625, 'learning_rate': 2.8166819359794457e-05, 'epoch': 5.766738660907127}\n",
      "{'loss': 0.3134, 'grad_norm': 2.875, 'learning_rate': 2.7894439325370375e-05, 'epoch': 5.788336933045357}\n",
      "{'loss': 0.3588, 'grad_norm': 18.875, 'learning_rate': 2.762205929094629e-05, 'epoch': 5.809935205183585}\n",
      "{'loss': 0.3877, 'grad_norm': 1.2734375, 'learning_rate': 2.7349679256522206e-05, 'epoch': 5.831533477321814}\n",
      "{'loss': 0.3331, 'grad_norm': 0.390625, 'learning_rate': 2.7077299222098127e-05, 'epoch': 5.853131749460044}\n",
      "{'loss': 0.398, 'grad_norm': 0.578125, 'learning_rate': 2.6804919187674038e-05, 'epoch': 5.874730021598272}\n",
      "{'loss': 0.4077, 'grad_norm': 0.6640625, 'learning_rate': 2.6532539153249958e-05, 'epoch': 5.896328293736501}\n",
      "{'loss': 0.4005, 'grad_norm': 21.25, 'learning_rate': 2.6260159118825876e-05, 'epoch': 5.91792656587473}\n",
      "{'loss': 0.3936, 'grad_norm': 45.75, 'learning_rate': 2.598777908440179e-05, 'epoch': 5.939524838012959}\n",
      "{'loss': 0.3593, 'grad_norm': 21.625, 'learning_rate': 2.5715399049977707e-05, 'epoch': 5.961123110151188}\n",
      "{'loss': 0.3869, 'grad_norm': 0.5390625, 'learning_rate': 2.5443019015553624e-05, 'epoch': 5.982721382289417}\n",
      "{'eval_loss': 0.6010882258415222, 'eval_accuracy': 0.8398931000971818, 'eval_precision': 0.8401535413287204, 'eval_recall': 0.8526153170491556, 'eval_f1': 0.8442704024013906, 'eval_runtime': 8.2935, 'eval_samples_per_second': 496.293, 'eval_steps_per_second': 62.097, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.8399 f1=0.8443 p=0.8402 r=0.8526\n",
      "{'loss': 0.2708, 'grad_norm': 28.25, 'learning_rate': 2.517063898112954e-05, 'epoch': 6.004319654427646}\n",
      "{'loss': 0.3558, 'grad_norm': 18.625, 'learning_rate': 2.4898258946705456e-05, 'epoch': 6.025917926565874}\n",
      "{'loss': 0.364, 'grad_norm': 0.9296875, 'learning_rate': 2.4625878912281373e-05, 'epoch': 6.047516198704104}\n",
      "{'loss': 0.3223, 'grad_norm': 17.375, 'learning_rate': 2.4353498877857287e-05, 'epoch': 6.069114470842333}\n",
      "{'loss': 0.3832, 'grad_norm': 6.28125, 'learning_rate': 2.4081118843433208e-05, 'epoch': 6.090712742980561}\n",
      "{'loss': 0.3311, 'grad_norm': 15.375, 'learning_rate': 2.3808738809009125e-05, 'epoch': 6.112311015118791}\n",
      "{'loss': 0.3915, 'grad_norm': 0.357421875, 'learning_rate': 2.353635877458504e-05, 'epoch': 6.133909287257019}\n",
      "{'loss': 0.3198, 'grad_norm': 25.5, 'learning_rate': 2.3263978740160956e-05, 'epoch': 6.155507559395248}\n",
      "{'loss': 0.3384, 'grad_norm': 34.25, 'learning_rate': 2.2991598705736874e-05, 'epoch': 6.177105831533478}\n",
      "{'loss': 0.3575, 'grad_norm': 83.5, 'learning_rate': 2.271921867131279e-05, 'epoch': 6.198704103671706}\n",
      "{'loss': 0.3979, 'grad_norm': 46.0, 'learning_rate': 2.2446838636888705e-05, 'epoch': 6.220302375809935}\n",
      "{'loss': 0.3661, 'grad_norm': 13.8125, 'learning_rate': 2.2174458602464623e-05, 'epoch': 6.241900647948164}\n",
      "{'loss': 0.352, 'grad_norm': 6.375, 'learning_rate': 2.190207856804054e-05, 'epoch': 6.263498920086393}\n",
      "{'loss': 0.3822, 'grad_norm': 0.7890625, 'learning_rate': 2.1629698533616454e-05, 'epoch': 6.285097192224622}\n",
      "{'loss': 0.3543, 'grad_norm': 10.625, 'learning_rate': 2.1357318499192375e-05, 'epoch': 6.306695464362851}\n",
      "{'loss': 0.348, 'grad_norm': 16.5, 'learning_rate': 2.108493846476829e-05, 'epoch': 6.32829373650108}\n",
      "{'loss': 0.3706, 'grad_norm': 15.6875, 'learning_rate': 2.081255843034421e-05, 'epoch': 6.3498920086393085}\n",
      "{'loss': 0.3559, 'grad_norm': 26.75, 'learning_rate': 2.0540178395920123e-05, 'epoch': 6.371490280777538}\n",
      "{'loss': 0.351, 'grad_norm': 41.5, 'learning_rate': 2.026779836149604e-05, 'epoch': 6.393088552915767}\n",
      "{'loss': 0.3093, 'grad_norm': 4.125, 'learning_rate': 1.9995418327071958e-05, 'epoch': 6.4146868250539955}\n",
      "{'loss': 0.3017, 'grad_norm': 5.96875, 'learning_rate': 1.9723038292647872e-05, 'epoch': 6.436285097192225}\n",
      "{'loss': 0.4062, 'grad_norm': 27.625, 'learning_rate': 1.945065825822379e-05, 'epoch': 6.457883369330453}\n",
      "{'loss': 0.3521, 'grad_norm': 39.25, 'learning_rate': 1.9178278223799707e-05, 'epoch': 6.479481641468682}\n",
      "{'loss': 0.3484, 'grad_norm': 19.625, 'learning_rate': 1.8905898189375624e-05, 'epoch': 6.501079913606912}\n",
      "{'loss': 0.3358, 'grad_norm': 0.640625, 'learning_rate': 1.8633518154951538e-05, 'epoch': 6.52267818574514}\n",
      "{'loss': 0.2869, 'grad_norm': 1.2734375, 'learning_rate': 1.836113812052746e-05, 'epoch': 6.544276457883369}\n",
      "{'loss': 0.3454, 'grad_norm': 16.0, 'learning_rate': 1.8088758086103373e-05, 'epoch': 6.565874730021598}\n",
      "{'loss': 0.415, 'grad_norm': 31.75, 'learning_rate': 1.781637805167929e-05, 'epoch': 6.587473002159827}\n",
      "{'loss': 0.3688, 'grad_norm': 55.5, 'learning_rate': 1.7543998017255208e-05, 'epoch': 6.609071274298056}\n",
      "{'loss': 0.3135, 'grad_norm': 61.25, 'learning_rate': 1.727161798283112e-05, 'epoch': 6.630669546436285}\n",
      "{'loss': 0.3371, 'grad_norm': 26.375, 'learning_rate': 1.6999237948407042e-05, 'epoch': 6.652267818574514}\n",
      "{'loss': 0.3584, 'grad_norm': 0.7421875, 'learning_rate': 1.6726857913982956e-05, 'epoch': 6.6738660907127425}\n",
      "{'loss': 0.354, 'grad_norm': 29.5, 'learning_rate': 1.6454477879558874e-05, 'epoch': 6.695464362850972}\n",
      "{'loss': 0.3479, 'grad_norm': 20.0, 'learning_rate': 1.618209784513479e-05, 'epoch': 6.717062634989201}\n",
      "{'loss': 0.2862, 'grad_norm': 91.0, 'learning_rate': 1.5909717810710705e-05, 'epoch': 6.7386609071274295}\n",
      "{'loss': 0.3432, 'grad_norm': 2.125, 'learning_rate': 1.5637337776286622e-05, 'epoch': 6.760259179265659}\n",
      "{'loss': 0.335, 'grad_norm': 42.75, 'learning_rate': 1.536495774186254e-05, 'epoch': 6.781857451403888}\n",
      "{'loss': 0.3021, 'grad_norm': 5.28125, 'learning_rate': 1.5092577707438457e-05, 'epoch': 6.8034557235421165}\n",
      "{'loss': 0.3734, 'grad_norm': 3.609375, 'learning_rate': 1.4820197673014374e-05, 'epoch': 6.825053995680346}\n",
      "{'loss': 0.3832, 'grad_norm': 27.75, 'learning_rate': 1.454781763859029e-05, 'epoch': 6.846652267818574}\n",
      "{'loss': 0.3355, 'grad_norm': 49.5, 'learning_rate': 1.4275437604166206e-05, 'epoch': 6.8682505399568035}\n",
      "{'loss': 0.3543, 'grad_norm': 30.625, 'learning_rate': 1.4003057569742125e-05, 'epoch': 6.889848812095033}\n",
      "{'loss': 0.3545, 'grad_norm': 35.75, 'learning_rate': 1.373067753531804e-05, 'epoch': 6.911447084233261}\n",
      "{'loss': 0.4013, 'grad_norm': 10.4375, 'learning_rate': 1.3458297500893956e-05, 'epoch': 6.93304535637149}\n",
      "{'loss': 0.269, 'grad_norm': 1.796875, 'learning_rate': 1.3185917466469874e-05, 'epoch': 6.954643628509719}\n",
      "{'loss': 0.3446, 'grad_norm': 26.375, 'learning_rate': 1.291353743204579e-05, 'epoch': 6.976241900647948}\n",
      "{'loss': 0.3901, 'grad_norm': 0.703125, 'learning_rate': 1.2641157397621705e-05, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 0.601289689540863, 'eval_accuracy': 0.8403790087463557, 'eval_precision': 0.8412681380691577, 'eval_recall': 0.8523495601127886, 'eval_f1': 0.844716851004196, 'eval_runtime': 7.8586, 'eval_samples_per_second': 523.759, 'eval_steps_per_second': 65.534, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.8404 f1=0.8447 p=0.8413 r=0.8523\n",
      "{'loss': 0.3864, 'grad_norm': 8.6875, 'learning_rate': 1.2368777363197624e-05, 'epoch': 7.019438444924406}\n",
      "{'loss': 0.3351, 'grad_norm': 23.0, 'learning_rate': 1.209639732877354e-05, 'epoch': 7.041036717062635}\n",
      "{'loss': 0.4547, 'grad_norm': 27.75, 'learning_rate': 1.1824017294349457e-05, 'epoch': 7.0626349892008635}\n",
      "{'loss': 0.331, 'grad_norm': 36.75, 'learning_rate': 1.1551637259925374e-05, 'epoch': 7.084233261339093}\n",
      "{'loss': 0.3088, 'grad_norm': 17.875, 'learning_rate': 1.1279257225501292e-05, 'epoch': 7.105831533477322}\n",
      "{'loss': 0.3674, 'grad_norm': 46.75, 'learning_rate': 1.1006877191077207e-05, 'epoch': 7.1274298056155505}\n",
      "{'loss': 0.3039, 'grad_norm': 1.2890625, 'learning_rate': 1.0734497156653123e-05, 'epoch': 7.14902807775378}\n",
      "{'loss': 0.377, 'grad_norm': 1.4140625, 'learning_rate': 1.046211712222904e-05, 'epoch': 7.170626349892009}\n",
      "{'loss': 0.4166, 'grad_norm': 21.625, 'learning_rate': 1.0189737087804958e-05, 'epoch': 7.1922246220302375}\n",
      "{'loss': 0.3655, 'grad_norm': 1.09375, 'learning_rate': 9.917357053380874e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 0.3828, 'grad_norm': 13.9375, 'learning_rate': 9.644977018956791e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 0.3038, 'grad_norm': 89.5, 'learning_rate': 9.372596984532708e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 0.3528, 'grad_norm': 1.609375, 'learning_rate': 9.100216950108624e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 0.3685, 'grad_norm': 1.3125, 'learning_rate': 8.82783691568454e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 0.3126, 'grad_norm': 7.75, 'learning_rate': 8.555456881260457e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 0.3687, 'grad_norm': 0.58203125, 'learning_rate': 8.283076846836374e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 0.3382, 'grad_norm': 4.21875, 'learning_rate': 8.01069681241229e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 0.3776, 'grad_norm': 26.75, 'learning_rate': 7.738316777988207e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 0.2761, 'grad_norm': 0.6796875, 'learning_rate': 7.465936743564124e-06, 'epoch': 7.408207343412527}\n",
      "{'loss': 0.3342, 'grad_norm': 0.96484375, 'learning_rate': 7.193556709140041e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 0.3456, 'grad_norm': 53.5, 'learning_rate': 6.921176674715957e-06, 'epoch': 7.4514038876889845}\n",
      "{'loss': 0.3434, 'grad_norm': 0.71875, 'learning_rate': 6.648796640291874e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 0.3502, 'grad_norm': 5.46875, 'learning_rate': 6.376416605867791e-06, 'epoch': 7.494600431965443}\n",
      "{'loss': 0.3814, 'grad_norm': 2.46875, 'learning_rate': 6.1040365714437065e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 0.3146, 'grad_norm': 38.25, 'learning_rate': 5.831656537019624e-06, 'epoch': 7.537796976241901}\n",
      "{'loss': 0.3695, 'grad_norm': 55.0, 'learning_rate': 5.559276502595541e-06, 'epoch': 7.559395248380129}\n",
      "{'loss': 0.3333, 'grad_norm': 10.625, 'learning_rate': 5.286896468171457e-06, 'epoch': 7.5809935205183585}\n",
      "{'loss': 0.3361, 'grad_norm': 1.375, 'learning_rate': 5.014516433747374e-06, 'epoch': 7.602591792656588}\n",
      "{'loss': 0.2842, 'grad_norm': 46.25, 'learning_rate': 4.742136399323291e-06, 'epoch': 7.624190064794816}\n",
      "{'loss': 0.3255, 'grad_norm': 49.25, 'learning_rate': 4.469756364899207e-06, 'epoch': 7.6457883369330455}\n",
      "{'loss': 0.3412, 'grad_norm': 5.6875, 'learning_rate': 4.197376330475124e-06, 'epoch': 7.667386609071274}\n",
      "{'loss': 0.4283, 'grad_norm': 4.78125, 'learning_rate': 3.924996296051041e-06, 'epoch': 7.688984881209503}\n",
      "{'loss': 0.3341, 'grad_norm': 37.75, 'learning_rate': 3.6526162616269573e-06, 'epoch': 7.7105831533477325}\n",
      "{'loss': 0.3481, 'grad_norm': 20.25, 'learning_rate': 3.3802362272028738e-06, 'epoch': 7.732181425485961}\n",
      "{'loss': 0.2879, 'grad_norm': 16.875, 'learning_rate': 3.1078561927787907e-06, 'epoch': 7.75377969762419}\n",
      "{'loss': 0.3445, 'grad_norm': 70.0, 'learning_rate': 2.8354761583547077e-06, 'epoch': 7.775377969762419}\n",
      "{'loss': 0.3867, 'grad_norm': 57.5, 'learning_rate': 2.563096123930624e-06, 'epoch': 7.796976241900648}\n",
      "{'loss': 0.3429, 'grad_norm': 20.625, 'learning_rate': 2.2907160895065407e-06, 'epoch': 7.818574514038877}\n",
      "{'loss': 0.31, 'grad_norm': 10.75, 'learning_rate': 2.0183360550824572e-06, 'epoch': 7.840172786177106}\n",
      "{'loss': 0.3616, 'grad_norm': 37.75, 'learning_rate': 1.745956020658374e-06, 'epoch': 7.861771058315335}\n",
      "{'loss': 0.322, 'grad_norm': 2.59375, 'learning_rate': 1.4735759862342907e-06, 'epoch': 7.883369330453563}\n",
      "{'loss': 0.3756, 'grad_norm': 16.75, 'learning_rate': 1.2011959518102074e-06, 'epoch': 7.9049676025917925}\n",
      "{'loss': 0.3332, 'grad_norm': 116.5, 'learning_rate': 9.288159173861241e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 0.3487, 'grad_norm': 19.5, 'learning_rate': 6.564358829620409e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 0.3404, 'grad_norm': 34.5, 'learning_rate': 3.8405584853795744e-07, 'epoch': 7.9697624190064795}\n",
      "{'loss': 0.3262, 'grad_norm': 5.15625, 'learning_rate': 1.1167581411387416e-07, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 0.6087741255760193, 'eval_accuracy': 0.8401360544217688, 'eval_precision': 0.8408697750937149, 'eval_recall': 0.8525065628943229, 'eval_f1': 0.8445044433074725, 'eval_runtime': 7.5048, 'eval_samples_per_second': 548.446, 'eval_steps_per_second': 68.622, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8401 f1=0.8445 p=0.8409 r=0.8525\n",
      "{'train_runtime': 1575.8202, 'train_samples_per_second': 188.037, 'train_steps_per_second': 23.505, 'train_loss': 0.4915098949333504, 'epoch': 8.0}\n",
      "{'eval_loss': 0.587822437286377, 'eval_accuracy': 0.8428085519922255, 'eval_precision': 0.8444036375137512, 'eval_recall': 0.8525357878974995, 'eval_f1': 0.8470683080135002, 'eval_runtime': 7.5543, 'eval_samples_per_second': 544.853, 'eval_steps_per_second': 68.173, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8428 f1=0.8471 p=0.8444 r=0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñà‚ñÖ‚ñÇ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñá‚ñÜ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÅ‚ñÑ‚ñá‚ñÜ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÉ‚ñá‚ñÑ‚ñÉ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.84707</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.84281</td></tr><tr><td>eval/f1</td><td>0.84707</td></tr><tr><td>eval/loss</td><td>0.58782</td></tr><tr><td>eval/precision</td><td>0.8444</td></tr><tr><td>eval/recall</td><td>0.85254</td></tr><tr><td>eval/runtime</td><td>7.5543</td></tr><tr><td>eval/samples_per_second</td><td>544.853</td></tr><tr><td>eval/steps_per_second</td><td>68.173</td></tr><tr><td>total_flos</td><td>1.1916090232884816e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>37040</td></tr><tr><td>train/grad_norm</td><td>5.15625</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3262</td></tr><tr><td>train_loss</td><td>0.49151</td></tr><tr><td>train_runtime</td><td>1575.8202</td></tr><tr><td>train_samples_per_second</td><td>188.037</td></tr><tr><td>train_steps_per_second</td><td>23.505</td></tr><tr><td>trial/accuracy</td><td>0.84281</td></tr><tr><td>trial/f1</td><td>0.84707</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t13</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nv60aodc' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/nv60aodc</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_051250-nv60aodc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.858824:  27%|‚ñà‚ñà‚ñã       | 4/15 [1:04:20<3:23:02, 1107.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=13 f1=0.8471\n",
      "[I 2025-08-17 05:39:18,074] Trial 3 finished with value: 0.8470683080135002 and parameters: {'lr': 9.483455658543309e-05, 'weight_decay': 6.0257538149883206e-05, 'unfreeze_last_k': 11, 'batch_size': 8}. Best is trial 2 with value: 0.8588243859157035.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_053918-a0tp2n0d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/a0tp2n0d' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t14</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/a0tp2n0d' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/a0tp2n0d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=14 | epochs=8 bs=8 lr=9.10e-05 wd=1.3e-05 k=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 57,297,413 / 278,813,189 (20.55%) ; unfreeze_last_k=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=8 lr=9.10e-05 wd=1.3e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/4630] loss=1.6397 lr=4.05e-06\n",
      "{'loss': 1.6397, 'grad_norm': 4.0625, 'learning_rate': 4.054367211723492e-06, 'epoch': 0.02159827213822894}\n",
      "[e0 b200/4630] loss=1.6435 lr=8.15e-06\n",
      "{'loss': 1.6435, 'grad_norm': 5.0625, 'learning_rate': 8.149687627605807e-06, 'epoch': 0.04319654427645788}\n",
      "[e0 b300/4630] loss=1.6385 lr=1.22e-05\n",
      "{'loss': 1.6385, 'grad_norm': 3.5, 'learning_rate': 1.2245008043488122e-05, 'epoch': 0.06479481641468683}\n",
      "[e0 b400/4630] loss=1.6190 lr=1.63e-05\n",
      "{'loss': 1.619, 'grad_norm': 2.40625, 'learning_rate': 1.634032845937044e-05, 'epoch': 0.08639308855291576}\n",
      "[e0 b500/4630] loss=1.5936 lr=2.04e-05\n",
      "{'loss': 1.5936, 'grad_norm': 6.21875, 'learning_rate': 2.0435648875252754e-05, 'epoch': 0.1079913606911447}\n",
      "[e0 b600/4630] loss=1.5735 lr=2.45e-05\n",
      "{'loss': 1.5735, 'grad_norm': 7.46875, 'learning_rate': 2.4530969291135068e-05, 'epoch': 0.12958963282937366}\n",
      "[e0 b700/4630] loss=1.5477 lr=2.86e-05\n",
      "{'loss': 1.5477, 'grad_norm': 11.0, 'learning_rate': 2.8626289707017385e-05, 'epoch': 0.1511879049676026}\n",
      "[e0 b800/4630] loss=1.5075 lr=3.27e-05\n",
      "{'loss': 1.5075, 'grad_norm': 7.5625, 'learning_rate': 3.27216101228997e-05, 'epoch': 0.17278617710583152}\n",
      "[e0 b900/4630] loss=1.4917 lr=3.68e-05\n",
      "{'loss': 1.4917, 'grad_norm': 7.75, 'learning_rate': 3.681693053878202e-05, 'epoch': 0.19438444924406048}\n",
      "[e0 b1000/4630] loss=1.4796 lr=4.09e-05\n",
      "{'loss': 1.4796, 'grad_norm': 13.6875, 'learning_rate': 4.0912250954664334e-05, 'epoch': 0.2159827213822894}\n",
      "[e0 b1100/4630] loss=1.3846 lr=4.50e-05\n",
      "{'loss': 1.3846, 'grad_norm': 16.25, 'learning_rate': 4.5007571370546645e-05, 'epoch': 0.23758099352051837}\n",
      "[e0 b1200/4630] loss=1.3337 lr=4.91e-05\n",
      "{'loss': 1.3337, 'grad_norm': 14.5, 'learning_rate': 4.910289178642896e-05, 'epoch': 0.2591792656587473}\n",
      "[e0 b1300/4630] loss=1.2939 lr=5.32e-05\n",
      "{'loss': 1.2939, 'grad_norm': 20.25, 'learning_rate': 5.319821220231127e-05, 'epoch': 0.28077753779697623}\n",
      "[e0 b1400/4630] loss=1.2645 lr=5.73e-05\n",
      "{'loss': 1.2645, 'grad_norm': 22.625, 'learning_rate': 5.72935326181936e-05, 'epoch': 0.3023758099352052}\n",
      "[e0 b1500/4630] loss=1.2174 lr=6.14e-05\n",
      "{'loss': 1.2174, 'grad_norm': 17.375, 'learning_rate': 6.138885303407591e-05, 'epoch': 0.32397408207343414}\n",
      "[e0 b1600/4630] loss=1.2145 lr=6.55e-05\n",
      "{'loss': 1.2145, 'grad_norm': 13.8125, 'learning_rate': 6.548417344995822e-05, 'epoch': 0.34557235421166305}\n",
      "[e0 b1700/4630] loss=1.1321 lr=6.96e-05\n",
      "{'loss': 1.1321, 'grad_norm': 15.875, 'learning_rate': 6.957949386584054e-05, 'epoch': 0.367170626349892}\n",
      "[e0 b1800/4630] loss=1.1013 lr=7.37e-05\n",
      "{'loss': 1.1013, 'grad_norm': 12.4375, 'learning_rate': 7.367481428172285e-05, 'epoch': 0.38876889848812096}\n",
      "[e0 b1900/4630] loss=1.0639 lr=7.78e-05\n",
      "{'loss': 1.0639, 'grad_norm': 13.0, 'learning_rate': 7.777013469760517e-05, 'epoch': 0.4103671706263499}\n",
      "[e0 b2000/4630] loss=1.1241 lr=8.19e-05\n",
      "{'loss': 1.1241, 'grad_norm': 22.375, 'learning_rate': 8.18654551134875e-05, 'epoch': 0.4319654427645788}\n",
      "[e0 b2100/4630] loss=1.0039 lr=8.60e-05\n",
      "{'loss': 1.0039, 'grad_norm': 16.875, 'learning_rate': 8.59607755293698e-05, 'epoch': 0.4535637149028078}\n",
      "[e0 b2200/4630] loss=1.0415 lr=9.01e-05\n",
      "{'loss': 1.0415, 'grad_norm': 19.375, 'learning_rate': 9.005609594525212e-05, 'epoch': 0.47516198704103674}\n",
      "[e0 b2300/4630] loss=0.9469 lr=9.08e-05\n",
      "{'loss': 0.9469, 'grad_norm': 19.125, 'learning_rate': 9.084024917742379e-05, 'epoch': 0.49676025917926564}\n",
      "[e0 b2400/4630] loss=1.0236 lr=9.06e-05\n",
      "{'loss': 1.0236, 'grad_norm': 12.25, 'learning_rate': 9.057877066737104e-05, 'epoch': 0.5183585313174947}\n",
      "[e0 b2500/4630] loss=0.9215 lr=9.03e-05\n",
      "{'loss': 0.9215, 'grad_norm': 24.75, 'learning_rate': 9.03172921573183e-05, 'epoch': 0.5399568034557235}\n",
      "[e0 b2600/4630] loss=0.9420 lr=9.01e-05\n",
      "{'loss': 0.942, 'grad_norm': 13.4375, 'learning_rate': 9.005581364726556e-05, 'epoch': 0.5615550755939525}\n",
      "[e0 b2700/4630] loss=0.9512 lr=8.98e-05\n",
      "{'loss': 0.9512, 'grad_norm': 17.25, 'learning_rate': 8.979433513721281e-05, 'epoch': 0.5831533477321814}\n",
      "[e0 b2800/4630] loss=0.9511 lr=8.95e-05\n",
      "{'loss': 0.9511, 'grad_norm': 21.25, 'learning_rate': 8.953285662716006e-05, 'epoch': 0.6047516198704104}\n",
      "[e0 b2900/4630] loss=0.9488 lr=8.93e-05\n",
      "{'loss': 0.9488, 'grad_norm': 16.5, 'learning_rate': 8.927137811710732e-05, 'epoch': 0.6263498920086393}\n",
      "[e0 b3000/4630] loss=0.8647 lr=8.90e-05\n",
      "{'loss': 0.8647, 'grad_norm': 13.4375, 'learning_rate': 8.900989960705458e-05, 'epoch': 0.6479481641468683}\n",
      "[e0 b3100/4630] loss=0.8852 lr=8.87e-05\n",
      "{'loss': 0.8852, 'grad_norm': 14.0625, 'learning_rate': 8.874842109700183e-05, 'epoch': 0.6695464362850972}\n",
      "[e0 b3200/4630] loss=0.9041 lr=8.85e-05\n",
      "{'loss': 0.9041, 'grad_norm': 11.9375, 'learning_rate': 8.848694258694909e-05, 'epoch': 0.6911447084233261}\n",
      "[e0 b3300/4630] loss=0.8936 lr=8.82e-05\n",
      "{'loss': 0.8936, 'grad_norm': 15.25, 'learning_rate': 8.822546407689635e-05, 'epoch': 0.712742980561555}\n",
      "[e0 b3400/4630] loss=0.8142 lr=8.80e-05\n",
      "{'loss': 0.8142, 'grad_norm': 10.0, 'learning_rate': 8.79639855668436e-05, 'epoch': 0.734341252699784}\n",
      "[e0 b3500/4630] loss=0.8084 lr=8.77e-05\n",
      "{'loss': 0.8084, 'grad_norm': 17.875, 'learning_rate': 8.770250705679085e-05, 'epoch': 0.755939524838013}\n",
      "[e0 b3600/4630] loss=0.8410 lr=8.74e-05\n",
      "{'loss': 0.841, 'grad_norm': 23.375, 'learning_rate': 8.744102854673811e-05, 'epoch': 0.7775377969762419}\n",
      "[e0 b3700/4630] loss=0.8134 lr=8.72e-05\n",
      "{'loss': 0.8134, 'grad_norm': 11.8125, 'learning_rate': 8.717955003668537e-05, 'epoch': 0.7991360691144709}\n",
      "[e0 b3800/4630] loss=0.7513 lr=8.69e-05\n",
      "{'loss': 0.7513, 'grad_norm': 13.0625, 'learning_rate': 8.691807152663262e-05, 'epoch': 0.8207343412526998}\n",
      "[e0 b3900/4630] loss=0.8201 lr=8.67e-05\n",
      "{'loss': 0.8201, 'grad_norm': 19.625, 'learning_rate': 8.665659301657988e-05, 'epoch': 0.8423326133909287}\n",
      "[e0 b4000/4630] loss=0.7996 lr=8.64e-05\n",
      "{'loss': 0.7996, 'grad_norm': 33.25, 'learning_rate': 8.639511450652715e-05, 'epoch': 0.8639308855291576}\n",
      "[e0 b4100/4630] loss=0.7857 lr=8.61e-05\n",
      "{'loss': 0.7857, 'grad_norm': 8.0, 'learning_rate': 8.61336359964744e-05, 'epoch': 0.8855291576673866}\n",
      "[e0 b4200/4630] loss=0.7599 lr=8.59e-05\n",
      "{'loss': 0.7599, 'grad_norm': 12.9375, 'learning_rate': 8.587215748642164e-05, 'epoch': 0.9071274298056156}\n",
      "[e0 b4300/4630] loss=0.7879 lr=8.56e-05\n",
      "{'loss': 0.7879, 'grad_norm': 10.0, 'learning_rate': 8.56106789763689e-05, 'epoch': 0.9287257019438445}\n",
      "[e0 b4400/4630] loss=0.7126 lr=8.53e-05\n",
      "{'loss': 0.7126, 'grad_norm': 8.8125, 'learning_rate': 8.534920046631617e-05, 'epoch': 0.9503239740820735}\n",
      "[e0 b4500/4630] loss=0.7902 lr=8.51e-05\n",
      "{'loss': 0.7902, 'grad_norm': 13.1875, 'learning_rate': 8.508772195626342e-05, 'epoch': 0.9719222462203023}\n",
      "[e0 b4600/4630] loss=0.6751 lr=8.48e-05\n",
      "{'loss': 0.6751, 'grad_norm': 10.125, 'learning_rate': 8.482624344621068e-05, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 0.77837735414505, 'eval_accuracy': 0.7232750242954324, 'eval_precision': 0.7445106779597268, 'eval_recall': 0.7222413615191615, 'eval_f1': 0.7289907840523722, 'eval_runtime': 7.7444, 'eval_samples_per_second': 531.478, 'eval_steps_per_second': 66.499, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.7233 f1=0.7290 p=0.7445 r=0.7222\n",
      "{'loss': 0.6791, 'grad_norm': 7.75, 'learning_rate': 8.456476493615793e-05, 'epoch': 1.0151187904967602}\n",
      "{'loss': 0.7082, 'grad_norm': 8.875, 'learning_rate': 8.430328642610519e-05, 'epoch': 1.0367170626349893}\n",
      "{'loss': 0.6812, 'grad_norm': 9.375, 'learning_rate': 8.404180791605244e-05, 'epoch': 1.0583153347732182}\n",
      "{'loss': 0.6789, 'grad_norm': 19.875, 'learning_rate': 8.37803294059997e-05, 'epoch': 1.079913606911447}\n",
      "{'loss': 0.6919, 'grad_norm': 6.0625, 'learning_rate': 8.351885089594696e-05, 'epoch': 1.101511879049676}\n",
      "{'loss': 0.7114, 'grad_norm': 17.125, 'learning_rate': 8.325737238589421e-05, 'epoch': 1.123110151187905}\n",
      "{'loss': 0.6174, 'grad_norm': 6.71875, 'learning_rate': 8.299589387584146e-05, 'epoch': 1.144708423326134}\n",
      "{'loss': 0.6438, 'grad_norm': 14.0, 'learning_rate': 8.273441536578872e-05, 'epoch': 1.1663066954643628}\n",
      "{'loss': 0.6604, 'grad_norm': 8.8125, 'learning_rate': 8.247293685573598e-05, 'epoch': 1.187904967602592}\n",
      "{'loss': 0.6536, 'grad_norm': 11.125, 'learning_rate': 8.221145834568323e-05, 'epoch': 1.2095032397408207}\n",
      "{'loss': 0.6807, 'grad_norm': 32.25, 'learning_rate': 8.194997983563049e-05, 'epoch': 1.2311015118790496}\n",
      "{'loss': 0.6677, 'grad_norm': 12.9375, 'learning_rate': 8.168850132557775e-05, 'epoch': 1.2526997840172787}\n",
      "{'loss': 0.6791, 'grad_norm': 9.75, 'learning_rate': 8.1427022815525e-05, 'epoch': 1.2742980561555075}\n",
      "{'loss': 0.6802, 'grad_norm': 16.375, 'learning_rate': 8.116554430547225e-05, 'epoch': 1.2958963282937366}\n",
      "{'loss': 0.6988, 'grad_norm': 14.9375, 'learning_rate': 8.090406579541951e-05, 'epoch': 1.3174946004319654}\n",
      "{'loss': 0.6755, 'grad_norm': 4.875, 'learning_rate': 8.064258728536677e-05, 'epoch': 1.3390928725701943}\n",
      "{'loss': 0.6408, 'grad_norm': 20.5, 'learning_rate': 8.038110877531402e-05, 'epoch': 1.3606911447084233}\n",
      "{'loss': 0.6568, 'grad_norm': 19.125, 'learning_rate': 8.011963026526128e-05, 'epoch': 1.3822894168466522}\n",
      "{'loss': 0.6875, 'grad_norm': 33.0, 'learning_rate': 7.985815175520854e-05, 'epoch': 1.4038876889848813}\n",
      "{'loss': 0.6867, 'grad_norm': 9.3125, 'learning_rate': 7.959667324515579e-05, 'epoch': 1.42548596112311}\n",
      "{'loss': 0.6139, 'grad_norm': 18.125, 'learning_rate': 7.933519473510304e-05, 'epoch': 1.4470842332613392}\n",
      "{'loss': 0.6471, 'grad_norm': 24.375, 'learning_rate': 7.90737162250503e-05, 'epoch': 1.468682505399568}\n",
      "{'loss': 0.6005, 'grad_norm': 15.625, 'learning_rate': 7.881223771499757e-05, 'epoch': 1.490280777537797}\n",
      "{'loss': 0.6379, 'grad_norm': 17.375, 'learning_rate': 7.855075920494483e-05, 'epoch': 1.511879049676026}\n",
      "{'loss': 0.6285, 'grad_norm': 14.4375, 'learning_rate': 7.828928069489208e-05, 'epoch': 1.5334773218142548}\n",
      "{'loss': 0.6485, 'grad_norm': 46.25, 'learning_rate': 7.802780218483932e-05, 'epoch': 1.5550755939524838}\n",
      "{'loss': 0.656, 'grad_norm': 4.375, 'learning_rate': 7.776632367478659e-05, 'epoch': 1.5766738660907127}\n",
      "{'loss': 0.6526, 'grad_norm': 11.5625, 'learning_rate': 7.750484516473385e-05, 'epoch': 1.5982721382289418}\n",
      "{'loss': 0.5848, 'grad_norm': 11.0, 'learning_rate': 7.72433666546811e-05, 'epoch': 1.6198704103671706}\n",
      "{'loss': 0.6633, 'grad_norm': 12.875, 'learning_rate': 7.698188814462836e-05, 'epoch': 1.6414686825053995}\n",
      "{'loss': 0.6183, 'grad_norm': 11.5625, 'learning_rate': 7.672040963457562e-05, 'epoch': 1.6630669546436285}\n",
      "{'loss': 0.5983, 'grad_norm': 21.5, 'learning_rate': 7.645893112452287e-05, 'epoch': 1.6846652267818576}\n",
      "{'loss': 0.6449, 'grad_norm': 19.25, 'learning_rate': 7.619745261447012e-05, 'epoch': 1.7062634989200864}\n",
      "{'loss': 0.6674, 'grad_norm': 10.6875, 'learning_rate': 7.593597410441738e-05, 'epoch': 1.7278617710583153}\n",
      "{'loss': 0.597, 'grad_norm': 42.5, 'learning_rate': 7.567449559436464e-05, 'epoch': 1.7494600431965441}\n",
      "{'loss': 0.6249, 'grad_norm': 31.125, 'learning_rate': 7.541301708431189e-05, 'epoch': 1.7710583153347732}\n",
      "{'loss': 0.6549, 'grad_norm': 8.25, 'learning_rate': 7.515153857425915e-05, 'epoch': 1.7926565874730023}\n",
      "{'loss': 0.6101, 'grad_norm': 27.5, 'learning_rate': 7.489006006420641e-05, 'epoch': 1.8142548596112311}\n",
      "{'loss': 0.5433, 'grad_norm': 4.03125, 'learning_rate': 7.462858155415366e-05, 'epoch': 1.83585313174946}\n",
      "{'loss': 0.6298, 'grad_norm': 24.375, 'learning_rate': 7.436710304410091e-05, 'epoch': 1.857451403887689}\n",
      "{'loss': 0.5644, 'grad_norm': 14.75, 'learning_rate': 7.410562453404817e-05, 'epoch': 1.8790496760259179}\n",
      "{'loss': 0.6146, 'grad_norm': 19.75, 'learning_rate': 7.384414602399543e-05, 'epoch': 1.900647948164147}\n",
      "{'loss': 0.5972, 'grad_norm': 18.625, 'learning_rate': 7.358266751394268e-05, 'epoch': 1.9222462203023758}\n",
      "{'loss': 0.5851, 'grad_norm': 10.5, 'learning_rate': 7.332118900388994e-05, 'epoch': 1.9438444924406046}\n",
      "{'loss': 0.5817, 'grad_norm': 9.0625, 'learning_rate': 7.305971049383719e-05, 'epoch': 1.9654427645788337}\n",
      "{'loss': 0.4958, 'grad_norm': 8.6875, 'learning_rate': 7.279823198378445e-05, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 0.6119539141654968, 'eval_accuracy': 0.783284742468416, 'eval_precision': 0.7913492373631643, 'eval_recall': 0.7957098001325236, 'eval_f1': 0.7906925476576231, 'eval_runtime': 7.5084, 'eval_samples_per_second': 548.185, 'eval_steps_per_second': 68.59, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.7833 f1=0.7907 p=0.7913 r=0.7957\n",
      "{'loss': 0.6305, 'grad_norm': 14.9375, 'learning_rate': 7.25367534737317e-05, 'epoch': 2.0086393088552916}\n",
      "{'loss': 0.5381, 'grad_norm': 12.4375, 'learning_rate': 7.227527496367896e-05, 'epoch': 2.0302375809935205}\n",
      "{'loss': 0.5425, 'grad_norm': 17.375, 'learning_rate': 7.201379645362623e-05, 'epoch': 2.0518358531317493}\n",
      "{'loss': 0.5881, 'grad_norm': 15.375, 'learning_rate': 7.175231794357347e-05, 'epoch': 2.0734341252699786}\n",
      "{'loss': 0.5427, 'grad_norm': 1.7421875, 'learning_rate': 7.149083943352072e-05, 'epoch': 2.0950323974082075}\n",
      "{'loss': 0.5889, 'grad_norm': 21.25, 'learning_rate': 7.122936092346798e-05, 'epoch': 2.1166306695464363}\n",
      "{'loss': 0.592, 'grad_norm': 31.75, 'learning_rate': 7.096788241341525e-05, 'epoch': 2.138228941684665}\n",
      "{'loss': 0.5138, 'grad_norm': 11.25, 'learning_rate': 7.07064039033625e-05, 'epoch': 2.159827213822894}\n",
      "{'loss': 0.509, 'grad_norm': 20.25, 'learning_rate': 7.044492539330976e-05, 'epoch': 2.1814254859611233}\n",
      "{'loss': 0.545, 'grad_norm': 11.125, 'learning_rate': 7.018344688325702e-05, 'epoch': 2.203023758099352}\n",
      "{'loss': 0.5328, 'grad_norm': 9.5, 'learning_rate': 6.992196837320427e-05, 'epoch': 2.224622030237581}\n",
      "{'loss': 0.554, 'grad_norm': 18.75, 'learning_rate': 6.966048986315151e-05, 'epoch': 2.24622030237581}\n",
      "{'loss': 0.5657, 'grad_norm': 2.765625, 'learning_rate': 6.939901135309878e-05, 'epoch': 2.267818574514039}\n",
      "{'loss': 0.5011, 'grad_norm': 9.5, 'learning_rate': 6.913753284304604e-05, 'epoch': 2.289416846652268}\n",
      "{'loss': 0.5037, 'grad_norm': 41.75, 'learning_rate': 6.887605433299329e-05, 'epoch': 2.311015118790497}\n",
      "{'loss': 0.5446, 'grad_norm': 18.625, 'learning_rate': 6.861457582294055e-05, 'epoch': 2.3326133909287257}\n",
      "{'loss': 0.5382, 'grad_norm': 3.265625, 'learning_rate': 6.835309731288781e-05, 'epoch': 2.3542116630669545}\n",
      "{'loss': 0.5013, 'grad_norm': 8.0625, 'learning_rate': 6.809161880283506e-05, 'epoch': 2.375809935205184}\n",
      "{'loss': 0.5556, 'grad_norm': 3.03125, 'learning_rate': 6.783014029278231e-05, 'epoch': 2.3974082073434126}\n",
      "{'loss': 0.5381, 'grad_norm': 7.875, 'learning_rate': 6.756866178272957e-05, 'epoch': 2.4190064794816415}\n",
      "{'loss': 0.5607, 'grad_norm': 20.0, 'learning_rate': 6.730718327267683e-05, 'epoch': 2.4406047516198703}\n",
      "{'loss': 0.5111, 'grad_norm': 13.9375, 'learning_rate': 6.704570476262408e-05, 'epoch': 2.462203023758099}\n",
      "{'loss': 0.5244, 'grad_norm': 83.0, 'learning_rate': 6.678422625257134e-05, 'epoch': 2.4838012958963285}\n",
      "{'loss': 0.5587, 'grad_norm': 31.5, 'learning_rate': 6.652274774251859e-05, 'epoch': 2.5053995680345573}\n",
      "{'loss': 0.5321, 'grad_norm': 17.5, 'learning_rate': 6.626126923246585e-05, 'epoch': 2.526997840172786}\n",
      "{'loss': 0.585, 'grad_norm': 3.0, 'learning_rate': 6.59997907224131e-05, 'epoch': 2.548596112311015}\n",
      "{'loss': 0.5471, 'grad_norm': 5.125, 'learning_rate': 6.573831221236036e-05, 'epoch': 2.570194384449244}\n",
      "{'loss': 0.4943, 'grad_norm': 17.375, 'learning_rate': 6.547683370230762e-05, 'epoch': 2.591792656587473}\n",
      "{'loss': 0.5207, 'grad_norm': 2.578125, 'learning_rate': 6.521535519225487e-05, 'epoch': 2.613390928725702}\n",
      "{'loss': 0.4668, 'grad_norm': 17.25, 'learning_rate': 6.495387668220212e-05, 'epoch': 2.634989200863931}\n",
      "{'loss': 0.5004, 'grad_norm': 15.3125, 'learning_rate': 6.469239817214938e-05, 'epoch': 2.6565874730021597}\n",
      "{'loss': 0.5493, 'grad_norm': 20.875, 'learning_rate': 6.443091966209664e-05, 'epoch': 2.6781857451403885}\n",
      "{'loss': 0.5088, 'grad_norm': 15.75, 'learning_rate': 6.416944115204389e-05, 'epoch': 2.699784017278618}\n",
      "{'loss': 0.5399, 'grad_norm': 17.875, 'learning_rate': 6.390796264199115e-05, 'epoch': 2.7213822894168467}\n",
      "{'loss': 0.5225, 'grad_norm': 3.390625, 'learning_rate': 6.364648413193842e-05, 'epoch': 2.7429805615550755}\n",
      "{'loss': 0.5079, 'grad_norm': 7.71875, 'learning_rate': 6.338500562188568e-05, 'epoch': 2.7645788336933044}\n",
      "{'loss': 0.5275, 'grad_norm': 31.75, 'learning_rate': 6.312352711183291e-05, 'epoch': 2.786177105831533}\n",
      "{'loss': 0.5223, 'grad_norm': 4.96875, 'learning_rate': 6.286204860178018e-05, 'epoch': 2.8077753779697625}\n",
      "{'loss': 0.5369, 'grad_norm': 15.375, 'learning_rate': 6.260057009172744e-05, 'epoch': 2.8293736501079914}\n",
      "{'loss': 0.5753, 'grad_norm': 13.8125, 'learning_rate': 6.23390915816747e-05, 'epoch': 2.85097192224622}\n",
      "{'loss': 0.4959, 'grad_norm': 23.875, 'learning_rate': 6.207761307162195e-05, 'epoch': 2.8725701943844495}\n",
      "{'loss': 0.5714, 'grad_norm': 5.8125, 'learning_rate': 6.181613456156921e-05, 'epoch': 2.8941684665226783}\n",
      "{'loss': 0.4997, 'grad_norm': 22.375, 'learning_rate': 6.155465605151646e-05, 'epoch': 2.915766738660907}\n",
      "{'loss': 0.5129, 'grad_norm': 25.875, 'learning_rate': 6.129317754146372e-05, 'epoch': 2.937365010799136}\n",
      "{'loss': 0.5533, 'grad_norm': 8.4375, 'learning_rate': 6.103169903141097e-05, 'epoch': 2.958963282937365}\n",
      "{'loss': 0.5459, 'grad_norm': 11.75, 'learning_rate': 6.077022052135823e-05, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 0.6176232099533081, 'eval_accuracy': 0.7959183673469388, 'eval_precision': 0.8018489354604428, 'eval_recall': 0.8097527108687312, 'eval_f1': 0.8023920435551096, 'eval_runtime': 8.2222, 'eval_samples_per_second': 500.593, 'eval_steps_per_second': 62.635, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.7959 f1=0.8024 p=0.8018 r=0.8098\n",
      "{'loss': 0.5239, 'grad_norm': 21.5, 'learning_rate': 6.0508742011305485e-05, 'epoch': 3.002159827213823}\n",
      "{'loss': 0.4682, 'grad_norm': 21.0, 'learning_rate': 6.024726350125275e-05, 'epoch': 3.023758099352052}\n",
      "{'loss': 0.4497, 'grad_norm': 13.875, 'learning_rate': 5.998578499119999e-05, 'epoch': 3.0453563714902807}\n",
      "{'loss': 0.5742, 'grad_norm': 17.0, 'learning_rate': 5.972430648114725e-05, 'epoch': 3.0669546436285096}\n",
      "{'loss': 0.4937, 'grad_norm': 26.375, 'learning_rate': 5.9462827971094505e-05, 'epoch': 3.088552915766739}\n",
      "{'loss': 0.477, 'grad_norm': 9.75, 'learning_rate': 5.920134946104177e-05, 'epoch': 3.1101511879049677}\n",
      "{'loss': 0.4617, 'grad_norm': 31.25, 'learning_rate': 5.893987095098902e-05, 'epoch': 3.1317494600431965}\n",
      "{'loss': 0.521, 'grad_norm': 7.375, 'learning_rate': 5.867839244093628e-05, 'epoch': 3.1533477321814254}\n",
      "{'loss': 0.5103, 'grad_norm': 15.5, 'learning_rate': 5.841691393088354e-05, 'epoch': 3.1749460043196542}\n",
      "{'loss': 0.5214, 'grad_norm': 22.0, 'learning_rate': 5.815543542083079e-05, 'epoch': 3.1965442764578835}\n",
      "{'loss': 0.5406, 'grad_norm': 17.625, 'learning_rate': 5.789395691077804e-05, 'epoch': 3.2181425485961124}\n",
      "{'loss': 0.4234, 'grad_norm': 8.3125, 'learning_rate': 5.76324784007253e-05, 'epoch': 3.239740820734341}\n",
      "{'loss': 0.4742, 'grad_norm': 21.875, 'learning_rate': 5.737099989067256e-05, 'epoch': 3.26133909287257}\n",
      "{'loss': 0.4707, 'grad_norm': 24.25, 'learning_rate': 5.7109521380619815e-05, 'epoch': 3.282937365010799}\n",
      "{'loss': 0.5163, 'grad_norm': 26.5, 'learning_rate': 5.684804287056707e-05, 'epoch': 3.304535637149028}\n",
      "{'loss': 0.5521, 'grad_norm': 23.5, 'learning_rate': 5.658656436051432e-05, 'epoch': 3.326133909287257}\n",
      "{'loss': 0.4761, 'grad_norm': 36.25, 'learning_rate': 5.632508585046158e-05, 'epoch': 3.347732181425486}\n",
      "{'loss': 0.459, 'grad_norm': 23.75, 'learning_rate': 5.6063607340408835e-05, 'epoch': 3.3693304535637147}\n",
      "{'loss': 0.4996, 'grad_norm': 4.59375, 'learning_rate': 5.580212883035609e-05, 'epoch': 3.390928725701944}\n",
      "{'loss': 0.5368, 'grad_norm': 21.875, 'learning_rate': 5.554065032030335e-05, 'epoch': 3.412526997840173}\n",
      "{'loss': 0.4261, 'grad_norm': 22.75, 'learning_rate': 5.527917181025061e-05, 'epoch': 3.4341252699784017}\n",
      "{'loss': 0.5116, 'grad_norm': 8.1875, 'learning_rate': 5.5017693300197856e-05, 'epoch': 3.4557235421166306}\n",
      "{'loss': 0.4923, 'grad_norm': 13.875, 'learning_rate': 5.475621479014511e-05, 'epoch': 3.4773218142548594}\n",
      "{'loss': 0.4689, 'grad_norm': 14.875, 'learning_rate': 5.449473628009237e-05, 'epoch': 3.4989200863930887}\n",
      "{'loss': 0.5431, 'grad_norm': 19.0, 'learning_rate': 5.423325777003963e-05, 'epoch': 3.5205183585313176}\n",
      "{'loss': 0.4816, 'grad_norm': 11.625, 'learning_rate': 5.397177925998688e-05, 'epoch': 3.5421166306695464}\n",
      "{'loss': 0.5202, 'grad_norm': 5.90625, 'learning_rate': 5.3710300749934145e-05, 'epoch': 3.5637149028077753}\n",
      "{'loss': 0.4474, 'grad_norm': 3.59375, 'learning_rate': 5.344882223988139e-05, 'epoch': 3.5853131749460045}\n",
      "{'loss': 0.4753, 'grad_norm': 10.3125, 'learning_rate': 5.318734372982865e-05, 'epoch': 3.6069114470842334}\n",
      "{'loss': 0.4686, 'grad_norm': 8.3125, 'learning_rate': 5.2925865219775904e-05, 'epoch': 3.6285097192224622}\n",
      "{'loss': 0.5499, 'grad_norm': 2.0625, 'learning_rate': 5.2664386709723165e-05, 'epoch': 3.650107991360691}\n",
      "{'loss': 0.4925, 'grad_norm': 28.0, 'learning_rate': 5.240290819967042e-05, 'epoch': 3.67170626349892}\n",
      "{'loss': 0.4651, 'grad_norm': 17.75, 'learning_rate': 5.214142968961768e-05, 'epoch': 3.693304535637149}\n",
      "{'loss': 0.4902, 'grad_norm': 28.375, 'learning_rate': 5.187995117956494e-05, 'epoch': 3.714902807775378}\n",
      "{'loss': 0.4852, 'grad_norm': 8.0625, 'learning_rate': 5.1618472669512186e-05, 'epoch': 3.736501079913607}\n",
      "{'loss': 0.4827, 'grad_norm': 35.25, 'learning_rate': 5.135699415945944e-05, 'epoch': 3.7580993520518358}\n",
      "{'loss': 0.4526, 'grad_norm': 17.5, 'learning_rate': 5.10955156494067e-05, 'epoch': 3.7796976241900646}\n",
      "{'loss': 0.515, 'grad_norm': 26.0, 'learning_rate': 5.083403713935396e-05, 'epoch': 3.801295896328294}\n",
      "{'loss': 0.4981, 'grad_norm': 3.203125, 'learning_rate': 5.057255862930121e-05, 'epoch': 3.8228941684665227}\n",
      "{'loss': 0.4468, 'grad_norm': 7.125, 'learning_rate': 5.0311080119248475e-05, 'epoch': 3.8444924406047516}\n",
      "{'loss': 0.4791, 'grad_norm': 20.875, 'learning_rate': 5.0049601609195723e-05, 'epoch': 3.8660907127429804}\n",
      "{'loss': 0.5002, 'grad_norm': 16.625, 'learning_rate': 4.978812309914298e-05, 'epoch': 3.8876889848812093}\n",
      "{'loss': 0.5236, 'grad_norm': 18.25, 'learning_rate': 4.9526644589090234e-05, 'epoch': 3.9092872570194386}\n",
      "{'loss': 0.4485, 'grad_norm': 17.375, 'learning_rate': 4.9265166079037496e-05, 'epoch': 3.9308855291576674}\n",
      "{'loss': 0.5033, 'grad_norm': 30.625, 'learning_rate': 4.900368756898475e-05, 'epoch': 3.9524838012958963}\n",
      "{'loss': 0.4621, 'grad_norm': 14.5, 'learning_rate': 4.8742209058932006e-05, 'epoch': 3.974082073434125}\n",
      "{'loss': 0.4105, 'grad_norm': 16.75, 'learning_rate': 4.8480730548879254e-05, 'epoch': 3.995680345572354}\n",
      "{'eval_loss': 0.6334839463233948, 'eval_accuracy': 0.7985908649173955, 'eval_precision': 0.8029372582150864, 'eval_recall': 0.8137873156669426, 'eval_f1': 0.8047468783662325, 'eval_runtime': 7.6536, 'eval_samples_per_second': 537.788, 'eval_steps_per_second': 67.289, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.7986 f1=0.8047 p=0.8029 r=0.8138\n",
      "{'loss': 0.4418, 'grad_norm': 48.75, 'learning_rate': 4.8219252038826516e-05, 'epoch': 4.017278617710583}\n",
      "{'loss': 0.4569, 'grad_norm': 22.875, 'learning_rate': 4.795777352877377e-05, 'epoch': 4.038876889848812}\n",
      "{'loss': 0.5212, 'grad_norm': 27.125, 'learning_rate': 4.7696295018721026e-05, 'epoch': 4.060475161987041}\n",
      "{'loss': 0.5099, 'grad_norm': 15.8125, 'learning_rate': 4.743481650866829e-05, 'epoch': 4.08207343412527}\n",
      "{'loss': 0.4836, 'grad_norm': 13.5625, 'learning_rate': 4.717333799861554e-05, 'epoch': 4.103671706263499}\n",
      "{'loss': 0.4654, 'grad_norm': 23.375, 'learning_rate': 4.69118594885628e-05, 'epoch': 4.125269978401728}\n",
      "{'loss': 0.4574, 'grad_norm': 13.375, 'learning_rate': 4.665038097851005e-05, 'epoch': 4.146868250539957}\n",
      "{'loss': 0.4631, 'grad_norm': 7.625, 'learning_rate': 4.638890246845731e-05, 'epoch': 4.168466522678186}\n",
      "{'loss': 0.4829, 'grad_norm': 12.375, 'learning_rate': 4.6127423958404564e-05, 'epoch': 4.190064794816415}\n",
      "{'loss': 0.5218, 'grad_norm': 20.625, 'learning_rate': 4.586594544835182e-05, 'epoch': 4.211663066954643}\n",
      "{'loss': 0.4913, 'grad_norm': 20.0, 'learning_rate': 4.560446693829908e-05, 'epoch': 4.233261339092873}\n",
      "{'loss': 0.4501, 'grad_norm': 23.25, 'learning_rate': 4.534298842824633e-05, 'epoch': 4.254859611231102}\n",
      "{'loss': 0.4567, 'grad_norm': 32.25, 'learning_rate': 4.508150991819359e-05, 'epoch': 4.27645788336933}\n",
      "{'loss': 0.4689, 'grad_norm': 18.25, 'learning_rate': 4.482003140814084e-05, 'epoch': 4.29805615550756}\n",
      "{'loss': 0.4588, 'grad_norm': 42.25, 'learning_rate': 4.45585528980881e-05, 'epoch': 4.319654427645788}\n",
      "{'loss': 0.4279, 'grad_norm': 2.140625, 'learning_rate': 4.4297074388035356e-05, 'epoch': 4.341252699784017}\n",
      "{'loss': 0.4498, 'grad_norm': 24.375, 'learning_rate': 4.403559587798262e-05, 'epoch': 4.362850971922247}\n",
      "{'loss': 0.531, 'grad_norm': 18.875, 'learning_rate': 4.377411736792987e-05, 'epoch': 4.384449244060475}\n",
      "{'loss': 0.4981, 'grad_norm': 3.90625, 'learning_rate': 4.351263885787713e-05, 'epoch': 4.406047516198704}\n",
      "{'loss': 0.4383, 'grad_norm': 10.1875, 'learning_rate': 4.3251160347824384e-05, 'epoch': 4.427645788336933}\n",
      "{'loss': 0.4999, 'grad_norm': 5.3125, 'learning_rate': 4.298968183777164e-05, 'epoch': 4.449244060475162}\n",
      "{'loss': 0.4684, 'grad_norm': 26.125, 'learning_rate': 4.2728203327718894e-05, 'epoch': 4.470842332613391}\n",
      "{'loss': 0.4099, 'grad_norm': 13.375, 'learning_rate': 4.246672481766615e-05, 'epoch': 4.49244060475162}\n",
      "{'loss': 0.4433, 'grad_norm': 11.625, 'learning_rate': 4.2205246307613404e-05, 'epoch': 4.514038876889849}\n",
      "{'loss': 0.4259, 'grad_norm': 5.75, 'learning_rate': 4.194376779756066e-05, 'epoch': 4.535637149028078}\n",
      "{'loss': 0.4377, 'grad_norm': 17.125, 'learning_rate': 4.168228928750792e-05, 'epoch': 4.557235421166307}\n",
      "{'loss': 0.4597, 'grad_norm': 17.625, 'learning_rate': 4.142081077745517e-05, 'epoch': 4.578833693304536}\n",
      "{'loss': 0.4907, 'grad_norm': 21.375, 'learning_rate': 4.115933226740243e-05, 'epoch': 4.600431965442764}\n",
      "{'loss': 0.4664, 'grad_norm': 35.0, 'learning_rate': 4.089785375734969e-05, 'epoch': 4.622030237580994}\n",
      "{'loss': 0.482, 'grad_norm': 7.84375, 'learning_rate': 4.063637524729694e-05, 'epoch': 4.643628509719223}\n",
      "{'loss': 0.4776, 'grad_norm': 21.125, 'learning_rate': 4.03748967372442e-05, 'epoch': 4.665226781857451}\n",
      "{'loss': 0.4983, 'grad_norm': 18.875, 'learning_rate': 4.011341822719145e-05, 'epoch': 4.686825053995681}\n",
      "{'loss': 0.5074, 'grad_norm': 29.25, 'learning_rate': 3.985193971713871e-05, 'epoch': 4.708423326133909}\n",
      "{'loss': 0.4385, 'grad_norm': 13.125, 'learning_rate': 3.959046120708596e-05, 'epoch': 4.730021598272138}\n",
      "{'loss': 0.4137, 'grad_norm': 3.40625, 'learning_rate': 3.9328982697033224e-05, 'epoch': 4.751619870410368}\n",
      "{'loss': 0.4743, 'grad_norm': 2.71875, 'learning_rate': 3.906750418698048e-05, 'epoch': 4.773218142548596}\n",
      "{'loss': 0.4725, 'grad_norm': 29.0, 'learning_rate': 3.8806025676927734e-05, 'epoch': 4.794816414686825}\n",
      "{'loss': 0.4546, 'grad_norm': 19.25, 'learning_rate': 3.854454716687499e-05, 'epoch': 4.816414686825054}\n",
      "{'loss': 0.4834, 'grad_norm': 14.3125, 'learning_rate': 3.8283068656822245e-05, 'epoch': 4.838012958963283}\n",
      "{'loss': 0.4864, 'grad_norm': 25.0, 'learning_rate': 3.80215901467695e-05, 'epoch': 4.859611231101512}\n",
      "{'loss': 0.3856, 'grad_norm': 27.75, 'learning_rate': 3.7760111636716755e-05, 'epoch': 4.881209503239741}\n",
      "{'loss': 0.4871, 'grad_norm': 21.125, 'learning_rate': 3.749863312666402e-05, 'epoch': 4.90280777537797}\n",
      "{'loss': 0.4386, 'grad_norm': 13.0625, 'learning_rate': 3.7237154616611265e-05, 'epoch': 4.924406047516198}\n",
      "{'loss': 0.5023, 'grad_norm': 30.5, 'learning_rate': 3.697567610655853e-05, 'epoch': 4.946004319654428}\n",
      "{'loss': 0.4942, 'grad_norm': 24.0, 'learning_rate': 3.671419759650578e-05, 'epoch': 4.967602591792657}\n",
      "{'loss': 0.4966, 'grad_norm': 17.375, 'learning_rate': 3.645271908645304e-05, 'epoch': 4.989200863930885}\n",
      "{'eval_loss': 0.6094524264335632, 'eval_accuracy': 0.8070942662779398, 'eval_precision': 0.8136308880525653, 'eval_recall': 0.8191044616620449, 'eval_f1': 0.8139152199680504, 'eval_runtime': 8.0548, 'eval_samples_per_second': 511.001, 'eval_steps_per_second': 63.937, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.8071 f1=0.8139 p=0.8136 r=0.8191\n",
      "{'loss': 0.4212, 'grad_norm': 15.75, 'learning_rate': 3.619124057640029e-05, 'epoch': 5.010799136069115}\n",
      "{'loss': 0.4587, 'grad_norm': 10.9375, 'learning_rate': 3.5929762066347554e-05, 'epoch': 5.032397408207343}\n",
      "{'loss': 0.4845, 'grad_norm': 15.75, 'learning_rate': 3.56682835562948e-05, 'epoch': 5.053995680345572}\n",
      "{'loss': 0.4543, 'grad_norm': 5.90625, 'learning_rate': 3.5406805046242065e-05, 'epoch': 5.075593952483802}\n",
      "{'loss': 0.4569, 'grad_norm': 16.125, 'learning_rate': 3.514532653618932e-05, 'epoch': 5.09719222462203}\n",
      "{'loss': 0.3947, 'grad_norm': 32.5, 'learning_rate': 3.4883848026136575e-05, 'epoch': 5.118790496760259}\n",
      "{'loss': 0.454, 'grad_norm': 1.3828125, 'learning_rate': 3.462236951608383e-05, 'epoch': 5.140388768898488}\n",
      "{'loss': 0.4669, 'grad_norm': 6.90625, 'learning_rate': 3.4360891006031085e-05, 'epoch': 5.161987041036717}\n",
      "{'loss': 0.4677, 'grad_norm': 14.8125, 'learning_rate': 3.409941249597834e-05, 'epoch': 5.183585313174946}\n",
      "{'loss': 0.432, 'grad_norm': 9.625, 'learning_rate': 3.3837933985925595e-05, 'epoch': 5.205183585313175}\n",
      "{'loss': 0.4197, 'grad_norm': 2.15625, 'learning_rate': 3.357645547587286e-05, 'epoch': 5.226781857451404}\n",
      "{'loss': 0.4443, 'grad_norm': 24.0, 'learning_rate': 3.331497696582011e-05, 'epoch': 5.248380129589632}\n",
      "{'loss': 0.4315, 'grad_norm': 23.25, 'learning_rate': 3.305349845576737e-05, 'epoch': 5.269978401727862}\n",
      "{'loss': 0.4104, 'grad_norm': 15.875, 'learning_rate': 3.279201994571462e-05, 'epoch': 5.291576673866091}\n",
      "{'loss': 0.4299, 'grad_norm': 24.0, 'learning_rate': 3.253054143566188e-05, 'epoch': 5.313174946004319}\n",
      "{'loss': 0.4886, 'grad_norm': 22.75, 'learning_rate': 3.226906292560913e-05, 'epoch': 5.334773218142549}\n",
      "{'loss': 0.4318, 'grad_norm': 8.9375, 'learning_rate': 3.200758441555639e-05, 'epoch': 5.356371490280777}\n",
      "{'loss': 0.5019, 'grad_norm': 18.125, 'learning_rate': 3.174610590550365e-05, 'epoch': 5.377969762419006}\n",
      "{'loss': 0.4005, 'grad_norm': 13.5, 'learning_rate': 3.14846273954509e-05, 'epoch': 5.399568034557236}\n",
      "{'loss': 0.4725, 'grad_norm': 37.75, 'learning_rate': 3.122314888539816e-05, 'epoch': 5.421166306695464}\n",
      "{'loss': 0.4324, 'grad_norm': 8.0625, 'learning_rate': 3.0961670375345415e-05, 'epoch': 5.442764578833693}\n",
      "{'loss': 0.4515, 'grad_norm': 9.1875, 'learning_rate': 3.070019186529267e-05, 'epoch': 5.464362850971923}\n",
      "{'loss': 0.4016, 'grad_norm': 0.70703125, 'learning_rate': 3.0438713355239925e-05, 'epoch': 5.485961123110151}\n",
      "{'loss': 0.4008, 'grad_norm': 30.125, 'learning_rate': 3.0177234845187184e-05, 'epoch': 5.50755939524838}\n",
      "{'loss': 0.5048, 'grad_norm': 1.203125, 'learning_rate': 2.9915756335134436e-05, 'epoch': 5.529157667386609}\n",
      "{'loss': 0.4768, 'grad_norm': 13.25, 'learning_rate': 2.9654277825081694e-05, 'epoch': 5.550755939524838}\n",
      "{'loss': 0.4766, 'grad_norm': 5.34375, 'learning_rate': 2.9392799315028953e-05, 'epoch': 5.572354211663067}\n",
      "{'loss': 0.4825, 'grad_norm': 18.25, 'learning_rate': 2.9131320804976204e-05, 'epoch': 5.593952483801296}\n",
      "{'loss': 0.4503, 'grad_norm': 14.1875, 'learning_rate': 2.8869842294923463e-05, 'epoch': 5.615550755939525}\n",
      "{'loss': 0.5536, 'grad_norm': 64.5, 'learning_rate': 2.8608363784870718e-05, 'epoch': 5.637149028077753}\n",
      "{'loss': 0.4238, 'grad_norm': 13.5, 'learning_rate': 2.8346885274817973e-05, 'epoch': 5.658747300215983}\n",
      "{'loss': 0.4401, 'grad_norm': 36.5, 'learning_rate': 2.8085406764765228e-05, 'epoch': 5.680345572354212}\n",
      "{'loss': 0.4232, 'grad_norm': 16.5, 'learning_rate': 2.7823928254712487e-05, 'epoch': 5.70194384449244}\n",
      "{'loss': 0.4822, 'grad_norm': 16.375, 'learning_rate': 2.7562449744659745e-05, 'epoch': 5.72354211663067}\n",
      "{'loss': 0.4465, 'grad_norm': 24.625, 'learning_rate': 2.7300971234606997e-05, 'epoch': 5.745140388768899}\n",
      "{'loss': 0.4159, 'grad_norm': 15.625, 'learning_rate': 2.7039492724554256e-05, 'epoch': 5.766738660907127}\n",
      "{'loss': 0.4107, 'grad_norm': 0.6875, 'learning_rate': 2.6778014214501514e-05, 'epoch': 5.788336933045357}\n",
      "{'loss': 0.4885, 'grad_norm': 23.0, 'learning_rate': 2.6516535704448766e-05, 'epoch': 5.809935205183585}\n",
      "{'loss': 0.4845, 'grad_norm': 16.0, 'learning_rate': 2.6255057194396024e-05, 'epoch': 5.831533477321814}\n",
      "{'loss': 0.4466, 'grad_norm': 2.09375, 'learning_rate': 2.599357868434328e-05, 'epoch': 5.853131749460044}\n",
      "{'loss': 0.4388, 'grad_norm': 5.28125, 'learning_rate': 2.5732100174290534e-05, 'epoch': 5.874730021598272}\n",
      "{'loss': 0.4602, 'grad_norm': 6.53125, 'learning_rate': 2.547062166423779e-05, 'epoch': 5.896328293736501}\n",
      "{'loss': 0.4594, 'grad_norm': 14.4375, 'learning_rate': 2.5209143154185048e-05, 'epoch': 5.91792656587473}\n",
      "{'loss': 0.4763, 'grad_norm': 15.9375, 'learning_rate': 2.49476646441323e-05, 'epoch': 5.939524838012959}\n",
      "{'loss': 0.4302, 'grad_norm': 27.25, 'learning_rate': 2.468618613407956e-05, 'epoch': 5.961123110151188}\n",
      "{'loss': 0.4687, 'grad_norm': 2.109375, 'learning_rate': 2.4424707624026817e-05, 'epoch': 5.982721382289417}\n",
      "{'eval_loss': 0.618075430393219, 'eval_accuracy': 0.8034499514091351, 'eval_precision': 0.808481065329816, 'eval_recall': 0.8176719198075982, 'eval_f1': 0.8098099904165238, 'eval_runtime': 7.8351, 'eval_samples_per_second': 525.329, 'eval_steps_per_second': 65.73, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.8034 f1=0.8098 p=0.8085 r=0.8177\n",
      "{'loss': 0.4355, 'grad_norm': 30.25, 'learning_rate': 2.416322911397407e-05, 'epoch': 6.004319654427646}\n",
      "{'loss': 0.4292, 'grad_norm': 4.03125, 'learning_rate': 2.3901750603921327e-05, 'epoch': 6.025917926565874}\n",
      "{'loss': 0.476, 'grad_norm': 15.8125, 'learning_rate': 2.3640272093868586e-05, 'epoch': 6.047516198704104}\n",
      "{'loss': 0.4613, 'grad_norm': 18.625, 'learning_rate': 2.3378793583815837e-05, 'epoch': 6.069114470842333}\n",
      "{'loss': 0.4966, 'grad_norm': 11.0, 'learning_rate': 2.3117315073763096e-05, 'epoch': 6.090712742980561}\n",
      "{'loss': 0.423, 'grad_norm': 16.375, 'learning_rate': 2.285583656371035e-05, 'epoch': 6.112311015118791}\n",
      "{'loss': 0.4763, 'grad_norm': 1.7890625, 'learning_rate': 2.2594358053657606e-05, 'epoch': 6.133909287257019}\n",
      "{'loss': 0.4325, 'grad_norm': 11.125, 'learning_rate': 2.233287954360486e-05, 'epoch': 6.155507559395248}\n",
      "{'loss': 0.3931, 'grad_norm': 13.8125, 'learning_rate': 2.207140103355212e-05, 'epoch': 6.177105831533478}\n",
      "{'loss': 0.4323, 'grad_norm': 19.0, 'learning_rate': 2.1809922523499375e-05, 'epoch': 6.198704103671706}\n",
      "{'loss': 0.4671, 'grad_norm': 24.125, 'learning_rate': 2.154844401344663e-05, 'epoch': 6.220302375809935}\n",
      "{'loss': 0.4834, 'grad_norm': 11.0, 'learning_rate': 2.128696550339389e-05, 'epoch': 6.241900647948164}\n",
      "{'loss': 0.4359, 'grad_norm': 14.625, 'learning_rate': 2.1025486993341144e-05, 'epoch': 6.263498920086393}\n",
      "{'loss': 0.4472, 'grad_norm': 2.375, 'learning_rate': 2.07640084832884e-05, 'epoch': 6.285097192224622}\n",
      "{'loss': 0.4295, 'grad_norm': 14.25, 'learning_rate': 2.0502529973235657e-05, 'epoch': 6.306695464362851}\n",
      "{'loss': 0.4461, 'grad_norm': 9.6875, 'learning_rate': 2.0241051463182912e-05, 'epoch': 6.32829373650108}\n",
      "{'loss': 0.4531, 'grad_norm': 11.8125, 'learning_rate': 1.9979572953130168e-05, 'epoch': 6.3498920086393085}\n",
      "{'loss': 0.4361, 'grad_norm': 38.5, 'learning_rate': 1.9718094443077423e-05, 'epoch': 6.371490280777538}\n",
      "{'loss': 0.4364, 'grad_norm': 20.125, 'learning_rate': 1.9456615933024678e-05, 'epoch': 6.393088552915767}\n",
      "{'loss': 0.406, 'grad_norm': 7.1875, 'learning_rate': 1.9195137422971936e-05, 'epoch': 6.4146868250539955}\n",
      "{'loss': 0.4608, 'grad_norm': 20.25, 'learning_rate': 1.893365891291919e-05, 'epoch': 6.436285097192225}\n",
      "{'loss': 0.4645, 'grad_norm': 55.75, 'learning_rate': 1.8672180402866447e-05, 'epoch': 6.457883369330453}\n",
      "{'loss': 0.432, 'grad_norm': 42.25, 'learning_rate': 1.8410701892813705e-05, 'epoch': 6.479481641468682}\n",
      "{'loss': 0.4308, 'grad_norm': 11.5625, 'learning_rate': 1.814922338276096e-05, 'epoch': 6.501079913606912}\n",
      "{'loss': 0.4451, 'grad_norm': 2.140625, 'learning_rate': 1.7887744872708215e-05, 'epoch': 6.52267818574514}\n",
      "{'loss': 0.3966, 'grad_norm': 2.9375, 'learning_rate': 1.762626636265547e-05, 'epoch': 6.544276457883369}\n",
      "{'loss': 0.4488, 'grad_norm': 11.0625, 'learning_rate': 1.7364787852602725e-05, 'epoch': 6.565874730021598}\n",
      "{'loss': 0.496, 'grad_norm': 13.1875, 'learning_rate': 1.7103309342549984e-05, 'epoch': 6.587473002159827}\n",
      "{'loss': 0.4354, 'grad_norm': 25.125, 'learning_rate': 1.684183083249724e-05, 'epoch': 6.609071274298056}\n",
      "{'loss': 0.448, 'grad_norm': 30.25, 'learning_rate': 1.6580352322444494e-05, 'epoch': 6.630669546436285}\n",
      "{'loss': 0.4378, 'grad_norm': 18.0, 'learning_rate': 1.6318873812391753e-05, 'epoch': 6.652267818574514}\n",
      "{'loss': 0.431, 'grad_norm': 16.625, 'learning_rate': 1.6057395302339008e-05, 'epoch': 6.6738660907127425}\n",
      "{'loss': 0.447, 'grad_norm': 21.875, 'learning_rate': 1.5795916792286263e-05, 'epoch': 6.695464362850972}\n",
      "{'loss': 0.457, 'grad_norm': 24.625, 'learning_rate': 1.553443828223352e-05, 'epoch': 6.717062634989201}\n",
      "{'loss': 0.4439, 'grad_norm': 23.5, 'learning_rate': 1.5272959772180777e-05, 'epoch': 6.7386609071274295}\n",
      "{'loss': 0.4197, 'grad_norm': 6.25, 'learning_rate': 1.501148126212803e-05, 'epoch': 6.760259179265659}\n",
      "{'loss': 0.4133, 'grad_norm': 25.875, 'learning_rate': 1.4750002752075289e-05, 'epoch': 6.781857451403888}\n",
      "{'loss': 0.4112, 'grad_norm': 6.5, 'learning_rate': 1.4488524242022544e-05, 'epoch': 6.8034557235421165}\n",
      "{'loss': 0.5085, 'grad_norm': 17.875, 'learning_rate': 1.42270457319698e-05, 'epoch': 6.825053995680346}\n",
      "{'loss': 0.5128, 'grad_norm': 20.625, 'learning_rate': 1.3965567221917056e-05, 'epoch': 6.846652267818574}\n",
      "{'loss': 0.4298, 'grad_norm': 2.375, 'learning_rate': 1.370408871186431e-05, 'epoch': 6.8682505399568035}\n",
      "{'loss': 0.4419, 'grad_norm': 1.9296875, 'learning_rate': 1.344261020181157e-05, 'epoch': 6.889848812095033}\n",
      "{'loss': 0.451, 'grad_norm': 32.5, 'learning_rate': 1.3181131691758824e-05, 'epoch': 6.911447084233261}\n",
      "{'loss': 0.4712, 'grad_norm': 11.9375, 'learning_rate': 1.291965318170608e-05, 'epoch': 6.93304535637149}\n",
      "{'loss': 0.3759, 'grad_norm': 44.25, 'learning_rate': 1.2658174671653336e-05, 'epoch': 6.954643628509719}\n",
      "{'loss': 0.4417, 'grad_norm': 29.5, 'learning_rate': 1.2396696161600591e-05, 'epoch': 6.976241900647948}\n",
      "{'loss': 0.4857, 'grad_norm': 1.0625, 'learning_rate': 1.2135217651547847e-05, 'epoch': 6.997840172786177}\n",
      "{'eval_loss': 0.6165176033973694, 'eval_accuracy': 0.8044217687074829, 'eval_precision': 0.8090323840416025, 'eval_recall': 0.8181973802324801, 'eval_f1': 0.8106694743148847, 'eval_runtime': 7.5627, 'eval_samples_per_second': 544.251, 'eval_steps_per_second': 68.098, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.8044 f1=0.8107 p=0.8090 r=0.8182\n",
      "{'loss': 0.5104, 'grad_norm': 27.0, 'learning_rate': 1.1873739141495105e-05, 'epoch': 7.019438444924406}\n",
      "{'loss': 0.396, 'grad_norm': 13.4375, 'learning_rate': 1.161226063144236e-05, 'epoch': 7.041036717062635}\n",
      "{'loss': 0.4592, 'grad_norm': 22.0, 'learning_rate': 1.1350782121389615e-05, 'epoch': 7.0626349892008635}\n",
      "{'loss': 0.4698, 'grad_norm': 10.1875, 'learning_rate': 1.1089303611336872e-05, 'epoch': 7.084233261339093}\n",
      "{'loss': 0.4237, 'grad_norm': 15.4375, 'learning_rate': 1.0827825101284129e-05, 'epoch': 7.105831533477322}\n",
      "{'loss': 0.4396, 'grad_norm': 23.875, 'learning_rate': 1.0566346591231384e-05, 'epoch': 7.1274298056155505}\n",
      "{'loss': 0.3983, 'grad_norm': 9.9375, 'learning_rate': 1.0304868081178641e-05, 'epoch': 7.14902807775378}\n",
      "{'loss': 0.5122, 'grad_norm': 5.875, 'learning_rate': 1.0043389571125896e-05, 'epoch': 7.170626349892009}\n",
      "{'loss': 0.4653, 'grad_norm': 14.5625, 'learning_rate': 9.781911061073153e-06, 'epoch': 7.1922246220302375}\n",
      "{'loss': 0.4686, 'grad_norm': 7.0, 'learning_rate': 9.520432551020408e-06, 'epoch': 7.213822894168467}\n",
      "{'loss': 0.4938, 'grad_norm': 9.0, 'learning_rate': 9.258954040967665e-06, 'epoch': 7.235421166306695}\n",
      "{'loss': 0.4221, 'grad_norm': 22.125, 'learning_rate': 8.99747553091492e-06, 'epoch': 7.2570194384449245}\n",
      "{'loss': 0.4697, 'grad_norm': 30.5, 'learning_rate': 8.735997020862175e-06, 'epoch': 7.278617710583154}\n",
      "{'loss': 0.4326, 'grad_norm': 25.625, 'learning_rate': 8.474518510809432e-06, 'epoch': 7.300215982721382}\n",
      "{'loss': 0.4315, 'grad_norm': 17.75, 'learning_rate': 8.213040000756689e-06, 'epoch': 7.3218142548596115}\n",
      "{'loss': 0.4224, 'grad_norm': 6.4375, 'learning_rate': 7.951561490703945e-06, 'epoch': 7.34341252699784}\n",
      "{'loss': 0.4863, 'grad_norm': 22.25, 'learning_rate': 7.6900829806512e-06, 'epoch': 7.365010799136069}\n",
      "{'loss': 0.4518, 'grad_norm': 11.375, 'learning_rate': 7.4286044705984565e-06, 'epoch': 7.386609071274298}\n",
      "{'loss': 0.3903, 'grad_norm': 13.0, 'learning_rate': 7.1671259605457125e-06, 'epoch': 7.408207343412527}\n",
      "{'loss': 0.4003, 'grad_norm': 11.5625, 'learning_rate': 6.905647450492969e-06, 'epoch': 7.429805615550756}\n",
      "{'loss': 0.4194, 'grad_norm': 21.125, 'learning_rate': 6.6441689404402245e-06, 'epoch': 7.4514038876889845}\n",
      "{'loss': 0.423, 'grad_norm': 21.625, 'learning_rate': 6.38269043038748e-06, 'epoch': 7.473002159827214}\n",
      "{'loss': 0.4335, 'grad_norm': 13.8125, 'learning_rate': 6.121211920334737e-06, 'epoch': 7.494600431965443}\n",
      "{'loss': 0.4494, 'grad_norm': 45.5, 'learning_rate': 5.859733410281992e-06, 'epoch': 7.5161987041036715}\n",
      "{'loss': 0.4449, 'grad_norm': 28.125, 'learning_rate': 5.598254900229248e-06, 'epoch': 7.537796976241901}\n",
      "{'loss': 0.4573, 'grad_norm': 21.625, 'learning_rate': 5.336776390176504e-06, 'epoch': 7.559395248380129}\n",
      "{'loss': 0.4086, 'grad_norm': 22.0, 'learning_rate': 5.07529788012376e-06, 'epoch': 7.5809935205183585}\n",
      "{'loss': 0.4503, 'grad_norm': 20.375, 'learning_rate': 4.813819370071017e-06, 'epoch': 7.602591792656588}\n",
      "{'loss': 0.4649, 'grad_norm': 8.5, 'learning_rate': 4.552340860018272e-06, 'epoch': 7.624190064794816}\n",
      "{'loss': 0.4409, 'grad_norm': 32.5, 'learning_rate': 4.290862349965529e-06, 'epoch': 7.6457883369330455}\n",
      "{'loss': 0.4513, 'grad_norm': 48.0, 'learning_rate': 4.029383839912785e-06, 'epoch': 7.667386609071274}\n",
      "{'loss': 0.4917, 'grad_norm': 7.5625, 'learning_rate': 3.767905329860041e-06, 'epoch': 7.688984881209503}\n",
      "{'loss': 0.404, 'grad_norm': 6.40625, 'learning_rate': 3.506426819807297e-06, 'epoch': 7.7105831533477325}\n",
      "{'loss': 0.4786, 'grad_norm': 17.75, 'learning_rate': 3.2449483097545525e-06, 'epoch': 7.732181425485961}\n",
      "{'loss': 0.3973, 'grad_norm': 23.875, 'learning_rate': 2.983469799701809e-06, 'epoch': 7.75377969762419}\n",
      "{'loss': 0.4461, 'grad_norm': 19.875, 'learning_rate': 2.721991289649065e-06, 'epoch': 7.775377969762419}\n",
      "{'loss': 0.4275, 'grad_norm': 32.25, 'learning_rate': 2.460512779596321e-06, 'epoch': 7.796976241900648}\n",
      "{'loss': 0.4121, 'grad_norm': 24.375, 'learning_rate': 2.1990342695435768e-06, 'epoch': 7.818574514038877}\n",
      "{'loss': 0.4057, 'grad_norm': 11.8125, 'learning_rate': 1.9375557594908327e-06, 'epoch': 7.840172786177106}\n",
      "{'loss': 0.4477, 'grad_norm': 7.96875, 'learning_rate': 1.6760772494380887e-06, 'epoch': 7.861771058315335}\n",
      "{'loss': 0.4571, 'grad_norm': 23.75, 'learning_rate': 1.4145987393853449e-06, 'epoch': 7.883369330453563}\n",
      "{'loss': 0.447, 'grad_norm': 18.25, 'learning_rate': 1.1531202293326009e-06, 'epoch': 7.9049676025917925}\n",
      "{'loss': 0.4071, 'grad_norm': 44.75, 'learning_rate': 8.916417192798569e-07, 'epoch': 7.926565874730022}\n",
      "{'loss': 0.4005, 'grad_norm': 12.5625, 'learning_rate': 6.30163209227113e-07, 'epoch': 7.94816414686825}\n",
      "{'loss': 0.4959, 'grad_norm': 44.75, 'learning_rate': 3.68684699174369e-07, 'epoch': 7.9697624190064795}\n",
      "{'loss': 0.48, 'grad_norm': 26.875, 'learning_rate': 1.0720618912162502e-07, 'epoch': 7.991360691144708}\n",
      "{'eval_loss': 0.6203649044036865, 'eval_accuracy': 0.8049076773566569, 'eval_precision': 0.8091414644119885, 'eval_recall': 0.8199392443459897, 'eval_f1': 0.8111619559212903, 'eval_runtime': 8.0672, 'eval_samples_per_second': 510.213, 'eval_steps_per_second': 63.839, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8049 f1=0.8112 p=0.8091 r=0.8199\n",
      "{'train_runtime': 1365.5156, 'train_samples_per_second': 216.996, 'train_steps_per_second': 27.125, 'train_loss': 0.5704191037954573, 'epoch': 8.0}\n",
      "{'eval_loss': 0.6094524264335632, 'eval_accuracy': 0.8070942662779398, 'eval_precision': 0.8136308880525653, 'eval_recall': 0.8191044616620449, 'eval_f1': 0.8139152199680504, 'eval_runtime': 7.9214, 'eval_samples_per_second': 519.606, 'eval_steps_per_second': 65.014, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8071 f1=0.8139 p=0.8136 r=0.8191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÉ‚ñÅ‚ñà‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÜ‚ñÖ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÇ‚ñÑ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÜ‚ñà‚ñÅ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÇ‚ñÑ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÅ‚ñÑ‚ñÜ‚ñÇ‚ñÖ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñÅ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñÜ‚ñÖ</td></tr><tr><td>train/learning_rate</td><td>‚ñÉ‚ñÜ‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.81392</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.80709</td></tr><tr><td>eval/f1</td><td>0.81392</td></tr><tr><td>eval/loss</td><td>0.60945</td></tr><tr><td>eval/precision</td><td>0.81363</td></tr><tr><td>eval/recall</td><td>0.8191</td></tr><tr><td>eval/runtime</td><td>7.9214</td></tr><tr><td>eval/samples_per_second</td><td>519.606</td></tr><tr><td>eval/steps_per_second</td><td>65.014</td></tr><tr><td>total_flos</td><td>1.1916090232884816e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>37040</td></tr><tr><td>train/grad_norm</td><td>26.875</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.48</td></tr><tr><td>train_loss</td><td>0.57042</td></tr><tr><td>train_runtime</td><td>1365.5156</td></tr><tr><td>train_samples_per_second</td><td>216.996</td></tr><tr><td>train_steps_per_second</td><td>27.125</td></tr><tr><td>trial/accuracy</td><td>0.80709</td></tr><tr><td>trial/f1</td><td>0.81392</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t14</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/a0tp2n0d' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/a0tp2n0d</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_053918-a0tp2n0d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.858824:  33%|‚ñà‚ñà‚ñà‚ñé      | 5/15 [1:27:19<3:20:50, 1205.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=14 f1=0.8139\n",
      "[I 2025-08-17 06:02:16,175] Trial 4 finished with value: 0.8139152199680504 and parameters: {'lr': 9.103897284506387e-05, 'weight_decay': 1.33939343338835e-05, 'unfreeze_last_k': 8, 'batch_size': 8}. Best is trial 2 with value: 0.8588243859157035.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_060216-qkmvqzzv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/qkmvqzzv' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t15</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/qkmvqzzv' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/qkmvqzzv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=15 | epochs=8 bs=16 lr=3.76e-04 wd=5.7e-06 k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=8 bs=16 lr=3.76e-04 wd=5.7e-06 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/2315] loss=1.6335 lr=3.35e-05\n",
      "{'loss': 1.6335, 'grad_norm': 2.3125, 'learning_rate': 3.345477547562474e-05, 'epoch': 0.04319654427645788}\n",
      "[e0 b200/2315] loss=1.5646 lr=6.72e-05\n",
      "{'loss': 1.5646, 'grad_norm': 6.125, 'learning_rate': 6.724747797625579e-05, 'epoch': 0.08639308855291576}\n",
      "[e0 b300/2315] loss=1.5564 lr=1.01e-04\n",
      "{'loss': 1.5564, 'grad_norm': 1.9375, 'learning_rate': 0.00010104018047688683, 'epoch': 0.12958963282937366}\n",
      "[e0 b400/2315] loss=1.5355 lr=1.35e-04\n",
      "{'loss': 1.5355, 'grad_norm': 5.65625, 'learning_rate': 0.00013483288297751788, 'epoch': 0.17278617710583152}\n",
      "[e0 b500/2315] loss=1.5601 lr=1.69e-04\n",
      "{'loss': 1.5601, 'grad_norm': 1.96875, 'learning_rate': 0.00016862558547814895, 'epoch': 0.2159827213822894}\n",
      "[e0 b600/2315] loss=1.5900 lr=2.02e-04\n",
      "{'loss': 1.59, 'grad_norm': 1.3359375, 'learning_rate': 0.00020241828797877998, 'epoch': 0.2591792656587473}\n",
      "[e0 b700/2315] loss=1.5881 lr=2.36e-04\n",
      "{'loss': 1.5881, 'grad_norm': 1.8046875, 'learning_rate': 0.00023621099047941104, 'epoch': 0.3023758099352052}\n",
      "[e0 b800/2315] loss=1.5870 lr=2.70e-04\n",
      "{'loss': 1.587, 'grad_norm': 1.84375, 'learning_rate': 0.0002700036929800421, 'epoch': 0.34557235421166305}\n",
      "[e0 b900/2315] loss=1.5705 lr=3.04e-04\n",
      "{'loss': 1.5705, 'grad_norm': 1.0, 'learning_rate': 0.0003037963954806731, 'epoch': 0.38876889848812096}\n",
      "[e0 b1000/2315] loss=1.5925 lr=3.38e-04\n",
      "{'loss': 1.5925, 'grad_norm': 0.78515625, 'learning_rate': 0.0003375890979813042, 'epoch': 0.4319654427645788}\n",
      "[e0 b1100/2315] loss=1.5896 lr=3.71e-04\n",
      "{'loss': 1.5896, 'grad_norm': 1.078125, 'learning_rate': 0.00037138180048193524, 'epoch': 0.47516198704103674}\n",
      "[e0 b1200/2315] loss=1.5838 lr=3.74e-04\n",
      "{'loss': 1.5838, 'grad_norm': 0.625, 'learning_rate': 0.00037389684100122626, 'epoch': 0.5183585313174947}\n",
      "[e0 b1300/2315] loss=1.5796 lr=3.72e-04\n",
      "{'loss': 1.5796, 'grad_norm': 1.0625, 'learning_rate': 0.00037173820789112156, 'epoch': 0.5615550755939525}\n",
      "[e0 b1400/2315] loss=1.5775 lr=3.70e-04\n",
      "{'loss': 1.5775, 'grad_norm': 0.953125, 'learning_rate': 0.0003695795747810169, 'epoch': 0.6047516198704104}\n",
      "[e0 b1500/2315] loss=1.5788 lr=3.67e-04\n",
      "{'loss': 1.5788, 'grad_norm': 1.296875, 'learning_rate': 0.0003674209416709123, 'epoch': 0.6479481641468683}\n",
      "[e0 b1600/2315] loss=1.5795 lr=3.65e-04\n",
      "{'loss': 1.5795, 'grad_norm': 1.109375, 'learning_rate': 0.00036526230856080764, 'epoch': 0.6911447084233261}\n",
      "[e0 b1700/2315] loss=1.5812 lr=3.63e-04\n",
      "{'loss': 1.5812, 'grad_norm': 0.82421875, 'learning_rate': 0.000363103675450703, 'epoch': 0.734341252699784}\n",
      "[e0 b1800/2315] loss=1.5682 lr=3.61e-04\n",
      "{'loss': 1.5682, 'grad_norm': 1.078125, 'learning_rate': 0.00036094504234059836, 'epoch': 0.7775377969762419}\n",
      "[e0 b1900/2315] loss=1.6037 lr=3.59e-04\n",
      "{'loss': 1.6037, 'grad_norm': 0.8828125, 'learning_rate': 0.0003587864092304937, 'epoch': 0.8207343412526998}\n",
      "[e0 b2000/2315] loss=1.5698 lr=3.57e-04\n",
      "{'loss': 1.5698, 'grad_norm': 1.3515625, 'learning_rate': 0.0003566277761203891, 'epoch': 0.8639308855291576}\n",
      "[e0 b2100/2315] loss=1.5807 lr=3.54e-04\n",
      "{'loss': 1.5807, 'grad_norm': 0.71484375, 'learning_rate': 0.00035446914301028444, 'epoch': 0.9071274298056156}\n",
      "[e0 b2200/2315] loss=1.5829 lr=3.52e-04\n",
      "{'loss': 1.5829, 'grad_norm': 1.09375, 'learning_rate': 0.00035231050990017974, 'epoch': 0.9503239740820735}\n",
      "[e0 b2300/2315] loss=1.5786 lr=3.50e-04\n",
      "{'loss': 1.5786, 'grad_norm': 0.80078125, 'learning_rate': 0.0003501518767900751, 'epoch': 0.9935205183585313}\n",
      "{'eval_loss': 1.577258586883545, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.9512, 'eval_samples_per_second': 1041.722, 'eval_steps_per_second': 65.297, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5829, 'grad_norm': 1.0625, 'learning_rate': 0.0003479932436799704, 'epoch': 1.0367170626349893}\n",
      "{'loss': 1.5857, 'grad_norm': 1.1640625, 'learning_rate': 0.00034583461056986577, 'epoch': 1.079913606911447}\n",
      "{'loss': 1.5808, 'grad_norm': 1.921875, 'learning_rate': 0.0003436759774597611, 'epoch': 1.123110151187905}\n",
      "{'loss': 1.5672, 'grad_norm': 0.5703125, 'learning_rate': 0.0003415173443496565, 'epoch': 1.1663066954643628}\n",
      "{'loss': 1.5672, 'grad_norm': 0.91015625, 'learning_rate': 0.00033935871123955184, 'epoch': 1.2095032397408207}\n",
      "{'loss': 1.589, 'grad_norm': 1.15625, 'learning_rate': 0.0003372000781294472, 'epoch': 1.2526997840172787}\n",
      "{'loss': 1.5826, 'grad_norm': 1.234375, 'learning_rate': 0.00033504144501934256, 'epoch': 1.2958963282937366}\n",
      "{'loss': 1.5822, 'grad_norm': 0.81640625, 'learning_rate': 0.0003328828119092379, 'epoch': 1.3390928725701943}\n",
      "{'loss': 1.5844, 'grad_norm': 0.71484375, 'learning_rate': 0.0003307241787991333, 'epoch': 1.3822894168466522}\n",
      "{'loss': 1.5762, 'grad_norm': 0.57421875, 'learning_rate': 0.00032856554568902864, 'epoch': 1.42548596112311}\n",
      "{'loss': 1.5655, 'grad_norm': 0.68359375, 'learning_rate': 0.00032640691257892395, 'epoch': 1.468682505399568}\n",
      "{'loss': 1.5982, 'grad_norm': 1.28125, 'learning_rate': 0.0003242482794688193, 'epoch': 1.511879049676026}\n",
      "{'loss': 1.5745, 'grad_norm': 0.82421875, 'learning_rate': 0.00032208964635871466, 'epoch': 1.5550755939524838}\n",
      "{'loss': 1.5812, 'grad_norm': 0.94921875, 'learning_rate': 0.00031993101324861, 'epoch': 1.5982721382289418}\n",
      "{'loss': 1.5855, 'grad_norm': 0.8359375, 'learning_rate': 0.0003177723801385054, 'epoch': 1.6414686825053995}\n",
      "{'loss': 1.5686, 'grad_norm': 1.3046875, 'learning_rate': 0.0003156137470284007, 'epoch': 1.6846652267818576}\n",
      "{'loss': 1.5722, 'grad_norm': 0.7265625, 'learning_rate': 0.00031345511391829605, 'epoch': 1.7278617710583153}\n",
      "{'loss': 1.5766, 'grad_norm': 0.64453125, 'learning_rate': 0.0003112964808081914, 'epoch': 1.7710583153347732}\n",
      "{'loss': 1.5879, 'grad_norm': 0.70703125, 'learning_rate': 0.00030913784769808677, 'epoch': 1.8142548596112311}\n",
      "{'loss': 1.5672, 'grad_norm': 0.65234375, 'learning_rate': 0.0003069792145879821, 'epoch': 1.857451403887689}\n",
      "{'loss': 1.5805, 'grad_norm': 1.1953125, 'learning_rate': 0.0003048205814778775, 'epoch': 1.900647948164147}\n",
      "{'loss': 1.5834, 'grad_norm': 1.109375, 'learning_rate': 0.00030266194836777284, 'epoch': 1.9438444924406046}\n",
      "{'loss': 1.5737, 'grad_norm': 0.53515625, 'learning_rate': 0.00030050331525766815, 'epoch': 1.9870410367170628}\n",
      "{'eval_loss': 1.578637719154358, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.8687, 'eval_samples_per_second': 1063.929, 'eval_steps_per_second': 66.689, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5818, 'grad_norm': 0.6640625, 'learning_rate': 0.0002983446821475635, 'epoch': 2.0302375809935205}\n",
      "{'loss': 1.585, 'grad_norm': 0.8828125, 'learning_rate': 0.00029618604903745887, 'epoch': 2.0734341252699786}\n",
      "{'loss': 1.5796, 'grad_norm': 0.55078125, 'learning_rate': 0.0002940274159273542, 'epoch': 2.1166306695464363}\n",
      "{'loss': 1.5785, 'grad_norm': 0.63671875, 'learning_rate': 0.0002918687828172496, 'epoch': 2.159827213822894}\n",
      "{'loss': 1.578, 'grad_norm': 0.431640625, 'learning_rate': 0.00028971014970714494, 'epoch': 2.203023758099352}\n",
      "{'loss': 1.5688, 'grad_norm': 0.765625, 'learning_rate': 0.0002875515165970403, 'epoch': 2.24622030237581}\n",
      "{'loss': 1.5688, 'grad_norm': 0.7890625, 'learning_rate': 0.00028539288348693566, 'epoch': 2.289416846652268}\n",
      "{'loss': 1.5727, 'grad_norm': 0.96484375, 'learning_rate': 0.000283234250376831, 'epoch': 2.3326133909287257}\n",
      "{'loss': 1.5822, 'grad_norm': 0.7734375, 'learning_rate': 0.00028107561726672633, 'epoch': 2.375809935205184}\n",
      "{'loss': 1.5834, 'grad_norm': 0.5703125, 'learning_rate': 0.0002789169841566217, 'epoch': 2.4190064794816415}\n",
      "{'loss': 1.5731, 'grad_norm': 1.0, 'learning_rate': 0.000276758351046517, 'epoch': 2.462203023758099}\n",
      "{'loss': 1.577, 'grad_norm': 1.0390625, 'learning_rate': 0.00027459971793641235, 'epoch': 2.5053995680345573}\n",
      "{'loss': 1.5788, 'grad_norm': 0.390625, 'learning_rate': 0.0002724410848263077, 'epoch': 2.548596112311015}\n",
      "{'loss': 1.5774, 'grad_norm': 0.6875, 'learning_rate': 0.00027028245171620307, 'epoch': 2.591792656587473}\n",
      "{'loss': 1.5822, 'grad_norm': 0.76953125, 'learning_rate': 0.00026812381860609843, 'epoch': 2.634989200863931}\n",
      "{'loss': 1.5794, 'grad_norm': 0.87109375, 'learning_rate': 0.0002659651854959938, 'epoch': 2.6781857451403885}\n",
      "{'loss': 1.5804, 'grad_norm': 1.1875, 'learning_rate': 0.00026380655238588915, 'epoch': 2.7213822894168467}\n",
      "{'loss': 1.5795, 'grad_norm': 0.484375, 'learning_rate': 0.0002616479192757845, 'epoch': 2.7645788336933044}\n",
      "{'loss': 1.5756, 'grad_norm': 0.5078125, 'learning_rate': 0.00025948928616567987, 'epoch': 2.8077753779697625}\n",
      "{'loss': 1.5772, 'grad_norm': 0.609375, 'learning_rate': 0.0002573306530555752, 'epoch': 2.85097192224622}\n",
      "{'loss': 1.5786, 'grad_norm': 0.77734375, 'learning_rate': 0.00025517201994547053, 'epoch': 2.8941684665226783}\n",
      "{'loss': 1.5779, 'grad_norm': 0.6953125, 'learning_rate': 0.0002530133868353659, 'epoch': 2.937365010799136}\n",
      "{'loss': 1.5797, 'grad_norm': 0.88671875, 'learning_rate': 0.00025085475372526125, 'epoch': 2.980561555075594}\n",
      "{'eval_loss': 1.5761891603469849, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.8991, 'eval_samples_per_second': 1055.617, 'eval_steps_per_second': 66.168, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5743, 'grad_norm': 0.419921875, 'learning_rate': 0.0002486961206151566, 'epoch': 3.023758099352052}\n",
      "{'loss': 1.576, 'grad_norm': 0.65234375, 'learning_rate': 0.00024653748750505197, 'epoch': 3.0669546436285096}\n",
      "{'loss': 1.5643, 'grad_norm': 0.87109375, 'learning_rate': 0.0002443788543949473, 'epoch': 3.1101511879049677}\n",
      "{'loss': 1.5779, 'grad_norm': 0.85546875, 'learning_rate': 0.00024222022128484263, 'epoch': 3.1533477321814254}\n",
      "{'loss': 1.5876, 'grad_norm': 0.63671875, 'learning_rate': 0.000240061588174738, 'epoch': 3.1965442764578835}\n",
      "{'loss': 1.5935, 'grad_norm': 1.0703125, 'learning_rate': 0.00023790295506463335, 'epoch': 3.239740820734341}\n",
      "{'loss': 1.5688, 'grad_norm': 0.52734375, 'learning_rate': 0.00023574432195452868, 'epoch': 3.282937365010799}\n",
      "{'loss': 1.5776, 'grad_norm': 0.51953125, 'learning_rate': 0.00023358568884442404, 'epoch': 3.326133909287257}\n",
      "{'loss': 1.5794, 'grad_norm': 0.83984375, 'learning_rate': 0.0002314270557343194, 'epoch': 3.3693304535637147}\n",
      "{'loss': 1.5814, 'grad_norm': 1.015625, 'learning_rate': 0.00022926842262421476, 'epoch': 3.412526997840173}\n",
      "{'loss': 1.5784, 'grad_norm': 0.78515625, 'learning_rate': 0.00022710978951411012, 'epoch': 3.4557235421166306}\n",
      "{'loss': 1.5654, 'grad_norm': 1.125, 'learning_rate': 0.00022495115640400545, 'epoch': 3.4989200863930887}\n",
      "{'loss': 1.5792, 'grad_norm': 0.734375, 'learning_rate': 0.0002227925232939008, 'epoch': 3.5421166306695464}\n",
      "{'loss': 1.5648, 'grad_norm': 0.98046875, 'learning_rate': 0.00022063389018379617, 'epoch': 3.5853131749460045}\n",
      "{'loss': 1.573, 'grad_norm': 0.76953125, 'learning_rate': 0.00021847525707369153, 'epoch': 3.6285097192224622}\n",
      "{'loss': 1.5705, 'grad_norm': 0.78125, 'learning_rate': 0.0002163166239635869, 'epoch': 3.67170626349892}\n",
      "{'loss': 1.5722, 'grad_norm': 0.74609375, 'learning_rate': 0.00021415799085348222, 'epoch': 3.714902807775378}\n",
      "{'loss': 1.5763, 'grad_norm': 0.83203125, 'learning_rate': 0.00021199935774337758, 'epoch': 3.7580993520518358}\n",
      "{'loss': 1.5683, 'grad_norm': 0.89453125, 'learning_rate': 0.00020984072463327294, 'epoch': 3.801295896328294}\n",
      "{'loss': 1.5798, 'grad_norm': 1.0078125, 'learning_rate': 0.0002076820915231683, 'epoch': 3.8444924406047516}\n",
      "{'loss': 1.5871, 'grad_norm': 0.7265625, 'learning_rate': 0.0002055234584130636, 'epoch': 3.8876889848812093}\n",
      "{'loss': 1.5794, 'grad_norm': 1.21875, 'learning_rate': 0.00020336482530295896, 'epoch': 3.9308855291576674}\n",
      "{'loss': 1.5827, 'grad_norm': 1.015625, 'learning_rate': 0.00020120619219285432, 'epoch': 3.974082073434125}\n",
      "{'eval_loss': 1.575498342514038, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 3.9293, 'eval_samples_per_second': 1047.513, 'eval_steps_per_second': 65.66, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5766, 'grad_norm': 0.671875, 'learning_rate': 0.00019904755908274965, 'epoch': 4.017278617710583}\n",
      "{'loss': 1.569, 'grad_norm': 0.66015625, 'learning_rate': 0.000196888925972645, 'epoch': 4.060475161987041}\n",
      "{'loss': 1.5681, 'grad_norm': 0.69140625, 'learning_rate': 0.00019473029286254037, 'epoch': 4.103671706263499}\n",
      "{'loss': 1.5699, 'grad_norm': 0.86328125, 'learning_rate': 0.00019257165975243573, 'epoch': 4.146868250539957}\n",
      "{'loss': 1.5694, 'grad_norm': 0.765625, 'learning_rate': 0.0001904130266423311, 'epoch': 4.190064794816415}\n",
      "{'loss': 1.5726, 'grad_norm': 0.7109375, 'learning_rate': 0.00018825439353222642, 'epoch': 4.233261339092873}\n",
      "{'loss': 1.5798, 'grad_norm': 0.9765625, 'learning_rate': 0.00018609576042212178, 'epoch': 4.27645788336933}\n",
      "{'loss': 1.5761, 'grad_norm': 0.7734375, 'learning_rate': 0.00018393712731201714, 'epoch': 4.319654427645788}\n",
      "{'loss': 1.5757, 'grad_norm': 0.8125, 'learning_rate': 0.0001817784942019125, 'epoch': 4.362850971922247}\n",
      "{'loss': 1.583, 'grad_norm': 1.015625, 'learning_rate': 0.00017961986109180783, 'epoch': 4.406047516198704}\n",
      "{'loss': 1.5769, 'grad_norm': 0.462890625, 'learning_rate': 0.00017746122798170316, 'epoch': 4.449244060475162}\n",
      "{'loss': 1.5703, 'grad_norm': 0.68359375, 'learning_rate': 0.00017530259487159852, 'epoch': 4.49244060475162}\n",
      "{'loss': 1.5801, 'grad_norm': 0.9140625, 'learning_rate': 0.00017314396176149388, 'epoch': 4.535637149028078}\n",
      "{'loss': 1.5806, 'grad_norm': 0.82421875, 'learning_rate': 0.00017098532865138924, 'epoch': 4.578833693304536}\n",
      "{'loss': 1.5817, 'grad_norm': 0.66796875, 'learning_rate': 0.0001688266955412846, 'epoch': 4.622030237580994}\n",
      "{'loss': 1.5805, 'grad_norm': 1.0390625, 'learning_rate': 0.00016666806243117993, 'epoch': 4.665226781857451}\n",
      "{'loss': 1.5809, 'grad_norm': 0.51171875, 'learning_rate': 0.0001645094293210753, 'epoch': 4.708423326133909}\n",
      "{'loss': 1.576, 'grad_norm': 0.765625, 'learning_rate': 0.00016235079621097065, 'epoch': 4.751619870410368}\n",
      "{'loss': 1.5793, 'grad_norm': 0.88671875, 'learning_rate': 0.00016019216310086598, 'epoch': 4.794816414686825}\n",
      "{'loss': 1.5732, 'grad_norm': 0.7421875, 'learning_rate': 0.00015803352999076134, 'epoch': 4.838012958963283}\n",
      "{'loss': 1.5838, 'grad_norm': 0.90625, 'learning_rate': 0.0001558748968806567, 'epoch': 4.881209503239741}\n",
      "{'loss': 1.5642, 'grad_norm': 0.5234375, 'learning_rate': 0.00015371626377055203, 'epoch': 4.924406047516198}\n",
      "{'loss': 1.5757, 'grad_norm': 1.1875, 'learning_rate': 0.0001515576306604474, 'epoch': 4.967602591792657}\n",
      "{'eval_loss': 1.576022982597351, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 4.0014, 'eval_samples_per_second': 1028.629, 'eval_steps_per_second': 64.477, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'train_runtime': 502.5416, 'train_samples_per_second': 589.627, 'train_steps_per_second': 36.853, 'train_loss': 1.57782256025475, 'epoch': 5.0}\n",
      "{'eval_loss': 1.577258586883545, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 4.0067, 'eval_samples_per_second': 1027.282, 'eval_steps_per_second': 64.392, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñÖ‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÖ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÖ‚ñÅ‚ñÉ‚ñÑ‚ñà‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÑ‚ñà‚ñÜ‚ñÖ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ</td></tr><tr><td>train/loss</td><td>‚ñÅ‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÇ‚ñà‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.08688</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.27745</td></tr><tr><td>eval/f1</td><td>0.08688</td></tr><tr><td>eval/loss</td><td>1.57726</td></tr><tr><td>eval/precision</td><td>0.05549</td></tr><tr><td>eval/recall</td><td>0.2</td></tr><tr><td>eval/runtime</td><td>4.0067</td></tr><tr><td>eval/samples_per_second</td><td>1027.282</td></tr><tr><td>eval/steps_per_second</td><td>64.392</td></tr><tr><td>total_flos</td><td>7868409591839808.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>11575</td></tr><tr><td>train/grad_norm</td><td>1.1875</td></tr><tr><td>train/learning_rate</td><td>0.00015</td></tr><tr><td>train/loss</td><td>1.5757</td></tr><tr><td>train_loss</td><td>1.57782</td></tr><tr><td>train_runtime</td><td>502.5416</td></tr><tr><td>train_samples_per_second</td><td>589.627</td></tr><tr><td>train_steps_per_second</td><td>36.853</td></tr><tr><td>trial/accuracy</td><td>0.27745</td></tr><tr><td>trial/f1</td><td>0.08688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t15</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/qkmvqzzv' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/qkmvqzzv</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_060216-qkmvqzzv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.858824:  40%|‚ñà‚ñà‚ñà‚ñà      | 6/15 [1:35:50<2:25:22, 969.20s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=15 f1=0.0869\n",
      "[I 2025-08-17 06:10:47,524] Trial 5 finished with value: 0.08687713959680486 and parameters: {'lr': 0.0003757748518070173, 'weight_decay': 5.704980917227296e-06, 'unfreeze_last_k': 11, 'batch_size': 16}. Best is trial 2 with value: 0.8588243859157035.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_061047-tdv9g692</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/tdv9g692' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t16</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/tdv9g692' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/tdv9g692</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=16 | epochs=8 bs=32 lr=2.74e-04 wd=4.1e-06 k=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[Run] epochs=8 bs=32 lr=2.74e-04 wd=4.1e-06 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/1158] loss=1.5953 lr=4.88e-05\n",
      "{'loss': 1.5953, 'grad_norm': 2.34375, 'learning_rate': 4.876471793523061e-05, 'epoch': 0.08635578583765112}\n",
      "[e0 b200/1158] loss=1.5252 lr=9.80e-05\n",
      "{'loss': 1.5252, 'grad_norm': 9.4375, 'learning_rate': 9.802200877889789e-05, 'epoch': 0.17271157167530224}\n",
      "[e0 b300/1158] loss=1.2682 lr=1.47e-04\n",
      "{'loss': 1.2682, 'grad_norm': 7.875, 'learning_rate': 0.00014727929962256517, 'epoch': 0.25906735751295334}\n",
      "[e0 b400/1158] loss=1.1961 lr=1.97e-04\n",
      "{'loss': 1.1961, 'grad_norm': 10.25, 'learning_rate': 0.00019653659046623244, 'epoch': 0.3454231433506045}\n",
      "[e0 b500/1158] loss=1.1168 lr=2.46e-04\n",
      "{'loss': 1.1168, 'grad_norm': 5.5, 'learning_rate': 0.00024579388130989974, 'epoch': 0.4317789291882556}\n",
      "[e0 b600/1158] loss=1.1080 lr=2.73e-04\n",
      "{'loss': 1.108, 'grad_norm': 6.40625, 'learning_rate': 0.0002725181676494828, 'epoch': 0.5181347150259067}\n",
      "[e0 b700/1158] loss=1.2229 lr=2.69e-04\n",
      "{'loss': 1.2229, 'grad_norm': 7.84375, 'learning_rate': 0.00026937312243714023, 'epoch': 0.6044905008635578}\n",
      "[e0 b800/1158] loss=1.1728 lr=2.66e-04\n",
      "{'loss': 1.1728, 'grad_norm': 8.375, 'learning_rate': 0.00026622807722479766, 'epoch': 0.690846286701209}\n",
      "[e0 b900/1158] loss=1.1011 lr=2.63e-04\n",
      "{'loss': 1.1011, 'grad_norm': 10.75, 'learning_rate': 0.00026308303201245513, 'epoch': 0.7772020725388601}\n",
      "[e0 b1000/1158] loss=1.0842 lr=2.60e-04\n",
      "{'loss': 1.0842, 'grad_norm': 25.125, 'learning_rate': 0.00025993798680011256, 'epoch': 0.8635578583765112}\n",
      "[e0 b1100/1158] loss=1.3399 lr=2.57e-04\n",
      "{'loss': 1.3399, 'grad_norm': 19.375, 'learning_rate': 0.00025679294158777, 'epoch': 0.9499136442141624}\n",
      "{'eval_loss': 1.5893945693969727, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 1.9998, 'eval_samples_per_second': 2058.17, 'eval_steps_per_second': 64.505, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.6063, 'grad_norm': 0.90234375, 'learning_rate': 0.0002536478963754274, 'epoch': 1.0362694300518134}\n",
      "{'loss': 1.5873, 'grad_norm': 0.71875, 'learning_rate': 0.0002505028511630849, 'epoch': 1.1226252158894645}\n",
      "{'loss': 1.5696, 'grad_norm': 0.5625, 'learning_rate': 0.0002473578059507423, 'epoch': 1.2089810017271156}\n",
      "{'loss': 1.5848, 'grad_norm': 0.515625, 'learning_rate': 0.00024421276073839974, 'epoch': 1.2953367875647668}\n",
      "{'loss': 1.5824, 'grad_norm': 0.69140625, 'learning_rate': 0.0002410677155260572, 'epoch': 1.381692573402418}\n",
      "{'loss': 1.5696, 'grad_norm': 1.03125, 'learning_rate': 0.0002379226703137146, 'epoch': 1.468048359240069}\n",
      "{'loss': 1.5848, 'grad_norm': 0.6328125, 'learning_rate': 0.00023477762510137206, 'epoch': 1.5544041450777202}\n",
      "{'loss': 1.5826, 'grad_norm': 0.734375, 'learning_rate': 0.00023163257988902952, 'epoch': 1.6407599309153713}\n",
      "{'loss': 1.5713, 'grad_norm': 0.94140625, 'learning_rate': 0.00022848753467668694, 'epoch': 1.7271157167530224}\n",
      "{'loss': 1.5814, 'grad_norm': 0.76171875, 'learning_rate': 0.0002253424894643444, 'epoch': 1.8134715025906736}\n",
      "{'loss': 1.5743, 'grad_norm': 0.609375, 'learning_rate': 0.00022219744425200184, 'epoch': 1.8998272884283247}\n",
      "{'loss': 1.5795, 'grad_norm': 0.6015625, 'learning_rate': 0.00021905239903965927, 'epoch': 1.9861830742659758}\n",
      "{'eval_loss': 1.5759564638137817, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 2.0341, 'eval_samples_per_second': 2023.494, 'eval_steps_per_second': 63.419, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.583, 'grad_norm': 0.435546875, 'learning_rate': 0.00021590735382731672, 'epoch': 2.0725388601036268}\n",
      "{'loss': 1.5797, 'grad_norm': 0.76953125, 'learning_rate': 0.00021276230861497417, 'epoch': 2.158894645941278}\n",
      "{'loss': 1.5721, 'grad_norm': 0.63671875, 'learning_rate': 0.0002096172634026316, 'epoch': 2.245250431778929}\n",
      "{'loss': 1.5723, 'grad_norm': 0.484375, 'learning_rate': 0.00020647221819028902, 'epoch': 2.33160621761658}\n",
      "{'loss': 1.5811, 'grad_norm': 0.8828125, 'learning_rate': 0.00020332717297794647, 'epoch': 2.4179620034542313}\n",
      "{'loss': 1.5737, 'grad_norm': 0.5390625, 'learning_rate': 0.00020018212776560393, 'epoch': 2.5043177892918824}\n",
      "{'loss': 1.5776, 'grad_norm': 0.490234375, 'learning_rate': 0.00019703708255326135, 'epoch': 2.5906735751295336}\n",
      "{'loss': 1.5815, 'grad_norm': 0.77734375, 'learning_rate': 0.0001938920373409188, 'epoch': 2.6770293609671847}\n",
      "{'loss': 1.5787, 'grad_norm': 0.77734375, 'learning_rate': 0.00019074699212857625, 'epoch': 2.763385146804836}\n",
      "{'loss': 1.5745, 'grad_norm': 0.953125, 'learning_rate': 0.00018760194691623368, 'epoch': 2.849740932642487}\n",
      "{'loss': 1.5779, 'grad_norm': 0.4921875, 'learning_rate': 0.00018445690170389113, 'epoch': 2.936096718480138}\n",
      "{'eval_loss': 1.5761754512786865, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 2.308, 'eval_samples_per_second': 1783.349, 'eval_steps_per_second': 55.892, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5748, 'grad_norm': 0.96875, 'learning_rate': 0.00018131185649154858, 'epoch': 3.0224525043177892}\n",
      "{'loss': 1.572, 'grad_norm': 0.89453125, 'learning_rate': 0.00017816681127920598, 'epoch': 3.1088082901554404}\n",
      "{'loss': 1.5818, 'grad_norm': 0.451171875, 'learning_rate': 0.00017502176606686343, 'epoch': 3.1951640759930915}\n",
      "{'loss': 1.5801, 'grad_norm': 0.41796875, 'learning_rate': 0.00017187672085452088, 'epoch': 3.2815198618307426}\n",
      "{'loss': 1.5786, 'grad_norm': 0.8515625, 'learning_rate': 0.0001687316756421783, 'epoch': 3.3678756476683938}\n",
      "{'loss': 1.5798, 'grad_norm': 0.484375, 'learning_rate': 0.00016558663042983576, 'epoch': 3.454231433506045}\n",
      "{'loss': 1.5708, 'grad_norm': 0.48046875, 'learning_rate': 0.0001624415852174932, 'epoch': 3.540587219343696}\n",
      "{'loss': 1.569, 'grad_norm': 0.50390625, 'learning_rate': 0.00015929654000515064, 'epoch': 3.626943005181347}\n",
      "{'loss': 1.5715, 'grad_norm': 0.5703125, 'learning_rate': 0.0001561514947928081, 'epoch': 3.7132987910189983}\n",
      "{'loss': 1.572, 'grad_norm': 0.41015625, 'learning_rate': 0.00015300644958046554, 'epoch': 3.7996545768566494}\n",
      "{'loss': 1.5834, 'grad_norm': 0.703125, 'learning_rate': 0.00014986140436812294, 'epoch': 3.8860103626943006}\n",
      "{'loss': 1.5803, 'grad_norm': 0.671875, 'learning_rate': 0.0001467163591557804, 'epoch': 3.9723661485319517}\n",
      "{'eval_loss': 1.5754284858703613, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 1.9922, 'eval_samples_per_second': 2066.048, 'eval_steps_per_second': 64.752, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'loss': 1.5732, 'grad_norm': 0.640625, 'learning_rate': 0.00014357131394343784, 'epoch': 4.058721934369602}\n",
      "{'loss': 1.5673, 'grad_norm': 0.5546875, 'learning_rate': 0.0001404262687310953, 'epoch': 4.1450777202072535}\n",
      "{'loss': 1.5708, 'grad_norm': 0.396484375, 'learning_rate': 0.00013728122351875272, 'epoch': 4.231433506044905}\n",
      "{'loss': 1.5781, 'grad_norm': 0.625, 'learning_rate': 0.00013413617830641017, 'epoch': 4.317789291882556}\n",
      "{'loss': 1.5779, 'grad_norm': 0.328125, 'learning_rate': 0.0001309911330940676, 'epoch': 4.404145077720207}\n",
      "{'loss': 1.5736, 'grad_norm': 0.53515625, 'learning_rate': 0.00012784608788172505, 'epoch': 4.490500863557858}\n",
      "{'loss': 1.579, 'grad_norm': 0.5625, 'learning_rate': 0.0001247010426693825, 'epoch': 4.576856649395509}\n",
      "{'loss': 1.5808, 'grad_norm': 0.59765625, 'learning_rate': 0.00012155599745703992, 'epoch': 4.66321243523316}\n",
      "{'loss': 1.5783, 'grad_norm': 0.95703125, 'learning_rate': 0.00011841095224469737, 'epoch': 4.7495682210708114}\n",
      "{'loss': 1.5757, 'grad_norm': 0.80078125, 'learning_rate': 0.00011526590703235481, 'epoch': 4.835924006908463}\n",
      "{'loss': 1.5746, 'grad_norm': 0.72265625, 'learning_rate': 0.00011212086182001225, 'epoch': 4.922279792746114}\n",
      "{'eval_loss': 1.5755701065063477, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 2.2502, 'eval_samples_per_second': 1829.196, 'eval_steps_per_second': 57.329, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n",
      "{'train_runtime': 289.1028, 'train_samples_per_second': 1024.936, 'train_steps_per_second': 32.044, 'train_loss': 1.5150597659623273, 'epoch': 5.0}\n",
      "{'eval_loss': 1.5893945693969727, 'eval_accuracy': 0.27745383867832846, 'eval_precision': 0.05549076773566569, 'eval_recall': 0.2, 'eval_f1': 0.08687713959680486, 'eval_runtime': 2.074, 'eval_samples_per_second': 1984.587, 'eval_steps_per_second': 62.199, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.2775 f1=0.0869 p=0.0555 r=0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñá‚ñÉ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñá‚ñÅ‚ñà‚ñÇ‚ñÜ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñá‚ñÅ‚ñà‚ñÇ‚ñÜ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñà‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÑ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.08688</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.27745</td></tr><tr><td>eval/f1</td><td>0.08688</td></tr><tr><td>eval/loss</td><td>1.58939</td></tr><tr><td>eval/precision</td><td>0.05549</td></tr><tr><td>eval/recall</td><td>0.2</td></tr><tr><td>eval/runtime</td><td>2.074</td></tr><tr><td>eval/samples_per_second</td><td>1984.587</td></tr><tr><td>eval/steps_per_second</td><td>62.199</td></tr><tr><td>total_flos</td><td>8220533760648000.0</td></tr><tr><td>train/epoch</td><td>5</td></tr><tr><td>train/global_step</td><td>5790</td></tr><tr><td>train/grad_norm</td><td>0.72266</td></tr><tr><td>train/learning_rate</td><td>0.00011</td></tr><tr><td>train/loss</td><td>1.5746</td></tr><tr><td>train_loss</td><td>1.51506</td></tr><tr><td>train_runtime</td><td>289.1028</td></tr><tr><td>train_samples_per_second</td><td>1024.936</td></tr><tr><td>train_steps_per_second</td><td>32.044</td></tr><tr><td>trial/accuracy</td><td>0.27745</td></tr><tr><td>trial/f1</td><td>0.08688</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t16</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/tdv9g692' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/tdv9g692</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_061047-tdv9g692\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.858824:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 7/15 [1:40:46<1:39:51, 749.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=16 f1=0.0869\n",
      "[I 2025-08-17 06:15:43,149] Trial 6 finished with value: 0.08687713959680486 and parameters: {'lr': 0.0002738705370907901, 'weight_decay': 4.122290115221052e-06, 'unfreeze_last_k': 12, 'batch_size': 32}. Best is trial 2 with value: 0.8588243859157035.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_061543-0jmz2fph</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/0jmz2fph' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t17</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/0jmz2fph' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/0jmz2fph</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=17 | epochs=8 bs=32 lr=8.59e-05 wd=1.2e-05 k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[Run] epochs=8 bs=32 lr=8.59e-05 wd=1.2e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/1158] loss=1.6139 lr=1.53e-05\n",
      "{'loss': 1.6139, 'grad_norm': 1.2109375, 'learning_rate': 1.5288851703800565e-05, 'epoch': 0.08635578583765112}\n",
      "[e0 b200/1158] loss=1.5723 lr=3.07e-05\n",
      "{'loss': 1.5723, 'grad_norm': 4.125, 'learning_rate': 3.073213625309406e-05, 'epoch': 0.17271157167530224}\n",
      "[e0 b300/1158] loss=1.5126 lr=4.62e-05\n",
      "{'loss': 1.5126, 'grad_norm': 5.1875, 'learning_rate': 4.617542080238756e-05, 'epoch': 0.25906735751295334}\n",
      "[e0 b400/1158] loss=1.4145 lr=6.16e-05\n",
      "{'loss': 1.4145, 'grad_norm': 11.75, 'learning_rate': 6.161870535168107e-05, 'epoch': 0.3454231433506045}\n",
      "[e0 b500/1158] loss=1.1977 lr=7.71e-05\n",
      "{'loss': 1.1977, 'grad_norm': 14.0625, 'learning_rate': 7.706198990097456e-05, 'epoch': 0.4317789291882556}\n",
      "[e0 b600/1158] loss=1.0439 lr=8.54e-05\n",
      "{'loss': 1.0439, 'grad_norm': 11.9375, 'learning_rate': 8.54406634181365e-05, 'epoch': 0.5181347150259067}\n",
      "[e0 b700/1158] loss=0.9686 lr=8.45e-05\n",
      "{'loss': 0.9686, 'grad_norm': 10.625, 'learning_rate': 8.44546199857287e-05, 'epoch': 0.6044905008635578}\n",
      "[e0 b800/1158] loss=0.8929 lr=8.35e-05\n",
      "{'loss': 0.8929, 'grad_norm': 15.3125, 'learning_rate': 8.346857655332088e-05, 'epoch': 0.690846286701209}\n",
      "[e0 b900/1158] loss=0.8414 lr=8.25e-05\n",
      "{'loss': 0.8414, 'grad_norm': 14.5, 'learning_rate': 8.248253312091308e-05, 'epoch': 0.7772020725388601}\n",
      "[e0 b1000/1158] loss=0.7975 lr=8.15e-05\n",
      "{'loss': 0.7975, 'grad_norm': 14.5, 'learning_rate': 8.149648968850527e-05, 'epoch': 0.8635578583765112}\n",
      "[e0 b1100/1158] loss=0.7498 lr=8.05e-05\n",
      "{'loss': 0.7498, 'grad_norm': 13.75, 'learning_rate': 8.051044625609746e-05, 'epoch': 0.9499136442141624}\n",
      "{'eval_loss': 0.7152377367019653, 'eval_accuracy': 0.7337220602526725, 'eval_precision': 0.7577616014821785, 'eval_recall': 0.7342597038724578, 'eval_f1': 0.7400883220116266, 'eval_runtime': 2.0998, 'eval_samples_per_second': 1960.224, 'eval_steps_per_second': 61.436, 'epoch': 1.0}\n",
      "[val @ epoch 1] acc=0.7337 f1=0.7401 p=0.7578 r=0.7343\n",
      "{'loss': 0.7048, 'grad_norm': 7.90625, 'learning_rate': 7.952440282368965e-05, 'epoch': 1.0362694300518134}\n",
      "{'loss': 0.6639, 'grad_norm': 9.6875, 'learning_rate': 7.853835939128186e-05, 'epoch': 1.1226252158894645}\n",
      "{'loss': 0.6355, 'grad_norm': 12.875, 'learning_rate': 7.755231595887405e-05, 'epoch': 1.2089810017271156}\n",
      "{'loss': 0.6526, 'grad_norm': 7.34375, 'learning_rate': 7.656627252646624e-05, 'epoch': 1.2953367875647668}\n",
      "{'loss': 0.6243, 'grad_norm': 16.75, 'learning_rate': 7.558022909405843e-05, 'epoch': 1.381692573402418}\n",
      "{'loss': 0.612, 'grad_norm': 10.875, 'learning_rate': 7.459418566165062e-05, 'epoch': 1.468048359240069}\n",
      "{'loss': 0.5755, 'grad_norm': 15.75, 'learning_rate': 7.360814222924282e-05, 'epoch': 1.5544041450777202}\n",
      "{'loss': 0.5963, 'grad_norm': 10.25, 'learning_rate': 7.262209879683501e-05, 'epoch': 1.6407599309153713}\n",
      "{'loss': 0.5742, 'grad_norm': 13.125, 'learning_rate': 7.16360553644272e-05, 'epoch': 1.7271157167530224}\n",
      "{'loss': 0.5901, 'grad_norm': 9.6875, 'learning_rate': 7.065001193201939e-05, 'epoch': 1.8134715025906736}\n",
      "{'loss': 0.5326, 'grad_norm': 11.5, 'learning_rate': 6.966396849961159e-05, 'epoch': 1.8998272884283247}\n",
      "{'loss': 0.5192, 'grad_norm': 8.25, 'learning_rate': 6.867792506720377e-05, 'epoch': 1.9861830742659758}\n",
      "{'eval_loss': 0.607849657535553, 'eval_accuracy': 0.7849854227405247, 'eval_precision': 0.8004037832130987, 'eval_recall': 0.7966758736703514, 'eval_f1': 0.7940514352980285, 'eval_runtime': 2.1045, 'eval_samples_per_second': 1955.843, 'eval_steps_per_second': 61.298, 'epoch': 2.0}\n",
      "[val @ epoch 2] acc=0.7850 f1=0.7941 p=0.8004 r=0.7967\n",
      "{'loss': 0.5073, 'grad_norm': 14.25, 'learning_rate': 6.769188163479597e-05, 'epoch': 2.0725388601036268}\n",
      "{'loss': 0.5088, 'grad_norm': 8.625, 'learning_rate': 6.670583820238816e-05, 'epoch': 2.158894645941278}\n",
      "{'loss': 0.4848, 'grad_norm': 16.875, 'learning_rate': 6.571979476998036e-05, 'epoch': 2.245250431778929}\n",
      "{'loss': 0.4712, 'grad_norm': 12.25, 'learning_rate': 6.473375133757255e-05, 'epoch': 2.33160621761658}\n",
      "{'loss': 0.4709, 'grad_norm': 11.3125, 'learning_rate': 6.374770790516474e-05, 'epoch': 2.4179620034542313}\n",
      "{'loss': 0.4793, 'grad_norm': 15.375, 'learning_rate': 6.276166447275693e-05, 'epoch': 2.5043177892918824}\n",
      "{'loss': 0.4869, 'grad_norm': 9.4375, 'learning_rate': 6.177562104034912e-05, 'epoch': 2.5906735751295336}\n",
      "{'loss': 0.4564, 'grad_norm': 11.3125, 'learning_rate': 6.078957760794132e-05, 'epoch': 2.6770293609671847}\n",
      "{'loss': 0.4584, 'grad_norm': 11.9375, 'learning_rate': 5.9803534175533515e-05, 'epoch': 2.763385146804836}\n",
      "{'loss': 0.445, 'grad_norm': 8.25, 'learning_rate': 5.8817490743125704e-05, 'epoch': 2.849740932642487}\n",
      "{'loss': 0.4676, 'grad_norm': 14.5, 'learning_rate': 5.78314473107179e-05, 'epoch': 2.936096718480138}\n",
      "{'eval_loss': 0.5459805130958557, 'eval_accuracy': 0.8158406219630709, 'eval_precision': 0.8203195908277733, 'eval_recall': 0.8297402960886983, 'eval_f1': 0.8214641740546286, 'eval_runtime': 2.0575, 'eval_samples_per_second': 2000.469, 'eval_steps_per_second': 62.697, 'epoch': 3.0}\n",
      "[val @ epoch 3] acc=0.8158 f1=0.8215 p=0.8203 r=0.8297\n",
      "{'loss': 0.4271, 'grad_norm': 12.875, 'learning_rate': 5.684540387831009e-05, 'epoch': 3.0224525043177892}\n",
      "{'loss': 0.3934, 'grad_norm': 11.1875, 'learning_rate': 5.585936044590228e-05, 'epoch': 3.1088082901554404}\n",
      "{'loss': 0.4236, 'grad_norm': 6.5625, 'learning_rate': 5.487331701349448e-05, 'epoch': 3.1951640759930915}\n",
      "{'loss': 0.3947, 'grad_norm': 4.21875, 'learning_rate': 5.388727358108667e-05, 'epoch': 3.2815198618307426}\n",
      "{'loss': 0.4105, 'grad_norm': 10.75, 'learning_rate': 5.2901230148678856e-05, 'epoch': 3.3678756476683938}\n",
      "{'loss': 0.3999, 'grad_norm': 12.0625, 'learning_rate': 5.191518671627105e-05, 'epoch': 3.454231433506045}\n",
      "{'loss': 0.4018, 'grad_norm': 14.5, 'learning_rate': 5.092914328386325e-05, 'epoch': 3.540587219343696}\n",
      "{'loss': 0.3916, 'grad_norm': 13.1875, 'learning_rate': 4.994309985145544e-05, 'epoch': 3.626943005181347}\n",
      "{'loss': 0.4068, 'grad_norm': 11.0, 'learning_rate': 4.895705641904763e-05, 'epoch': 3.7132987910189983}\n",
      "{'loss': 0.3953, 'grad_norm': 5.09375, 'learning_rate': 4.7971012986639825e-05, 'epoch': 3.7996545768566494}\n",
      "{'loss': 0.3975, 'grad_norm': 14.625, 'learning_rate': 4.6984969554232014e-05, 'epoch': 3.8860103626943006}\n",
      "{'loss': 0.4016, 'grad_norm': 11.875, 'learning_rate': 4.599892612182421e-05, 'epoch': 3.9723661485319517}\n",
      "{'eval_loss': 0.5122537612915039, 'eval_accuracy': 0.8233722060252673, 'eval_precision': 0.8266567562292051, 'eval_recall': 0.8360410901302817, 'eval_f1': 0.8287550879690034, 'eval_runtime': 1.9951, 'eval_samples_per_second': 2063.087, 'eval_steps_per_second': 64.659, 'epoch': 4.0}\n",
      "[val @ epoch 4] acc=0.8234 f1=0.8288 p=0.8267 r=0.8360\n",
      "{'loss': 0.362, 'grad_norm': 18.625, 'learning_rate': 4.501288268941641e-05, 'epoch': 4.058721934369602}\n",
      "{'loss': 0.3685, 'grad_norm': 11.1875, 'learning_rate': 4.4026839257008604e-05, 'epoch': 4.1450777202072535}\n",
      "{'loss': 0.3711, 'grad_norm': 6.15625, 'learning_rate': 4.304079582460079e-05, 'epoch': 4.231433506044905}\n",
      "{'loss': 0.3711, 'grad_norm': 11.5, 'learning_rate': 4.205475239219298e-05, 'epoch': 4.317789291882556}\n",
      "{'loss': 0.3763, 'grad_norm': 17.875, 'learning_rate': 4.106870895978517e-05, 'epoch': 4.404145077720207}\n",
      "{'loss': 0.3642, 'grad_norm': 11.6875, 'learning_rate': 4.008266552737736e-05, 'epoch': 4.490500863557858}\n",
      "{'loss': 0.342, 'grad_norm': 4.375, 'learning_rate': 3.909662209496956e-05, 'epoch': 4.576856649395509}\n",
      "{'loss': 0.3723, 'grad_norm': 14.25, 'learning_rate': 3.811057866256175e-05, 'epoch': 4.66321243523316}\n",
      "{'loss': 0.3574, 'grad_norm': 12.0625, 'learning_rate': 3.7124535230153945e-05, 'epoch': 4.7495682210708114}\n",
      "{'loss': 0.3784, 'grad_norm': 9.25, 'learning_rate': 3.6138491797746135e-05, 'epoch': 4.835924006908463}\n",
      "{'loss': 0.3614, 'grad_norm': 8.6875, 'learning_rate': 3.515244836533833e-05, 'epoch': 4.922279792746114}\n",
      "{'eval_loss': 0.5236790180206299, 'eval_accuracy': 0.825801749271137, 'eval_precision': 0.8285725959579953, 'eval_recall': 0.8400454041687645, 'eval_f1': 0.8309785162132848, 'eval_runtime': 1.9834, 'eval_samples_per_second': 2075.274, 'eval_steps_per_second': 65.041, 'epoch': 5.0}\n",
      "[val @ epoch 5] acc=0.8258 f1=0.8310 p=0.8286 r=0.8400\n",
      "{'loss': 0.3652, 'grad_norm': 12.125, 'learning_rate': 3.416640493293053e-05, 'epoch': 5.008635578583765}\n",
      "{'loss': 0.3481, 'grad_norm': 11.0625, 'learning_rate': 3.318036150052272e-05, 'epoch': 5.094991364421416}\n",
      "{'loss': 0.3329, 'grad_norm': 17.25, 'learning_rate': 3.219431806811491e-05, 'epoch': 5.181347150259067}\n",
      "{'loss': 0.3347, 'grad_norm': 19.75, 'learning_rate': 3.12082746357071e-05, 'epoch': 5.267702936096718}\n",
      "{'loss': 0.3262, 'grad_norm': 10.6875, 'learning_rate': 3.0222231203299293e-05, 'epoch': 5.354058721934369}\n",
      "{'loss': 0.3537, 'grad_norm': 5.5, 'learning_rate': 2.9236187770891483e-05, 'epoch': 5.4404145077720205}\n",
      "{'loss': 0.3188, 'grad_norm': 15.25, 'learning_rate': 2.825014433848368e-05, 'epoch': 5.526770293609672}\n",
      "{'loss': 0.3712, 'grad_norm': 9.5625, 'learning_rate': 2.7264100906075872e-05, 'epoch': 5.613126079447323}\n",
      "{'loss': 0.3472, 'grad_norm': 15.4375, 'learning_rate': 2.6278057473668065e-05, 'epoch': 5.699481865284974}\n",
      "{'loss': 0.3451, 'grad_norm': 11.9375, 'learning_rate': 2.529201404126026e-05, 'epoch': 5.785837651122625}\n",
      "{'loss': 0.3499, 'grad_norm': 14.875, 'learning_rate': 2.4305970608852448e-05, 'epoch': 5.872193436960276}\n",
      "{'loss': 0.3626, 'grad_norm': 28.5, 'learning_rate': 2.3319927176444644e-05, 'epoch': 5.958549222797927}\n",
      "{'eval_loss': 0.5450320243835449, 'eval_accuracy': 0.8202137998056366, 'eval_precision': 0.8223408040227493, 'eval_recall': 0.8367624701478477, 'eval_f1': 0.824967095913979, 'eval_runtime': 2.0742, 'eval_samples_per_second': 1984.344, 'eval_steps_per_second': 62.192, 'epoch': 6.0}\n",
      "[val @ epoch 6] acc=0.8202 f1=0.8250 p=0.8223 r=0.8368\n",
      "{'loss': 0.3251, 'grad_norm': 12.8125, 'learning_rate': 2.2333883744036834e-05, 'epoch': 6.0449050086355784}\n",
      "{'loss': 0.3415, 'grad_norm': 12.625, 'learning_rate': 2.1347840311629027e-05, 'epoch': 6.13126079447323}\n",
      "{'loss': 0.3179, 'grad_norm': 8.0625, 'learning_rate': 2.036179687922122e-05, 'epoch': 6.217616580310881}\n",
      "{'loss': 0.3282, 'grad_norm': 16.0, 'learning_rate': 1.9375753446813413e-05, 'epoch': 6.303972366148532}\n",
      "{'loss': 0.3453, 'grad_norm': 5.75, 'learning_rate': 1.8389710014405606e-05, 'epoch': 6.390328151986183}\n",
      "{'loss': 0.3122, 'grad_norm': 12.0, 'learning_rate': 1.74036665819978e-05, 'epoch': 6.476683937823834}\n",
      "{'loss': 0.3212, 'grad_norm': 8.0625, 'learning_rate': 1.6417623149589992e-05, 'epoch': 6.563039723661485}\n",
      "{'loss': 0.3545, 'grad_norm': 8.0625, 'learning_rate': 1.5431579717182186e-05, 'epoch': 6.649395509499136}\n",
      "{'loss': 0.3232, 'grad_norm': 9.0, 'learning_rate': 1.4445536284774377e-05, 'epoch': 6.7357512953367875}\n",
      "{'loss': 0.3272, 'grad_norm': 13.5625, 'learning_rate': 1.3459492852366568e-05, 'epoch': 6.822107081174439}\n",
      "{'loss': 0.3487, 'grad_norm': 7.90625, 'learning_rate': 1.2473449419958761e-05, 'epoch': 6.90846286701209}\n",
      "{'loss': 0.3277, 'grad_norm': 14.5625, 'learning_rate': 1.1487405987550956e-05, 'epoch': 6.994818652849741}\n",
      "{'eval_loss': 0.5377060770988464, 'eval_accuracy': 0.8248299319727891, 'eval_precision': 0.8262562437024876, 'eval_recall': 0.840859645929004, 'eval_f1': 0.829604905641407, 'eval_runtime': 2.0068, 'eval_samples_per_second': 2050.991, 'eval_steps_per_second': 64.28, 'epoch': 7.0}\n",
      "[val @ epoch 7] acc=0.8248 f1=0.8296 p=0.8263 r=0.8409\n",
      "{'loss': 0.3275, 'grad_norm': 11.8125, 'learning_rate': 1.0501362555143147e-05, 'epoch': 7.081174438687392}\n",
      "{'loss': 0.3234, 'grad_norm': 10.125, 'learning_rate': 9.51531912273534e-06, 'epoch': 7.167530224525043}\n",
      "{'loss': 0.354, 'grad_norm': 13.75, 'learning_rate': 8.529275690327534e-06, 'epoch': 7.253886010362694}\n",
      "{'loss': 0.3283, 'grad_norm': 11.9375, 'learning_rate': 7.543232257919727e-06, 'epoch': 7.3402417962003454}\n",
      "{'loss': 0.3053, 'grad_norm': 5.8125, 'learning_rate': 6.557188825511919e-06, 'epoch': 7.426597582037997}\n",
      "{'loss': 0.3329, 'grad_norm': 17.0, 'learning_rate': 5.571145393104111e-06, 'epoch': 7.512953367875648}\n",
      "{'loss': 0.3254, 'grad_norm': 22.25, 'learning_rate': 4.585101960696304e-06, 'epoch': 7.599309153713299}\n",
      "{'loss': 0.3385, 'grad_norm': 19.875, 'learning_rate': 3.5990585282884968e-06, 'epoch': 7.68566493955095}\n",
      "{'loss': 0.3313, 'grad_norm': 11.625, 'learning_rate': 2.6130150958806894e-06, 'epoch': 7.772020725388601}\n",
      "{'loss': 0.3138, 'grad_norm': 11.875, 'learning_rate': 1.626971663472882e-06, 'epoch': 7.858376511226252}\n",
      "{'loss': 0.3216, 'grad_norm': 9.9375, 'learning_rate': 6.409282310650748e-07, 'epoch': 7.944732297063903}\n",
      "{'eval_loss': 0.5325564742088318, 'eval_accuracy': 0.8275024295432458, 'eval_precision': 0.8292475408845918, 'eval_recall': 0.842555375954254, 'eval_f1': 0.832148005149769, 'eval_runtime': 2.0783, 'eval_samples_per_second': 1980.47, 'eval_steps_per_second': 62.07, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8275 f1=0.8321 p=0.8292 r=0.8426\n",
      "{'train_runtime': 436.6378, 'train_samples_per_second': 678.622, 'train_steps_per_second': 21.217, 'train_loss': 0.4960534066116254, 'epoch': 8.0}\n",
      "{'eval_loss': 0.5325564742088318, 'eval_accuracy': 0.8275024295432458, 'eval_precision': 0.8292475408845918, 'eval_recall': 0.842555375954254, 'eval_f1': 0.832148005149769, 'eval_runtime': 2.006, 'eval_samples_per_second': 2051.865, 'eval_steps_per_second': 64.308, 'epoch': 8.0}\n",
      "[val @ epoch 8] acc=0.8275 f1=0.8321 p=0.8292 r=0.8426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÜ‚ñÇ‚ñÜ‚ñÇ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÉ‚ñá‚ñÇ‚ñá</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñá‚ñà‚ñÉ‚ñá‚ñÇ‚ñá</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÅ‚ñÖ‚ñÑ‚ñá‚ñà‚ñÖ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñá‚ñà‚ñÑ</td></tr><tr><td>train/learning_rate</td><td>‚ñÖ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>trial/accuracy</td><td>‚ñÅ</td></tr><tr><td>trial/f1</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_eval_f1</td><td>0.83215</td></tr><tr><td>best_model_ckpt</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.8275</td></tr><tr><td>eval/f1</td><td>0.83215</td></tr><tr><td>eval/loss</td><td>0.53256</td></tr><tr><td>eval/precision</td><td>0.82925</td></tr><tr><td>eval/recall</td><td>0.84256</td></tr><tr><td>eval/runtime</td><td>2.006</td></tr><tr><td>eval/samples_per_second</td><td>2051.865</td></tr><tr><td>eval/steps_per_second</td><td>64.308</td></tr><tr><td>total_flos</td><td>1.3154490312328992e+16</td></tr><tr><td>train/epoch</td><td>8</td></tr><tr><td>train/global_step</td><td>9264</td></tr><tr><td>train/grad_norm</td><td>9.9375</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3216</td></tr><tr><td>train_loss</td><td>0.49605</td></tr><tr><td>train_runtime</td><td>436.6378</td></tr><tr><td>train_samples_per_second</td><td>678.622</td></tr><tr><td>train_steps_per_second</td><td>21.217</td></tr><tr><td>trial/accuracy</td><td>0.8275</td></tr><tr><td>trial/f1</td><td>0.83215</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__t17</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/0jmz2fph' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/0jmz2fph</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_061543-0jmz2fph\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.858824:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 8/15 [1:48:09<1:16:01, 651.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE-END] trial=17 f1=0.8321\n",
      "[I 2025-08-17 06:23:06,152] Trial 7 finished with value: 0.832148005149769 and parameters: {'lr': 8.586466209407186e-05, 'weight_decay': 1.1815206011878946e-05, 'unfreeze_last_k': 11, 'batch_size': 32}. Best is trial 2 with value: 0.8588243859157035.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_062306-mhjhs6xu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/mhjhs6xu' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__t18</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/mhjhs6xu' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/mhjhs6xu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TUNE] trial=18 | epochs=8 bs=128 lr=1.04e-04 wd=4.6e-05 k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py:106: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[Run] epochs=8 bs=128 lr=1.04e-04 wd=4.6e-05 warmup_ratio=0.06 grad_accum=1\n",
      "[e0 b100/290] loss=1.5677 lr=7.38e-05\n",
      "{'loss': 1.5677, 'grad_norm': 3.515625, 'learning_rate': 7.378932090650918e-05, 'epoch': 0.3448275862068966}\n",
      "[e0 b200/290] loss=1.1213 lr=1.02e-04\n",
      "{'loss': 1.1213, 'grad_norm': 11.5, 'learning_rate': 0.00010152442289861383, 'epoch': 0.6896551724137931}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.858824:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 8/15 [1:49:57<1:36:13, 824.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-08-17 06:24:55,040] Trial 8 failed with parameters: {'lr': 0.00010434853461526551, 'weight_decay': 4.6344760009348534e-05, 'unfreeze_last_k': 11, 'batch_size': 128} because of the following error: RuntimeError('CUDA error: out of memory\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n').\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_29804\\3473936884.py\", line 151, in objective\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py\", line 2238, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py\", line 2582, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py\", line 3796, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py\", line 3884, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\accelerate\\utils\\operations.py\", line 818, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\accelerate\\utils\\operations.py\", line 806, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\amp\\autocast_mode.py\", line 44, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 1079, in forward\n",
      "    outputs = self.deberta(\n",
      "              ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 786, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "                      ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 659, in forward\n",
      "    output_states, attn_weights = layer_module(\n",
      "                                  ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 438, in forward\n",
      "    attention_output, att_matrix = self.attention(\n",
      "                                   ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 371, in forward\n",
      "    self_output, att_matrix = self.self(\n",
      "                              ^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 251, in forward\n",
      "    rel_att = self.disentangled_attention_bias(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py\", line 343, in disentangled_attention_bias\n",
      "    p2c_att = torch.bmm(key_layer, pos_query_layer.transpose(-1, -2))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: CUDA error: out of memory\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "[W 2025-08-17 06:24:55,047] Trial 8 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 172\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Study] Starting Optuna: trials=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_TRIALS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, epochs/trial=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREFINE_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ; centered at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    170\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLR\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, wd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mWEIGHT_DECAY\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1e\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, k=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mUNFREEZE_LAST_K\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, bs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    171\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# Persist best params\u001b[39;00m\n\u001b[32m    175\u001b[39m best = {\n\u001b[32m    176\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(study.best_trial.params[\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    177\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(study.best_trial.params[\u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m   (...)\u001b[39m\u001b[32m    180\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mepochs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(EPOCHS),   \u001b[38;5;66;03m# you can bump this later for final train if you want\u001b[39;00m\n\u001b[32m    181\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 151\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    148\u001b[39m trainer = build_trainer_for_trial(lr, wd, k, bs, run_name, REFINE_EPOCHS)\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# Train + epoch evals (printed by callback; logged to W&B automatically)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Final eval for the objective score\u001b[39;00m\n\u001b[32m    154\u001b[39m metrics = trainer.evaluate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py:2582\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py:3796\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3793\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3795\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3796\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3798\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3799\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3800\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3801\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3802\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py:3884\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3882\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3883\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3885\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3886\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1079\u001b[39m, in \u001b[36mDebertaV2ForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1071\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1072\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m   1073\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m   1074\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m   1075\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1076\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1077\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1079\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1090\u001b[39m encoder_layer = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1091\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(encoder_layer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:786\u001b[39m, in \u001b[36mDebertaV2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    776\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    778\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    779\u001b[39m     input_ids=input_ids,\n\u001b[32m    780\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    783\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    784\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m786\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    787\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    793\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:659\u001b[39m, in \u001b[36mDebertaV2Encoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    657\u001b[39m rel_embeddings = \u001b[38;5;28mself\u001b[39m.get_rel_embedding()\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     output_states, attn_weights = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[32m    669\u001b[39m         all_attentions = all_attentions + (attn_weights,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:438\u001b[39m, in \u001b[36mDebertaV2Layer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    430\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    431\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    436\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    437\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    447\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:371\u001b[39m, in \u001b[36mDebertaV2Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    363\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    364\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    370\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    380\u001b[39m         query_states = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:251\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention:\n\u001b[32m    250\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     rel_att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    256\u001b[39m     attention_scores = attention_scores + rel_att\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:343\u001b[39m, in \u001b[36mDisentangledSelfAttention.disentangled_attention_bias\u001b[39m\u001b[34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[39m\n\u001b[32m    335\u001b[39m r_pos = build_rpos(\n\u001b[32m    336\u001b[39m     query_layer,\n\u001b[32m    337\u001b[39m     key_layer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    340\u001b[39m     \u001b[38;5;28mself\u001b[39m.position_buckets,\n\u001b[32m    341\u001b[39m )\n\u001b[32m    342\u001b[39m p2c_pos = torch.clamp(-r_pos + att_span, \u001b[32m0\u001b[39m, att_span * \u001b[32m2\u001b[39m - \u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m p2c_att = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_query_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m p2c_att = torch.gather(\n\u001b[32m    345\u001b[39m     p2c_att,\n\u001b[32m    346\u001b[39m     dim=-\u001b[32m1\u001b[39m,\n\u001b[32m    347\u001b[39m     index=p2c_pos.squeeze(\u001b[32m0\u001b[39m).expand([query_layer.size(\u001b[32m0\u001b[39m), key_layer.size(-\u001b[32m2\u001b[39m), key_layer.size(-\u001b[32m2\u001b[39m)]),\n\u001b[32m    348\u001b[39m ).transpose(-\u001b[32m1\u001b[39m, -\u001b[32m2\u001b[39m)\n\u001b[32m    349\u001b[39m score += p2c_att / scale.to(dtype=p2c_att.dtype)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "    # --- Ex.5: Optuna study (12 trials √ó 12 epochs) around your current best HPs ---\n",
    "\n",
    "import optuna\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "STUDY_GROUP   = BASE_RUN_NAME + \"_study\"\n",
    "REFINE_EPOCHS = 8        # <- per your request\n",
    "N_TRIALS      = 15        # <- per your request\n",
    "LOG_STEPS     = 100\n",
    "\n",
    "\n",
    "\n",
    "# Objective: search narrowly around your current best HPs\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    lr = trial.suggest_float(\"lr\", 7e-5, 5e-4, log=True)\n",
    "    wd = trial.suggest_float(\"weight_decay\",1e-5, 1e-4, log=True)\n",
    "    k  = trial.suggest_int(\"unfreeze_last_k\", 8,12)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [8, 16, 32,64,128])\n",
    "\n",
    "    run_name = f\"{BASE_RUN_NAME}__t{trial.number+10}\"\n",
    "\n",
    "    # W&B run per trial (grouped under the study for easy comparison)\n",
    "    wb = wandb.init(\n",
    "        project=WANDB_PROJECT,\n",
    "        name=run_name,\n",
    "        group=STUDY_GROUP,\n",
    "        job_type=\"optuna-trial\",\n",
    "        tags=[\"optuna\",\"trainer\",\"ex5\"],\n",
    "        config={\n",
    "            \"trial\": trial.number+10, \"model\": MODEL_NAME,\n",
    "            \"lr\": lr, \"weight_decay\": wd, \"unfreeze_last_k\": k,\n",
    "            \"batch_size\": bs, \"epochs\": REFINE_EPOCHS,\n",
    "            \"warmup_ratio\": WARMUP_RATIO, \"max_len\": MAX_LEN,\n",
    "        },\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "    )\n",
    "\n",
    "    print(f\"[TUNE] trial={trial.number+10} | epochs={REFINE_EPOCHS} bs={bs} lr={lr:.2e} wd={wd:.1e} k={k}\")\n",
    "\n",
    "    trainer = build_trainer_for_trial(lr, wd, k, bs, run_name, REFINE_EPOCHS)\n",
    "\n",
    "    # Train + epoch evals (printed by callback; logged to W&B automatically)\n",
    "    trainer.train()\n",
    "\n",
    "    # Final eval for the objective score\n",
    "    metrics = trainer.evaluate()\n",
    "    f1 = float(metrics.get(\"eval_f1\", 0.0))\n",
    "    acc = float(metrics.get(\"eval_accuracy\", 0.0))\n",
    "\n",
    "    # Extra trial-level logging\n",
    "    wandb.log({\"trial/f1\": f1, \"trial/accuracy\": acc})\n",
    "    wb.summary[\"best_model_ckpt\"] = trainer.state.best_model_checkpoint\n",
    "    wb.summary[\"best_eval_f1\"] = f1\n",
    "    wb.summary[\"params\"] = dict(lr=lr, weight_decay=wd, unfreeze_last_k=k, batch_size=bs, epochs=REFINE_EPOCHS)\n",
    "    wandb.finish()\n",
    "\n",
    "    print(f\"[TUNE-END] trial={trial.number+10} f1={f1:.4f}\")\n",
    "    return f1\n",
    "\n",
    "# Run the study\n",
    "print(f\"[Study] Starting Optuna: trials={N_TRIALS}, epochs/trial={REFINE_EPOCHS} ; centered at \"\n",
    "      f\"lr={LR:.2e}, wd={WEIGHT_DECAY:.1e}, k={UNFREEZE_LAST_K}, bs={BATCH_SIZE}\")\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)\n",
    "\n",
    "# Persist best params\n",
    "best = {\n",
    "    \"lr\": float(study.best_trial.params[\"lr\"]),\n",
    "    \"weight_decay\": float(study.best_trial.params[\"weight_decay\"]),\n",
    "    \"num_unfreeze_last_layers\": int(study.best_trial.params[\"unfreeze_last_k\"]),\n",
    "    \"batch_size\": int(study.best_trial.params[\"batch_size\"]),\n",
    "    \"epochs\": int(EPOCHS),   # you can bump this later for final train if you want\n",
    "}\n",
    "os.makedirs(\"hf_ckpts\", exist_ok=True)\n",
    "with open(\"hf_ckpts/best_params_optuna_1closer.json\", \"w\") as f:\n",
    "    json.dump(best, f, indent=2)\n",
    "\n",
    "print(\"[Study best]\", best)\n",
    "print(\"Saved best params ‚Üí hf_ckpts/best_params_optuna1closer.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87f9da-61ec-4dee-931d-ec152a0687b1",
   "metadata": {},
   "source": [
    "# üìä Ex.5 ‚Äî Optuna Trials (11‚Äì18): Extended Results  \n",
    "\n",
    "After the first 10 runs, we extended the Optuna search to more trials.  \n",
    "Here‚Äôs what we observed:  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Stronger Trials\n",
    "- **Trial 12** ‚Üí LR ‚âà 1.0e-4, WD ‚âà 5e-5, batch size 16, unfreeze 12 ‚Üí **F1 = 0.859**  \n",
    "  ‚Üí This is our **best so far**. Confirms that pushing LR slightly above 9e-5 can work if weight decay is kept moderate.  \n",
    "- **Trial 13** ‚Üí LR ‚âà 9.4e-5, WD ‚âà 6e-5, batch size 8, unfreeze 11 ‚Üí **F1 = 0.847**  \n",
    "  ‚Üí Stable, good consistency. Smaller batch helped here.  \n",
    "- **Trial 11** ‚Üí LR ‚âà 1.2e-4, WD ‚âà 8.6e-5, batch size 16, unfreeze 8 ‚Üí **F1 = 0.836**  \n",
    "  ‚Üí Still decent, but shallower unfreezing (only 8 layers) seems to cap performance.  \n",
    "- **Trial 17** ‚Üí LR ‚âà 8.6e-5, WD ‚âà 1e-5, batch size 32, unfreeze 11 ‚Üí **F1 = 0.832**  \n",
    "  ‚Üí Confirms the LR ‚Äúsweet spot.‚Äù Batch size 32 still works fine.  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Medium Results\n",
    "- **Trial 14** ‚Üí LR ‚âà 9.1e-5, WD ‚âà 1.3e-5, batch size 8, unfreeze 8 ‚Üí **F1 = 0.814**  \n",
    "  ‚Üí Again, too shallow unfreezing limits the score.  \n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Collapse Zone\n",
    "Several trials **completely collapsed (F1 ‚âà 0.086)**:  \n",
    "- Trial 10, 15, 16, 11 (second run), 18 (no result)  \n",
    "  - All used **very large LR (‚â•4e-4)**.  \n",
    "  - This confirms that once LR crosses ~1.5e-4, the model becomes unstable and fails to learn.  \n",
    "\n",
    "---\n",
    "\n",
    "### üßæ Takeaways\n",
    "- **Best range** remains **LR ‚âà 8e-5 ‚Üí 1e-4**, with **weight decay ~1e-5 ‚Üí 6e-5**.  \n",
    "- **Deep unfreezing (10‚Äì12 layers)** is almost always required for high F1.  \n",
    "- **Batch size flexibility**: both 16 and 32 work, 8 sometimes helps but not always.  \n",
    "- **Large LR (‚â•2e-4)** ‚Üí catastrophic collapse (F1 ~ 0.08).  \n",
    "\n",
    "---\n",
    "\n",
    "üëâ Overall, Trials 12 and 13 show we can **push beyond 0.85 F1** with careful tuning.  \n",
    "But the search space is still fragile ‚Äî a small step in LR can cause collapse.  \n",
    "Next step: maybe test **schedulers / warmup ratio tweaks** to stabilize training.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba57e6a-4611-44f6-9603-fe71c4d88594",
   "metadata": {},
   "source": [
    "# üèÅ Ex.5 ‚Äî Final Training & Test Evaluation  \n",
    "\n",
    "Now that we finished **hyperparameter tuning with Optuna**, it‚Äôs time for the **final run**:  \n",
    "we take the **best HPs** (learning rate, weight decay, batch size, unfreezing depth, epochs)  \n",
    "and retrain a **fresh model from scratch** with them.  \n",
    "\n",
    "---\n",
    "\n",
    "### üî® What happens in this stage:\n",
    "1. **Load best hyperparameters** ‚Äî either directly from the Optuna study in memory,  \n",
    "   or from the saved JSON (`hf_ckpts/best_params_optuna1.json`).  \n",
    "2. **Rebuild the model** ‚Äî freeze everything, then unfreeze the best `k` last layers,  \n",
    "   keeping the classifier head trainable.  \n",
    "3. **Training setup** ‚Äî longer training (15 epochs), early stopping, bf16/fp16 acceleration,  \n",
    "   and fused AdamW optimizer for speed.  \n",
    "4. **Train & save checkpoints** ‚Äî log everything to W&B, keep the best model at the end.  \n",
    "5. **Evaluate** ‚Äî report metrics on both validation and the clean translated **test set**.  \n",
    "   We include accuracy, precision, recall, macro-F1, plus full per-class report and confusion matrix.  \n",
    "\n",
    "---\n",
    "\n",
    "üëâ This gives us the **final performance numbers** of our pipeline,  \n",
    "and also saves the best model checkpoint so we can reuse it later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f71301-1ef7-4855-afc0-dc58c253fcc4",
   "metadata": {},
   "source": [
    "# Load Best Model EX5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66896c920d37ec45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:00:31.721409Z",
     "start_time": "2025-08-17T07:40:11.589818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[final] Loaded best params from file: {'lr': 0.00010491108358128862, 'weight_decay': 5.114700076417737e-05, 'num_unfreeze_last_layers': 12, 'batch_size': 16, 'epochs': 12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[final] Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÖ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>0.77754</td></tr><tr><td>train/global_step</td><td>1800</td></tr><tr><td>train/grad_norm</td><td>13.4375</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>0.8664</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_103841</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/c5fkxtcs' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/c5fkxtcs</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_103841-c5fkxtcs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_104013-o8smm6mo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/o8smm6mo' target=\"_blank\">microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/o8smm6mo' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/o8smm6mo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[final] device=cuda bf16=True fp16=False | out_dir=hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\n",
      "[final] HP ‚Üí bs=16 lr=1.05e-04 wd=5.1e-05 epochs=12 unfreeze_last_k=12\n",
      "[final] Starting training ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_30920\\2488745322.py:139: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run] epochs=12 bs=16 lr=1.05e-04 wd=5.1e-05 warmup_ratio=0.06 grad_accum=1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25465' max='27780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25465/27780 20:06 < 01:49, 21.10 it/s, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.743100</td>\n",
       "      <td>0.713724</td>\n",
       "      <td>0.729106</td>\n",
       "      <td>0.761852</td>\n",
       "      <td>0.721231</td>\n",
       "      <td>0.736664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.468500</td>\n",
       "      <td>0.510003</td>\n",
       "      <td>0.827017</td>\n",
       "      <td>0.843368</td>\n",
       "      <td>0.824059</td>\n",
       "      <td>0.831787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.442800</td>\n",
       "      <td>0.587996</td>\n",
       "      <td>0.811710</td>\n",
       "      <td>0.811926</td>\n",
       "      <td>0.834801</td>\n",
       "      <td>0.814972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.340900</td>\n",
       "      <td>0.469016</td>\n",
       "      <td>0.850826</td>\n",
       "      <td>0.852043</td>\n",
       "      <td>0.858283</td>\n",
       "      <td>0.854500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.487433</td>\n",
       "      <td>0.853984</td>\n",
       "      <td>0.856803</td>\n",
       "      <td>0.861751</td>\n",
       "      <td>0.857645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.275800</td>\n",
       "      <td>0.460352</td>\n",
       "      <td>0.861516</td>\n",
       "      <td>0.860913</td>\n",
       "      <td>0.870439</td>\n",
       "      <td>0.864878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.248500</td>\n",
       "      <td>0.500496</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>0.862110</td>\n",
       "      <td>0.875398</td>\n",
       "      <td>0.867273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.243500</td>\n",
       "      <td>0.506808</td>\n",
       "      <td>0.860787</td>\n",
       "      <td>0.857589</td>\n",
       "      <td>0.872901</td>\n",
       "      <td>0.863752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.497311</td>\n",
       "      <td>0.863946</td>\n",
       "      <td>0.862095</td>\n",
       "      <td>0.874121</td>\n",
       "      <td>0.867178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>0.514455</td>\n",
       "      <td>0.859572</td>\n",
       "      <td>0.855997</td>\n",
       "      <td>0.872231</td>\n",
       "      <td>0.862454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>0.518526</td>\n",
       "      <td>0.859815</td>\n",
       "      <td>0.856653</td>\n",
       "      <td>0.872196</td>\n",
       "      <td>0.862831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e0 b100/2315] loss=1.6420 lr=6.23e-06\n",
      "[e0 b200/2315] loss=1.6353 lr=1.25e-05\n",
      "[e0 b300/2315] loss=1.6074 lr=1.88e-05\n",
      "[e0 b400/2315] loss=1.5622 lr=2.51e-05\n",
      "[e0 b500/2315] loss=1.5431 lr=3.14e-05\n",
      "[e0 b600/2315] loss=1.4967 lr=3.77e-05\n",
      "[e0 b700/2315] loss=1.4243 lr=4.40e-05\n",
      "[e0 b800/2315] loss=1.3462 lr=5.03e-05\n",
      "[e0 b900/2315] loss=1.2049 lr=5.66e-05\n",
      "[e0 b1000/2315] loss=1.1781 lr=6.29e-05\n",
      "[e0 b1100/2315] loss=1.0698 lr=6.92e-05\n",
      "[e0 b1200/2315] loss=1.0031 lr=7.55e-05\n",
      "[e0 b1300/2315] loss=0.9656 lr=8.18e-05\n",
      "[e0 b1400/2315] loss=0.9317 lr=8.80e-05\n",
      "[e0 b1500/2315] loss=0.9780 lr=9.43e-05\n",
      "[e0 b1600/2315] loss=0.9174 lr=1.01e-04\n",
      "[e0 b1700/2315] loss=0.8946 lr=1.05e-04\n",
      "[e0 b1800/2315] loss=0.8692 lr=1.04e-04\n",
      "[e0 b1900/2315] loss=0.8118 lr=1.04e-04\n",
      "[e0 b2000/2315] loss=0.8220 lr=1.04e-04\n",
      "[e0 b2100/2315] loss=0.7980 lr=1.03e-04\n",
      "[e0 b2200/2315] loss=0.7537 lr=1.03e-04\n",
      "[e0 b2300/2315] loss=0.7431 lr=1.02e-04\n",
      "[val @ epoch 1] acc=0.7291 f1=0.7367 p=0.7619 r=0.7212\n",
      "[val @ epoch 2] acc=0.8270 f1=0.8318 p=0.8434 r=0.8241\n",
      "[val @ epoch 3] acc=0.8117 f1=0.8150 p=0.8119 r=0.8348\n",
      "[val @ epoch 4] acc=0.8508 f1=0.8545 p=0.8520 r=0.8583\n",
      "[val @ epoch 5] acc=0.8540 f1=0.8576 p=0.8568 r=0.8618\n",
      "[val @ epoch 6] acc=0.8615 f1=0.8649 p=0.8609 r=0.8704\n",
      "[val @ epoch 7] acc=0.8639 f1=0.8673 p=0.8621 r=0.8754\n",
      "[val @ epoch 8] acc=0.8608 f1=0.8638 p=0.8576 r=0.8729\n",
      "[val @ epoch 9] acc=0.8639 f1=0.8672 p=0.8621 r=0.8741\n",
      "[val @ epoch 10] acc=0.8596 f1=0.8625 p=0.8560 r=0.8722\n",
      "[val @ epoch 11] acc=0.8598 f1=0.8628 p=0.8567 r=0.8722\n",
      "[final] TrainOutput: TrainOutput(global_step=25465, training_loss=0.3875152890115315, metrics={'train_runtime': 1206.8692, 'train_samples_per_second': 368.282, 'train_steps_per_second': 23.018, 'total_flos': 1.7303469143585856e+16, 'train_loss': 0.3875152890115315, 'epoch': 11.0})\n",
      "[final] Best checkpoint: hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\checkpoint-16205\n",
      "[final] Evaluating on validation set ‚Ä¶\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[val @ epoch 11] acc=0.8639 f1=0.8673 p=0.8621 r=0.8754\n",
      "[final][val] ‚Üí {'eval_loss': 0.5004957914352417, 'eval_accuracy': 0.8639455782312925, 'eval_precision': 0.8621102868494844, 'eval_recall': 0.8753979815762152, 'eval_f1': 0.8672727518751845, 'eval_runtime': 4.4479, 'eval_samples_per_second': 925.389, 'eval_steps_per_second': 58.005, 'epoch': 11.0}\n",
      "[final] Evaluating on TEST set ‚Ä¶\n",
      "[final][test] ‚Üí {'test_accuracy': 0.8433385992627699, 'test_precision_macro': 0.8464462108232367, 'test_recall_macro': 0.8524153135872158, 'test_f1_macro': 0.8476267133422806}\n"
     ]
    }
   ],
   "source": [
    "# ===== Final train on best HPs, save model, and evaluate on test =====\n",
    "import os, json, time, importlib.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import wandb\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback, AutoModelForSequenceClassification\n",
    "\n",
    "\n",
    "# --- 1) Load best hyperparameters (from current study or from disk) ---\n",
    "best_params_path = \"hf_ckpts/best_params_optuna1.json\"\n",
    "if 'study' in globals() and study.best_trial is not None:\n",
    "    bp = {\n",
    "        \"lr\": float(study.best_trial.params[\"lr\"]),\n",
    "        \"weight_decay\": float(study.best_trial.params[\"weight_decay\"]),\n",
    "        \"num_unfreeze_last_layers\": int(study.best_trial.params[\"unfreeze_last_k\"]),\n",
    "        \"batch_size\": int(study.best_trial.params[\"batch_size\"]),\n",
    "        \"epochs\": int(15),  # keep your requested final epochs\n",
    "    }\n",
    "    print(\"[final] Using best params from in-memory study:\", bp)\n",
    "    os.makedirs(os.path.dirname(best_params_path), exist_ok=True)\n",
    "    with open(best_params_path, \"w\") as f: json.dump(bp, f, indent=2)\n",
    "else:\n",
    "    with open(best_params_path, \"r\") as f:\n",
    "        bp = json.load(f)\n",
    "    print(\"[final] Loaded best params from file:\", bp)\n",
    "\n",
    "FINAL_EPOCHS   = int(bp.get(\"epochs\", EPOCHS))\n",
    "FINAL_LR       = float(bp[\"lr\"])\n",
    "FINAL_WD       = float(bp[\"weight_decay\"])\n",
    "FINAL_BS       = int(bp[\"batch_size\"])\n",
    "FINAL_UNFREEZE = int(bp[\"num_unfreeze_last_layers\"])\n",
    "\n",
    "# --- 2) Rebuild a fresh model with the chosen unfreezing plan ---\n",
    "def build_model_for_final(unfreeze_last_k: int):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(ORDER),\n",
    "        id2label=ID2LABEL,\n",
    "        label2id=LABEL2ID,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else None),\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "    base = getattr(model, \"deberta\", None) or getattr(model, \"roberta\", None) or getattr(model, \"bert\", None)\n",
    "    if base is not None and hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "        # freeze all, then unfreeze last k transformer blocks\n",
    "        for p in base.parameters(): p.requires_grad = False\n",
    "        for layer in base.encoder.layer[-int(unfreeze_last_k):]:\n",
    "            for p in layer.parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"[final] Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%) ; unfreeze_last_k={unfreeze_last_k}\")\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "model = build_model_for_final(FINAL_UNFREEZE)\n",
    "\n",
    "# --- 3) Final TrainingArguments (bf16 on 4090; fused AdamW; no torch.compile) ---\n",
    "bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_run_name = f\"{BASE_RUN_NAME}__final_{timestamp}\"\n",
    "final_out_dir  = os.path.join(\"hf_ckpts\", final_run_name)\n",
    "os.makedirs(final_out_dir, exist_ok=True)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=final_out_dir,\n",
    "    run_name=final_run_name,\n",
    "    report_to=[\"wandb\"],\n",
    "\n",
    "    learning_rate=FINAL_LR,\n",
    "    weight_decay=FINAL_WD,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    num_train_epochs=FINAL_EPOCHS,\n",
    "\n",
    "    per_device_train_batch_size=FINAL_BS,\n",
    "    per_device_eval_batch_size=FINAL_BS,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "\n",
    "    dataloader_num_workers=max(0, NUM_WORKERS),   # keep your Windows-stable choice\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_prefetch_factor=2 if NUM_WORKERS > 0 else None,\n",
    "\n",
    "    fp16=(torch.cuda.is_available() and not bf16_ok),\n",
    "    bf16=bf16_ok,\n",
    "    optim=(\"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\"),\n",
    "    save_safetensors=True,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    torch_compile=False,   # keep off on Windows unless Triton is present & stable\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# --- 4) W&B run for the final train ---\n",
    "wandb_run = wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    name=final_run_name,\n",
    "    group=BASE_RUN_NAME + \"_final\",\n",
    "    job_type=\"final-train\",\n",
    "    tags=[\"final\",\"trainer\",\"ex5\"],\n",
    "    config={\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"max_len\": MAX_LEN,\n",
    "        \"epochs\": FINAL_EPOCHS,\n",
    "        \"lr\": FINAL_LR,\n",
    "        \"weight_decay\": FINAL_WD,\n",
    "        \"warmup_ratio\": WARMUP_RATIO,\n",
    "        \"batch_size\": FINAL_BS,\n",
    "        \"grad_accum\": GRAD_ACCUM,\n",
    "        \"unfreeze_last_k\": FINAL_UNFREEZE,\n",
    "        \"num_workers\": NUM_WORKERS,\n",
    "        \"bf16\": bf16_ok,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"[final] device={DEVICE} bf16={bf16_ok} fp16={training_args.fp16} | out_dir={final_out_dir}\")\n",
    "print(f\"[final] HP ‚Üí bs={FINAL_BS} lr={FINAL_LR:.2e} wd={FINAL_WD:.1e} epochs={FINAL_EPOCHS} unfreeze_last_k={FINAL_UNFREEZE}\")\n",
    "\n",
    "# --- 5) Trainer + train ---\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,                 # ok; deprecates in v5 (processing_class later)\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[PrintAndWBCallback(print_every=100),\n",
    "               EarlyStoppingCallback(early_stopping_patience=PATIENCE)],\n",
    ")\n",
    "\n",
    "print(\"[final] Starting training ‚Ä¶\")\n",
    "train_out = trainer.train()\n",
    "print(\"[final] TrainOutput:\", train_out)\n",
    "print(\"[final] Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "# --- 6) Evaluate on validation & test, save detailed reports ---\n",
    "print(\"[final] Evaluating on validation set ‚Ä¶\")\n",
    "val_metrics = trainer.evaluate(eval_dataset=val_ds)\n",
    "print(\"[final][val] ‚Üí\", val_metrics)\n",
    "wandb.log({f\"final/val_{k}\": v for k, v in val_metrics.items()})\n",
    "\n",
    "print(\"[final] Evaluating on TEST set ‚Ä¶\")\n",
    "test_pred = trainer.predict(test_ds)\n",
    "test_logits = test_pred.predictions\n",
    "test_labels = test_pred.label_ids\n",
    "test_preds  = np.argmax(test_logits, axis=-1)\n",
    "\n",
    "test_acc = accuracy_score(test_labels, test_preds)\n",
    "p, r, f1, _ = precision_recall_fscore_support(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
    "test_metrics = {\"test_accuracy\": float(test_acc), \"test_precision_macro\": float(p), \"test_recall_macro\": float(r), \"test_f1_macro\": float(f1)}\n",
    "print(\"[final][test] ‚Üí\", test_metrics)\n",
    "wandb.log({f\"final/{k}\": v for k, v in test_metrics.items()})\n",
    "\n",
    "# per-class report & confusion matrix\n",
    "report_dict = classification_report(test_labels, test_preds, target_names=ORDER, digits=4, output_dict=True)\n",
    "cm = confusion_matrix(test_labels, test_preds, labels=list(range(len(ORDER))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4cd8a-d406-467e-bd74-338ee1f21c58",
   "metadata": {},
   "source": [
    "# üìä Ex.5 ‚Äî Final Results (Validation & Test)\n",
    "\n",
    "After retraining the model with the **best hyperparameters** from Optuna,  \n",
    "we get the following performance:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Validation set\n",
    "- **Accuracy:** 0.8640  \n",
    "- **Macro F1:** 0.8673  \n",
    "- **Macro Precision:** 0.8621  \n",
    "- **Macro Recall:** 0.8754  \n",
    "\n",
    "> The model is very consistent on validation, reaching ~86‚Äì87% across all main metrics.  \n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Test set (clean translated)\n",
    "- **Accuracy:** 0.8433  \n",
    "- **Macro F1:** 0.8476  \n",
    "- **Macro Precision:** 0.8464  \n",
    "- **Macro Recall:** 0.8524  \n",
    "\n",
    "> On the unseen test set, performance drops slightly (‚âà2% lower than validation),  \n",
    "> but results are still strong and balanced across classes.  \n",
    "\n",
    "---\n",
    "\n",
    "### üéì Takeaway\n",
    "- Compared to our **Ex.4 training**, where the test **Macro F1 was 0.8685**,  \n",
    "  this run is a bit **weaker (~0.85 F1)**.  \n",
    "- Still, the model shows **good generalization and stable performance**.  \n",
    "- The Validation ‚Üî Test gap remains small ‚Üí no major overfitting.  \n",
    "- Overall, with **macro F1 ~0.85**, the model captures all sentiment classes fairly well.\n",
    "---\n",
    "After finishing the final training and test evaluation,  \n",
    "we saved **all important artifacts** for reproducibility and later analysis.  \n",
    "\n",
    "\n",
    "\n",
    "### üìÇ What we saved\n",
    "- **Model + Tokenizer** (best checkpoint) ‚Üí can be reloaded for inference or fine-tuning.  \n",
    "- **Test metrics JSON** ‚Üí contains summary scores (accuracy, precision, recall, macro F1).  \n",
    "- **Classification report (CSV)** ‚Üí per-class metrics (precision, recall, F1, support).  \n",
    "- **Confusion matrix (CSV)** ‚Üí class-wise confusion analysis.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c090001d2dec5ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:06:56.133452Z",
     "start_time": "2025-08-17T08:05:59.741602Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (.\\hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model)... Done. 0.9s\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/f1</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ</td></tr><tr><td>eval/precision</td><td>‚ñÅ‚ñá‚ñÑ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/recall</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÖ‚ñà‚ñÖ‚ñá‚ñÇ‚ñÉ‚ñÇ‚ñÑ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñá‚ñà‚ñÅ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñá‚ñÑ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñá‚ñà‚ñÅ‚ñá‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñá‚ñÜ‚ñá‚ñÑ</td></tr><tr><td>final/test_accuracy</td><td>‚ñÅ</td></tr><tr><td>final/test_f1_macro</td><td>‚ñÅ</td></tr><tr><td>final/test_precision_macro</td><td>‚ñÅ</td></tr><tr><td>final/test_recall_macro</td><td>‚ñÅ</td></tr><tr><td>final/val_epoch</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_accuracy</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_f1</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_loss</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_precision</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_recall</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_runtime</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>final/val_eval_steps_per_second</td><td>‚ñÅ</td></tr><tr><td>test/accuracy</td><td>‚ñÅ</td></tr><tr><td>test/f1</td><td>‚ñÅ</td></tr><tr><td>test/loss</td><td>‚ñÅ</td></tr><tr><td>test/precision</td><td>‚ñÅ</td></tr><tr><td>test/recall</td><td>‚ñÅ</td></tr><tr><td>test/runtime</td><td>‚ñÅ</td></tr><tr><td>test/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>test/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_dir</td><td>hf_ckpts\\microsoft__...</td></tr><tr><td>eval/accuracy</td><td>0.86395</td></tr><tr><td>eval/f1</td><td>0.86727</td></tr><tr><td>eval/loss</td><td>0.5005</td></tr><tr><td>eval/precision</td><td>0.86211</td></tr><tr><td>eval/recall</td><td>0.8754</td></tr><tr><td>eval/runtime</td><td>4.4479</td></tr><tr><td>eval/samples_per_second</td><td>925.389</td></tr><tr><td>eval/steps_per_second</td><td>58.005</td></tr><tr><td>final/test_accuracy</td><td>0.84334</td></tr><tr><td>final/test_f1_macro</td><td>0.84763</td></tr><tr><td>final/test_precision_macro</td><td>0.84645</td></tr><tr><td>final/test_recall_macro</td><td>0.85242</td></tr><tr><td>final/val_epoch</td><td>11</td></tr><tr><td>final/val_eval_accuracy</td><td>0.86395</td></tr><tr><td>final/val_eval_f1</td><td>0.86727</td></tr><tr><td>final/val_eval_loss</td><td>0.5005</td></tr><tr><td>final/val_eval_precision</td><td>0.86211</td></tr><tr><td>final/val_eval_recall</td><td>0.8754</td></tr><tr><td>final/val_eval_runtime</td><td>4.4479</td></tr><tr><td>final/val_eval_samples_per_second</td><td>925.389</td></tr><tr><td>final/val_eval_steps_per_second</td><td>58.005</td></tr><tr><td>test/accuracy</td><td>0.84334</td></tr><tr><td>test/f1</td><td>0.84763</td></tr><tr><td>test/loss</td><td>0.56451</td></tr><tr><td>test/precision</td><td>0.84645</td></tr><tr><td>test/recall</td><td>0.85242</td></tr><tr><td>test/runtime</td><td>4.4325</td></tr><tr><td>test/samples_per_second</td><td>856.846</td></tr><tr><td>test/steps_per_second</td><td>53.694</td></tr><tr><td>total_flos</td><td>1.7303469143585856e+16</td></tr><tr><td>train/epoch</td><td>11</td></tr><tr><td>train/global_step</td><td>25465</td></tr><tr><td>train/grad_norm</td><td>3.625</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.2172</td></tr><tr><td>train_loss</td><td>0.38752</td></tr><tr><td>train_runtime</td><td>1206.8692</td></tr><tr><td>train_samples_per_second</td><td>368.282</td></tr><tr><td>train_steps_per_second</td><td>23.018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/o8smm6mo' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try/runs/o8smm6mo</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-ex-5-deberta-16-08-try</a><br>Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_104013-o8smm6mo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DONE ===\n",
      "Saved:\n",
      " ‚Ä¢ Model + tokenizer ‚Üí hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model\n",
      " ‚Ä¢ Test metrics JSON ‚Üí hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\test_metrics.json\n",
      " ‚Ä¢ Classification report CSV ‚Üí hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\classification_report_test.csv\n",
      " ‚Ä¢ Confusion matrix CSV ‚Üí hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\confusion_matrix_test.csv\n"
     ]
    }
   ],
   "source": [
    "# --- 6) Save evaluation artifacts (Windows-safe: use W&B Artifacts, no symlinks) ---\n",
    "import time\n",
    "rep_df = pd.DataFrame(report_dict).transpose()\n",
    "cm_df  = pd.DataFrame(cm, index=[f\"true_{c}\" for c in ORDER], columns=[f\"pred_{c}\" for c in ORDER])\n",
    "\n",
    "rep_path  = os.path.join(final_out_dir, \"classification_report_test.csv\")\n",
    "cm_path   = os.path.join(final_out_dir, \"confusion_matrix_test.csv\")\n",
    "json_path = os.path.join(final_out_dir, \"test_metrics.json\")\n",
    "\n",
    "rep_df.to_csv(rep_path, index=True)\n",
    "cm_df.to_csv(cm_path, index=True)\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "# ‚õîÔ∏è Remove wandb.save(...) ‚Äî it uses symlinks on Windows and fails.\n",
    "# Use an Artifact instead:\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "eval_art = wandb.Artifact(f\"{BASE_RUN_NAME}-eval-{ts}\", type=\"evaluation\")\n",
    "eval_art.add_file(rep_path,  name=\"classification_report_test.csv\")\n",
    "eval_art.add_file(cm_path,   name=\"confusion_matrix_test.csv\")\n",
    "eval_art.add_file(json_path, name=\"test_metrics.json\")\n",
    "wandb_run.log_artifact(eval_art)\n",
    "\n",
    "# --- 7) Save the final model (best weights) + tokenizer ---\n",
    "save_dir = os.path.join(final_out_dir, \"best_model\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "trainer.save_model(save_dir)        # saves model + trainer state\n",
    "tokenizer.save_pretrained(save_dir) # saves tokenizer\n",
    "\n",
    "# a tiny README for the checkpoint\n",
    "with open(os.path.join(save_dir, \"README.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        f\"Model: {MODEL_NAME}\\n\"\n",
    "        f\"Labels: {ORDER}\\n\"\n",
    "        f\"hp: lr={FINAL_LR}, weight_decay={FINAL_WD}, batch_size={FINAL_BS}, \"\n",
    "        f\"epochs={FINAL_EPOCHS}, unfreeze_last_k={FINAL_UNFREEZE}\\n\"\n",
    "        f\"Val metrics: {val_metrics}\\n\"\n",
    "        f\"Test metrics: {test_metrics}\\n\"\n",
    "    )\n",
    "\n",
    "# (optional but nice) Log the whole model directory as a model artifact too\n",
    "model_art = wandb.Artifact(f\"{BASE_RUN_NAME}-model-{ts}\", type=\"model\")\n",
    "model_art.add_dir(save_dir)\n",
    "wandb_run.log_artifact(model_art)\n",
    "\n",
    "# Summaries + finish\n",
    "wandb_run.summary[\"best_checkpoint_dir\"] = trainer.state.best_model_checkpoint\n",
    "wandb_run.summary.update({f\"final/{k}\": v for k, v in test_metrics.items()})\n",
    "wandb.finish()\n",
    "\n",
    "print(\"\\n=== DONE ===\")\n",
    "print(\"Saved:\")\n",
    "print(\" ‚Ä¢ Model + tokenizer ‚Üí\", save_dir)\n",
    "print(\" ‚Ä¢ Test metrics JSON ‚Üí\", json_path)\n",
    "print(\" ‚Ä¢ Classification report CSV ‚Üí\", rep_path)\n",
    "print(\" ‚Ä¢ Confusion matrix CSV ‚Üí\", cm_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f95d57f47fac81b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:11:41.942700Z",
     "start_time": "2025-08-17T08:11:41.934220Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_extremely negative</th>\n",
       "      <th>pred_negative</th>\n",
       "      <th>pred_neutral</th>\n",
       "      <th>pred_positive</th>\n",
       "      <th>pred_extremely positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>true_extremely negative</th>\n",
       "      <td>538</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_negative</th>\n",
       "      <td>104</td>\n",
       "      <td>873</td>\n",
       "      <td>19</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_neutral</th>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>506</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_positive</th>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>32</td>\n",
       "      <td>733</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true_extremely positive</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         pred_extremely negative  pred_negative  pred_neutral  \\\n",
       "true_extremely negative                      538             50             2   \n",
       "true_negative                                104            873            19   \n",
       "true_neutral                                   5             64           506   \n",
       "true_positive                                  4             64            32   \n",
       "true_extremely positive                        1              1             0   \n",
       "\n",
       "                         pred_positive  pred_extremely positive  \n",
       "true_extremely negative              2                        0  \n",
       "true_negative                       41                        4  \n",
       "true_neutral                        42                        2  \n",
       "true_positive                      733                      114  \n",
       "true_extremely positive             44                      553  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce3a67fc423b5d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T08:57:26.403863Z",
     "start_time": "2025-08-17T08:57:25.813742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved state_dict ‚Üí hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\n"
     ]
    }
   ],
   "source": [
    "pt_path = os.path.join(final_out_dir, \"best_model_ex5.pt\")\n",
    "torch.save(trainer.model.state_dict(), pt_path)\n",
    "print(\"Saved state_dict ‚Üí\", pt_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef963113ed55ae5a",
   "metadata": {},
   "source": [
    "# ‚öñÔ∏è  Weighted Loss Experiment (mid-class emphasis)\n",
    "\n",
    "In this extra experiment we re-run the **full training loop (Ex.4 style)**,  \n",
    "but we make one important change: **the loss function now gives more weight to the ‚Äúmiddle‚Äù classes**  \n",
    "(`negative`, `neutral`, `positive`).  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Why we do this\n",
    "- In earlier runs, the model was already strong on the **extreme classes** (`extremely negative`, `extremely positive`).  \n",
    "- Performance on the **middle classes** was weaker (more confusion between neutral/positive/negative).  \n",
    "- To balance this, we **boost the loss weight** for the mid-classes so that mistakes on them are penalized more.  \n",
    "  ‚Üí This should encourage the model to learn better boundaries between these subtle sentiments.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîß How we apply it\n",
    "- We start from the **inverse-frequency class weights** (baseline).  \n",
    "- Then, we multiply the weights of the mid-classes by a fixed **boost factor (e.g. 1.5√ó)**.  \n",
    "- Finally, we normalize so the average weight stays ~1.0, keeping training stable.  \n",
    "- Loss = **Weighted CrossEntropy**, optionally with small label smoothing.  \n",
    "\n",
    "---\n",
    "\n",
    "### üìä Hyperparameters\n",
    "- For this **first run** we kept the HP search range the same as before:  \n",
    "  - `lr` in **8e-5 ‚Äì 1e-3** (log scale)  \n",
    "  - `weight_decay` in **1e-6 ‚Äì 1e-4**  \n",
    "  - Unfreeze depth: **8‚Äì12 layers**  \n",
    "  - Batch size: {4, 8, 16, 32, 64}  \n",
    "- The only difference is the **loss weighting strategy**.  \n",
    "\n",
    "---\n",
    "\n",
    "### üîÆ Next step\n",
    "- After this first weighted-loss run, we may also include the **mid-class boost ratio itself**  \n",
    "as a tunable hyperparameter in Optuna (instead of fixing it to 1.5).  \n",
    "- That way, the study can find the **optimal balance** between extremes vs. middle classes.  \n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ So in short: same training process as Ex.4,  \n",
    "but **loss re-weighting** to focus on the harder mid classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2873383337d8b71a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T13:29:18.822573Z",
     "start_time": "2025-08-17T13:29:13.275042Z"
    }
   },
   "outputs": [],
   "source": [
    "# # =========================\n",
    "# # ADV DL ‚Äì Part B: Monolingual baseline (RoBERTa) ‚Äì Exercise-4 style\n",
    "# # Custom loop + early stopping + W&B + Optuna ONLY; freeze base, unfreeze last k layers\n",
    "# # Uses df_train / df_test with columns: OriginalTweet (str), Sentiment (str)\n",
    "# # =========================\n",
    "\n",
    "# import os, math, random, time, json\n",
    "# from typing import Dict, List, Tuple\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "# import torch.nn.functional as F\n",
    "# from collections import Counter\n",
    "\n",
    "# # ---- deps ----\n",
    "# # !pip -q install transformers==4.43.3 optuna==3.6.1 wandb==0.17.5 >/dev/null\n",
    "\n",
    "# import transformers\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModelForSequenceClassification,\n",
    "#     DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "# )\n",
    "# import os\n",
    "# os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "# os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "# import optuna\n",
    "# import wandb\n",
    "\n",
    "# # -------------------------\n",
    "# # Constants (no CFG, Optuna-only workflow)\n",
    "# # -------------------------\n",
    "# MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "# MAX_LEN = 512\n",
    "# BATCH_SIZE = 16\n",
    "# WARMUP_RATIO = 0.06\n",
    "# GRAD_CLIP = 1.0\n",
    "# USE_AMP = True\n",
    "\n",
    "# # ‚ùó New W&B project & run base (to keep things separate)\n",
    "# PROJECT = \"adv-dl-p2-deberta-midf1\"\n",
    "# BASE_RUN_NAME = \"microsoft/mdeberta-v3-base_full_ex_4_midf1\"\n",
    "\n",
    "# TRIALS = 20\n",
    "# SEED = 42\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed); np.random.seed(seed)\n",
    "#     torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set_seed(SEED)\n",
    "\n",
    "# # ---- GPU perf toggles (Windows-safe) ----\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "# try:\n",
    "#     torch.set_float32_matmul_precision(\"high\")\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# # -------------------------\n",
    "# # Label mapping (5-way sentiment)\n",
    "# # -------------------------\n",
    "# CANON = {\n",
    "#     \"extremely negative\": \"extremely negative\",\n",
    "#     \"negative\": \"negative\",\n",
    "#     \"neutral\": \"neutral\",\n",
    "#     \"positive\": \"positive\",\n",
    "#     \"extremely positive\": \"extremely positive\",\n",
    "# }\n",
    "# ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "# LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "# ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "# def normalize_label(s: str) -> str:\n",
    "#     s = str(s).strip().lower()\n",
    "#     s = s.replace(\"very negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"very positive\", \"extremely positive\")\n",
    "#     s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "#     return CANON.get(s, s)\n",
    "\n",
    "# # -------------------------\n",
    "# # Expect df_train, df_test in memory\n",
    "# # -------------------------\n",
    "# assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns, \"df_train missing required columns\"\n",
    "# assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns, \"df_test missing required columns\"\n",
    "\n",
    "# def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df = df.copy()\n",
    "#     df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "#     df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "#     df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "#     df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "#     df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "#     return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "# dftrain_ = prep_df(df_train)\n",
    "# dftest_  = prep_df(df_test)\n",
    "\n",
    "# train_df, val_df = train_test_split(\n",
    "#     dftrain_, test_size=0.1, stratify=dftrain_[\"label\"], random_state=SEED\n",
    "# )\n",
    "# print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n",
    "\n",
    "# # # --- class weights for mid-class emphasis (computed from training split) ---\n",
    "# # _counts = Counter(train_df[\"label\"].tolist())\n",
    "# # _num_classes = len(ORDER)\n",
    "# # _total = sum(_counts.values())\n",
    "# # # inverse-freq: N / (K * n_c)\n",
    "# # CLASS_WEIGHTS = torch.tensor(\n",
    "# #     [_total / (_num_classes * _counts[i]) for i in range(_num_classes)],\n",
    "# #     dtype=torch.float,\n",
    "# #     device=DEVICE,\n",
    "# # )\n",
    "# # --- class weights with explicit mid-class boost ---\n",
    "# from collections import Counter\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# _counts = Counter(train_df[\"label\"].tolist())\n",
    "# _num_classes = len(ORDER)\n",
    "# _total = sum(_counts.values())\n",
    "\n",
    "# # inverse frequency baseline: N / (K * n_c)\n",
    "# inv = np.array([_total / (_num_classes * max(1, _counts.get(i, 0))) for i in range(_num_classes)],\n",
    "#                dtype=np.float32)\n",
    "\n",
    "# # indices\n",
    "# IDX_EXT_NEG = LABEL2ID[\"extremely negative\"]\n",
    "# IDX_NEG     = LABEL2ID[\"negative\"]\n",
    "# IDX_NEU     = LABEL2ID[\"neutral\"]\n",
    "# IDX_POS     = LABEL2ID[\"positive\"]\n",
    "# IDX_EXT_POS = LABEL2ID[\"extremely positive\"]\n",
    "\n",
    "# # multiplicative boost: mids > extremes (tune 1.25‚Äì2.0 as you like)\n",
    "# MID_BOOST = 1.5\n",
    "# mult = np.ones(_num_classes, dtype=np.float32)\n",
    "# mult[[IDX_NEG, IDX_NEU, IDX_POS]] = MID_BOOST   # boost the mid classes\n",
    "\n",
    "# weights = inv * mult\n",
    "\n",
    "# # optional: renormalize so the mean weight is 1.0 (keeps loss scale stable)\n",
    "# weights = weights * (_num_classes / weights.sum())\n",
    "\n",
    "# CLASS_WEIGHTS = torch.tensor(weights, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "# print(\"[loss] Using class-weighted CrossEntropy; weights per label:\")\n",
    "# print({ID2LABEL[i]: float(CLASS_WEIGHTS[i].item()) for i in range(_num_classes)})\n",
    "\n",
    "\n",
    "# # -------------------------\n",
    "# # Dataset & Collator\n",
    "# # -------------------------\n",
    "# class TweetDataset(Dataset):\n",
    "#     def __init__(self, df: pd.DataFrame, tokenizer: transformers.PreTrainedTokenizerBase, max_len: int):\n",
    "#         self.texts = df[\"text\"].tolist()\n",
    "#         self.labels = df[\"label\"].tolist()\n",
    "#         self.tok = tokenizer\n",
    "#         self.max_len = max_len\n",
    "#     def __len__(self): return len(self.texts)\n",
    "#     def __getitem__(self, idx):\n",
    "#         enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "#         enc[\"labels\"] = self.labels[idx]\n",
    "#         return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# train_ds = TweetDataset(train_df, tokenizer, MAX_LEN)\n",
    "# val_ds   = TweetDataset(val_df, tokenizer, MAX_LEN)\n",
    "# test_ds  = TweetDataset(dftest_, tokenizer, MAX_LEN)\n",
    "\n",
    "# BATCH_SIZE=16\n",
    "# # ---- pad_to_multiple_of=8 for Tensor Cores; Windows: workers=0 is often faster ----\n",
    "# collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "# train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "# val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "# # -------------------------\n",
    "# # Model & Freeze/Unfreeze strategy\n",
    "# # -------------------------\n",
    "# def build_model(num_unfreeze_last_layers: int = 4):\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         MODEL_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "#     )\n",
    "#     base = getattr(model, \"roberta\", None) or getattr(model, \"bert\", None) or getattr(model, \"deberta\", None)\n",
    "#     if base is not None:\n",
    "#         for p in base.parameters(): p.requires_grad = False\n",
    "#         if hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "#             k = num_unfreeze_last_layers\n",
    "#             if k > 0:\n",
    "#                 for layer in base.encoder.layer[-k:]:\n",
    "#                     for p in layer.parameters(): p.requires_grad = True\n",
    "#     for p in model.classifier.parameters(): p.requires_grad = True\n",
    "#     return model.to(DEVICE)\n",
    "\n",
    "# # -------------------------\n",
    "# # Train / Eval utilities\n",
    "# # -------------------------\n",
    "# def get_optimizer_scheduler(model, num_training_steps: int, lr: float, weight_decay: float):\n",
    "#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "#     ]\n",
    "#     # try fused AdamW on CUDA (faster step) ‚Äî falls back if unavailable\n",
    "#     try:\n",
    "#         optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay, fused=(DEVICE==\"cuda\"))\n",
    "#     except TypeError:\n",
    "#         optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay)\n",
    "#     num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps)\n",
    "#     return optimizer, scheduler\n",
    "\n",
    "# MIDS = [LABEL2ID[\"negative\"], LABEL2ID[\"neutral\"], LABEL2ID[\"positive\"]]\n",
    "\n",
    "# def evaluate(model, loader) -> Dict[str, float]:\n",
    "#     model.eval()\n",
    "#     preds, labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "#             # AMP autocast for faster eval math\n",
    "#             with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "#                           enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "#                 logits = model(**batch).logits\n",
    "#             preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "#             labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "#     # extra: mid-class F1 (negative/neutral/positive)\n",
    "#     p_mid, r_mid, f1_mid, _ = precision_recall_fscore_support(\n",
    "#         labels, preds, labels=MIDS, average=\"macro\", zero_division=0\n",
    "#     )\n",
    "#     return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"f1_mid\": f1_mid}\n",
    "\n",
    "# def train_one_run(hp: Dict) -> Tuple[str, Dict[str, float]]:\n",
    "#     \"\"\"\n",
    "#     hp keys: run_name, num_unfreeze_last_layers, lr, weight_decay, epochs, patience, trial_number\n",
    "#     \"\"\"\n",
    "#     run_name = hp[\"run_name\"]\n",
    "#     num_unfreeze = int(hp[\"num_unfreeze_last_layers\"])\n",
    "#     lr = float(hp[\"lr\"])\n",
    "#     wd = float(hp[\"weight_decay\"])\n",
    "#     epochs   = int(hp.get(\"epochs\",   FIXED_EPOCHS))\n",
    "#     patience = int(hp.get(\"patience\", FIXED_PATIENCE))\n",
    "#     model = build_model(num_unfreeze)\n",
    "#     total_steps = int(math.ceil(len(train_loader) * epochs))\n",
    "#     optimizer, scheduler = get_optimizer_scheduler(model, total_steps, lr, wd)\n",
    "\n",
    "#     scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
    "#     best_metric = -1.0\n",
    "#     no_improve = 0\n",
    "\n",
    "#     # ‚ùó save to a different folder + name to avoid collisions\n",
    "#     safe_run_name = run_name.replace(\"/\", \"__\").replace(\"\\\\\", \"__\")\n",
    "#     ckpt_dir = \"checkpoints_midf1\"\n",
    "#     os.makedirs(ckpt_dir, exist_ok=True)\n",
    "#     best_path = os.path.join(ckpt_dir, f\"best_midf1_{safe_run_name}.pt\")\n",
    "\n",
    "#     wandb_run = wandb.init(\n",
    "#         project=PROJECT,\n",
    "#         name=run_name,\n",
    "#         config={\n",
    "#             \"model\": MODEL_NAME,\n",
    "#             \"max_len\": MAX_LEN,\n",
    "#             \"batch_size\": BATCH_SIZE,\n",
    "#             \"epochs\": epochs,\n",
    "#             \"lr\": lr,\n",
    "#             \"weight_decay\": wd,\n",
    "#             \"warmup_ratio\": WARMUP_RATIO,\n",
    "#             \"grad_clip\": GRAD_CLIP,\n",
    "#             \"num_unfreeze_last_layers\": num_unfreeze,\n",
    "#             \"trial_number\": hp.get(\"trial_number\", -1),\n",
    "#             \"suggested_batch_size\": hp.get(\"batch_size\", BATCH_SIZE),\n",
    "#         },\n",
    "#         reinit=True,\n",
    "#     )\n",
    "\n",
    "#     # nicer W&B charts\n",
    "#     wandb.define_metric(\"epoch\")\n",
    "#     wandb.define_metric(\"step\")\n",
    "#     wandb.define_metric(\"train/*\", step_metric=\"step\")\n",
    "#     wandb.define_metric(\"val/*\",   step_metric=\"epoch\")\n",
    "\n",
    "#     # print + log trainable params\n",
    "#     total_params     = sum(p.numel() for p in model.parameters())\n",
    "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     print(f\"Trainable params: {trainable_params:,} / {total_params:,} \"\n",
    "#           f\"({100.0*trainable_params/total_params:.2f}%) ; unfreeze_last_k={num_unfreeze}\")\n",
    "#     wandb.log({\"params/total\": total_params,\n",
    "#                \"params/trainable\": trainable_params,\n",
    "#                \"params/ratio\": trainable_params/max(1,total_params)}, step=0)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         t0 = time.time()\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         for step, batch in enumerate(train_loader):\n",
    "#             batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "#             labels = batch.pop(\"labels\")  # we compute weighted loss ourselves\n",
    "\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             # use BF16 if supported; else FP16\n",
    "#             with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "#                           enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "#                 outputs = model(**batch)\n",
    "#                 logits = outputs.logits\n",
    "#                 # weighted CE (+ smoothing if available)\n",
    "#                 try:\n",
    "#                     loss = F.cross_entropy(logits, labels, weight=CLASS_WEIGHTS, label_smoothing=0.05)\n",
    "#                 except TypeError:\n",
    "#                     loss = F.cross_entropy(logits, labels, weight=CLASS_WEIGHTS)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             if GRAD_CLIP is not None:\n",
    "#                 scaler.unscale_(optimizer)\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "#             scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             if step % 20 == 0:\n",
    "#                 wandb.log({\"train/loss\": loss.item(), \"step\": step + 1, \"epoch\": epoch + 1})\n",
    "\n",
    "#             # periodic console + throughput log (about 10x per epoch)\n",
    "#             if step % max(1, len(train_loader)//10) == 0 or step == 1:\n",
    "#                 avg_loss = running_loss / max(1, (step + 1))\n",
    "#                 elapsed  = time.time() - t0\n",
    "#                 items    = (step + 1) * BATCH_SIZE\n",
    "#                 itps     = items / max(elapsed, 1e-6)\n",
    "#                 print(f\"[e{epoch+1} b{step+1}/{len(train_loader)}] loss={loss.item():.4f} avg={avg_loss:.4f} it/s={itps:.1f}\")\n",
    "#                 wandb.log({\"train/avg_loss_so_far\": avg_loss,\n",
    "#                            \"train/items_per_sec\": itps,\n",
    "#                            \"step\": (epoch * len(train_loader)) + (step + 1),\n",
    "#                            \"epoch\": epoch + 1})\n",
    "\n",
    "#         # epoch-end validation\n",
    "#         val_metrics = evaluate(model, val_loader)\n",
    "#         elapsed = time.time() - t0\n",
    "\n",
    "#         epoch_loss = running_loss / max(1, len(train_loader))\n",
    "#         current_lr = scheduler.get_last_lr()[0]\n",
    "#         wandb.log({\n",
    "#             \"train/epoch_loss\": epoch_loss,\n",
    "#             \"val/acc\": val_metrics[\"acc\"],\n",
    "#             \"val/precision\": val_metrics[\"precision\"],\n",
    "#             \"val/recall\": val_metrics[\"recall\"],\n",
    "#             \"val/f1\": val_metrics[\"f1\"],\n",
    "#             \"val/mid_f1\": val_metrics[\"f1_mid\"],   # extra log (format unchanged elsewhere)\n",
    "#             \"lr\": current_lr,\n",
    "#             \"time/epoch_sec\": elapsed,\n",
    "#             \"epoch\": epoch + 1,\n",
    "#         })\n",
    "\n",
    "#         # Early stopping on mid-class F1 (prints stay the same)\n",
    "#         target_metric = val_metrics[\"f1_mid\"]\n",
    "#         if target_metric > best_metric:\n",
    "#             best_metric = target_metric\n",
    "#             torch.save(model.state_dict(), best_path)\n",
    "#             no_improve = 0\n",
    "#             wandb_run.summary[\"best_val_f1\"] = best_metric  # kept same key for compatibility\n",
    "#             wandb_run.summary[\"best_val_mid_f1\"] = best_metric\n",
    "#             wandb_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "#             wandb.log({\"val/best_f1_so_far\": best_metric, \"val/best_epoch\": epoch + 1})\n",
    "#         else:\n",
    "#             no_improve += 1\n",
    "#             if no_improve >= patience:\n",
    "#                 print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#                 break\n",
    "\n",
    "#         # console print line unchanged\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "#               f\"loss={epoch_loss:.4f} | \"\n",
    "#               f\"val_acc={val_metrics['acc']:.4f} | val_f1={val_metrics['f1']:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "#     wandb.finish()\n",
    "\n",
    "#     # Load best and return path + metrics on val for reference\n",
    "#     model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "#     final_val = evaluate(model, val_loader)\n",
    "\n",
    "#     # store final val in W&B summary for quick sorting\n",
    "#     if wandb.run is not None:\n",
    "#         wandb.run.summary[\"final_val_acc\"] = final_val[\"acc\"]\n",
    "#         wandb.run.summary[\"final_val_precision\"] = final_val[\"precision\"]\n",
    "#         wandb.run.summary[\"final_val_recall\"] = final_val[\"recall\"]\n",
    "#         wandb.run.summary[\"final_val_f1\"] = final_val[\"f1\"]\n",
    "#         wandb.run.summary[\"final_val_mid_f1\"] = final_val[\"f1_mid\"]\n",
    "\n",
    "#     return best_path, final_val\n",
    "\n",
    "# # -------------------------\n",
    "# # Optuna hyperparameter tuning (ALWAYS ON)\n",
    "# # -------------------------\n",
    "\n",
    "# # Constants\n",
    "# FIXED_EPOCHS = 12\n",
    "# FIXED_PATIENCE = 4\n",
    "\n",
    "# def objective(trial: optuna.trial.Trial):\n",
    "#     params = {\n",
    "#         \"run_name\": f\"{BASE_RUN_NAME}_optuna_trial_{trial.number}\",\n",
    "#         \"num_unfreeze_last_layers\": trial.suggest_int(\"num_unfreeze_last_layers\", 8, 12),\n",
    "#         \"lr\": trial.suggest_float(\"lr\", 8e-5, 1e-3, log=True),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True),\n",
    "#         \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64]),\n",
    "#         \"epochs\": FIXED_EPOCHS,\n",
    "#         \"patience\": FIXED_PATIENCE,\n",
    "#         \"trial_number\": trial.number,\n",
    "#     }\n",
    "#     path, val_metrics = train_one_run(params)\n",
    "#     # console visibility per trial (unchanged)\n",
    "#     print(f\"[Trial {trial.number}] f1={val_metrics['f1']:.4f} | \"\n",
    "#           f\"unfreeze_k={params['num_unfreeze_last_layers']} lr={params['lr']:.2e} \"\n",
    "#           f\"wd={params['weight_decay']:.1e} suggested_bs={params['batch_size']}\")\n",
    "#     # report intermediate value for pruning if enabled (keep macro f1 for study objective)\n",
    "#     trial.report(val_metrics[\"f1\"], step=1)\n",
    "#     return val_metrics[\"f1\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)\n",
    "# print(\"Best trial:\", study.best_trial.number, \"F1:\", study.best_value)\n",
    "# best_params = {\"run_name\": f\"{BASE_RUN_NAME}_best_optuna\", **study.best_trial.params}\n",
    "\n",
    "# # also persist best params to a different folder/name\n",
    "# os.makedirs(\"checkpoints_midf1\", exist_ok=True)\n",
    "# with open(os.path.join(\"checkpoints_midf1\", \"best_hparams_optuna_midf1.json\"), \"w\") as f:\n",
    "#     json.dump({**best_params, \"epochs\": FIXED_EPOCHS, \"patience\": FIXED_PATIENCE}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd0fa90d15cb5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json, os\n",
    "# os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "# # best hparams from Optuna (only suggested ones live here)\n",
    "# best_hparams = study.best_trial.params\n",
    "\n",
    "# # if your train_one_run expects epochs/patience and they were fixed (not suggested),\n",
    "# # add them explicitly:\n",
    "# best_hparams_complete = {\n",
    "#     **best_hparams,\n",
    "#     \"epochs\": FIXED_EPOCHS,       # or whatever you used\n",
    "#     \"patience\": FIXED_PATIENCE,   # \"\n",
    "# }\n",
    "# # hp_path = os.path.join(\"checkpoints\", \"best_hparams_optuna.json\")\n",
    "# hp_path = os.path.join(\"checkpoints\", \"best_hparams_optuna_2.json\")\n",
    "# with open(hp_path, \"w\") as f:\n",
    "#     json.dump(best_hparams_complete, f, indent=2)\n",
    "# print(\"Saved best hparams to:\", hp_path)\n",
    "# print(\"Best trial number:\", study.best_trial.number, \" value:\", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948bea5c6cdf0751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T06:47:11.064422Z",
     "start_time": "2025-08-18T06:47:10.931533Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Load CSVs (your files have columns: ['UserName','ScreenName','Location','TweetAt','OriginalTweet','Sentiment'])\n",
    "TRAIN_CSV = \"Corona_NLP_train_cleaned_translated.csv\"   # or \"Corona_NLP_train.csv\"\n",
    "TEST_CSV  = \"Corona_NLP_test_cleaned_translated.csv\"    # or \"Corona_NLP_test.csv\"\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV, encoding=\"utf-8\", engine=\"python\")\n",
    "df_test  = pd.read_csv(TEST_CSV,  encoding=\"utf-8\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780a5550576f5f74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T18:21:18.535359Z",
     "start_time": "2025-08-17T14:48:01.408720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 37039/4116/3798\n",
      "[loss] Using tiered class weights; per label:\n",
      "{'extremely negative': 0.6557376980781555, 'negative': 1.2295081615447998, 'neutral': 1.2295081615447998, 'positive': 1.2295081615447998, 'extremely positive': 0.6557376980781555}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "[I 2025-08-17 17:48:02,852] A new study created in memory with name: no-name-03ead3fe-dcdf-4b18-bc81-53020fce6c05\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_174804-dqr0ejdg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/dqr0ejdg' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_0</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/dqr0ejdg' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/dqr0ejdg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 28,945,925 / 278,813,189 (10.38%) ; unfreeze_last_k=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b1/2315] loss=1.6733 avg=1.6733 it/s=41.6\n",
      "[e1 b2/2315] loss=1.6255 avg=1.6494 it/s=65.9\n",
      "[e1 b232/2315] loss=1.5370 avg=1.5309 it/s=522.7\n",
      "[e1 b463/2315] loss=1.3853 avg=1.4602 it/s=541.8\n",
      "[e1 b694/2315] loss=1.0948 avg=1.3923 it/s=543.0\n",
      "[e1 b925/2315] loss=1.0755 avg=1.3458 it/s=532.2\n",
      "[e1 b1156/2315] loss=0.8271 avg=1.3049 it/s=529.5\n",
      "[e1 b1387/2315] loss=0.9937 avg=1.2741 it/s=529.8\n",
      "[e1 b1618/2315] loss=0.7183 avg=1.2491 it/s=529.8\n",
      "[e1 b1849/2315] loss=1.2618 avg=1.2267 it/s=535.6\n",
      "[e1 b2080/2315] loss=1.1084 avg=1.2071 it/s=540.1\n",
      "[e1 b2311/2315] loss=1.3135 avg=1.1931 it/s=543.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.1930 | val_acc=0.6045 | val_f1=0.6184 | time=72.8s\n",
      "[e2 b1/2315] loss=1.0361 avg=1.0361 it/s=574.6\n",
      "[e2 b2/2315] loss=0.7388 avg=0.8875 it/s=607.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.9810 avg=1.0016 it/s=548.4\n",
      "[e2 b463/2315] loss=0.9185 avg=1.0022 it/s=538.3\n",
      "[e2 b694/2315] loss=1.0811 avg=0.9982 it/s=536.7\n",
      "[e2 b925/2315] loss=1.1991 avg=0.9994 it/s=539.2\n",
      "[e2 b1156/2315] loss=1.0520 avg=0.9887 it/s=541.1\n",
      "[e2 b1387/2315] loss=1.1179 avg=0.9828 it/s=545.9\n",
      "[e2 b1618/2315] loss=0.7024 avg=0.9805 it/s=550.4\n",
      "[e2 b1849/2315] loss=0.9609 avg=0.9753 it/s=553.9\n",
      "[e2 b2080/2315] loss=1.1449 avg=0.9726 it/s=557.1\n",
      "[e2 b2311/2315] loss=1.0524 avg=0.9694 it/s=557.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.9694 | val_acc=0.6540 | val_f1=0.6598 | time=71.1s\n",
      "[e3 b1/2315] loss=0.9008 avg=0.9008 it/s=475.1\n",
      "[e3 b2/2315] loss=1.0006 avg=0.9507 it/s=512.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.8123 avg=0.9059 it/s=560.1\n",
      "[e3 b463/2315] loss=0.9463 avg=0.9069 it/s=555.8\n",
      "[e3 b694/2315] loss=0.8118 avg=0.9048 it/s=558.0\n",
      "[e3 b925/2315] loss=0.8971 avg=0.8977 it/s=552.8\n",
      "[e3 b1156/2315] loss=0.8305 avg=0.8912 it/s=552.1\n",
      "[e3 b1387/2315] loss=0.8575 avg=0.8902 it/s=552.0\n",
      "[e3 b1618/2315] loss=0.9606 avg=0.8869 it/s=553.6\n",
      "[e3 b1849/2315] loss=0.8780 avg=0.8857 it/s=554.5\n",
      "[e3 b2080/2315] loss=0.9512 avg=0.8838 it/s=556.4\n",
      "[e3 b2311/2315] loss=0.6447 avg=0.8835 it/s=558.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.8835 | val_acc=0.6771 | val_f1=0.6883 | time=70.9s\n",
      "[e4 b1/2315] loss=0.8827 avg=0.8827 it/s=475.6\n",
      "[e4 b2/2315] loss=0.7565 avg=0.8196 it/s=584.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.7485 avg=0.8111 it/s=578.4\n",
      "[e4 b463/2315] loss=0.8027 avg=0.8259 it/s=581.7\n",
      "[e4 b694/2315] loss=1.2966 avg=0.8357 it/s=582.5\n",
      "[e4 b925/2315] loss=0.7232 avg=0.8380 it/s=583.2\n",
      "[e4 b1156/2315] loss=0.7594 avg=0.8384 it/s=580.9\n",
      "[e4 b1387/2315] loss=0.7920 avg=0.8375 it/s=562.6\n",
      "[e4 b1618/2315] loss=0.6902 avg=0.8361 it/s=513.8\n",
      "[e4 b1849/2315] loss=0.8842 avg=0.8365 it/s=492.1\n",
      "[e4 b2080/2315] loss=0.7169 avg=0.8346 it/s=469.3\n",
      "[e4 b2311/2315] loss=0.7919 avg=0.8334 it/s=466.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.8333 | val_acc=0.6878 | val_f1=0.6987 | time=84.0s\n",
      "[e5 b1/2315] loss=0.6985 avg=0.6985 it/s=523.6\n",
      "[e5 b2/2315] loss=0.7175 avg=0.7080 it/s=512.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.5846 avg=0.7906 it/s=573.6\n",
      "[e5 b463/2315] loss=0.5905 avg=0.7949 it/s=574.1\n",
      "[e5 b694/2315] loss=0.7464 avg=0.7880 it/s=574.9\n",
      "[e5 b925/2315] loss=0.8627 avg=0.7908 it/s=576.9\n",
      "[e5 b1156/2315] loss=0.8003 avg=0.7881 it/s=578.7\n",
      "[e5 b1387/2315] loss=0.9702 avg=0.7803 it/s=579.0\n",
      "[e5 b1618/2315] loss=1.3750 avg=0.7793 it/s=578.9\n",
      "[e5 b1849/2315] loss=0.7967 avg=0.7804 it/s=579.0\n",
      "[e5 b2080/2315] loss=1.6527 avg=0.7832 it/s=579.8\n",
      "[e5 b2311/2315] loss=1.0325 avg=0.7822 it/s=579.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.7822 | val_acc=0.7021 | val_f1=0.7124 | time=68.5s\n",
      "[e6 b1/2315] loss=0.7811 avg=0.7811 it/s=530.9\n",
      "[e6 b2/2315] loss=0.7376 avg=0.7593 it/s=590.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.8131 avg=0.7399 it/s=562.7\n",
      "[e6 b463/2315] loss=0.7439 avg=0.7363 it/s=532.3\n",
      "[e6 b694/2315] loss=0.7031 avg=0.7400 it/s=532.4\n",
      "[e6 b925/2315] loss=0.7046 avg=0.7426 it/s=528.0\n",
      "[e6 b1156/2315] loss=0.5494 avg=0.7434 it/s=530.8\n",
      "[e6 b1387/2315] loss=0.8384 avg=0.7432 it/s=537.2\n",
      "[e6 b1618/2315] loss=0.7435 avg=0.7409 it/s=543.0\n",
      "[e6 b1849/2315] loss=0.8034 avg=0.7402 it/s=547.1\n",
      "[e6 b2080/2315] loss=0.6485 avg=0.7375 it/s=551.5\n",
      "[e6 b2311/2315] loss=1.0068 avg=0.7374 it/s=551.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.7375 | val_acc=0.7102 | val_f1=0.7190 | time=71.8s\n",
      "[e7 b1/2315] loss=0.9489 avg=0.9489 it/s=739.5\n",
      "[e7 b2/2315] loss=0.5953 avg=0.7721 it/s=519.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.3971 avg=0.7104 it/s=536.0\n",
      "[e7 b463/2315] loss=0.3893 avg=0.7008 it/s=543.2\n",
      "[e7 b694/2315] loss=0.9501 avg=0.7126 it/s=542.7\n",
      "[e7 b925/2315] loss=0.7576 avg=0.7151 it/s=553.1\n",
      "[e7 b1156/2315] loss=0.8048 avg=0.7120 it/s=559.6\n",
      "[e7 b1387/2315] loss=0.7710 avg=0.7090 it/s=563.6\n",
      "[e7 b1618/2315] loss=0.7235 avg=0.7059 it/s=567.7\n",
      "[e7 b1849/2315] loss=0.7555 avg=0.7040 it/s=569.4\n",
      "[e7 b2080/2315] loss=0.6464 avg=0.7071 it/s=569.1\n",
      "[e7 b2311/2315] loss=0.7173 avg=0.7086 it/s=568.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.7083 | val_acc=0.7199 | val_f1=0.7284 | time=69.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÅ‚ñÅ‚ñÜ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñà‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÉ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÉ‚ñÇ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.71433</td></tr><tr><td>best_val_mid_f1</td><td>0.71433</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.10382</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>28945925</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>69.69054</td></tr><tr><td>train/avg_loss_so_far</td><td>0.70857</td></tr><tr><td>train/epoch_loss</td><td>0.70831</td></tr><tr><td>train/items_per_sec</td><td>568.94388</td></tr><tr><td>train/loss</td><td>0.51615</td></tr><tr><td>val/acc</td><td>0.71987</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.71433</td></tr><tr><td>val/f1</td><td>0.72839</td></tr><tr><td>val/mid_f1</td><td>0.71433</td></tr><tr><td>val/precision</td><td>0.73471</td></tr><tr><td>val/recall</td><td>0.72485</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_0</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/dqr0ejdg' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/dqr0ejdg</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_174804-dqr0ejdg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.728393:   5%|‚ñå         | 1/20 [08:45<2:46:29, 525.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 0] f1=0.7284 | unfreeze_k=4 lr=5.30e-05 wd=4.2e-07 suggested_bs=16\n",
      "[I 2025-08-17 17:56:48,587] Trial 0 finished with value: 0.7283928888695556 and parameters: {'num_unfreeze_last_layers': 4, 'lr': 5.2983095099764093e-05, 'weight_decay': 4.176723737706654e-07, 'batch_size': 16}. Best is trial 0 with value: 0.7283928888695556.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_175649-e7ex3pdi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e7ex3pdi' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_1</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e7ex3pdi' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e7ex3pdi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 50,209,541 / 278,813,189 (18.01%) ; unfreeze_last_k=7\n",
      "[e1 b1/2315] loss=1.5927 avg=1.5927 it/s=282.6\n",
      "[e1 b2/2315] loss=1.6036 avg=1.5981 it/s=324.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6425 avg=1.4912 it/s=446.7\n",
      "[e1 b463/2315] loss=1.4284 avg=1.3818 it/s=445.6\n",
      "[e1 b694/2315] loss=1.3784 avg=1.3423 it/s=447.6\n",
      "[e1 b925/2315] loss=1.3820 avg=1.3459 it/s=447.6\n",
      "[e1 b1156/2315] loss=1.4607 avg=1.3760 it/s=452.1\n",
      "[e1 b1387/2315] loss=1.5962 avg=1.3978 it/s=455.4\n",
      "[e1 b1618/2315] loss=1.4588 avg=1.4137 it/s=458.5\n",
      "[e1 b1849/2315] loss=1.5740 avg=1.4251 it/s=460.3\n",
      "[e1 b2080/2315] loss=1.4836 avg=1.4328 it/s=462.0\n",
      "[e1 b2311/2315] loss=1.3922 avg=1.4391 it/s=463.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.4390 | val_acc=0.2775 | val_f1=0.0869 | time=84.5s\n",
      "[e2 b1/2315] loss=1.5984 avg=1.5984 it/s=415.1\n",
      "[e2 b2/2315] loss=1.4512 avg=1.5248 it/s=540.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.5860 avg=1.4878 it/s=469.7\n",
      "[e2 b463/2315] loss=1.5286 avg=1.4901 it/s=456.8\n",
      "[e2 b694/2315] loss=1.5053 avg=1.4940 it/s=456.9\n",
      "[e2 b925/2315] loss=1.5490 avg=1.4965 it/s=455.8\n",
      "[e2 b1156/2315] loss=1.4608 avg=1.4969 it/s=456.7\n",
      "[e2 b1387/2315] loss=1.4401 avg=1.4976 it/s=460.2\n",
      "[e2 b1618/2315] loss=1.5514 avg=1.4972 it/s=462.9\n",
      "[e2 b1849/2315] loss=1.6076 avg=1.4981 it/s=465.2\n",
      "[e2 b2080/2315] loss=1.7110 avg=1.4969 it/s=467.1\n",
      "[e2 b2311/2315] loss=1.6137 avg=1.4972 it/s=467.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=1.4972 | val_acc=0.2775 | val_f1=0.0869 | time=83.8s\n",
      "[e3 b1/2315] loss=1.5409 avg=1.5409 it/s=471.3\n",
      "[e3 b2/2315] loss=1.3996 avg=1.4703 it/s=516.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.4941 avg=1.4933 it/s=473.3\n",
      "[e3 b463/2315] loss=1.5508 avg=1.4902 it/s=473.7\n",
      "[e3 b694/2315] loss=1.5227 avg=1.4926 it/s=477.4\n",
      "[e3 b925/2315] loss=1.4828 avg=1.4944 it/s=479.4\n",
      "[e3 b1156/2315] loss=1.5496 avg=1.4951 it/s=480.7\n",
      "[e3 b1387/2315] loss=1.6558 avg=1.4947 it/s=476.0\n",
      "[e3 b1618/2315] loss=1.6504 avg=1.4964 it/s=466.0\n",
      "[e3 b1849/2315] loss=1.4449 avg=1.4960 it/s=460.9\n",
      "[e3 b2080/2315] loss=1.5094 avg=1.4960 it/s=458.2\n",
      "[e3 b2311/2315] loss=1.5682 avg=1.4960 it/s=459.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=1.4961 | val_acc=0.2775 | val_f1=0.0869 | time=85.3s\n",
      "[e4 b1/2315] loss=1.4526 avg=1.4526 it/s=478.1\n",
      "[e4 b2/2315] loss=1.5177 avg=1.4851 it/s=510.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.4132 avg=1.4895 it/s=473.7\n",
      "[e4 b463/2315] loss=1.4076 avg=1.4942 it/s=472.0\n",
      "[e4 b694/2315] loss=1.4747 avg=1.4962 it/s=464.3\n",
      "[e4 b925/2315] loss=1.4289 avg=1.4980 it/s=456.9\n",
      "[e4 b1156/2315] loss=1.5809 avg=1.4975 it/s=454.2\n",
      "[e4 b1387/2315] loss=1.5313 avg=1.4981 it/s=451.4\n",
      "[e4 b1618/2315] loss=1.4691 avg=1.4976 it/s=454.6\n",
      "[e4 b1849/2315] loss=1.4441 avg=1.4962 it/s=457.3\n",
      "[e4 b2080/2315] loss=1.4313 avg=1.4963 it/s=452.5\n",
      "[e4 b2311/2315] loss=1.5769 avg=1.4958 it/s=448.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñÇ‚ñÅ‚ñÑ‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñÖ‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.1448</td></tr><tr><td>best_val_mid_f1</td><td>0.1448</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>lr</td><td>9e-05</td></tr><tr><td>params/ratio</td><td>0.18008</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>50209541</td></tr><tr><td>step</td><td>9256</td></tr><tr><td>time/epoch_sec</td><td>87.33525</td></tr><tr><td>train/avg_loss_so_far</td><td>1.49584</td></tr><tr><td>train/epoch_loss</td><td>1.49584</td></tr><tr><td>train/items_per_sec</td><td>448.40592</td></tr><tr><td>train/loss</td><td>1.4575</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.1448</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/mid_f1</td><td>0.1448</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_1</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e7ex3pdi' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e7ex3pdi</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_175649-e7ex3pdi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 0. Best value: 0.728393:  10%|‚ñà         | 2/20 [14:36<2:06:47, 422.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 1] f1=0.0869 | unfreeze_k=7 lr=1.98e-04 wd=2.5e-06 suggested_bs=64\n",
      "[I 2025-08-17 18:02:39,039] Trial 1 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 7, 'lr': 0.00019844028445501068, 'weight_decay': 2.540025143840635e-06, 'batch_size': 64}. Best is trial 0 with value: 0.7283928888695556.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_180240-gkx9qian</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gkx9qian' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_2</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gkx9qian' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gkx9qian</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[e1 b1/2315] loss=1.5376 avg=1.5376 it/s=120.0\n",
      "[e1 b2/2315] loss=1.6045 avg=1.5710 it/s=151.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4637 avg=1.5481 it/s=336.4\n",
      "[e1 b463/2315] loss=1.4402 avg=1.5091 it/s=337.7\n",
      "[e1 b694/2315] loss=1.3643 avg=1.4736 it/s=339.9\n",
      "[e1 b925/2315] loss=1.1458 avg=1.4203 it/s=344.2\n",
      "[e1 b1156/2315] loss=0.7086 avg=1.3655 it/s=347.5\n",
      "[e1 b1387/2315] loss=1.1630 avg=1.3137 it/s=349.6\n",
      "[e1 b1618/2315] loss=0.7530 avg=1.2621 it/s=348.3\n",
      "[e1 b1849/2315] loss=0.6535 avg=1.2161 it/s=348.7\n",
      "[e1 b2080/2315] loss=0.7317 avg=1.1761 it/s=349.3\n",
      "[e1 b2311/2315] loss=0.9213 avg=1.1459 it/s=349.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.1452 | val_acc=0.7279 | val_f1=0.7383 | time=110.8s\n",
      "[e2 b1/2315] loss=0.6773 avg=0.6773 it/s=313.0\n",
      "[e2 b2/2315] loss=0.8443 avg=0.7608 it/s=323.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.7139 avg=0.7861 it/s=348.1\n",
      "[e2 b463/2315] loss=0.6314 avg=0.7738 it/s=346.7\n",
      "[e2 b694/2315] loss=0.5623 avg=0.7664 it/s=350.1\n",
      "[e2 b925/2315] loss=0.4747 avg=0.7551 it/s=352.4\n",
      "[e2 b1156/2315] loss=1.0590 avg=0.7501 it/s=354.7\n",
      "[e2 b1387/2315] loss=0.4271 avg=0.7390 it/s=356.9\n",
      "[e2 b1618/2315] loss=0.5642 avg=0.7342 it/s=358.1\n",
      "[e2 b1849/2315] loss=0.4317 avg=0.7293 it/s=359.0\n",
      "[e2 b2080/2315] loss=0.5881 avg=0.7224 it/s=359.7\n",
      "[e2 b2311/2315] loss=0.9817 avg=0.7175 it/s=360.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.7174 | val_acc=0.8110 | val_f1=0.8168 | time=107.2s\n",
      "[e3 b1/2315] loss=0.3580 avg=0.3580 it/s=287.6\n",
      "[e3 b2/2315] loss=0.2944 avg=0.3262 it/s=295.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.6506 avg=0.6019 it/s=329.3\n",
      "[e3 b463/2315] loss=0.5186 avg=0.6213 it/s=330.0\n",
      "[e3 b694/2315] loss=0.5284 avg=0.6172 it/s=329.0\n",
      "[e3 b925/2315] loss=0.4438 avg=0.6087 it/s=336.2\n",
      "[e3 b1156/2315] loss=1.0252 avg=0.6083 it/s=341.0\n",
      "[e3 b1387/2315] loss=0.3283 avg=0.6082 it/s=342.5\n",
      "[e3 b1618/2315] loss=0.9633 avg=0.6093 it/s=342.1\n",
      "[e3 b1849/2315] loss=0.6438 avg=0.6075 it/s=339.6\n",
      "[e3 b2080/2315] loss=0.5634 avg=0.6054 it/s=341.7\n",
      "[e3 b2311/2315] loss=0.7712 avg=0.6040 it/s=343.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.6041 | val_acc=0.8190 | val_f1=0.8245 | time=112.4s\n",
      "[e4 b1/2315] loss=0.4660 avg=0.4660 it/s=327.7\n",
      "[e4 b2/2315] loss=0.7206 avg=0.5933 it/s=327.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.5206 avg=0.5501 it/s=357.1\n",
      "[e4 b463/2315] loss=0.5363 avg=0.5441 it/s=352.0\n",
      "[e4 b694/2315] loss=0.3362 avg=0.5441 it/s=348.9\n",
      "[e4 b925/2315] loss=0.5773 avg=0.5481 it/s=345.2\n",
      "[e4 b1156/2315] loss=0.4762 avg=0.5490 it/s=345.3\n",
      "[e4 b1387/2315] loss=0.4456 avg=0.5460 it/s=345.1\n",
      "[e4 b1618/2315] loss=0.5660 avg=0.5432 it/s=345.7\n",
      "[e4 b1849/2315] loss=0.3619 avg=0.5438 it/s=341.6\n",
      "[e4 b2080/2315] loss=0.6659 avg=0.5401 it/s=324.7\n",
      "[e4 b2311/2315] loss=0.5478 avg=0.5388 it/s=314.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.5388 | val_acc=0.8379 | val_f1=0.8433 | time=125.9s\n",
      "[e5 b1/2315] loss=0.5390 avg=0.5390 it/s=277.0\n",
      "[e5 b2/2315] loss=0.2956 avg=0.4173 it/s=285.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.4916 avg=0.5022 it/s=236.5\n",
      "[e5 b463/2315] loss=0.4255 avg=0.4956 it/s=222.1\n",
      "[e5 b694/2315] loss=0.7153 avg=0.4974 it/s=251.2\n",
      "[e5 b925/2315] loss=0.6022 avg=0.4979 it/s=272.3\n",
      "[e5 b1156/2315] loss=0.3790 avg=0.4996 it/s=286.3\n",
      "[e5 b1387/2315] loss=0.6434 avg=0.4961 it/s=296.7\n",
      "[e5 b1618/2315] loss=0.7472 avg=0.4968 it/s=304.8\n",
      "[e5 b1849/2315] loss=0.5014 avg=0.4974 it/s=311.1\n",
      "[e5 b2080/2315] loss=0.2798 avg=0.4955 it/s=316.2\n",
      "[e5 b2311/2315] loss=0.2311 avg=0.4937 it/s=320.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4939 | val_acc=0.8491 | val_f1=0.8533 | time=120.2s\n",
      "[e6 b1/2315] loss=0.7703 avg=0.7703 it/s=257.9\n",
      "[e6 b2/2315] loss=0.6195 avg=0.6949 it/s=281.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2677 avg=0.4626 it/s=313.4\n",
      "[e6 b463/2315] loss=0.7682 avg=0.4562 it/s=324.7\n",
      "[e6 b694/2315] loss=0.3227 avg=0.4628 it/s=289.9\n",
      "[e6 b925/2315] loss=0.6322 avg=0.4657 it/s=304.0\n",
      "[e6 b1156/2315] loss=0.2930 avg=0.4632 it/s=313.7\n",
      "[e6 b1387/2315] loss=0.2503 avg=0.4632 it/s=316.3\n",
      "[e6 b1618/2315] loss=0.5553 avg=0.4647 it/s=319.2\n",
      "[e6 b1849/2315] loss=0.3833 avg=0.4656 it/s=322.6\n",
      "[e6 b2080/2315] loss=0.4047 avg=0.4644 it/s=326.6\n",
      "[e6 b2311/2315] loss=0.6370 avg=0.4648 it/s=330.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.4647 | val_acc=0.8557 | val_f1=0.8595 | time=116.6s\n",
      "[e7 b1/2315] loss=0.2544 avg=0.2544 it/s=409.5\n",
      "[e7 b2/2315] loss=0.3090 avg=0.2817 it/s=361.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.4812 avg=0.4353 it/s=349.2\n",
      "[e7 b463/2315] loss=0.3045 avg=0.4461 it/s=347.7\n",
      "[e7 b694/2315] loss=0.5692 avg=0.4438 it/s=345.9\n",
      "[e7 b925/2315] loss=0.2287 avg=0.4469 it/s=345.3\n",
      "[e7 b1156/2315] loss=0.2801 avg=0.4441 it/s=346.7\n",
      "[e7 b1387/2315] loss=0.4281 avg=0.4426 it/s=343.3\n",
      "[e7 b1618/2315] loss=0.2483 avg=0.4458 it/s=326.8\n",
      "[e7 b1849/2315] loss=0.2959 avg=0.4472 it/s=330.3\n",
      "[e7 b2080/2315] loss=0.4078 avg=0.4447 it/s=333.9\n",
      "[e7 b2311/2315] loss=0.3339 avg=0.4427 it/s=336.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.4427 | val_acc=0.8516 | val_f1=0.8557 | time=114.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÑ‚ñÅ‚ñá‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÇ‚ñÅ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÑ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñá‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÜ‚ñÅ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.84848</td></tr><tr><td>best_val_mid_f1</td><td>0.84848</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>114.78056</td></tr><tr><td>train/avg_loss_so_far</td><td>0.44272</td></tr><tr><td>train/epoch_loss</td><td>0.44265</td></tr><tr><td>train/items_per_sec</td><td>336.74721</td></tr><tr><td>train/loss</td><td>0.56327</td></tr><tr><td>val/acc</td><td>0.85155</td></tr><tr><td>val/best_epoch</td><td>6</td></tr><tr><td>val/best_f1_so_far</td><td>0.84848</td></tr><tr><td>val/f1</td><td>0.85566</td></tr><tr><td>val/mid_f1</td><td>0.84273</td></tr><tr><td>val/precision</td><td>0.85322</td></tr><tr><td>val/recall</td><td>0.86043</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_2</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gkx9qian' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gkx9qian</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_180240-gkx9qian\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 2. Best value: 0.85947:  15%|‚ñà‚ñå        | 3/20 [28:19<2:51:31, 605.40s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 2] f1=0.8595 | unfreeze_k=12 lr=1.06e-05 wd=1.7e-06 suggested_bs=32\n",
      "[I 2025-08-17 18:16:21,939] Trial 2 finished with value: 0.8594704602547099 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 1.0637481988876116e-05, 'weight_decay': 1.653206260053585e-06, 'batch_size': 32}. Best is trial 2 with value: 0.8594704602547099.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_181623-zl7ojt9f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zl7ojt9f' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_3</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zl7ojt9f' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zl7ojt9f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[e1 b1/2315] loss=1.6446 avg=1.6446 it/s=136.1\n",
      "[e1 b2/2315] loss=1.6214 avg=1.6330 it/s=185.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.3890 avg=1.5095 it/s=342.7\n",
      "[e1 b463/2315] loss=1.1294 avg=1.3942 it/s=339.9\n",
      "[e1 b694/2315] loss=1.1207 avg=1.3047 it/s=341.5\n",
      "[e1 b925/2315] loss=1.0801 avg=1.2418 it/s=345.8\n",
      "[e1 b1156/2315] loss=0.9710 avg=1.2069 it/s=346.9\n",
      "[e1 b1387/2315] loss=1.0736 avg=1.1785 it/s=348.9\n",
      "[e1 b1618/2315] loss=1.1132 avg=1.1593 it/s=351.2\n",
      "[e1 b1849/2315] loss=1.1802 avg=1.1434 it/s=353.0\n",
      "[e1 b2080/2315] loss=1.1731 avg=1.1300 it/s=354.3\n",
      "[e1 b2311/2315] loss=0.9765 avg=1.1218 it/s=355.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.1219 | val_acc=0.6368 | val_f1=0.6380 | time=109.0s\n",
      "[e2 b1/2315] loss=0.8502 avg=0.8502 it/s=286.5\n",
      "[e2 b2/2315] loss=0.9686 avg=0.9094 it/s=315.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.1656 avg=1.0495 it/s=349.0\n",
      "[e2 b463/2315] loss=1.1218 avg=1.0487 it/s=340.1\n",
      "[e2 b694/2315] loss=0.9700 avg=1.0386 it/s=319.8\n",
      "[e2 b925/2315] loss=0.8910 avg=1.0220 it/s=294.8\n",
      "[e2 b1156/2315] loss=1.2190 avg=1.0135 it/s=290.0\n",
      "[e2 b1387/2315] loss=0.5802 avg=0.9955 it/s=298.8\n",
      "[e2 b1618/2315] loss=0.8224 avg=0.9776 it/s=304.1\n",
      "[e2 b1849/2315] loss=1.2742 avg=0.9673 it/s=307.2\n",
      "[e2 b2080/2315] loss=0.9030 avg=0.9574 it/s=312.0\n",
      "[e2 b2311/2315] loss=1.1679 avg=0.9525 it/s=316.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.9523 | val_acc=0.7021 | val_f1=0.7119 | time=121.6s\n",
      "[e3 b1/2315] loss=0.9251 avg=0.9251 it/s=291.6\n",
      "[e3 b2/2315] loss=0.7322 avg=0.8287 it/s=300.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.3976 avg=0.9818 it/s=346.9\n",
      "[e3 b463/2315] loss=0.7354 avg=0.9315 it/s=346.5\n",
      "[e3 b694/2315] loss=1.0098 avg=0.9605 it/s=347.0\n",
      "[e3 b925/2315] loss=0.8022 avg=0.9674 it/s=346.9\n",
      "[e3 b1156/2315] loss=1.0177 avg=0.9771 it/s=347.1\n",
      "[e3 b1387/2315] loss=1.3215 avg=0.9897 it/s=347.4\n",
      "[e3 b1618/2315] loss=1.0416 avg=1.0002 it/s=347.6\n",
      "[e3 b1849/2315] loss=1.4204 avg=1.0080 it/s=349.4\n",
      "[e3 b2080/2315] loss=0.7321 avg=1.0137 it/s=350.7\n",
      "[e3 b2311/2315] loss=1.1182 avg=1.0088 it/s=351.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=1.0091 | val_acc=0.6579 | val_f1=0.6675 | time=109.9s\n",
      "[e4 b1/2315] loss=0.8839 avg=0.8839 it/s=370.7\n",
      "[e4 b2/2315] loss=0.7811 avg=0.8325 it/s=429.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.8668 avg=0.8935 it/s=365.7\n",
      "[e4 b463/2315] loss=0.5863 avg=0.8775 it/s=359.4\n",
      "[e4 b694/2315] loss=0.6483 avg=0.8682 it/s=354.1\n",
      "[e4 b925/2315] loss=0.7832 avg=0.8607 it/s=352.4\n",
      "[e4 b1156/2315] loss=1.4853 avg=0.8520 it/s=352.2\n",
      "[e4 b1387/2315] loss=0.7102 avg=0.8478 it/s=354.7\n",
      "[e4 b1618/2315] loss=0.9793 avg=0.8475 it/s=355.2\n",
      "[e4 b1849/2315] loss=0.9759 avg=0.8454 it/s=354.7\n",
      "[e4 b2080/2315] loss=0.6764 avg=0.8398 it/s=355.3\n",
      "[e4 b2311/2315] loss=1.0068 avg=0.8352 it/s=355.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.8350 | val_acc=0.7379 | val_f1=0.7442 | time=108.8s\n",
      "[e5 b1/2315] loss=0.4744 avg=0.4744 it/s=293.3\n",
      "[e5 b2/2315] loss=0.5321 avg=0.5033 it/s=341.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.7824 avg=0.7374 it/s=358.3\n",
      "[e5 b463/2315] loss=0.9482 avg=0.7567 it/s=357.0\n",
      "[e5 b694/2315] loss=0.8095 avg=0.7811 it/s=350.7\n",
      "[e5 b925/2315] loss=0.7145 avg=0.7835 it/s=344.8\n",
      "[e5 b1156/2315] loss=0.6692 avg=0.7846 it/s=342.8\n",
      "[e5 b1387/2315] loss=0.9102 avg=0.7837 it/s=344.6\n",
      "[e5 b1618/2315] loss=0.5888 avg=0.7837 it/s=346.7\n",
      "[e5 b1849/2315] loss=0.6274 avg=0.7807 it/s=346.6\n",
      "[e5 b2080/2315] loss=0.8852 avg=0.7770 it/s=344.6\n",
      "[e5 b2311/2315] loss=0.6755 avg=0.7712 it/s=341.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.7708 | val_acc=0.7592 | val_f1=0.7659 | time=113.2s\n",
      "[e6 b1/2315] loss=0.7883 avg=0.7883 it/s=292.7\n",
      "[e6 b2/2315] loss=0.3054 avg=0.5468 it/s=292.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.8757 avg=0.6823 it/s=342.6\n",
      "[e6 b463/2315] loss=0.6216 avg=0.6851 it/s=353.6\n",
      "[e6 b694/2315] loss=1.2101 avg=0.6821 it/s=353.0\n",
      "[e6 b925/2315] loss=0.4845 avg=0.6761 it/s=351.3\n",
      "[e6 b1156/2315] loss=0.7987 avg=0.6750 it/s=349.4\n",
      "[e6 b1387/2315] loss=0.3886 avg=0.6714 it/s=350.0\n",
      "[e6 b1618/2315] loss=1.0782 avg=0.6723 it/s=347.5\n",
      "[e6 b1849/2315] loss=0.2431 avg=0.6703 it/s=345.8\n",
      "[e6 b2080/2315] loss=0.4843 avg=0.6681 it/s=344.8\n",
      "[e6 b2311/2315] loss=0.6948 avg=0.6634 it/s=344.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.6637 | val_acc=0.8015 | val_f1=0.8077 | time=112.3s\n",
      "[e7 b1/2315] loss=0.6468 avg=0.6468 it/s=318.5\n",
      "[e7 b2/2315] loss=0.4950 avg=0.5709 it/s=325.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.8180 avg=0.6094 it/s=355.0\n",
      "[e7 b463/2315] loss=0.2843 avg=0.6000 it/s=354.5\n",
      "[e7 b694/2315] loss=0.5402 avg=0.6000 it/s=354.8\n",
      "[e7 b925/2315] loss=0.3634 avg=0.5979 it/s=352.0\n",
      "[e7 b1156/2315] loss=0.6099 avg=0.5971 it/s=344.6\n",
      "[e7 b1387/2315] loss=1.0924 avg=0.5962 it/s=340.7\n",
      "[e7 b1618/2315] loss=0.7196 avg=0.5950 it/s=339.2\n",
      "[e7 b1849/2315] loss=0.5517 avg=0.5933 it/s=336.4\n",
      "[e7 b2080/2315] loss=0.4896 avg=0.5906 it/s=338.0\n",
      "[e7 b2311/2315] loss=0.2794 avg=0.5900 it/s=339.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.5898 | val_acc=0.8088 | val_f1=0.8138 | time=114.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñà‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÅ‚ñà‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÑ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñá‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÑ‚ñÇ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÉ‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÉ‚ñÖ‚ñÜ‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.80278</td></tr><tr><td>best_val_mid_f1</td><td>0.80278</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>113.96431</td></tr><tr><td>train/avg_loss_so_far</td><td>0.59001</td></tr><tr><td>train/epoch_loss</td><td>0.58979</td></tr><tr><td>train/items_per_sec</td><td>339.37964</td></tr><tr><td>train/loss</td><td>0.97755</td></tr><tr><td>val/acc</td><td>0.80879</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.80278</td></tr><tr><td>val/f1</td><td>0.81381</td></tr><tr><td>val/mid_f1</td><td>0.80278</td></tr><tr><td>val/precision</td><td>0.81325</td></tr><tr><td>val/recall</td><td>0.81451</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_3</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zl7ojt9f' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zl7ojt9f</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_181623-zl7ojt9f\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 2. Best value: 0.85947:  20%|‚ñà‚ñà        | 4/20 [41:42<3:02:18, 683.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 3] f1=0.8138 | unfreeze_k=12 lr=1.16e-04 wd=1.1e-07 suggested_bs=32\n",
      "[I 2025-08-17 18:29:45,520] Trial 3 finished with value: 0.8138106051208209 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 0.0001163037715315009, 'weight_decay': 1.1385339326222966e-07, 'batch_size': 32}. Best is trial 2 with value: 0.8594704602547099.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_182946-pi3dwsxy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/pi3dwsxy' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_4</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/pi3dwsxy' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/pi3dwsxy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 50,209,541 / 278,813,189 (18.01%) ; unfreeze_last_k=7\n",
      "[e1 b1/2315] loss=1.5843 avg=1.5843 it/s=153.4\n",
      "[e1 b2/2315] loss=1.5670 avg=1.5756 it/s=211.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5370 avg=1.5106 it/s=445.5\n",
      "[e1 b463/2315] loss=1.2510 avg=1.4568 it/s=457.4\n",
      "[e1 b694/2315] loss=1.0037 avg=1.3826 it/s=458.4\n",
      "[e1 b925/2315] loss=1.2288 avg=1.3159 it/s=453.7\n",
      "[e1 b1156/2315] loss=0.9966 avg=1.2571 it/s=451.6\n",
      "[e1 b1387/2315] loss=1.0067 avg=1.2120 it/s=444.3\n",
      "[e1 b1618/2315] loss=0.9697 avg=1.1740 it/s=441.9\n",
      "[e1 b1849/2315] loss=0.9515 avg=1.1437 it/s=441.2\n",
      "[e1 b2080/2315] loss=0.8614 avg=1.1153 it/s=443.4\n",
      "[e1 b2311/2315] loss=1.2049 avg=1.0904 it/s=443.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0903 | val_acc=0.7184 | val_f1=0.7261 | time=88.5s\n",
      "[e2 b1/2315] loss=0.9772 avg=0.9772 it/s=342.1\n",
      "[e2 b2/2315] loss=0.8922 avg=0.9347 it/s=424.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.7104 avg=0.7933 it/s=409.9\n",
      "[e2 b463/2315] loss=1.1932 avg=0.8014 it/s=413.8\n",
      "[e2 b694/2315] loss=0.8278 avg=0.7956 it/s=421.1\n",
      "[e2 b925/2315] loss=0.6018 avg=0.7864 it/s=429.8\n",
      "[e2 b1156/2315] loss=0.9132 avg=0.7812 it/s=437.7\n",
      "[e2 b1387/2315] loss=0.5784 avg=0.7759 it/s=443.9\n",
      "[e2 b1618/2315] loss=1.1907 avg=0.7705 it/s=448.4\n",
      "[e2 b1849/2315] loss=0.9949 avg=0.7661 it/s=446.9\n",
      "[e2 b2080/2315] loss=0.7466 avg=0.7621 it/s=447.5\n",
      "[e2 b2311/2315] loss=0.5054 avg=0.7579 it/s=446.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.7579 | val_acc=0.7838 | val_f1=0.7909 | time=87.9s\n",
      "[e3 b1/2315] loss=0.8056 avg=0.8056 it/s=667.7\n",
      "[e3 b2/2315] loss=0.5521 avg=0.6788 it/s=401.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.5205 avg=0.6551 it/s=432.5\n",
      "[e3 b463/2315] loss=0.4878 avg=0.6470 it/s=433.4\n",
      "[e3 b694/2315] loss=0.8300 avg=0.6589 it/s=431.9\n",
      "[e3 b925/2315] loss=0.7498 avg=0.6594 it/s=432.5\n",
      "[e3 b1156/2315] loss=0.7103 avg=0.6561 it/s=435.8\n",
      "[e3 b1387/2315] loss=0.6494 avg=0.6577 it/s=440.5\n",
      "[e3 b1618/2315] loss=0.3162 avg=0.6509 it/s=439.1\n",
      "[e3 b1849/2315] loss=1.0125 avg=0.6457 it/s=443.2\n",
      "[e3 b2080/2315] loss=0.4619 avg=0.6407 it/s=442.8\n",
      "[e3 b2311/2315] loss=0.4783 avg=0.6378 it/s=443.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.6375 | val_acc=0.7884 | val_f1=0.7938 | time=88.3s\n",
      "[e4 b1/2315] loss=0.6501 avg=0.6501 it/s=416.9\n",
      "[e4 b2/2315] loss=0.2998 avg=0.4750 it/s=453.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.2392 avg=0.5663 it/s=452.9\n",
      "[e4 b463/2315] loss=0.4482 avg=0.5585 it/s=453.7\n",
      "[e4 b694/2315] loss=0.6198 avg=0.5641 it/s=452.3\n",
      "[e4 b925/2315] loss=0.3448 avg=0.5640 it/s=450.4\n",
      "[e4 b1156/2315] loss=0.4991 avg=0.5633 it/s=447.4\n",
      "[e4 b1387/2315] loss=0.5408 avg=0.5599 it/s=450.7\n",
      "[e4 b1618/2315] loss=0.5161 avg=0.5595 it/s=452.5\n",
      "[e4 b1849/2315] loss=0.6372 avg=0.5581 it/s=454.2\n",
      "[e4 b2080/2315] loss=0.4992 avg=0.5575 it/s=454.6\n",
      "[e4 b2311/2315] loss=0.3993 avg=0.5537 it/s=454.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.5536 | val_acc=0.8071 | val_f1=0.8131 | time=86.2s\n",
      "[e5 b1/2315] loss=0.2158 avg=0.2158 it/s=553.4\n",
      "[e5 b2/2315] loss=0.5483 avg=0.3821 it/s=417.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.7368 avg=0.4895 it/s=461.3\n",
      "[e5 b463/2315] loss=0.7909 avg=0.5012 it/s=463.0\n",
      "[e5 b694/2315] loss=0.4637 avg=0.4998 it/s=464.1\n",
      "[e5 b925/2315] loss=0.6852 avg=0.4970 it/s=465.3\n",
      "[e5 b1156/2315] loss=0.6541 avg=0.4991 it/s=465.8\n",
      "[e5 b1387/2315] loss=0.4896 avg=0.4981 it/s=461.2\n",
      "[e5 b1618/2315] loss=1.0467 avg=0.5002 it/s=455.5\n",
      "[e5 b1849/2315] loss=0.5786 avg=0.5014 it/s=448.6\n",
      "[e5 b2080/2315] loss=0.6722 avg=0.5009 it/s=447.3\n",
      "[e5 b2311/2315] loss=0.7440 avg=0.4998 it/s=448.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4995 | val_acc=0.8256 | val_f1=0.8299 | time=87.3s\n",
      "[e6 b1/2315] loss=0.5835 avg=0.5835 it/s=465.3\n",
      "[e6 b2/2315] loss=0.2142 avg=0.3988 it/s=489.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2293 avg=0.4460 it/s=479.8\n",
      "[e6 b463/2315] loss=0.2693 avg=0.4565 it/s=453.9\n",
      "[e6 b694/2315] loss=0.3337 avg=0.4551 it/s=444.6\n",
      "[e6 b925/2315] loss=0.3470 avg=0.4570 it/s=443.6\n",
      "[e6 b1156/2315] loss=0.9098 avg=0.4604 it/s=445.6\n",
      "[e6 b1387/2315] loss=0.7737 avg=0.4588 it/s=451.4\n",
      "[e6 b1618/2315] loss=0.2355 avg=0.4577 it/s=455.3\n",
      "[e6 b1849/2315] loss=1.0014 avg=0.4554 it/s=458.5\n",
      "[e6 b2080/2315] loss=0.3455 avg=0.4545 it/s=455.3\n",
      "[e6 b2311/2315] loss=0.3751 avg=0.4539 it/s=453.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.4538 | val_acc=0.8355 | val_f1=0.8390 | time=86.4s\n",
      "[e7 b1/2315] loss=0.6423 avg=0.6423 it/s=394.0\n",
      "[e7 b2/2315] loss=0.4174 avg=0.5299 it/s=431.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.3695 avg=0.4346 it/s=437.2\n",
      "[e7 b463/2315] loss=0.3073 avg=0.4237 it/s=436.7\n",
      "[e7 b694/2315] loss=0.2824 avg=0.4183 it/s=443.0\n",
      "[e7 b925/2315] loss=0.3586 avg=0.4159 it/s=445.6\n",
      "[e7 b1156/2315] loss=0.7816 avg=0.4162 it/s=442.7\n",
      "[e7 b1387/2315] loss=0.3165 avg=0.4155 it/s=438.3\n",
      "[e7 b1618/2315] loss=0.3477 avg=0.4138 it/s=440.1\n",
      "[e7 b1849/2315] loss=0.6615 avg=0.4146 it/s=444.4\n",
      "[e7 b2080/2315] loss=0.2646 avg=0.4131 it/s=447.4\n",
      "[e7 b2311/2315] loss=0.6028 avg=0.4138 it/s=449.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.4139 | val_acc=0.8389 | val_f1=0.8421 | time=88.0s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñá‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñÜ‚ñá‚ñÅ‚ñÑ‚ñÅ‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñá‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.83375</td></tr><tr><td>best_val_mid_f1</td><td>0.83375</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.18008</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>50209541</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>88.01029</td></tr><tr><td>train/avg_loss_so_far</td><td>0.41385</td></tr><tr><td>train/epoch_loss</td><td>0.4139</td></tr><tr><td>train/items_per_sec</td><td>449.13713</td></tr><tr><td>train/loss</td><td>0.34199</td></tr><tr><td>val/acc</td><td>0.83892</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.83375</td></tr><tr><td>val/f1</td><td>0.84208</td></tr><tr><td>val/mid_f1</td><td>0.83375</td></tr><tr><td>val/precision</td><td>0.83872</td></tr><tr><td>val/recall</td><td>0.84705</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_4</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/pi3dwsxy' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/pi3dwsxy</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_182946-pi3dwsxy\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 2. Best value: 0.85947:  25%|‚ñà‚ñà‚ñå       | 5/20 [52:11<2:45:57, 663.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 4] f1=0.8421 | unfreeze_k=7 lr=5.05e-05 wd=4.4e-07 suggested_bs=64\n",
      "[I 2025-08-17 18:40:14,211] Trial 4 finished with value: 0.8420752118483215 and parameters: {'num_unfreeze_last_layers': 7, 'lr': 5.0537788313468655e-05, 'weight_decay': 4.4345882224489024e-07, 'batch_size': 64}. Best is trial 2 with value: 0.8594704602547099.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_184015-s139170r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/s139170r' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_5</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/s139170r' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/s139170r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 43,121,669 / 278,813,189 (15.47%) ; unfreeze_last_k=6\n",
      "[e1 b1/2315] loss=1.6119 avg=1.6119 it/s=142.2\n",
      "[e1 b2/2315] loss=1.5824 avg=1.5971 it/s=199.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.1728 avg=1.4827 it/s=436.9\n",
      "[e1 b463/2315] loss=1.4331 avg=1.4333 it/s=437.5\n",
      "[e1 b694/2315] loss=1.3007 avg=1.4168 it/s=456.9\n",
      "[e1 b925/2315] loss=1.5468 avg=1.4107 it/s=463.5\n",
      "[e1 b1156/2315] loss=1.2210 avg=1.4115 it/s=466.1\n",
      "[e1 b1387/2315] loss=1.3763 avg=1.4245 it/s=464.0\n",
      "[e1 b1618/2315] loss=1.5808 avg=1.4345 it/s=459.7\n",
      "[e1 b1849/2315] loss=1.6804 avg=1.4442 it/s=464.2\n",
      "[e1 b2080/2315] loss=1.4458 avg=1.4509 it/s=467.1\n",
      "[e1 b2311/2315] loss=1.4866 avg=1.4559 it/s=464.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.4558 | val_acc=0.2775 | val_f1=0.0869 | time=84.4s\n",
      "[e2 b1/2315] loss=1.6422 avg=1.6422 it/s=468.1\n",
      "[e2 b2/2315] loss=1.5409 avg=1.5916 it/s=472.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.5395 avg=1.5056 it/s=495.4\n",
      "[e2 b463/2315] loss=1.5010 avg=1.5001 it/s=491.2\n",
      "[e2 b694/2315] loss=1.5701 avg=1.5012 it/s=492.1\n",
      "[e2 b925/2315] loss=1.5392 avg=1.5011 it/s=492.7\n",
      "[e2 b1156/2315] loss=1.6482 avg=1.4999 it/s=492.9\n",
      "[e2 b1387/2315] loss=1.5625 avg=1.4994 it/s=482.8\n",
      "[e2 b1618/2315] loss=1.4250 avg=1.4981 it/s=474.1\n",
      "[e2 b1849/2315] loss=1.4628 avg=1.4976 it/s=466.6\n",
      "[e2 b2080/2315] loss=1.5461 avg=1.4989 it/s=465.8\n",
      "[e2 b2311/2315] loss=1.7147 avg=1.4981 it/s=467.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=1.4980 | val_acc=0.2410 | val_f1=0.0777 | time=84.0s\n",
      "[e3 b1/2315] loss=1.4005 avg=1.4005 it/s=913.8\n",
      "[e3 b2/2315] loss=1.4206 avg=1.4105 it/s=497.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.4458 avg=1.4947 it/s=476.9\n",
      "[e3 b463/2315] loss=1.5363 avg=1.4951 it/s=484.3\n",
      "[e3 b694/2315] loss=1.5966 avg=1.4958 it/s=478.2\n",
      "[e3 b925/2315] loss=1.4265 avg=1.4953 it/s=474.9\n",
      "[e3 b1156/2315] loss=1.5487 avg=1.4955 it/s=472.6\n",
      "[e3 b1387/2315] loss=1.5232 avg=1.4949 it/s=472.0\n",
      "[e3 b1618/2315] loss=1.5481 avg=1.4956 it/s=475.7\n",
      "[e3 b1849/2315] loss=1.4103 avg=1.4943 it/s=479.1\n",
      "[e3 b2080/2315] loss=1.4563 avg=1.4953 it/s=480.9\n",
      "[e3 b2311/2315] loss=1.5767 avg=1.4961 it/s=483.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=1.4961 | val_acc=0.2775 | val_f1=0.0869 | time=81.5s\n",
      "[e4 b1/2315] loss=1.4743 avg=1.4743 it/s=516.4\n",
      "[e4 b2/2315] loss=1.5351 avg=1.5047 it/s=507.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.5218 avg=1.4992 it/s=461.0\n",
      "[e4 b463/2315] loss=1.4928 avg=1.4938 it/s=467.1\n",
      "[e4 b694/2315] loss=1.3570 avg=1.4937 it/s=470.4\n",
      "[e4 b925/2315] loss=1.4071 avg=1.4928 it/s=464.9\n",
      "[e4 b1156/2315] loss=1.4632 avg=1.4940 it/s=461.7\n",
      "[e4 b1387/2315] loss=1.4971 avg=1.4942 it/s=461.4\n",
      "[e4 b1618/2315] loss=1.3537 avg=1.4928 it/s=463.2\n",
      "[e4 b1849/2315] loss=1.5375 avg=1.4933 it/s=465.4\n",
      "[e4 b2080/2315] loss=1.5231 avg=1.4948 it/s=464.1\n",
      "[e4 b2311/2315] loss=1.3656 avg=1.4951 it/s=462.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñá‚ñÜ‚ñÅ‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñá‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñà‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>train/loss</td><td>‚ñÖ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñá‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ</td></tr><tr><td>val/acc</td><td>‚ñà‚ñÅ‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñà‚ñÅ‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñà‚ñÅ‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñà‚ñÅ‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.1448</td></tr><tr><td>best_val_mid_f1</td><td>0.1448</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>lr</td><td>0.0002</td></tr><tr><td>params/ratio</td><td>0.15466</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>43121669</td></tr><tr><td>step</td><td>9256</td></tr><tr><td>time/epoch_sec</td><td>85.032</td></tr><tr><td>train/avg_loss_so_far</td><td>1.4951</td></tr><tr><td>train/epoch_loss</td><td>1.49524</td></tr><tr><td>train/items_per_sec</td><td>462.35354</td></tr><tr><td>train/loss</td><td>1.33008</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.1448</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/mid_f1</td><td>0.1448</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_5</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/s139170r' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/s139170r</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_184015-s139170r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 2. Best value: 0.85947:  30%|‚ñà‚ñà‚ñà       | 6/20 [57:56<2:09:35, 555.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 5] f1=0.0869 | unfreeze_k=6 lr=4.46e-04 wd=1.5e-07 suggested_bs=64\n",
      "[I 2025-08-17 18:45:59,159] Trial 5 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 6, 'lr': 0.000446214650582172, 'weight_decay': 1.4923083393062914e-07, 'batch_size': 64}. Best is trial 2 with value: 0.8594704602547099.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_184600-065zcavo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/065zcavo' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_6</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/065zcavo' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/065zcavo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 28,945,925 / 278,813,189 (10.38%) ; unfreeze_last_k=4\n",
      "[e1 b1/2315] loss=1.6452 avg=1.6452 it/s=152.8\n",
      "[e1 b2/2315] loss=1.6148 avg=1.6300 it/s=231.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4870 avg=1.5740 it/s=508.0\n",
      "[e1 b463/2315] loss=1.5737 avg=1.5264 it/s=521.9\n",
      "[e1 b694/2315] loss=1.4736 avg=1.4940 it/s=533.6\n",
      "[e1 b925/2315] loss=1.2739 avg=1.4526 it/s=531.1\n",
      "[e1 b1156/2315] loss=1.3431 avg=1.4133 it/s=529.1\n",
      "[e1 b1387/2315] loss=1.0684 avg=1.3790 it/s=525.6\n",
      "[e1 b1618/2315] loss=1.2092 avg=1.3480 it/s=521.9\n",
      "[e1 b1849/2315] loss=1.3150 avg=1.3232 it/s=526.6\n",
      "[e1 b2080/2315] loss=1.2876 avg=1.3003 it/s=530.2\n",
      "[e1 b2311/2315] loss=0.9985 avg=1.2816 it/s=532.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.2812 | val_acc=0.5688 | val_f1=0.5838 | time=74.5s\n",
      "[e2 b1/2315] loss=1.1050 avg=1.1050 it/s=439.3\n",
      "[e2 b2/2315] loss=1.1648 avg=1.1349 it/s=499.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.0485 avg=1.0945 it/s=540.2\n",
      "[e2 b463/2315] loss=1.1071 avg=1.0819 it/s=552.7\n",
      "[e2 b694/2315] loss=1.1783 avg=1.0662 it/s=556.7\n",
      "[e2 b925/2315] loss=1.1136 avg=1.0603 it/s=559.5\n",
      "[e2 b1156/2315] loss=1.1078 avg=1.0547 it/s=561.3\n",
      "[e2 b1387/2315] loss=1.0662 avg=1.0500 it/s=561.7\n",
      "[e2 b1618/2315] loss=1.4172 avg=1.0454 it/s=562.4\n",
      "[e2 b1849/2315] loss=1.1393 avg=1.0437 it/s=562.7\n",
      "[e2 b2080/2315] loss=1.0408 avg=1.0425 it/s=563.4\n",
      "[e2 b2311/2315] loss=1.3159 avg=1.0391 it/s=564.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=1.0392 | val_acc=0.6147 | val_f1=0.6280 | time=70.4s\n",
      "[e3 b1/2315] loss=1.1742 avg=1.1742 it/s=492.4\n",
      "[e3 b2/2315] loss=1.2648 avg=1.2195 it/s=494.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.9644 avg=0.9748 it/s=497.6\n",
      "[e3 b463/2315] loss=1.3069 avg=0.9716 it/s=514.8\n",
      "[e3 b694/2315] loss=0.8331 avg=0.9754 it/s=516.1\n",
      "[e3 b925/2315] loss=1.1875 avg=0.9809 it/s=522.2\n",
      "[e3 b1156/2315] loss=0.9228 avg=0.9800 it/s=530.5\n",
      "[e3 b1387/2315] loss=1.0092 avg=0.9794 it/s=536.9\n",
      "[e3 b1618/2315] loss=1.0481 avg=0.9778 it/s=542.0\n",
      "[e3 b1849/2315] loss=1.0338 avg=0.9767 it/s=544.9\n",
      "[e3 b2080/2315] loss=0.8454 avg=0.9761 it/s=541.5\n",
      "[e3 b2311/2315] loss=0.9836 avg=0.9742 it/s=537.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.9741 | val_acc=0.6317 | val_f1=0.6430 | time=73.6s\n",
      "[e4 b1/2315] loss=1.3292 avg=1.3292 it/s=448.4\n",
      "[e4 b2/2315] loss=0.8172 avg=1.0732 it/s=479.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.0180 avg=0.9309 it/s=535.5\n",
      "[e4 b463/2315] loss=0.5214 avg=0.9399 it/s=546.2\n",
      "[e4 b694/2315] loss=1.0001 avg=0.9449 it/s=554.0\n",
      "[e4 b925/2315] loss=1.0522 avg=0.9341 it/s=557.8\n",
      "[e4 b1156/2315] loss=1.0924 avg=0.9331 it/s=558.7\n",
      "[e4 b1387/2315] loss=1.0997 avg=0.9363 it/s=558.1\n",
      "[e4 b1618/2315] loss=0.9961 avg=0.9331 it/s=555.5\n",
      "[e4 b1849/2315] loss=1.2708 avg=0.9330 it/s=554.0\n",
      "[e4 b2080/2315] loss=1.0516 avg=0.9317 it/s=553.4\n",
      "[e4 b2311/2315] loss=0.8338 avg=0.9309 it/s=551.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.9309 | val_acc=0.6351 | val_f1=0.6483 | time=71.9s\n",
      "[e5 b1/2315] loss=0.8333 avg=0.8333 it/s=547.9\n",
      "[e5 b2/2315] loss=0.7314 avg=0.7824 it/s=529.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.7912 avg=0.9201 it/s=560.5\n",
      "[e5 b463/2315] loss=1.0100 avg=0.9226 it/s=554.6\n",
      "[e5 b694/2315] loss=0.7032 avg=0.9102 it/s=521.8\n",
      "[e5 b925/2315] loss=0.8102 avg=0.9127 it/s=459.8\n",
      "[e5 b1156/2315] loss=0.7983 avg=0.9114 it/s=445.0\n",
      "[e5 b1387/2315] loss=0.8821 avg=0.9095 it/s=444.1\n",
      "[e5 b1618/2315] loss=0.7161 avg=0.9097 it/s=458.0\n",
      "[e5 b1849/2315] loss=1.1196 avg=0.9090 it/s=468.7\n",
      "[e5 b2080/2315] loss=1.1344 avg=0.9060 it/s=479.2\n",
      "[e5 b2311/2315] loss=0.8747 avg=0.9050 it/s=469.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.9053 | val_acc=0.6535 | val_f1=0.6644 | time=83.8s\n",
      "[e6 b1/2315] loss=1.0236 avg=1.0236 it/s=458.0\n",
      "[e6 b2/2315] loss=0.9302 avg=0.9769 it/s=509.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.7782 avg=0.8968 it/s=397.4\n",
      "[e6 b463/2315] loss=0.9731 avg=0.8796 it/s=457.2\n",
      "[e6 b694/2315] loss=0.9302 avg=0.8885 it/s=435.4\n",
      "[e6 b925/2315] loss=0.7699 avg=0.8892 it/s=455.9\n",
      "[e6 b1156/2315] loss=0.9374 avg=0.8898 it/s=440.8\n",
      "[e6 b1387/2315] loss=1.1619 avg=0.8872 it/s=439.0\n",
      "[e6 b1618/2315] loss=0.7270 avg=0.8867 it/s=441.9\n",
      "[e6 b1849/2315] loss=0.9083 avg=0.8864 it/s=434.6\n",
      "[e6 b2080/2315] loss=0.5783 avg=0.8844 it/s=441.3\n",
      "[e6 b2311/2315] loss=0.9322 avg=0.8845 it/s=438.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.8846 | val_acc=0.6550 | val_f1=0.6659 | time=89.1s\n",
      "[e7 b1/2315] loss=1.1452 avg=1.1452 it/s=524.6\n",
      "[e7 b2/2315] loss=0.7919 avg=0.9686 it/s=537.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.8839 avg=0.8753 it/s=410.3\n",
      "[e7 b463/2315] loss=0.7830 avg=0.8687 it/s=464.0\n",
      "[e7 b694/2315] loss=0.6566 avg=0.8689 it/s=440.9\n",
      "[e7 b925/2315] loss=0.8815 avg=0.8668 it/s=459.3\n",
      "[e7 b1156/2315] loss=0.7697 avg=0.8702 it/s=440.8\n",
      "[e7 b1387/2315] loss=0.7639 avg=0.8717 it/s=434.9\n",
      "[e7 b1618/2315] loss=0.7008 avg=0.8676 it/s=437.1\n",
      "[e7 b1849/2315] loss=1.1464 avg=0.8658 it/s=432.6\n",
      "[e7 b2080/2315] loss=0.7640 avg=0.8663 it/s=443.5\n",
      "[e7 b2311/2315] loss=0.7326 avg=0.8681 it/s=441.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.8680 | val_acc=0.6579 | val_f1=0.6692 | time=88.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñà‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÅ‚ñÑ‚ñÅ‚ñÜ‚ñÜ‚ñÑ‚ñÇ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñá‚ñÜ‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñà‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.65052</td></tr><tr><td>best_val_mid_f1</td><td>0.65052</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.10382</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>28945925</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>88.80332</td></tr><tr><td>train/avg_loss_so_far</td><td>0.86814</td></tr><tr><td>train/epoch_loss</td><td>0.86804</td></tr><tr><td>train/items_per_sec</td><td>441.02464</td></tr><tr><td>train/loss</td><td>0.93035</td></tr><tr><td>val/acc</td><td>0.65792</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.65052</td></tr><tr><td>val/f1</td><td>0.66923</td></tr><tr><td>val/mid_f1</td><td>0.65052</td></tr><tr><td>val/precision</td><td>0.68087</td></tr><tr><td>val/recall</td><td>0.66385</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_6</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/065zcavo' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/065zcavo</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_184600-065zcavo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 2. Best value: 0.85947:  35%|‚ñà‚ñà‚ñà‚ñå      | 7/20 [1:07:25<2:01:19, 559.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 6] f1=0.6692 | unfreeze_k=4 lr=1.53e-05 wd=3.7e-07 suggested_bs=64\n",
      "[I 2025-08-17 18:55:28,587] Trial 6 finished with value: 0.6692302476144392 and parameters: {'num_unfreeze_last_layers': 4, 'lr': 1.5302923725865004e-05, 'weight_decay': 3.682752200470921e-07, 'batch_size': 64}. Best is trial 2 with value: 0.8594704602547099.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_185529-nuscd2qx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/nuscd2qx' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_7</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/nuscd2qx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/nuscd2qx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 28,945,925 / 278,813,189 (10.38%) ; unfreeze_last_k=4\n",
      "[e1 b1/2315] loss=1.6640 avg=1.6640 it/s=129.2\n",
      "[e1 b2/2315] loss=1.6012 avg=1.6326 it/s=189.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.2755 avg=1.4771 it/s=444.7\n",
      "[e1 b463/2315] loss=1.1440 avg=1.3863 it/s=423.8\n",
      "[e1 b694/2315] loss=1.5321 avg=1.3374 it/s=419.8\n",
      "[e1 b925/2315] loss=1.3941 avg=1.3327 it/s=430.9\n",
      "[e1 b1156/2315] loss=1.5648 avg=1.3477 it/s=427.8\n",
      "[e1 b1387/2315] loss=1.3952 avg=1.3633 it/s=444.5\n",
      "[e1 b1618/2315] loss=1.3948 avg=1.3655 it/s=439.0\n",
      "[e1 b1849/2315] loss=1.6615 avg=1.3662 it/s=445.7\n",
      "[e1 b2080/2315] loss=1.2340 avg=1.3688 it/s=435.8\n",
      "[e1 b2311/2315] loss=1.1336 avg=1.3625 it/s=428.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.3622 | val_acc=0.4130 | val_f1=0.2925 | time=93.1s\n",
      "[e2 b1/2315] loss=1.1158 avg=1.1158 it/s=660.7\n",
      "[e2 b2/2315] loss=1.4474 avg=1.2816 it/s=477.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.1505 avg=1.2876 it/s=340.7\n",
      "[e2 b463/2315] loss=1.2546 avg=1.2991 it/s=362.4\n",
      "[e2 b694/2315] loss=1.4114 avg=1.2988 it/s=375.5\n",
      "[e2 b925/2315] loss=1.0107 avg=1.2978 it/s=370.6\n",
      "[e2 b1156/2315] loss=1.1883 avg=1.2984 it/s=363.9\n",
      "[e2 b1387/2315] loss=1.1614 avg=1.2913 it/s=386.3\n",
      "[e2 b1618/2315] loss=1.7033 avg=1.2873 it/s=405.0\n",
      "[e2 b1849/2315] loss=1.2886 avg=1.2856 it/s=420.7\n",
      "[e2 b2080/2315] loss=1.3015 avg=1.2896 it/s=433.5\n",
      "[e2 b2311/2315] loss=1.4152 avg=1.2894 it/s=441.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=1.2893 | val_acc=0.4507 | val_f1=0.3894 | time=88.6s\n",
      "[e3 b1/2315] loss=1.1576 avg=1.1576 it/s=570.7\n",
      "[e3 b2/2315] loss=1.3771 avg=1.2673 it/s=766.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.6178 avg=1.2419 it/s=402.1\n",
      "[e3 b463/2315] loss=1.1250 avg=1.2923 it/s=466.0\n",
      "[e3 b694/2315] loss=1.5177 avg=1.2949 it/s=446.8\n",
      "[e3 b925/2315] loss=1.3460 avg=1.3143 it/s=474.4\n",
      "[e3 b1156/2315] loss=1.3050 avg=1.3120 it/s=455.5\n",
      "[e3 b1387/2315] loss=1.1758 avg=1.3087 it/s=463.4\n",
      "[e3 b1618/2315] loss=1.2069 avg=1.3030 it/s=462.5\n",
      "[e3 b1849/2315] loss=1.3718 avg=1.2982 it/s=466.1\n",
      "[e3 b2080/2315] loss=1.2080 avg=1.2962 it/s=464.3\n",
      "[e3 b2311/2315] loss=1.3950 avg=1.2941 it/s=459.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=1.2940 | val_acc=0.4096 | val_f1=0.2783 | time=86.0s\n",
      "[e4 b1/2315] loss=1.1987 avg=1.1987 it/s=581.6\n",
      "[e4 b2/2315] loss=1.4514 avg=1.3250 it/s=724.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.4176 avg=1.2998 it/s=458.9\n",
      "[e4 b463/2315] loss=1.2092 avg=1.2742 it/s=462.1\n",
      "[e4 b694/2315] loss=1.2533 avg=1.2694 it/s=444.2\n",
      "[e4 b925/2315] loss=1.7008 avg=1.2683 it/s=455.8\n",
      "[e4 b1156/2315] loss=1.1732 avg=1.2619 it/s=444.0\n",
      "[e4 b1387/2315] loss=1.0574 avg=1.2589 it/s=453.7\n",
      "[e4 b1618/2315] loss=1.4307 avg=1.2553 it/s=445.7\n",
      "[e4 b1849/2315] loss=1.6608 avg=1.2528 it/s=434.5\n",
      "[e4 b2080/2315] loss=1.4488 avg=1.2526 it/s=445.9\n",
      "[e4 b2311/2315] loss=1.0142 avg=1.2531 it/s=455.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=1.2532 | val_acc=0.4402 | val_f1=0.3610 | time=86.1s\n",
      "[e5 b1/2315] loss=1.2360 avg=1.2360 it/s=544.3\n",
      "[e5 b2/2315] loss=1.3085 avg=1.2722 it/s=445.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=1.1573 avg=1.2935 it/s=522.1\n",
      "[e5 b463/2315] loss=1.3296 avg=1.2790 it/s=533.2\n",
      "[e5 b694/2315] loss=1.2959 avg=1.2734 it/s=534.7\n",
      "[e5 b925/2315] loss=1.1096 avg=1.2580 it/s=540.1\n",
      "[e5 b1156/2315] loss=1.1865 avg=1.2471 it/s=547.1\n",
      "[e5 b1387/2315] loss=0.8888 avg=1.2419 it/s=551.9\n",
      "[e5 b1618/2315] loss=1.2276 avg=1.2367 it/s=554.3\n",
      "[e5 b1849/2315] loss=1.2433 avg=1.2321 it/s=556.5\n",
      "[e5 b2080/2315] loss=1.2693 avg=1.2309 it/s=555.7\n",
      "[e5 b2311/2315] loss=1.0523 avg=1.2254 it/s=552.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=1.2256 | val_acc=0.4866 | val_f1=0.4621 | time=71.7s\n",
      "[e6 b1/2315] loss=1.0373 avg=1.0373 it/s=514.5\n",
      "[e6 b2/2315] loss=1.0960 avg=1.0667 it/s=538.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=1.1185 avg=1.1611 it/s=546.4\n",
      "[e6 b463/2315] loss=1.0538 avg=1.1635 it/s=550.7\n",
      "[e6 b694/2315] loss=1.1240 avg=1.1669 it/s=545.3\n",
      "[e6 b925/2315] loss=1.1766 avg=1.1670 it/s=546.1\n",
      "[e6 b1156/2315] loss=1.3895 avg=1.1702 it/s=544.5\n",
      "[e6 b1387/2315] loss=1.1469 avg=1.1706 it/s=528.3\n",
      "[e6 b1618/2315] loss=1.1224 avg=1.1692 it/s=499.1\n",
      "[e6 b1849/2315] loss=0.9653 avg=1.1674 it/s=506.5\n",
      "[e6 b2080/2315] loss=1.3382 avg=1.1649 it/s=512.4\n",
      "[e6 b2311/2315] loss=1.2507 avg=1.1652 it/s=517.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=1.1651 | val_acc=0.5141 | val_f1=0.4906 | time=76.2s\n",
      "[e7 b1/2315] loss=0.9426 avg=0.9426 it/s=529.8\n",
      "[e7 b2/2315] loss=0.9681 avg=0.9554 it/s=576.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=1.5436 avg=1.1435 it/s=556.5\n",
      "[e7 b463/2315] loss=0.9565 avg=1.1445 it/s=559.2\n",
      "[e7 b694/2315] loss=0.8839 avg=1.1370 it/s=559.5\n",
      "[e7 b925/2315] loss=1.0765 avg=1.1356 it/s=560.0\n",
      "[e7 b1156/2315] loss=1.0799 avg=1.1346 it/s=553.1\n",
      "[e7 b1387/2315] loss=0.8935 avg=1.1352 it/s=549.0\n",
      "[e7 b1618/2315] loss=1.3306 avg=1.1372 it/s=548.3\n",
      "[e7 b1849/2315] loss=1.0287 avg=1.1390 it/s=548.6\n",
      "[e7 b2080/2315] loss=0.8820 avg=1.1370 it/s=549.8\n",
      "[e7 b2311/2315] loss=1.0866 avg=1.1337 it/s=551.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=1.1339 | val_acc=0.5292 | val_f1=0.5295 | time=71.8s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñá‚ñÇ‚ñÉ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÅ‚ñÇ‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÇ‚ñÉ‚ñÉ‚ñá‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñá‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñá‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñà‚ñÑ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÖ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÖ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÉ‚ñÉ‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÑ‚ñÅ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.55514</td></tr><tr><td>best_val_mid_f1</td><td>0.55514</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.10382</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>28945925</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>71.83584</td></tr><tr><td>train/avg_loss_so_far</td><td>1.13374</td></tr><tr><td>train/epoch_loss</td><td>1.13389</td></tr><tr><td>train/items_per_sec</td><td>551.39741</td></tr><tr><td>train/loss</td><td>1.08177</td></tr><tr><td>val/acc</td><td>0.52915</td></tr><tr><td>val/best_epoch</td><td>6</td></tr><tr><td>val/best_f1_so_far</td><td>0.55514</td></tr><tr><td>val/f1</td><td>0.52949</td></tr><tr><td>val/mid_f1</td><td>0.55282</td></tr><tr><td>val/precision</td><td>0.56846</td></tr><tr><td>val/recall</td><td>0.51607</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_7</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/nuscd2qx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/nuscd2qx</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_185529-nuscd2qx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 2. Best value: 0.85947:  40%|‚ñà‚ñà‚ñà‚ñà      | 8/20 [1:17:12<1:53:41, 568.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 7] f1=0.4906 | unfreeze_k=4 lr=2.68e-04 wd=4.9e-06 suggested_bs=4\n",
      "[I 2025-08-17 19:05:15,120] Trial 7 finished with value: 0.49057592373421377 and parameters: {'num_unfreeze_last_layers': 4, 'lr': 0.0002678536574669438, 'weight_decay': 4.92687088956682e-06, 'batch_size': 4}. Best is trial 2 with value: 0.8594704602547099.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_190516-kiary75w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/kiary75w' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_8</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/kiary75w' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/kiary75w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[e1 b1/2315] loss=1.5869 avg=1.5869 it/s=124.2\n",
      "[e1 b2/2315] loss=1.6461 avg=1.6165 it/s=172.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5838 avg=1.5524 it/s=348.0\n",
      "[e1 b463/2315] loss=1.2076 avg=1.4984 it/s=354.5\n",
      "[e1 b694/2315] loss=0.9737 avg=1.4070 it/s=357.2\n",
      "[e1 b925/2315] loss=0.8820 avg=1.3202 it/s=359.0\n",
      "[e1 b1156/2315] loss=0.9148 avg=1.2488 it/s=360.8\n",
      "[e1 b1387/2315] loss=0.9874 avg=1.1901 it/s=360.9\n",
      "[e1 b1618/2315] loss=0.6321 avg=1.1409 it/s=357.6\n",
      "[e1 b1849/2315] loss=1.0019 avg=1.1005 it/s=353.5\n",
      "[e1 b2080/2315] loss=0.5528 avg=1.0660 it/s=348.7\n",
      "[e1 b2311/2315] loss=0.3606 avg=1.0331 it/s=349.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0327 | val_acc=0.7670 | val_f1=0.7736 | time=110.7s\n",
      "[e2 b1/2315] loss=0.6760 avg=0.6760 it/s=319.3\n",
      "[e2 b2/2315] loss=0.6643 avg=0.6701 it/s=329.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.7286 avg=0.6786 it/s=352.1\n",
      "[e2 b463/2315] loss=0.4689 avg=0.6860 it/s=333.5\n",
      "[e2 b694/2315] loss=1.1900 avg=0.6843 it/s=302.1\n",
      "[e2 b925/2315] loss=0.4295 avg=0.6780 it/s=312.6\n",
      "[e2 b1156/2315] loss=0.7583 avg=0.6702 it/s=321.9\n",
      "[e2 b1387/2315] loss=0.4107 avg=0.6648 it/s=328.1\n",
      "[e2 b1618/2315] loss=0.7407 avg=0.6606 it/s=332.0\n",
      "[e2 b1849/2315] loss=0.6449 avg=0.6571 it/s=334.8\n",
      "[e2 b2080/2315] loss=0.9956 avg=0.6515 it/s=335.9\n",
      "[e2 b2311/2315] loss=0.6844 avg=0.6473 it/s=337.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.6480 | val_acc=0.8241 | val_f1=0.8299 | time=114.5s\n",
      "[e3 b1/2315] loss=0.6339 avg=0.6339 it/s=313.0\n",
      "[e3 b2/2315] loss=0.6148 avg=0.6243 it/s=329.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.4370 avg=0.5583 it/s=339.5\n",
      "[e3 b463/2315] loss=0.4626 avg=0.5539 it/s=342.1\n",
      "[e3 b694/2315] loss=0.3570 avg=0.5536 it/s=347.3\n",
      "[e3 b925/2315] loss=1.3451 avg=0.5519 it/s=349.3\n",
      "[e3 b1156/2315] loss=0.4050 avg=0.5446 it/s=351.7\n",
      "[e3 b1387/2315] loss=0.7867 avg=0.5419 it/s=353.2\n",
      "[e3 b1618/2315] loss=0.4217 avg=0.5399 it/s=353.9\n",
      "[e3 b1849/2315] loss=0.5424 avg=0.5401 it/s=352.9\n",
      "[e3 b2080/2315] loss=0.6636 avg=0.5387 it/s=351.9\n",
      "[e3 b2311/2315] loss=0.4487 avg=0.5358 it/s=350.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5358 | val_acc=0.8540 | val_f1=0.8584 | time=110.3s\n",
      "[e4 b1/2315] loss=0.3715 avg=0.3715 it/s=350.3\n",
      "[e4 b2/2315] loss=0.4382 avg=0.4048 it/s=325.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.3335 avg=0.4644 it/s=361.0\n",
      "[e4 b463/2315] loss=0.3084 avg=0.4648 it/s=361.6\n",
      "[e4 b694/2315] loss=0.2484 avg=0.4654 it/s=359.4\n",
      "[e4 b925/2315] loss=0.7294 avg=0.4665 it/s=360.4\n",
      "[e4 b1156/2315] loss=0.3830 avg=0.4651 it/s=360.8\n",
      "[e4 b1387/2315] loss=0.2201 avg=0.4666 it/s=360.5\n",
      "[e4 b1618/2315] loss=0.2773 avg=0.4671 it/s=360.7\n",
      "[e4 b1849/2315] loss=0.4593 avg=0.4651 it/s=360.7\n",
      "[e4 b2080/2315] loss=0.2953 avg=0.4630 it/s=357.1\n",
      "[e4 b2311/2315] loss=0.5888 avg=0.4643 it/s=354.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.4644 | val_acc=0.8533 | val_f1=0.8561 | time=109.4s\n",
      "[e5 b1/2315] loss=0.8881 avg=0.8881 it/s=365.3\n",
      "[e5 b2/2315] loss=0.2639 avg=0.5760 it/s=370.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.4027 avg=0.4297 it/s=338.2\n",
      "[e5 b463/2315] loss=0.6736 avg=0.4242 it/s=336.3\n",
      "[e5 b694/2315] loss=0.2632 avg=0.4204 it/s=287.7\n",
      "[e5 b925/2315] loss=0.5794 avg=0.4171 it/s=274.6\n",
      "[e5 b1156/2315] loss=0.5390 avg=0.4173 it/s=286.1\n",
      "[e5 b1387/2315] loss=0.4480 avg=0.4148 it/s=295.6\n",
      "[e5 b1618/2315] loss=0.2828 avg=0.4155 it/s=303.7\n",
      "[e5 b1849/2315] loss=0.3955 avg=0.4142 it/s=309.9\n",
      "[e5 b2080/2315] loss=0.5345 avg=0.4156 it/s=312.5\n",
      "[e5 b2311/2315] loss=0.5164 avg=0.4155 it/s=315.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4156 | val_acc=0.8516 | val_f1=0.8557 | time=122.2s\n",
      "[e6 b1/2315] loss=0.2282 avg=0.2282 it/s=363.0\n",
      "[e6 b2/2315] loss=0.3870 avg=0.3076 it/s=339.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2177 avg=0.3746 it/s=345.1\n",
      "[e6 b463/2315] loss=0.2085 avg=0.3813 it/s=341.2\n",
      "[e6 b694/2315] loss=0.2628 avg=0.3799 it/s=342.4\n",
      "[e6 b925/2315] loss=0.5898 avg=0.3787 it/s=344.4\n",
      "[e6 b1156/2315] loss=0.2149 avg=0.3813 it/s=344.6\n",
      "[e6 b1387/2315] loss=0.3413 avg=0.3798 it/s=337.4\n",
      "[e6 b1618/2315] loss=0.2083 avg=0.3797 it/s=327.8\n",
      "[e6 b1849/2315] loss=0.2337 avg=0.3795 it/s=325.1\n",
      "[e6 b2080/2315] loss=0.3658 avg=0.3817 it/s=327.7\n",
      "[e6 b2311/2315] loss=0.3595 avg=0.3823 it/s=329.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.3826 | val_acc=0.8664 | val_f1=0.8691 | time=117.3s\n",
      "[e7 b1/2315] loss=0.2195 avg=0.2195 it/s=287.6\n",
      "[e7 b2/2315] loss=0.2153 avg=0.2174 it/s=268.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.5247 avg=0.3689 it/s=349.0\n",
      "[e7 b463/2315] loss=0.2537 avg=0.3598 it/s=353.9\n",
      "[e7 b694/2315] loss=0.5717 avg=0.3620 it/s=354.4\n",
      "[e7 b925/2315] loss=0.4997 avg=0.3599 it/s=324.0\n",
      "[e7 b1156/2315] loss=0.3738 avg=0.3582 it/s=330.2\n",
      "[e7 b1387/2315] loss=0.3398 avg=0.3553 it/s=334.6\n",
      "[e7 b1618/2315] loss=0.2102 avg=0.3527 it/s=338.1\n",
      "[e7 b1849/2315] loss=0.4984 avg=0.3524 it/s=341.5\n",
      "[e7 b2080/2315] loss=0.5323 avg=0.3513 it/s=344.0\n",
      "[e7 b2311/2315] loss=0.2315 avg=0.3503 it/s=341.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3502 | val_acc=0.8686 | val_f1=0.8714 | time=113.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñà‚ñÖ‚ñÉ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÖ‚ñÜ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñà‚ñÉ‚ñÇ‚ñá‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÅ‚ñÇ‚ñá‚ñÅ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñá‚ñà‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.86112</td></tr><tr><td>best_val_mid_f1</td><td>0.86112</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>113.17197</td></tr><tr><td>train/avg_loss_so_far</td><td>0.35034</td></tr><tr><td>train/epoch_loss</td><td>0.35022</td></tr><tr><td>train/items_per_sec</td><td>341.50981</td></tr><tr><td>train/loss</td><td>0.27837</td></tr><tr><td>val/acc</td><td>0.86856</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.86112</td></tr><tr><td>val/f1</td><td>0.87142</td></tr><tr><td>val/mid_f1</td><td>0.86112</td></tr><tr><td>val/precision</td><td>0.86551</td></tr><tr><td>val/recall</td><td>0.8793</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_8</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/kiary75w' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/kiary75w</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_190516-kiary75w\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 8. Best value: 0.871425:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 9/20 [1:30:43<1:58:08, 644.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 8] f1=0.8714 | unfreeze_k=12 lr=2.97e-05 wd=4.3e-06 suggested_bs=8\n",
      "[I 2025-08-17 19:18:46,667] Trial 8 finished with value: 0.8714249088849261 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 2.9687864289331742e-05, 'weight_decay': 4.253134472368673e-06, 'batch_size': 8}. Best is trial 8 with value: 0.8714249088849261.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_191847-l8vr36ra</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/l8vr36ra' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_9</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/l8vr36ra' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/l8vr36ra</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 85,648,901 / 278,813,189 (30.72%) ; unfreeze_last_k=12\n",
      "[e1 b1/2315] loss=1.6363 avg=1.6363 it/s=238.3\n",
      "[e1 b2/2315] loss=1.6445 avg=1.6404 it/s=265.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4707 avg=1.4974 it/s=339.4\n",
      "[e1 b463/2315] loss=1.3856 avg=1.5004 it/s=347.5\n",
      "[e1 b694/2315] loss=1.5249 avg=1.5037 it/s=351.1\n",
      "[e1 b925/2315] loss=1.3192 avg=1.5047 it/s=350.2\n",
      "[e1 b1156/2315] loss=1.5637 avg=1.5048 it/s=348.2\n",
      "[e1 b1387/2315] loss=1.3644 avg=1.5053 it/s=346.0\n",
      "[e1 b1618/2315] loss=1.3868 avg=1.5037 it/s=348.6\n",
      "[e1 b1849/2315] loss=1.5552 avg=1.5032 it/s=350.2\n",
      "[e1 b2080/2315] loss=1.4495 avg=1.5032 it/s=350.7\n",
      "[e1 b2311/2315] loss=1.3749 avg=1.5037 it/s=350.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.5037 | val_acc=0.2775 | val_f1=0.0869 | time=110.4s\n",
      "[e2 b1/2315] loss=1.7497 avg=1.7497 it/s=315.0\n",
      "[e2 b2/2315] loss=1.4981 avg=1.6239 it/s=320.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.3906 avg=1.5004 it/s=341.6\n",
      "[e2 b463/2315] loss=1.5096 avg=1.4963 it/s=343.4\n",
      "[e2 b694/2315] loss=1.3717 avg=1.4950 it/s=330.9\n",
      "[e2 b925/2315] loss=1.4459 avg=1.4961 it/s=304.1\n",
      "[e2 b1156/2315] loss=1.5988 avg=1.4980 it/s=289.1\n",
      "[e2 b1387/2315] loss=1.6096 avg=1.4977 it/s=285.9\n",
      "[e2 b1618/2315] loss=1.4001 avg=1.4979 it/s=294.3\n",
      "[e2 b1849/2315] loss=1.6339 avg=1.4983 it/s=301.2\n",
      "[e2 b2080/2315] loss=1.5350 avg=1.4978 it/s=305.4\n",
      "[e2 b2311/2315] loss=1.4536 avg=1.4983 it/s=308.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=1.4985 | val_acc=0.2775 | val_f1=0.0869 | time=124.9s\n",
      "[e3 b1/2315] loss=1.6302 avg=1.6302 it/s=363.8\n",
      "[e3 b2/2315] loss=1.4320 avg=1.5311 it/s=372.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.4877 avg=1.5083 it/s=345.4\n",
      "[e3 b463/2315] loss=1.5270 avg=1.4991 it/s=354.2\n",
      "[e3 b694/2315] loss=1.6186 avg=1.5019 it/s=357.6\n",
      "[e3 b925/2315] loss=1.3500 avg=1.4992 it/s=357.1\n",
      "[e3 b1156/2315] loss=1.4046 avg=1.4990 it/s=357.2\n",
      "[e3 b1387/2315] loss=1.5631 avg=1.4973 it/s=358.1\n",
      "[e3 b1618/2315] loss=1.6301 avg=1.4987 it/s=359.3\n",
      "[e3 b1849/2315] loss=1.6405 avg=1.4986 it/s=360.2\n",
      "[e3 b2080/2315] loss=1.4235 avg=1.4978 it/s=360.6\n",
      "[e3 b2311/2315] loss=1.5187 avg=1.4977 it/s=359.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=1.4976 | val_acc=0.2775 | val_f1=0.0869 | time=108.0s\n",
      "[e4 b1/2315] loss=1.4245 avg=1.4245 it/s=283.0\n",
      "[e4 b2/2315] loss=1.5576 avg=1.4910 it/s=281.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.6032 avg=1.4980 it/s=325.8\n",
      "[e4 b463/2315] loss=1.4053 avg=1.5003 it/s=331.9\n",
      "[e4 b694/2315] loss=1.7446 avg=1.5007 it/s=340.8\n",
      "[e4 b925/2315] loss=1.4561 avg=1.4981 it/s=344.0\n",
      "[e4 b1156/2315] loss=1.5108 avg=1.4988 it/s=343.7\n",
      "[e4 b1387/2315] loss=1.4383 avg=1.4994 it/s=341.5\n",
      "[e4 b1618/2315] loss=1.3477 avg=1.4992 it/s=337.3\n",
      "[e4 b1849/2315] loss=1.5024 avg=1.4985 it/s=316.3\n",
      "[e4 b2080/2315] loss=1.6564 avg=1.4974 it/s=320.7\n",
      "[e4 b2311/2315] loss=1.5096 avg=1.4962 it/s=323.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÇ‚ñà‚ñÅ‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñá‚ñÖ‚ñÅ‚ñÉ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñà‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÖ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñá‚ñÖ‚ñà</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.1448</td></tr><tr><td>best_val_mid_f1</td><td>0.1448</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>lr</td><td>0.00026</td></tr><tr><td>params/ratio</td><td>0.30719</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>85648901</td></tr><tr><td>step</td><td>9256</td></tr><tr><td>time/epoch_sec</td><td>119.21406</td></tr><tr><td>train/avg_loss_so_far</td><td>1.49617</td></tr><tr><td>train/epoch_loss</td><td>1.4961</td></tr><tr><td>train/items_per_sec</td><td>323.30411</td></tr><tr><td>train/loss</td><td>1.43002</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.1448</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/mid_f1</td><td>0.1448</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_9</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/l8vr36ra' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/l8vr36ra</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_191847-l8vr36ra\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 8. Best value: 0.871425:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10/20 [1:38:35<1:38:32, 591.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 9] f1=0.0869 | unfreeze_k=12 lr=5.68e-04 wd=4.1e-06 suggested_bs=16\n",
      "[I 2025-08-17 19:26:38,695] Trial 9 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 12, 'lr': 0.0005684036256649225, 'weight_decay': 4.108463704855652e-06, 'batch_size': 16}. Best is trial 8 with value: 0.8714249088849261.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_192640-vulg6x00</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/vulg6x00' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_10</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/vulg6x00' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/vulg6x00</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n",
      "[e1 b1/2315] loss=1.6156 avg=1.6156 it/s=124.1\n",
      "[e1 b2/2315] loss=1.5684 avg=1.5920 it/s=169.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6434 avg=1.5322 it/s=360.6\n",
      "[e1 b463/2315] loss=1.3232 avg=1.4731 it/s=373.1\n",
      "[e1 b694/2315] loss=1.2018 avg=1.3889 it/s=366.4\n",
      "[e1 b925/2315] loss=1.3401 avg=1.3098 it/s=331.4\n",
      "[e1 b1156/2315] loss=0.7554 avg=1.2457 it/s=306.5\n",
      "[e1 b1387/2315] loss=0.7943 avg=1.1893 it/s=319.1\n",
      "[e1 b1618/2315] loss=0.6966 avg=1.1444 it/s=328.4\n",
      "[e1 b1849/2315] loss=0.9882 avg=1.1038 it/s=335.8\n",
      "[e1 b2080/2315] loss=1.2701 avg=1.0674 it/s=341.6\n",
      "[e1 b2311/2315] loss=0.6656 avg=1.0369 it/s=345.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0364 | val_acc=0.7748 | val_f1=0.7785 | time=111.7s\n",
      "[e2 b1/2315] loss=0.4861 avg=0.4861 it/s=337.7\n",
      "[e2 b2/2315] loss=0.7544 avg=0.6202 it/s=313.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.7074 avg=0.6971 it/s=378.5\n",
      "[e2 b463/2315] loss=1.1013 avg=0.6856 it/s=330.6\n",
      "[e2 b694/2315] loss=0.9703 avg=0.6869 it/s=302.6\n",
      "[e2 b925/2315] loss=0.3881 avg=0.6806 it/s=306.4\n",
      "[e2 b1156/2315] loss=0.6920 avg=0.6762 it/s=319.3\n",
      "[e2 b1387/2315] loss=0.9065 avg=0.6737 it/s=330.8\n",
      "[e2 b1618/2315] loss=0.5932 avg=0.6703 it/s=339.2\n",
      "[e2 b1849/2315] loss=0.6494 avg=0.6671 it/s=345.5\n",
      "[e2 b2080/2315] loss=0.4909 avg=0.6624 it/s=350.9\n",
      "[e2 b2311/2315] loss=0.9188 avg=0.6587 it/s=355.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.6587 | val_acc=0.8207 | val_f1=0.8252 | time=108.9s\n",
      "[e3 b1/2315] loss=0.7678 avg=0.7678 it/s=389.0\n",
      "[e3 b2/2315] loss=0.4125 avg=0.5901 it/s=389.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.5112 avg=0.5857 it/s=359.4\n",
      "[e3 b463/2315] loss=0.4716 avg=0.5684 it/s=342.0\n",
      "[e3 b694/2315] loss=0.4176 avg=0.5617 it/s=301.8\n",
      "[e3 b925/2315] loss=0.6818 avg=0.5549 it/s=300.2\n",
      "[e3 b1156/2315] loss=0.5411 avg=0.5535 it/s=299.9\n",
      "[e3 b1387/2315] loss=0.5632 avg=0.5531 it/s=309.5\n",
      "[e3 b1618/2315] loss=0.6070 avg=0.5510 it/s=315.8\n",
      "[e3 b1849/2315] loss=0.4538 avg=0.5508 it/s=322.2\n",
      "[e3 b2080/2315] loss=0.8389 avg=0.5502 it/s=329.6\n",
      "[e3 b2311/2315] loss=0.5144 avg=0.5496 it/s=335.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5494 | val_acc=0.8353 | val_f1=0.8404 | time=114.9s\n",
      "[e4 b1/2315] loss=0.4670 avg=0.4670 it/s=353.6\n",
      "[e4 b2/2315] loss=0.2436 avg=0.3553 it/s=339.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.3907 avg=0.4733 it/s=380.8\n",
      "[e4 b463/2315] loss=0.3101 avg=0.4758 it/s=295.8\n",
      "[e4 b694/2315] loss=0.4941 avg=0.4766 it/s=285.0\n",
      "[e4 b925/2315] loss=0.5627 avg=0.4789 it/s=283.1\n",
      "[e4 b1156/2315] loss=0.4965 avg=0.4795 it/s=280.1\n",
      "[e4 b1387/2315] loss=0.3147 avg=0.4756 it/s=294.9\n",
      "[e4 b1618/2315] loss=0.3505 avg=0.4796 it/s=306.1\n",
      "[e4 b1849/2315] loss=0.2782 avg=0.4780 it/s=315.1\n",
      "[e4 b2080/2315] loss=0.3828 avg=0.4794 it/s=322.5\n",
      "[e4 b2311/2315] loss=0.5637 avg=0.4800 it/s=328.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.4801 | val_acc=0.8547 | val_f1=0.8590 | time=117.2s\n",
      "[e5 b1/2315] loss=0.3297 avg=0.3297 it/s=354.4\n",
      "[e5 b2/2315] loss=0.3668 avg=0.3482 it/s=410.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.4464 avg=0.4167 it/s=385.0\n",
      "[e5 b463/2315] loss=0.5801 avg=0.4234 it/s=373.9\n",
      "[e5 b694/2315] loss=0.4655 avg=0.4276 it/s=373.6\n",
      "[e5 b925/2315] loss=0.3504 avg=0.4260 it/s=354.3\n",
      "[e5 b1156/2315] loss=0.4696 avg=0.4254 it/s=344.2\n",
      "[e5 b1387/2315] loss=0.3001 avg=0.4275 it/s=352.3\n",
      "[e5 b1618/2315] loss=0.3307 avg=0.4307 it/s=358.4\n",
      "[e5 b1849/2315] loss=0.5193 avg=0.4299 it/s=362.6\n",
      "[e5 b2080/2315] loss=0.2426 avg=0.4288 it/s=366.9\n",
      "[e5 b2311/2315] loss=0.4049 avg=0.4296 it/s=370.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4296 | val_acc=0.8562 | val_f1=0.8605 | time=104.5s\n",
      "[e6 b1/2315] loss=0.2074 avg=0.2074 it/s=408.2\n",
      "[e6 b2/2315] loss=0.4505 avg=0.3289 it/s=412.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2002 avg=0.4069 it/s=408.0\n",
      "[e6 b463/2315] loss=0.5540 avg=0.3991 it/s=407.3\n",
      "[e6 b694/2315] loss=0.5322 avg=0.4026 it/s=381.4\n",
      "[e6 b925/2315] loss=0.2313 avg=0.3977 it/s=358.5\n",
      "[e6 b1156/2315] loss=0.2900 avg=0.3985 it/s=349.7\n",
      "[e6 b1387/2315] loss=0.6179 avg=0.3975 it/s=340.5\n",
      "[e6 b1618/2315] loss=0.3719 avg=0.3960 it/s=334.5\n",
      "[e6 b1849/2315] loss=0.3158 avg=0.3939 it/s=337.9\n",
      "[e6 b2080/2315] loss=0.6158 avg=0.3936 it/s=341.3\n",
      "[e6 b2311/2315] loss=0.3862 avg=0.3929 it/s=344.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.3932 | val_acc=0.8618 | val_f1=0.8655 | time=112.3s\n",
      "[e7 b1/2315] loss=0.2115 avg=0.2115 it/s=355.2\n",
      "[e7 b2/2315] loss=0.3424 avg=0.2769 it/s=367.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.5132 avg=0.3565 it/s=403.1\n",
      "[e7 b463/2315] loss=0.4525 avg=0.3623 it/s=400.0\n",
      "[e7 b694/2315] loss=0.6282 avg=0.3666 it/s=400.7\n",
      "[e7 b925/2315] loss=0.2071 avg=0.3701 it/s=393.5\n",
      "[e7 b1156/2315] loss=0.4035 avg=0.3654 it/s=390.8\n",
      "[e7 b1387/2315] loss=0.4652 avg=0.3666 it/s=387.7\n",
      "[e7 b1618/2315] loss=0.2111 avg=0.3651 it/s=385.5\n",
      "[e7 b1849/2315] loss=0.3821 avg=0.3662 it/s=384.9\n",
      "[e7 b2080/2315] loss=0.2196 avg=0.3670 it/s=384.9\n",
      "[e7 b2311/2315] loss=0.3492 avg=0.3673 it/s=385.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3671 | val_acc=0.8591 | val_f1=0.8622 | time=100.6s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÜ‚ñÑ‚ñá‚ñà‚ñÉ‚ñÜ‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñá‚ñá‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÉ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñá‚ñà‚ñá</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.8546</td></tr><tr><td>best_val_mid_f1</td><td>0.8546</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.25635</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>71473157</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>100.57654</td></tr><tr><td>train/avg_loss_so_far</td><td>0.36728</td></tr><tr><td>train/epoch_loss</td><td>0.36714</td></tr><tr><td>train/items_per_sec</td><td>385.83989</td></tr><tr><td>train/loss</td><td>0.43996</td></tr><tr><td>val/acc</td><td>0.85909</td></tr><tr><td>val/best_epoch</td><td>6</td></tr><tr><td>val/best_f1_so_far</td><td>0.8546</td></tr><tr><td>val/f1</td><td>0.86224</td></tr><tr><td>val/mid_f1</td><td>0.85263</td></tr><tr><td>val/precision</td><td>0.85642</td></tr><tr><td>val/recall</td><td>0.87046</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_10</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/vulg6x00' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/vulg6x00</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_192640-vulg6x00\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 8. Best value: 0.871425:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 11/20 [1:51:39<1:37:30, 650.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 10] f1=0.8655 | unfreeze_k=10 lr=3.00e-05 wd=9.1e-06 suggested_bs=8\n",
      "[I 2025-08-17 19:39:42,074] Trial 10 finished with value: 0.8654852884932753 and parameters: {'num_unfreeze_last_layers': 10, 'lr': 3.002191102461359e-05, 'weight_decay': 9.093319053371251e-06, 'batch_size': 8}. Best is trial 8 with value: 0.8714249088849261.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_193943-uxk1wgrx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/uxk1wgrx' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_11</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/uxk1wgrx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/uxk1wgrx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n",
      "[e1 b1/2315] loss=1.6056 avg=1.6056 it/s=278.7\n",
      "[e1 b2/2315] loss=1.6399 avg=1.6228 it/s=286.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6154 avg=1.5470 it/s=394.5\n",
      "[e1 b463/2315] loss=1.1292 avg=1.4666 it/s=400.3\n",
      "[e1 b694/2315] loss=1.2161 avg=1.3814 it/s=401.0\n",
      "[e1 b925/2315] loss=0.7353 avg=1.2981 it/s=400.4\n",
      "[e1 b1156/2315] loss=0.7831 avg=1.2329 it/s=395.6\n",
      "[e1 b1387/2315] loss=0.7322 avg=1.1810 it/s=391.6\n",
      "[e1 b1618/2315] loss=0.9182 avg=1.1303 it/s=388.5\n",
      "[e1 b1849/2315] loss=0.6913 avg=1.0909 it/s=389.8\n",
      "[e1 b2080/2315] loss=1.0932 avg=1.0564 it/s=390.9\n",
      "[e1 b2311/2315] loss=0.6780 avg=1.0283 it/s=391.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0282 | val_acc=0.7459 | val_f1=0.7571 | time=99.2s\n",
      "[e2 b1/2315] loss=0.5829 avg=0.5829 it/s=386.4\n",
      "[e2 b2/2315] loss=0.6910 avg=0.6370 it/s=416.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.6215 avg=0.6911 it/s=399.7\n",
      "[e2 b463/2315] loss=0.6489 avg=0.6914 it/s=397.7\n",
      "[e2 b694/2315] loss=0.5971 avg=0.6936 it/s=398.2\n",
      "[e2 b925/2315] loss=1.1517 avg=0.6874 it/s=395.4\n",
      "[e2 b1156/2315] loss=0.4284 avg=0.6815 it/s=395.0\n",
      "[e2 b1387/2315] loss=0.5785 avg=0.6747 it/s=393.0\n",
      "[e2 b1618/2315] loss=0.6605 avg=0.6664 it/s=387.3\n",
      "[e2 b1849/2315] loss=0.7512 avg=0.6617 it/s=383.0\n",
      "[e2 b2080/2315] loss=0.3534 avg=0.6553 it/s=379.0\n",
      "[e2 b2311/2315] loss=0.4304 avg=0.6541 it/s=378.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.6541 | val_acc=0.8277 | val_f1=0.8329 | time=102.8s\n",
      "[e3 b1/2315] loss=0.4406 avg=0.4406 it/s=328.7\n",
      "[e3 b2/2315] loss=0.3705 avg=0.4055 it/s=318.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.4205 avg=0.5639 it/s=395.0\n",
      "[e3 b463/2315] loss=0.4288 avg=0.5625 it/s=395.4\n",
      "[e3 b694/2315] loss=0.5728 avg=0.5613 it/s=377.5\n",
      "[e3 b925/2315] loss=0.6118 avg=0.5572 it/s=369.9\n",
      "[e3 b1156/2315] loss=0.8364 avg=0.5540 it/s=372.1\n",
      "[e3 b1387/2315] loss=0.7825 avg=0.5526 it/s=376.3\n",
      "[e3 b1618/2315] loss=0.2645 avg=0.5487 it/s=378.5\n",
      "[e3 b1849/2315] loss=0.4898 avg=0.5476 it/s=379.8\n",
      "[e3 b2080/2315] loss=0.5825 avg=0.5468 it/s=377.7\n",
      "[e3 b2311/2315] loss=0.5097 avg=0.5467 it/s=377.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5467 | val_acc=0.8479 | val_f1=0.8535 | time=103.0s\n",
      "[e4 b1/2315] loss=0.2435 avg=0.2435 it/s=350.6\n",
      "[e4 b2/2315] loss=0.7098 avg=0.4767 it/s=366.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.2520 avg=0.4891 it/s=393.5\n",
      "[e4 b463/2315] loss=0.7187 avg=0.4853 it/s=390.3\n",
      "[e4 b694/2315] loss=0.5101 avg=0.4759 it/s=383.8\n",
      "[e4 b925/2315] loss=0.4358 avg=0.4741 it/s=385.7\n",
      "[e4 b1156/2315] loss=0.5665 avg=0.4735 it/s=383.4\n",
      "[e4 b1387/2315] loss=0.2442 avg=0.4751 it/s=386.1\n",
      "[e4 b1618/2315] loss=0.4391 avg=0.4751 it/s=388.2\n",
      "[e4 b1849/2315] loss=0.2641 avg=0.4755 it/s=388.4\n",
      "[e4 b2080/2315] loss=0.5964 avg=0.4733 it/s=388.9\n",
      "[e4 b2311/2315] loss=0.5245 avg=0.4728 it/s=387.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.4729 | val_acc=0.8450 | val_f1=0.8500 | time=100.5s\n",
      "[e5 b1/2315] loss=0.3674 avg=0.3674 it/s=262.4\n",
      "[e5 b2/2315] loss=0.2416 avg=0.3045 it/s=352.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.3529 avg=0.4219 it/s=365.3\n",
      "[e5 b463/2315] loss=0.2224 avg=0.4354 it/s=374.7\n",
      "[e5 b694/2315] loss=0.2712 avg=0.4341 it/s=382.3\n",
      "[e5 b925/2315] loss=0.4261 avg=0.4301 it/s=387.1\n",
      "[e5 b1156/2315] loss=0.3929 avg=0.4290 it/s=388.4\n",
      "[e5 b1387/2315] loss=0.4022 avg=0.4281 it/s=390.8\n",
      "[e5 b1618/2315] loss=0.9989 avg=0.4261 it/s=390.9\n",
      "[e5 b1849/2315] loss=1.0038 avg=0.4238 it/s=390.8\n",
      "[e5 b2080/2315] loss=0.2381 avg=0.4239 it/s=391.5\n",
      "[e5 b2311/2315] loss=0.6660 avg=0.4234 it/s=391.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4234 | val_acc=0.8525 | val_f1=0.8564 | time=99.3s\n",
      "[e6 b1/2315] loss=0.3388 avg=0.3388 it/s=381.6\n",
      "[e6 b2/2315] loss=0.4331 avg=0.3859 it/s=294.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.4870 avg=0.3909 it/s=387.8\n",
      "[e6 b463/2315] loss=0.2140 avg=0.3866 it/s=373.9\n",
      "[e6 b694/2315] loss=0.4226 avg=0.3896 it/s=360.6\n",
      "[e6 b925/2315] loss=0.2380 avg=0.3919 it/s=359.7\n",
      "[e6 b1156/2315] loss=0.2009 avg=0.3912 it/s=362.6\n",
      "[e6 b1387/2315] loss=0.2794 avg=0.3901 it/s=365.1\n",
      "[e6 b1618/2315] loss=0.2292 avg=0.3892 it/s=365.1\n",
      "[e6 b1849/2315] loss=0.3050 avg=0.3872 it/s=364.0\n",
      "[e6 b2080/2315] loss=0.4279 avg=0.3872 it/s=364.4\n",
      "[e6 b2311/2315] loss=0.2362 avg=0.3870 it/s=365.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.3868 | val_acc=0.8610 | val_f1=0.8644 | time=105.7s\n",
      "[e7 b1/2315] loss=0.5216 avg=0.5216 it/s=333.7\n",
      "[e7 b2/2315] loss=0.2426 avg=0.3821 it/s=360.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.1916 avg=0.3635 it/s=398.3\n",
      "[e7 b463/2315] loss=0.7737 avg=0.3572 it/s=396.2\n",
      "[e7 b694/2315] loss=0.2829 avg=0.3478 it/s=393.8\n",
      "[e7 b925/2315] loss=0.5042 avg=0.3507 it/s=385.7\n",
      "[e7 b1156/2315] loss=0.6038 avg=0.3527 it/s=385.8\n",
      "[e7 b1387/2315] loss=0.3061 avg=0.3501 it/s=384.8\n",
      "[e7 b1618/2315] loss=0.3153 avg=0.3510 it/s=381.1\n",
      "[e7 b1849/2315] loss=0.3202 avg=0.3495 it/s=379.8\n",
      "[e7 b2080/2315] loss=0.2458 avg=0.3477 it/s=378.2\n",
      "[e7 b2311/2315] loss=0.4781 avg=0.3480 it/s=379.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3480 | val_acc=0.8698 | val_f1=0.8728 | time=102.6s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÇ‚ñÅ‚ñà‚ñÖ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÉ‚ñà‚ñÜ‚ñá‚ñá‚ñÖ‚ñÜ‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñà‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñà‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.86312</td></tr><tr><td>best_val_mid_f1</td><td>0.86312</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.25635</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>71473157</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>102.55042</td></tr><tr><td>train/avg_loss_so_far</td><td>0.34799</td></tr><tr><td>train/epoch_loss</td><td>0.34798</td></tr><tr><td>train/items_per_sec</td><td>378.96916</td></tr><tr><td>train/loss</td><td>0.40934</td></tr><tr><td>val/acc</td><td>0.86978</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.86312</td></tr><tr><td>val/f1</td><td>0.87279</td></tr><tr><td>val/mid_f1</td><td>0.86312</td></tr><tr><td>val/precision</td><td>0.86774</td></tr><tr><td>val/recall</td><td>0.87935</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_11</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/uxk1wgrx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/uxk1wgrx</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_193943-uxk1wgrx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 11. Best value: 0.872794:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 12/20 [2:03:47<1:29:50, 673.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 11] f1=0.8728 | unfreeze_k=10 lr=3.39e-05 wd=9.6e-06 suggested_bs=8\n",
      "[I 2025-08-17 19:51:50,183] Trial 11 finished with value: 0.8727940336019774 and parameters: {'num_unfreeze_last_layers': 10, 'lr': 3.392673551748963e-05, 'weight_decay': 9.567965176138777e-06, 'batch_size': 8}. Best is trial 11 with value: 0.8727940336019774.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_195151-why9nrda</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/why9nrda' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_12</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/why9nrda' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/why9nrda</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n",
      "[e1 b1/2315] loss=1.6529 avg=1.6529 it/s=257.4\n",
      "[e1 b2/2315] loss=1.6528 avg=1.6528 it/s=287.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4592 avg=1.5437 it/s=387.9\n",
      "[e1 b463/2315] loss=1.5142 avg=1.4827 it/s=393.1\n",
      "[e1 b694/2315] loss=1.0575 avg=1.4187 it/s=393.6\n",
      "[e1 b925/2315] loss=1.0251 avg=1.3451 it/s=387.8\n",
      "[e1 b1156/2315] loss=0.9380 avg=1.2777 it/s=383.5\n",
      "[e1 b1387/2315] loss=0.8234 avg=1.2225 it/s=382.2\n",
      "[e1 b1618/2315] loss=0.9640 avg=1.1721 it/s=383.5\n",
      "[e1 b1849/2315] loss=0.7916 avg=1.1324 it/s=385.0\n",
      "[e1 b2080/2315] loss=0.9532 avg=1.0976 it/s=387.1\n",
      "[e1 b2311/2315] loss=0.6214 avg=1.0651 it/s=388.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0645 | val_acc=0.7369 | val_f1=0.7450 | time=100.2s\n",
      "[e2 b1/2315] loss=0.4914 avg=0.4914 it/s=270.9\n",
      "[e2 b2/2315] loss=0.9000 avg=0.6957 it/s=320.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.7574 avg=0.7276 it/s=399.4\n",
      "[e2 b463/2315] loss=0.7938 avg=0.7142 it/s=396.6\n",
      "[e2 b694/2315] loss=0.3721 avg=0.7104 it/s=397.9\n",
      "[e2 b925/2315] loss=0.8552 avg=0.7051 it/s=399.5\n",
      "[e2 b1156/2315] loss=0.9236 avg=0.6980 it/s=399.2\n",
      "[e2 b1387/2315] loss=0.9837 avg=0.6924 it/s=393.7\n",
      "[e2 b1618/2315] loss=0.5770 avg=0.6873 it/s=386.5\n",
      "[e2 b1849/2315] loss=0.7878 avg=0.6798 it/s=382.0\n",
      "[e2 b2080/2315] loss=0.4129 avg=0.6720 it/s=381.7\n",
      "[e2 b2311/2315] loss=0.4433 avg=0.6676 it/s=382.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.6676 | val_acc=0.8195 | val_f1=0.8259 | time=101.4s\n",
      "[e3 b1/2315] loss=0.3504 avg=0.3504 it/s=308.8\n",
      "[e3 b2/2315] loss=0.4712 avg=0.4108 it/s=313.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.2953 avg=0.5444 it/s=384.0\n",
      "[e3 b463/2315] loss=0.2644 avg=0.5609 it/s=374.2\n",
      "[e3 b694/2315] loss=0.5595 avg=0.5595 it/s=373.5\n",
      "[e3 b925/2315] loss=0.4219 avg=0.5602 it/s=374.1\n",
      "[e3 b1156/2315] loss=0.7099 avg=0.5569 it/s=380.6\n",
      "[e3 b1387/2315] loss=0.4370 avg=0.5568 it/s=384.5\n",
      "[e3 b1618/2315] loss=0.5057 avg=0.5580 it/s=386.6\n",
      "[e3 b1849/2315] loss=0.6872 avg=0.5562 it/s=384.0\n",
      "[e3 b2080/2315] loss=0.3111 avg=0.5535 it/s=383.8\n",
      "[e3 b2311/2315] loss=0.5179 avg=0.5530 it/s=382.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5533 | val_acc=0.8404 | val_f1=0.8459 | time=101.5s\n",
      "[e4 b1/2315] loss=0.5237 avg=0.5237 it/s=269.2\n",
      "[e4 b2/2315] loss=0.3083 avg=0.4160 it/s=315.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.4022 avg=0.4929 it/s=374.3\n",
      "[e4 b463/2315] loss=0.9099 avg=0.4856 it/s=380.9\n",
      "[e4 b694/2315] loss=0.5428 avg=0.4829 it/s=376.6\n",
      "[e4 b925/2315] loss=0.3970 avg=0.4842 it/s=381.6\n",
      "[e4 b1156/2315] loss=0.3414 avg=0.4851 it/s=380.1\n",
      "[e4 b1387/2315] loss=0.3661 avg=0.4897 it/s=381.2\n",
      "[e4 b1618/2315] loss=0.6153 avg=0.4899 it/s=381.8\n",
      "[e4 b1849/2315] loss=0.2909 avg=0.4890 it/s=382.9\n",
      "[e4 b2080/2315] loss=0.6957 avg=0.4874 it/s=383.6\n",
      "[e4 b2311/2315] loss=0.5269 avg=0.4850 it/s=381.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.4850 | val_acc=0.8528 | val_f1=0.8562 | time=101.9s\n",
      "[e5 b1/2315] loss=0.2826 avg=0.2826 it/s=281.6\n",
      "[e5 b2/2315] loss=0.4084 avg=0.3455 it/s=345.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.2959 avg=0.4337 it/s=377.1\n",
      "[e5 b463/2315] loss=0.2076 avg=0.4284 it/s=387.4\n",
      "[e5 b694/2315] loss=0.2630 avg=0.4267 it/s=391.6\n",
      "[e5 b925/2315] loss=0.2785 avg=0.4268 it/s=394.8\n",
      "[e5 b1156/2315] loss=0.2251 avg=0.4310 it/s=394.7\n",
      "[e5 b1387/2315] loss=0.4678 avg=0.4326 it/s=394.7\n",
      "[e5 b1618/2315] loss=0.4111 avg=0.4348 it/s=395.0\n",
      "[e5 b1849/2315] loss=0.4143 avg=0.4332 it/s=395.4\n",
      "[e5 b2080/2315] loss=0.6073 avg=0.4332 it/s=396.6\n",
      "[e5 b2311/2315] loss=0.3493 avg=0.4332 it/s=396.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4332 | val_acc=0.8588 | val_f1=0.8622 | time=97.9s\n",
      "[e6 b1/2315] loss=0.2642 avg=0.2642 it/s=348.8\n",
      "[e6 b2/2315] loss=0.2015 avg=0.2329 it/s=344.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2759 avg=0.3931 it/s=377.4\n",
      "[e6 b463/2315] loss=0.3371 avg=0.3963 it/s=376.1\n",
      "[e6 b694/2315] loss=0.3198 avg=0.3997 it/s=370.1\n",
      "[e6 b925/2315] loss=0.2373 avg=0.3990 it/s=373.3\n",
      "[e6 b1156/2315] loss=0.9036 avg=0.3956 it/s=376.7\n",
      "[e6 b1387/2315] loss=0.4619 avg=0.3962 it/s=379.6\n",
      "[e6 b1618/2315] loss=0.4645 avg=0.3974 it/s=374.1\n",
      "[e6 b1849/2315] loss=0.3082 avg=0.3975 it/s=352.9\n",
      "[e6 b2080/2315] loss=0.3108 avg=0.3940 it/s=339.7\n",
      "[e6 b2311/2315] loss=0.6521 avg=0.3930 it/s=332.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.3931 | val_acc=0.8571 | val_f1=0.8613 | time=115.8s\n",
      "[e7 b1/2315] loss=0.3486 avg=0.3486 it/s=395.2\n",
      "[e7 b2/2315] loss=0.2439 avg=0.2963 it/s=360.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.5284 avg=0.3691 it/s=390.8\n",
      "[e7 b463/2315] loss=0.5373 avg=0.3746 it/s=373.5\n",
      "[e7 b694/2315] loss=0.7256 avg=0.3667 it/s=299.8\n",
      "[e7 b925/2315] loss=0.3398 avg=0.3646 it/s=315.3\n",
      "[e7 b1156/2315] loss=0.3876 avg=0.3640 it/s=326.4\n",
      "[e7 b1387/2315] loss=0.3804 avg=0.3645 it/s=334.9\n",
      "[e7 b1618/2315] loss=0.1964 avg=0.3638 it/s=343.2\n",
      "[e7 b1849/2315] loss=0.4921 avg=0.3647 it/s=349.3\n",
      "[e7 b2080/2315] loss=0.2040 avg=0.3657 it/s=354.7\n",
      "[e7 b2311/2315] loss=0.1993 avg=0.3646 it/s=359.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3644 | val_acc=0.8630 | val_f1=0.8662 | time=107.6s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÜ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñà‚ñÖ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÉ‚ñà‚ñà‚ñà‚ñá‚ñà‚ñÑ‚ñà‚ñá‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÖ‚ñÖ‚ñà‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÖ‚ñÇ‚ñÑ‚ñÜ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÇ‚ñÇ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÉ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.8561</td></tr><tr><td>best_val_mid_f1</td><td>0.8561</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.25635</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>71473157</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>107.62054</td></tr><tr><td>train/avg_loss_so_far</td><td>0.36457</td></tr><tr><td>train/epoch_loss</td><td>0.36442</td></tr><tr><td>train/items_per_sec</td><td>359.14606</td></tr><tr><td>train/loss</td><td>0.22149</td></tr><tr><td>val/acc</td><td>0.86297</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.8561</td></tr><tr><td>val/f1</td><td>0.86617</td></tr><tr><td>val/mid_f1</td><td>0.8561</td></tr><tr><td>val/precision</td><td>0.86188</td></tr><tr><td>val/recall</td><td>0.87216</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_12</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/why9nrda' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/why9nrda</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_195151-why9nrda\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 11. Best value: 0.872794:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 13/20 [2:16:07<1:20:57, 693.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 12] f1=0.8662 | unfreeze_k=10 lr=2.57e-05 wd=9.0e-06 suggested_bs=8\n",
      "[I 2025-08-17 20:04:10,668] Trial 12 finished with value: 0.8661696640081556 and parameters: {'num_unfreeze_last_layers': 10, 'lr': 2.572261191520793e-05, 'weight_decay': 9.03168405297932e-06, 'batch_size': 8}. Best is trial 11 with value: 0.8727940336019774.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_200411-zgp8rfc6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zgp8rfc6' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_13</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zgp8rfc6' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zgp8rfc6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 71,473,157 / 278,813,189 (25.63%) ; unfreeze_last_k=10\n",
      "[e1 b1/2315] loss=1.5939 avg=1.5939 it/s=253.6\n",
      "[e1 b2/2315] loss=1.5691 avg=1.5815 it/s=276.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.2325 avg=1.5084 it/s=395.7\n",
      "[e1 b463/2315] loss=1.2026 avg=1.4235 it/s=388.2\n",
      "[e1 b694/2315] loss=1.4262 avg=1.3136 it/s=382.8\n",
      "[e1 b925/2315] loss=0.7808 avg=1.2406 it/s=378.2\n",
      "[e1 b1156/2315] loss=0.8395 avg=1.1894 it/s=382.0\n",
      "[e1 b1387/2315] loss=1.0330 avg=1.1502 it/s=385.1\n",
      "[e1 b1618/2315] loss=0.5178 avg=1.1095 it/s=386.6\n",
      "[e1 b1849/2315] loss=0.8246 avg=1.0753 it/s=388.4\n",
      "[e1 b2080/2315] loss=1.1568 avg=1.0498 it/s=385.8\n",
      "[e1 b2311/2315] loss=0.7095 avg=1.0265 it/s=361.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0259 | val_acc=0.7515 | val_f1=0.7598 | time=106.9s\n",
      "[e2 b1/2315] loss=0.7537 avg=0.7537 it/s=300.6\n",
      "[e2 b2/2315] loss=0.7814 avg=0.7675 it/s=345.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.4803 avg=0.7329 it/s=398.7\n",
      "[e2 b463/2315] loss=0.7441 avg=0.7479 it/s=401.1\n",
      "[e2 b694/2315] loss=0.5703 avg=0.7457 it/s=389.3\n",
      "[e2 b925/2315] loss=0.6256 avg=0.7462 it/s=380.0\n",
      "[e2 b1156/2315] loss=0.6972 avg=0.7390 it/s=375.1\n",
      "[e2 b1387/2315] loss=0.5841 avg=0.7373 it/s=377.1\n",
      "[e2 b1618/2315] loss=0.2756 avg=0.7341 it/s=380.1\n",
      "[e2 b1849/2315] loss=0.9255 avg=0.7291 it/s=382.2\n",
      "[e2 b2080/2315] loss=0.6094 avg=0.7231 it/s=382.2\n",
      "[e2 b2311/2315] loss=0.7284 avg=0.7168 it/s=379.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.7166 | val_acc=0.7925 | val_f1=0.7982 | time=102.3s\n",
      "[e3 b1/2315] loss=0.7268 avg=0.7268 it/s=329.9\n",
      "[e3 b2/2315] loss=0.6951 avg=0.7109 it/s=336.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.7802 avg=0.6253 it/s=365.5\n",
      "[e3 b463/2315] loss=0.4502 avg=0.6118 it/s=384.3\n",
      "[e3 b694/2315] loss=0.6341 avg=0.6078 it/s=389.0\n",
      "[e3 b925/2315] loss=0.5422 avg=0.6079 it/s=391.2\n",
      "[e3 b1156/2315] loss=0.3794 avg=0.6071 it/s=390.2\n",
      "[e3 b1387/2315] loss=0.9885 avg=0.6075 it/s=389.9\n",
      "[e3 b1618/2315] loss=0.5476 avg=0.6045 it/s=388.6\n",
      "[e3 b1849/2315] loss=0.4910 avg=0.6043 it/s=386.5\n",
      "[e3 b2080/2315] loss=0.7537 avg=0.6030 it/s=386.0\n",
      "[e3 b2311/2315] loss=1.3027 avg=0.6017 it/s=383.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.6015 | val_acc=0.8399 | val_f1=0.8448 | time=101.3s\n",
      "[e4 b1/2315] loss=0.3310 avg=0.3310 it/s=385.1\n",
      "[e4 b2/2315] loss=0.7205 avg=0.5257 it/s=310.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.6330 avg=0.5367 it/s=393.3\n",
      "[e4 b463/2315] loss=0.3032 avg=0.5303 it/s=397.9\n",
      "[e4 b694/2315] loss=0.5564 avg=0.5244 it/s=395.7\n",
      "[e4 b925/2315] loss=0.9037 avg=0.5284 it/s=392.3\n",
      "[e4 b1156/2315] loss=0.5718 avg=0.5287 it/s=390.5\n",
      "[e4 b1387/2315] loss=0.5726 avg=0.5337 it/s=391.6\n",
      "[e4 b1618/2315] loss=0.4789 avg=0.5287 it/s=387.5\n",
      "[e4 b1849/2315] loss=0.3720 avg=0.5254 it/s=385.4\n",
      "[e4 b2080/2315] loss=0.6222 avg=0.5241 it/s=384.6\n",
      "[e4 b2311/2315] loss=0.3074 avg=0.5235 it/s=384.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.5239 | val_acc=0.8508 | val_f1=0.8553 | time=100.9s\n",
      "[e5 b1/2315] loss=0.6916 avg=0.6916 it/s=316.7\n",
      "[e5 b2/2315] loss=0.4122 avg=0.5519 it/s=339.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.3100 avg=0.4696 it/s=388.6\n",
      "[e5 b463/2315] loss=0.5657 avg=0.4717 it/s=386.1\n",
      "[e5 b694/2315] loss=0.2255 avg=0.4725 it/s=389.4\n",
      "[e5 b925/2315] loss=0.3970 avg=0.4709 it/s=390.4\n",
      "[e5 b1156/2315] loss=0.2670 avg=0.4718 it/s=392.5\n",
      "[e5 b1387/2315] loss=0.6212 avg=0.4703 it/s=394.0\n",
      "[e5 b1618/2315] loss=0.3172 avg=0.4709 it/s=394.2\n",
      "[e5 b1849/2315] loss=0.6562 avg=0.4707 it/s=394.5\n",
      "[e5 b2080/2315] loss=1.1021 avg=0.4687 it/s=390.2\n",
      "[e5 b2311/2315] loss=0.8951 avg=0.4677 it/s=385.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4680 | val_acc=0.8554 | val_f1=0.8593 | time=100.8s\n",
      "[e6 b1/2315] loss=0.5162 avg=0.5162 it/s=309.8\n",
      "[e6 b2/2315] loss=0.2833 avg=0.3998 it/s=320.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.4217 avg=0.4210 it/s=377.0\n",
      "[e6 b463/2315] loss=0.7848 avg=0.4265 it/s=386.2\n",
      "[e6 b694/2315] loss=0.4603 avg=0.4248 it/s=389.7\n",
      "[e6 b925/2315] loss=0.4472 avg=0.4236 it/s=384.5\n",
      "[e6 b1156/2315] loss=0.2200 avg=0.4176 it/s=380.5\n",
      "[e6 b1387/2315] loss=1.1225 avg=0.4170 it/s=378.0\n",
      "[e6 b1618/2315] loss=0.5099 avg=0.4169 it/s=378.8\n",
      "[e6 b1849/2315] loss=0.5078 avg=0.4165 it/s=382.0\n",
      "[e6 b2080/2315] loss=0.2219 avg=0.4143 it/s=383.5\n",
      "[e6 b2311/2315] loss=0.3634 avg=0.4140 it/s=383.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.4139 | val_acc=0.8664 | val_f1=0.8700 | time=101.3s\n",
      "[e7 b1/2315] loss=0.4834 avg=0.4834 it/s=632.4\n",
      "[e7 b2/2315] loss=0.4861 avg=0.4848 it/s=429.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.7490 avg=0.3861 it/s=383.9\n",
      "[e7 b463/2315] loss=0.2349 avg=0.3802 it/s=382.1\n",
      "[e7 b694/2315] loss=0.3068 avg=0.3742 it/s=379.1\n",
      "[e7 b925/2315] loss=0.2317 avg=0.3723 it/s=377.0\n",
      "[e7 b1156/2315] loss=0.4130 avg=0.3675 it/s=376.9\n",
      "[e7 b1387/2315] loss=0.3987 avg=0.3703 it/s=376.6\n",
      "[e7 b1618/2315] loss=0.2143 avg=0.3664 it/s=372.0\n",
      "[e7 b1849/2315] loss=0.2549 avg=0.3682 it/s=371.4\n",
      "[e7 b2080/2315] loss=0.2682 avg=0.3702 it/s=373.3\n",
      "[e7 b2311/2315] loss=0.2384 avg=0.3694 it/s=374.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3694 | val_acc=0.8661 | val_f1=0.8695 | time=103.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÇ‚ñá‚ñá‚ñÅ‚ñÇ‚ñÇ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÑ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÖ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÅ‚ñÇ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÅ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÇ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÑ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.85934</td></tr><tr><td>best_val_mid_f1</td><td>0.85934</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.25635</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>71473157</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>103.70838</td></tr><tr><td>train/avg_loss_so_far</td><td>0.36938</td></tr><tr><td>train/epoch_loss</td><td>0.36936</td></tr><tr><td>train/items_per_sec</td><td>374.13744</td></tr><tr><td>train/loss</td><td>0.2871</td></tr><tr><td>val/acc</td><td>0.86613</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.85934</td></tr><tr><td>val/f1</td><td>0.86948</td></tr><tr><td>val/mid_f1</td><td>0.85934</td></tr><tr><td>val/precision</td><td>0.86472</td></tr><tr><td>val/recall</td><td>0.87546</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_13</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zgp8rfc6' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zgp8rfc6</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_200411-zgp8rfc6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 11. Best value: 0.872794:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 14/20 [2:28:20<1:10:34, 705.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 13] f1=0.8695 | unfreeze_k=10 lr=7.53e-05 wd=1.1e-06 suggested_bs=8\n",
      "[I 2025-08-17 20:16:23,695] Trial 13 finished with value: 0.869482841547233 and parameters: {'num_unfreeze_last_layers': 10, 'lr': 7.527944983935378e-05, 'weight_decay': 1.0829233114724898e-06, 'batch_size': 8}. Best is trial 11 with value: 0.8727940336019774.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_201624-am4808rm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/am4808rm' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_14</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/am4808rm' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/am4808rm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.5763 avg=1.5763 it/s=127.6\n",
      "[e1 b2/2315] loss=1.6025 avg=1.5894 it/s=185.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.5313 avg=1.5433 it/s=375.3\n",
      "[e1 b463/2315] loss=1.4876 avg=1.4846 it/s=382.1\n",
      "[e1 b694/2315] loss=1.0125 avg=1.4124 it/s=386.3\n",
      "[e1 b925/2315] loss=1.0557 avg=1.3384 it/s=384.3\n",
      "[e1 b1156/2315] loss=0.8091 avg=1.2780 it/s=384.8\n",
      "[e1 b1387/2315] loss=1.1321 avg=1.2274 it/s=387.4\n",
      "[e1 b1618/2315] loss=1.0060 avg=1.1782 it/s=390.3\n",
      "[e1 b1849/2315] loss=0.7094 avg=1.1378 it/s=392.8\n",
      "[e1 b2080/2315] loss=0.8868 avg=1.1052 it/s=395.3\n",
      "[e1 b2311/2315] loss=0.5668 avg=1.0772 it/s=397.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0765 | val_acc=0.7600 | val_f1=0.7672 | time=97.8s\n",
      "[e2 b1/2315] loss=0.6886 avg=0.6886 it/s=341.5\n",
      "[e2 b2/2315] loss=0.8275 avg=0.7581 it/s=342.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.9294 avg=0.7543 it/s=410.6\n",
      "[e2 b463/2315] loss=0.5514 avg=0.7392 it/s=372.2\n",
      "[e2 b694/2315] loss=0.7800 avg=0.7344 it/s=334.3\n",
      "[e2 b925/2315] loss=0.8013 avg=0.7271 it/s=315.4\n",
      "[e2 b1156/2315] loss=0.8873 avg=0.7188 it/s=301.5\n",
      "[e2 b1387/2315] loss=0.8049 avg=0.7186 it/s=294.1\n",
      "[e2 b1618/2315] loss=0.6188 avg=0.7113 it/s=293.3\n",
      "[e2 b1849/2315] loss=0.4104 avg=0.7087 it/s=290.6\n",
      "[e2 b2080/2315] loss=0.4033 avg=0.7036 it/s=290.5\n",
      "[e2 b2311/2315] loss=0.9614 avg=0.6994 it/s=286.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.6995 | val_acc=0.8032 | val_f1=0.8086 | time=133.9s\n",
      "[e3 b1/2315] loss=0.6615 avg=0.6615 it/s=349.6\n",
      "[e3 b2/2315] loss=0.5340 avg=0.5977 it/s=351.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.5574 avg=0.5566 it/s=405.5\n",
      "[e3 b463/2315] loss=0.4115 avg=0.5833 it/s=381.8\n",
      "[e3 b694/2315] loss=0.8856 avg=0.5905 it/s=345.5\n",
      "[e3 b925/2315] loss=0.5188 avg=0.5935 it/s=328.9\n",
      "[e3 b1156/2315] loss=0.6576 avg=0.5932 it/s=322.2\n",
      "[e3 b1387/2315] loss=0.7486 avg=0.5901 it/s=319.3\n",
      "[e3 b1618/2315] loss=0.5963 avg=0.5891 it/s=310.2\n",
      "[e3 b1849/2315] loss=0.6771 avg=0.5866 it/s=320.3\n",
      "[e3 b2080/2315] loss=0.4683 avg=0.5847 it/s=329.1\n",
      "[e3 b2311/2315] loss=0.3503 avg=0.5834 it/s=336.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5834 | val_acc=0.8158 | val_f1=0.8215 | time=114.6s\n",
      "[e4 b1/2315] loss=0.3492 avg=0.3492 it/s=392.7\n",
      "[e4 b2/2315] loss=0.8180 avg=0.5836 it/s=396.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.9351 avg=0.5265 it/s=408.4\n",
      "[e4 b463/2315] loss=0.6130 avg=0.5173 it/s=400.8\n",
      "[e4 b694/2315] loss=0.4662 avg=0.5128 it/s=399.6\n",
      "[e4 b925/2315] loss=0.2753 avg=0.5110 it/s=400.2\n",
      "[e4 b1156/2315] loss=0.7577 avg=0.5117 it/s=404.1\n",
      "[e4 b1387/2315] loss=0.5856 avg=0.5128 it/s=407.3\n",
      "[e4 b1618/2315] loss=0.4218 avg=0.5141 it/s=409.6\n",
      "[e4 b1849/2315] loss=0.4169 avg=0.5136 it/s=410.4\n",
      "[e4 b2080/2315] loss=0.3733 avg=0.5126 it/s=412.4\n",
      "[e4 b2311/2315] loss=0.4518 avg=0.5133 it/s=413.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.5132 | val_acc=0.8455 | val_f1=0.8500 | time=94.1s\n",
      "[e5 b1/2315] loss=0.3517 avg=0.3517 it/s=380.4\n",
      "[e5 b2/2315] loss=0.2755 avg=0.3136 it/s=390.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.8273 avg=0.4722 it/s=423.8\n",
      "[e5 b463/2315] loss=0.7173 avg=0.4714 it/s=426.0\n",
      "[e5 b694/2315] loss=0.6931 avg=0.4681 it/s=426.8\n",
      "[e5 b925/2315] loss=0.3393 avg=0.4667 it/s=422.0\n",
      "[e5 b1156/2315] loss=0.7813 avg=0.4629 it/s=411.2\n",
      "[e5 b1387/2315] loss=0.7521 avg=0.4639 it/s=402.6\n",
      "[e5 b1618/2315] loss=0.5399 avg=0.4636 it/s=400.6\n",
      "[e5 b1849/2315] loss=0.5447 avg=0.4623 it/s=403.3\n",
      "[e5 b2080/2315] loss=0.6809 avg=0.4615 it/s=404.9\n",
      "[e5 b2311/2315] loss=0.5221 avg=0.4612 it/s=406.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4611 | val_acc=0.8477 | val_f1=0.8531 | time=95.8s\n",
      "[e6 b1/2315] loss=0.8156 avg=0.8156 it/s=396.3\n",
      "[e6 b2/2315] loss=0.2391 avg=0.5273 it/s=370.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2343 avg=0.4205 it/s=392.9\n",
      "[e6 b463/2315] loss=0.6515 avg=0.4256 it/s=378.2\n",
      "[e6 b694/2315] loss=0.2570 avg=0.4239 it/s=381.8\n",
      "[e6 b925/2315] loss=0.5569 avg=0.4238 it/s=390.8\n",
      "[e6 b1156/2315] loss=0.6020 avg=0.4256 it/s=383.9\n",
      "[e6 b1387/2315] loss=0.2543 avg=0.4246 it/s=365.1\n",
      "[e6 b1618/2315] loss=0.2957 avg=0.4246 it/s=341.8\n",
      "[e6 b1849/2315] loss=0.2735 avg=0.4239 it/s=348.3\n",
      "[e6 b2080/2315] loss=0.3734 avg=0.4220 it/s=353.5\n",
      "[e6 b2311/2315] loss=0.2381 avg=0.4230 it/s=357.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.4229 | val_acc=0.8511 | val_f1=0.8560 | time=108.2s\n",
      "[e7 b1/2315] loss=0.4932 avg=0.4932 it/s=566.4\n",
      "[e7 b2/2315] loss=0.5145 avg=0.5039 it/s=406.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.8898 avg=0.3956 it/s=398.3\n",
      "[e7 b463/2315] loss=0.3943 avg=0.3991 it/s=386.4\n",
      "[e7 b694/2315] loss=0.2238 avg=0.3974 it/s=363.5\n",
      "[e7 b925/2315] loss=0.6054 avg=0.3971 it/s=356.1\n",
      "[e7 b1156/2315] loss=0.2925 avg=0.3938 it/s=350.8\n",
      "[e7 b1387/2315] loss=0.4738 avg=0.3920 it/s=346.2\n",
      "[e7 b1618/2315] loss=0.4318 avg=0.3908 it/s=344.6\n",
      "[e7 b1849/2315] loss=0.3924 avg=0.3921 it/s=342.9\n",
      "[e7 b2080/2315] loss=0.3038 avg=0.3941 it/s=341.3\n",
      "[e7 b2311/2315] loss=0.2524 avg=0.3941 it/s=342.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3941 | val_acc=0.8567 | val_f1=0.8608 | time=113.7s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÇ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÇ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÇ‚ñà‚ñÖ‚ñÅ‚ñÅ‚ñÉ‚ñÑ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÜ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÉ‚ñÖ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñà‚ñá‚ñá‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.84748</td></tr><tr><td>best_val_mid_f1</td><td>0.84748</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>113.6974</td></tr><tr><td>train/avg_loss_so_far</td><td>0.39411</td></tr><tr><td>train/epoch_loss</td><td>0.39408</td></tr><tr><td>train/items_per_sec</td><td>342.64536</td></tr><tr><td>train/loss</td><td>0.60464</td></tr><tr><td>val/acc</td><td>0.85666</td></tr><tr><td>val/best_epoch</td><td>7</td></tr><tr><td>val/best_f1_so_far</td><td>0.84748</td></tr><tr><td>val/f1</td><td>0.86081</td></tr><tr><td>val/mid_f1</td><td>0.84748</td></tr><tr><td>val/precision</td><td>0.8557</td></tr><tr><td>val/recall</td><td>0.86861</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_14</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/am4808rm' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/am4808rm</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_201624-am4808rm\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 11. Best value: 0.872794:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 15/20 [2:41:13<1:00:29, 725.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 14] f1=0.8608 | unfreeze_k=9 lr=2.46e-05 wd=4.6e-06 suggested_bs=8\n",
      "[I 2025-08-17 20:29:16,420] Trial 14 finished with value: 0.8608142355635048 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 2.4550953525635824e-05, 'weight_decay': 4.557439822949102e-06, 'batch_size': 8}. Best is trial 11 with value: 0.8727940336019774.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_202917-qygrscew</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/qygrscew' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_15</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/qygrscew' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/qygrscew</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[e1 b1/2315] loss=1.5774 avg=1.5774 it/s=129.0\n",
      "[e1 b2/2315] loss=1.6572 avg=1.6173 it/s=170.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4284 avg=1.5252 it/s=260.4\n",
      "[e1 b463/2315] loss=1.1602 avg=1.4583 it/s=260.7\n",
      "[e1 b694/2315] loss=1.1941 avg=1.3616 it/s=278.2\n",
      "[e1 b925/2315] loss=0.7733 avg=1.2790 it/s=285.8\n",
      "[e1 b1156/2315] loss=0.6321 avg=1.2116 it/s=289.9\n",
      "[e1 b1387/2315] loss=1.0436 avg=1.1597 it/s=293.2\n",
      "[e1 b1618/2315] loss=0.6534 avg=1.1141 it/s=296.6\n",
      "[e1 b1849/2315] loss=1.0319 avg=1.0739 it/s=298.4\n",
      "[e1 b2080/2315] loss=0.7777 avg=1.0446 it/s=306.0\n",
      "[e1 b2311/2315] loss=0.5667 avg=1.0156 it/s=312.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0149 | val_acc=0.7799 | val_f1=0.7841 | time=123.2s\n",
      "[e2 b1/2315] loss=0.9026 avg=0.9026 it/s=320.5\n",
      "[e2 b2/2315] loss=0.6185 avg=0.7605 it/s=328.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.4643 avg=0.7057 it/s=346.7\n",
      "[e2 b463/2315] loss=0.5139 avg=0.6803 it/s=349.0\n",
      "[e2 b694/2315] loss=0.5763 avg=0.6754 it/s=357.2\n",
      "[e2 b925/2315] loss=0.6586 avg=0.6687 it/s=363.9\n",
      "[e2 b1156/2315] loss=0.5156 avg=0.6663 it/s=368.4\n",
      "[e2 b1387/2315] loss=0.4350 avg=0.6588 it/s=371.6\n",
      "[e2 b1618/2315] loss=0.8694 avg=0.6554 it/s=370.2\n",
      "[e2 b1849/2315] loss=0.3272 avg=0.6542 it/s=368.7\n",
      "[e2 b2080/2315] loss=0.7713 avg=0.6509 it/s=368.5\n",
      "[e2 b2311/2315] loss=0.4203 avg=0.6470 it/s=368.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.6469 | val_acc=0.8175 | val_f1=0.8205 | time=105.4s\n",
      "[e3 b1/2315] loss=0.6320 avg=0.6320 it/s=300.0\n",
      "[e3 b2/2315] loss=0.6271 avg=0.6295 it/s=324.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.8805 avg=0.5368 it/s=351.6\n",
      "[e3 b463/2315] loss=0.7453 avg=0.5428 it/s=359.3\n",
      "[e3 b694/2315] loss=0.4865 avg=0.5439 it/s=363.4\n",
      "[e3 b925/2315] loss=0.6788 avg=0.5458 it/s=365.6\n",
      "[e3 b1156/2315] loss=0.3652 avg=0.5462 it/s=368.1\n",
      "[e3 b1387/2315] loss=0.2399 avg=0.5428 it/s=369.7\n",
      "[e3 b1618/2315] loss=0.7000 avg=0.5424 it/s=371.8\n",
      "[e3 b1849/2315] loss=0.4892 avg=0.5409 it/s=371.1\n",
      "[e3 b2080/2315] loss=0.5939 avg=0.5376 it/s=369.5\n",
      "[e3 b2311/2315] loss=0.2455 avg=0.5372 it/s=369.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5368 | val_acc=0.8445 | val_f1=0.8491 | time=105.0s\n",
      "[e4 b1/2315] loss=0.3757 avg=0.3757 it/s=376.0\n",
      "[e4 b2/2315] loss=0.2757 avg=0.3257 it/s=328.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.7580 avg=0.4639 it/s=380.7\n",
      "[e4 b463/2315] loss=0.4461 avg=0.4631 it/s=380.9\n",
      "[e4 b694/2315] loss=0.3735 avg=0.4600 it/s=378.1\n",
      "[e4 b925/2315] loss=0.3211 avg=0.4606 it/s=377.9\n",
      "[e4 b1156/2315] loss=0.3641 avg=0.4605 it/s=376.9\n",
      "[e4 b1387/2315] loss=0.9307 avg=0.4652 it/s=367.1\n",
      "[e4 b1618/2315] loss=0.6017 avg=0.4691 it/s=344.5\n",
      "[e4 b1849/2315] loss=0.4384 avg=0.4690 it/s=349.7\n",
      "[e4 b2080/2315] loss=0.8200 avg=0.4700 it/s=349.9\n",
      "[e4 b2311/2315] loss=0.2765 avg=0.4682 it/s=349.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.4681 | val_acc=0.8584 | val_f1=0.8620 | time=110.8s\n",
      "[e5 b1/2315] loss=0.9398 avg=0.9398 it/s=327.5\n",
      "[e5 b2/2315] loss=0.8474 avg=0.8936 it/s=317.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.2155 avg=0.4226 it/s=353.3\n",
      "[e5 b463/2315] loss=0.5603 avg=0.4205 it/s=366.6\n",
      "[e5 b694/2315] loss=0.3718 avg=0.4222 it/s=347.3\n",
      "[e5 b925/2315] loss=0.2215 avg=0.4229 it/s=320.5\n",
      "[e5 b1156/2315] loss=0.4927 avg=0.4231 it/s=328.0\n",
      "[e5 b1387/2315] loss=0.4619 avg=0.4201 it/s=333.4\n",
      "[e5 b1618/2315] loss=0.4988 avg=0.4198 it/s=340.1\n",
      "[e5 b1849/2315] loss=0.2878 avg=0.4172 it/s=345.4\n",
      "[e5 b2080/2315] loss=0.2410 avg=0.4186 it/s=349.7\n",
      "[e5 b2311/2315] loss=0.6017 avg=0.4181 it/s=350.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4180 | val_acc=0.8598 | val_f1=0.8626 | time=110.2s\n",
      "[e6 b1/2315] loss=0.2644 avg=0.2644 it/s=344.1\n",
      "[e6 b2/2315] loss=0.4250 avg=0.3447 it/s=343.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.4880 avg=0.3569 it/s=362.9\n",
      "[e6 b463/2315] loss=0.4451 avg=0.3655 it/s=364.9\n",
      "[e6 b694/2315] loss=0.6100 avg=0.3642 it/s=361.1\n",
      "[e6 b925/2315] loss=0.2057 avg=0.3683 it/s=326.7\n",
      "[e6 b1156/2315] loss=0.5095 avg=0.3674 it/s=334.4\n",
      "[e6 b1387/2315] loss=0.3204 avg=0.3680 it/s=340.9\n",
      "[e6 b1618/2315] loss=0.7116 avg=0.3695 it/s=346.2\n",
      "[e6 b1849/2315] loss=0.2537 avg=0.3685 it/s=349.7\n",
      "[e6 b2080/2315] loss=0.3742 avg=0.3665 it/s=353.0\n",
      "[e6 b2311/2315] loss=0.4421 avg=0.3680 it/s=355.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.3681 | val_acc=0.8727 | val_f1=0.8763 | time=109.2s\n",
      "[e7 b1/2315] loss=0.2701 avg=0.2701 it/s=391.8\n",
      "[e7 b2/2315] loss=0.3360 avg=0.3030 it/s=365.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.3398 avg=0.3419 it/s=362.6\n",
      "[e7 b463/2315] loss=0.2341 avg=0.3364 it/s=361.9\n",
      "[e7 b694/2315] loss=0.2005 avg=0.3411 it/s=366.4\n",
      "[e7 b925/2315] loss=0.3279 avg=0.3414 it/s=370.1\n",
      "[e7 b1156/2315] loss=0.5047 avg=0.3391 it/s=372.8\n",
      "[e7 b1387/2315] loss=0.2734 avg=0.3411 it/s=373.6\n",
      "[e7 b1618/2315] loss=0.2130 avg=0.3396 it/s=374.9\n",
      "[e7 b1849/2315] loss=0.4805 avg=0.3376 it/s=376.0\n",
      "[e7 b2080/2315] loss=0.2138 avg=0.3372 it/s=377.2\n",
      "[e7 b2311/2315] loss=0.4046 avg=0.3370 it/s=378.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3369 | val_acc=0.8710 | val_f1=0.8744 | time=102.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñà‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÅ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñá‚ñà‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÖ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÖ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñá‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.86604</td></tr><tr><td>best_val_mid_f1</td><td>0.86604</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.28177</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>78561029</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>102.38467</td></tr><tr><td>train/avg_loss_so_far</td><td>0.33702</td></tr><tr><td>train/epoch_loss</td><td>0.3369</td></tr><tr><td>train/items_per_sec</td><td>378.44474</td></tr><tr><td>train/loss</td><td>0.24389</td></tr><tr><td>val/acc</td><td>0.87099</td></tr><tr><td>val/best_epoch</td><td>6</td></tr><tr><td>val/best_f1_so_far</td><td>0.86604</td></tr><tr><td>val/f1</td><td>0.87441</td></tr><tr><td>val/mid_f1</td><td>0.86389</td></tr><tr><td>val/precision</td><td>0.86902</td></tr><tr><td>val/recall</td><td>0.88134</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_15</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/qygrscew' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/qygrscew</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_202917-qygrscew\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 15. Best value: 0.876329:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 16/20 [2:54:14<49:29, 742.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 15] f1=0.8763 | unfreeze_k=11 lr=4.17e-05 wd=9.8e-06 suggested_bs=8\n",
      "[I 2025-08-17 20:42:17,307] Trial 15 finished with value: 0.8763286523329038 and parameters: {'num_unfreeze_last_layers': 11, 'lr': 4.171825599095043e-05, 'weight_decay': 9.830249183132208e-06, 'batch_size': 8}. Best is trial 15 with value: 0.8763286523329038.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_204218-zh8to955</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zh8to955' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_16</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zh8to955' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zh8to955</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.6327 avg=1.6327 it/s=144.6\n",
      "[e1 b2/2315] loss=1.5708 avg=1.6017 it/s=205.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.3444 avg=1.4967 it/s=367.6\n",
      "[e1 b463/2315] loss=1.3013 avg=1.3829 it/s=366.3\n",
      "[e1 b694/2315] loss=0.8946 avg=1.3013 it/s=373.0\n",
      "[e1 b925/2315] loss=1.8271 avg=1.2604 it/s=382.0\n",
      "[e1 b1156/2315] loss=1.6504 avg=1.2445 it/s=383.8\n",
      "[e1 b1387/2315] loss=1.6782 avg=1.2297 it/s=355.0\n",
      "[e1 b1618/2315] loss=0.9148 avg=1.2201 it/s=357.1\n",
      "[e1 b1849/2315] loss=1.4433 avg=1.2275 it/s=359.6\n",
      "[e1 b2080/2315] loss=1.6244 avg=1.2297 it/s=362.0\n",
      "[e1 b2311/2315] loss=1.2484 avg=1.2369 it/s=366.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.2368 | val_acc=0.4373 | val_f1=0.3556 | time=105.5s\n",
      "[e2 b1/2315] loss=1.3269 avg=1.3269 it/s=394.5\n",
      "[e2 b2/2315] loss=1.0305 avg=1.1787 it/s=389.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.1088 avg=1.2297 it/s=419.0\n",
      "[e2 b463/2315] loss=1.3101 avg=1.2198 it/s=419.5\n",
      "[e2 b694/2315] loss=1.4162 avg=1.2393 it/s=411.6\n",
      "[e2 b925/2315] loss=1.3581 avg=1.2787 it/s=411.5\n",
      "[e2 b1156/2315] loss=1.3582 avg=1.2990 it/s=408.2\n",
      "[e2 b1387/2315] loss=1.7065 avg=1.3158 it/s=404.8\n",
      "[e2 b1618/2315] loss=1.5540 avg=1.3176 it/s=405.4\n",
      "[e2 b1849/2315] loss=1.6738 avg=1.3181 it/s=404.3\n",
      "[e2 b2080/2315] loss=1.6574 avg=1.3365 it/s=404.3\n",
      "[e2 b2311/2315] loss=1.4494 avg=1.3525 it/s=405.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=1.3527 | val_acc=0.2775 | val_f1=0.0869 | time=96.0s\n",
      "[e3 b1/2315] loss=1.6925 avg=1.6925 it/s=629.7\n",
      "[e3 b2/2315] loss=1.4202 avg=1.5564 it/s=445.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.4016 avg=1.4989 it/s=420.5\n",
      "[e3 b463/2315] loss=1.6028 avg=1.4968 it/s=419.1\n",
      "[e3 b694/2315] loss=1.4826 avg=1.4954 it/s=420.2\n",
      "[e3 b925/2315] loss=1.4355 avg=1.4966 it/s=418.9\n",
      "[e3 b1156/2315] loss=1.3509 avg=1.4968 it/s=417.7\n",
      "[e3 b1387/2315] loss=1.4008 avg=1.4971 it/s=414.7\n",
      "[e3 b1618/2315] loss=1.5549 avg=1.4958 it/s=412.0\n",
      "[e3 b1849/2315] loss=1.6599 avg=1.4973 it/s=410.8\n",
      "[e3 b2080/2315] loss=1.5463 avg=1.4967 it/s=412.0\n",
      "[e3 b2311/2315] loss=1.5955 avg=1.4970 it/s=413.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=1.4970 | val_acc=0.2775 | val_f1=0.0869 | time=94.3s\n",
      "[e4 b1/2315] loss=1.4970 avg=1.4970 it/s=412.8\n",
      "[e4 b2/2315] loss=1.5150 avg=1.5060 it/s=392.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.4514 avg=1.4964 it/s=413.5\n",
      "[e4 b463/2315] loss=1.6099 avg=1.4953 it/s=414.2\n",
      "[e4 b694/2315] loss=1.6642 avg=1.4944 it/s=414.0\n",
      "[e4 b925/2315] loss=1.3729 avg=1.4942 it/s=406.1\n",
      "[e4 b1156/2315] loss=1.5168 avg=1.4959 it/s=408.4\n",
      "[e4 b1387/2315] loss=1.6167 avg=1.4965 it/s=410.6\n",
      "[e4 b1618/2315] loss=1.4290 avg=1.4952 it/s=411.9\n",
      "[e4 b1849/2315] loss=1.4295 avg=1.4949 it/s=409.1\n",
      "[e4 b2080/2315] loss=1.4462 avg=1.4952 it/s=404.5\n",
      "[e4 b2311/2315] loss=1.4626 avg=1.4960 it/s=401.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÇ‚ñà‚ñÇ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñà‚ñÇ‚ñÅ‚ñÉ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñá‚ñá‚ñÖ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/epoch_loss</td><td>‚ñÅ‚ñÑ‚ñà‚ñà</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñÜ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÖ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ</td></tr><tr><td>val/acc</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/mid_f1</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.42473</td></tr><tr><td>best_val_mid_f1</td><td>0.42473</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>lr</td><td>7e-05</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>9256</td></tr><tr><td>time/epoch_sec</td><td>97.13188</td></tr><tr><td>train/avg_loss_so_far</td><td>1.49605</td></tr><tr><td>train/epoch_loss</td><td>1.49605</td></tr><tr><td>train/items_per_sec</td><td>401.09951</td></tr><tr><td>train/loss</td><td>1.4811</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.42473</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/mid_f1</td><td>0.1448</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_16</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zh8to955' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/zh8to955</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_204218-zh8to955\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 15. Best value: 0.876329:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 17/20 [3:00:56<32:00, 640.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 16] f1=0.3556 | unfreeze_k=9 lr=1.44e-04 wd=8.9e-06 suggested_bs=4\n",
      "[I 2025-08-17 20:48:59,579] Trial 16 finished with value: 0.3555999440373106 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 0.00014358541335158907, 'weight_decay': 8.926002789778153e-06, 'batch_size': 4}. Best is trial 15 with value: 0.8763286523329038.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_204900-as358f3r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/as358f3r' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_17</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/as358f3r' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/as358f3r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[e1 b1/2315] loss=1.5932 avg=1.5932 it/s=246.0\n",
      "[e1 b2/2315] loss=1.5888 avg=1.5910 it/s=287.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6923 avg=1.5050 it/s=374.3\n",
      "[e1 b463/2315] loss=1.4619 avg=1.5132 it/s=375.2\n",
      "[e1 b694/2315] loss=1.6405 avg=1.5128 it/s=367.6\n",
      "[e1 b925/2315] loss=1.5371 avg=1.5135 it/s=364.4\n",
      "[e1 b1156/2315] loss=1.5179 avg=1.5119 it/s=359.6\n",
      "[e1 b1387/2315] loss=1.5355 avg=1.5109 it/s=363.1\n",
      "[e1 b1618/2315] loss=1.5542 avg=1.5099 it/s=366.2\n",
      "[e1 b1849/2315] loss=1.5140 avg=1.5095 it/s=368.3\n",
      "[e1 b2080/2315] loss=1.4892 avg=1.5082 it/s=367.8\n",
      "[e1 b2311/2315] loss=1.5955 avg=1.5080 it/s=367.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.5080 | val_acc=0.2775 | val_f1=0.0869 | time=105.4s\n",
      "[e2 b1/2315] loss=1.6071 avg=1.6071 it/s=301.8\n",
      "[e2 b2/2315] loss=1.5117 avg=1.5594 it/s=364.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=1.4869 avg=1.4944 it/s=357.0\n",
      "[e2 b463/2315] loss=1.6823 avg=1.4965 it/s=362.7\n",
      "[e2 b694/2315] loss=1.5379 avg=1.4987 it/s=361.1\n",
      "[e2 b925/2315] loss=1.5904 avg=1.4991 it/s=364.4\n",
      "[e2 b1156/2315] loss=1.4528 avg=1.5000 it/s=367.9\n",
      "[e2 b1387/2315] loss=1.5286 avg=1.4993 it/s=369.7\n",
      "[e2 b1618/2315] loss=1.4088 avg=1.5009 it/s=371.0\n",
      "[e2 b1849/2315] loss=1.3930 avg=1.5008 it/s=372.9\n",
      "[e2 b2080/2315] loss=1.5183 avg=1.4988 it/s=373.3\n",
      "[e2 b2311/2315] loss=1.5356 avg=1.4985 it/s=372.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=1.4984 | val_acc=0.2775 | val_f1=0.0869 | time=104.2s\n",
      "[e3 b1/2315] loss=1.6221 avg=1.6221 it/s=387.7\n",
      "[e3 b2/2315] loss=1.4445 avg=1.5333 it/s=440.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=1.5155 avg=1.4947 it/s=355.5\n",
      "[e3 b463/2315] loss=1.5087 avg=1.4967 it/s=351.4\n",
      "[e3 b694/2315] loss=1.3977 avg=1.4963 it/s=359.9\n",
      "[e3 b925/2315] loss=1.6559 avg=1.4972 it/s=364.4\n",
      "[e3 b1156/2315] loss=1.7723 avg=1.4964 it/s=365.9\n",
      "[e3 b1387/2315] loss=1.3913 avg=1.4962 it/s=366.2\n",
      "[e3 b1618/2315] loss=1.4368 avg=1.4970 it/s=367.3\n",
      "[e3 b1849/2315] loss=1.5313 avg=1.4964 it/s=367.9\n",
      "[e3 b2080/2315] loss=1.4332 avg=1.4964 it/s=369.1\n",
      "[e3 b2311/2315] loss=1.6576 avg=1.4965 it/s=371.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=1.4964 | val_acc=0.2775 | val_f1=0.0869 | time=104.4s\n",
      "[e4 b1/2315] loss=1.4178 avg=1.4178 it/s=391.4\n",
      "[e4 b2/2315] loss=1.4189 avg=1.4183 it/s=390.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=1.4883 avg=1.4874 it/s=366.6\n",
      "[e4 b463/2315] loss=1.6446 avg=1.5010 it/s=351.9\n",
      "[e4 b694/2315] loss=1.3531 avg=1.4991 it/s=348.7\n",
      "[e4 b925/2315] loss=1.5830 avg=1.4972 it/s=348.8\n",
      "[e4 b1156/2315] loss=1.5329 avg=1.4970 it/s=352.6\n",
      "[e4 b1387/2315] loss=1.6090 avg=1.4954 it/s=356.2\n",
      "[e4 b1618/2315] loss=1.3518 avg=1.4955 it/s=356.6\n",
      "[e4 b1849/2315] loss=1.6348 avg=1.4955 it/s=356.4\n",
      "[e4 b2080/2315] loss=1.4195 avg=1.4964 it/s=357.7\n",
      "[e4 b2311/2315] loss=1.4583 avg=1.4959 it/s=356.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 4\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñÜ‚ñÉ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñá‚ñÉ‚ñÉ‚ñà‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÉ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñá‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÅ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÉ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñà‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÖ‚ñÑ‚ñÉ‚ñÅ‚ñÇ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.1448</td></tr><tr><td>best_val_mid_f1</td><td>0.1448</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>lr</td><td>0.00044</td></tr><tr><td>params/ratio</td><td>0.28177</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>78561029</td></tr><tr><td>step</td><td>9256</td></tr><tr><td>time/epoch_sec</td><td>108.59882</td></tr><tr><td>train/avg_loss_so_far</td><td>1.49588</td></tr><tr><td>train/epoch_loss</td><td>1.4958</td></tr><tr><td>train/items_per_sec</td><td>356.05202</td></tr><tr><td>train/loss</td><td>1.61335</td></tr><tr><td>val/acc</td><td>0.27745</td></tr><tr><td>val/best_epoch</td><td>1</td></tr><tr><td>val/best_f1_so_far</td><td>0.1448</td></tr><tr><td>val/f1</td><td>0.08688</td></tr><tr><td>val/mid_f1</td><td>0.1448</td></tr><tr><td>val/precision</td><td>0.05549</td></tr><tr><td>val/recall</td><td>0.2</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_17</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/as358f3r' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/as358f3r</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_204900-as358f3r\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 15. Best value: 0.876329:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 18/20 [3:08:08<19:15, 577.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 17] f1=0.0869 | unfreeze_k=11 lr=9.73e-04 wd=2.3e-06 suggested_bs=8\n",
      "[I 2025-08-17 20:56:11,225] Trial 17 finished with value: 0.08687713959680486 and parameters: {'num_unfreeze_last_layers': 11, 'lr': 0.0009725811140454004, 'weight_decay': 2.29271284636435e-06, 'batch_size': 8}. Best is trial 15 with value: 0.8763286523329038.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_205612-z7etoqfe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/z7etoqfe' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_18</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/z7etoqfe' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/z7etoqfe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 64,385,285 / 278,813,189 (23.09%) ; unfreeze_last_k=9\n",
      "[e1 b1/2315] loss=1.6700 avg=1.6700 it/s=262.7\n",
      "[e1 b2/2315] loss=1.5500 avg=1.6100 it/s=300.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4500 avg=1.5251 it/s=415.1\n",
      "[e1 b463/2315] loss=1.0492 avg=1.4244 it/s=407.0\n",
      "[e1 b694/2315] loss=1.0734 avg=1.3174 it/s=409.0\n",
      "[e1 b925/2315] loss=0.9229 avg=1.2491 it/s=404.4\n",
      "[e1 b1156/2315] loss=1.3077 avg=1.1963 it/s=403.0\n",
      "[e1 b1387/2315] loss=0.7822 avg=1.1527 it/s=401.7\n",
      "[e1 b1618/2315] loss=1.2129 avg=1.1125 it/s=402.1\n",
      "[e1 b1849/2315] loss=0.8336 avg=1.0786 it/s=402.4\n",
      "[e1 b2080/2315] loss=0.7952 avg=1.0496 it/s=403.4\n",
      "[e1 b2311/2315] loss=0.6197 avg=1.0242 it/s=404.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.0235 | val_acc=0.7432 | val_f1=0.7494 | time=96.1s\n",
      "[e2 b1/2315] loss=0.4566 avg=0.4566 it/s=361.1\n",
      "[e2 b2/2315] loss=0.3759 avg=0.4162 it/s=340.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.4065 avg=0.7214 it/s=419.9\n",
      "[e2 b463/2315] loss=1.1747 avg=0.7230 it/s=421.8\n",
      "[e2 b694/2315] loss=0.7187 avg=0.7215 it/s=421.9\n",
      "[e2 b925/2315] loss=0.7976 avg=0.7172 it/s=419.3\n",
      "[e2 b1156/2315] loss=0.8276 avg=0.7194 it/s=414.2\n",
      "[e2 b1387/2315] loss=0.8591 avg=0.7138 it/s=410.0\n",
      "[e2 b1618/2315] loss=0.7675 avg=0.7126 it/s=409.1\n",
      "[e2 b1849/2315] loss=1.1426 avg=0.7089 it/s=410.9\n",
      "[e2 b2080/2315] loss=0.8516 avg=0.7048 it/s=411.7\n",
      "[e2 b2311/2315] loss=0.4289 avg=0.7020 it/s=412.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.7022 | val_acc=0.7930 | val_f1=0.7825 | time=94.5s\n",
      "[e3 b1/2315] loss=0.5390 avg=0.5390 it/s=384.3\n",
      "[e3 b2/2315] loss=0.6119 avg=0.5755 it/s=379.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.3831 avg=0.5935 it/s=411.6\n",
      "[e3 b463/2315] loss=0.4575 avg=0.6005 it/s=382.9\n",
      "[e3 b694/2315] loss=0.3675 avg=0.6080 it/s=335.2\n",
      "[e3 b925/2315] loss=0.3493 avg=0.6045 it/s=308.1\n",
      "[e3 b1156/2315] loss=0.4551 avg=0.6043 it/s=325.1\n",
      "[e3 b1387/2315] loss=0.5211 avg=0.6080 it/s=321.8\n",
      "[e3 b1618/2315] loss=0.4486 avg=0.6050 it/s=327.8\n",
      "[e3 b1849/2315] loss=0.5613 avg=0.6008 it/s=333.5\n",
      "[e3 b2080/2315] loss=0.6189 avg=0.5963 it/s=340.5\n",
      "[e3 b2311/2315] loss=0.3625 avg=0.5938 it/s=346.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5937 | val_acc=0.8256 | val_f1=0.8310 | time=111.4s\n",
      "[e4 b1/2315] loss=0.6803 avg=0.6803 it/s=366.8\n",
      "[e4 b2/2315] loss=0.3776 avg=0.5289 it/s=383.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.3853 avg=0.5163 it/s=406.4\n",
      "[e4 b463/2315] loss=0.5433 avg=0.5205 it/s=358.7\n",
      "[e4 b694/2315] loss=0.3997 avg=0.5212 it/s=319.0\n",
      "[e4 b925/2315] loss=0.4810 avg=0.5249 it/s=303.2\n",
      "[e4 b1156/2315] loss=0.4156 avg=0.5269 it/s=295.8\n",
      "[e4 b1387/2315] loss=0.8996 avg=0.5273 it/s=287.1\n",
      "[e4 b1618/2315] loss=0.3624 avg=0.5247 it/s=286.3\n",
      "[e4 b1849/2315] loss=0.2584 avg=0.5233 it/s=283.4\n",
      "[e4 b2080/2315] loss=0.4268 avg=0.5216 it/s=292.5\n",
      "[e4 b2311/2315] loss=0.3350 avg=0.5200 it/s=300.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.5200 | val_acc=0.8484 | val_f1=0.8538 | time=128.0s\n",
      "[e5 b1/2315] loss=0.3926 avg=0.3926 it/s=368.0\n",
      "[e5 b2/2315] loss=0.3205 avg=0.3566 it/s=378.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.5078 avg=0.4684 it/s=407.8\n",
      "[e5 b463/2315] loss=0.6210 avg=0.4703 it/s=383.0\n",
      "[e5 b694/2315] loss=0.2611 avg=0.4708 it/s=331.4\n",
      "[e5 b925/2315] loss=0.4972 avg=0.4662 it/s=318.8\n",
      "[e5 b1156/2315] loss=0.3055 avg=0.4657 it/s=334.5\n",
      "[e5 b1387/2315] loss=0.6059 avg=0.4635 it/s=342.8\n",
      "[e5 b1618/2315] loss=0.4560 avg=0.4599 it/s=348.2\n",
      "[e5 b1849/2315] loss=0.2458 avg=0.4593 it/s=354.3\n",
      "[e5 b2080/2315] loss=0.2819 avg=0.4589 it/s=360.6\n",
      "[e5 b2311/2315] loss=0.5226 avg=0.4583 it/s=366.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4582 | val_acc=0.8673 | val_f1=0.8708 | time=105.7s\n",
      "[e6 b1/2315] loss=0.3400 avg=0.3400 it/s=414.0\n",
      "[e6 b2/2315] loss=0.5729 avg=0.4564 it/s=417.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2943 avg=0.4160 it/s=419.4\n",
      "[e6 b463/2315] loss=0.2911 avg=0.4132 it/s=422.1\n",
      "[e6 b694/2315] loss=0.4893 avg=0.4157 it/s=402.5\n",
      "[e6 b925/2315] loss=0.5473 avg=0.4125 it/s=355.2\n",
      "[e6 b1156/2315] loss=0.3163 avg=0.4119 it/s=366.9\n",
      "[e6 b1387/2315] loss=0.2283 avg=0.4091 it/s=375.8\n",
      "[e6 b1618/2315] loss=0.4045 avg=0.4105 it/s=381.2\n",
      "[e6 b1849/2315] loss=0.6442 avg=0.4075 it/s=380.9\n",
      "[e6 b2080/2315] loss=0.4381 avg=0.4066 it/s=381.0\n",
      "[e6 b2311/2315] loss=0.2352 avg=0.4049 it/s=380.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.4049 | val_acc=0.8608 | val_f1=0.8646 | time=102.2s\n",
      "[e7 b1/2315] loss=0.3567 avg=0.3567 it/s=424.0\n",
      "[e7 b2/2315] loss=0.4434 avg=0.4000 it/s=500.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.1934 avg=0.3647 it/s=414.8\n",
      "[e7 b463/2315] loss=0.7131 avg=0.3663 it/s=376.5\n",
      "[e7 b694/2315] loss=0.4099 avg=0.3653 it/s=353.4\n",
      "[e7 b925/2315] loss=0.3282 avg=0.3637 it/s=352.5\n",
      "[e7 b1156/2315] loss=0.2050 avg=0.3637 it/s=345.2\n",
      "[e7 b1387/2315] loss=0.4780 avg=0.3665 it/s=339.9\n",
      "[e7 b1618/2315] loss=0.2018 avg=0.3658 it/s=346.8\n",
      "[e7 b1849/2315] loss=0.3226 avg=0.3656 it/s=355.1\n",
      "[e7 b2080/2315] loss=0.3388 avg=0.3640 it/s=361.0\n",
      "[e7 b2311/2315] loss=0.1863 avg=0.3639 it/s=363.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.3641 | val_acc=0.8632 | val_f1=0.8666 | time=106.4s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÑ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñà‚ñÉ‚ñÉ‚ñÉ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÅ‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÖ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñÇ‚ñÑ‚ñÜ‚ñÑ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÑ‚ñÖ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÇ‚ñÅ‚ñÉ‚ñÖ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÑ‚ñÇ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÉ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà‚ñá‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÇ‚ñÜ‚ñá‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.86123</td></tr><tr><td>best_val_mid_f1</td><td>0.86123</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.23093</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>64385285</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>106.38663</td></tr><tr><td>train/avg_loss_so_far</td><td>0.36388</td></tr><tr><td>train/epoch_loss</td><td>0.36407</td></tr><tr><td>train/items_per_sec</td><td>363.7357</td></tr><tr><td>train/loss</td><td>0.24791</td></tr><tr><td>val/acc</td><td>0.86322</td></tr><tr><td>val/best_epoch</td><td>5</td></tr><tr><td>val/best_f1_so_far</td><td>0.86123</td></tr><tr><td>val/f1</td><td>0.86658</td></tr><tr><td>val/mid_f1</td><td>0.85664</td></tr><tr><td>val/precision</td><td>0.86108</td></tr><tr><td>val/recall</td><td>0.87391</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_18</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/z7etoqfe' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/z7etoqfe</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_205612-z7etoqfe\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 15. Best value: 0.876329:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 19/20 [3:20:46<10:31, 631.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 18] f1=0.8708 | unfreeze_k=9 lr=6.36e-05 wd=7.9e-07 suggested_bs=8\n",
      "[I 2025-08-17 21:08:49,200] Trial 18 finished with value: 0.8707660920187703 and parameters: {'num_unfreeze_last_layers': 9, 'lr': 6.361064552508168e-05, 'weight_decay': 7.892964547831696e-07, 'batch_size': 8}. Best is trial 15 with value: 0.8763286523329038.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_210850-e8cqel6t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e8cqel6t' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_19</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e8cqel6t' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e8cqel6t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[e1 b1/2315] loss=1.6019 avg=1.6019 it/s=226.6\n",
      "[e1 b2/2315] loss=1.5452 avg=1.5735 it/s=255.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.6070 avg=1.5548 it/s=357.7\n",
      "[e1 b463/2315] loss=1.3850 avg=1.5079 it/s=358.5\n",
      "[e1 b694/2315] loss=1.3630 avg=1.4823 it/s=358.8\n",
      "[e1 b925/2315] loss=0.9542 avg=1.4355 it/s=362.3\n",
      "[e1 b1156/2315] loss=1.3222 avg=1.3682 it/s=366.8\n",
      "[e1 b1387/2315] loss=0.8654 avg=1.3106 it/s=368.9\n",
      "[e1 b1618/2315] loss=1.0405 avg=1.2579 it/s=371.2\n",
      "[e1 b1849/2315] loss=0.7652 avg=1.2099 it/s=371.8\n",
      "[e1 b2080/2315] loss=1.1210 avg=1.1717 it/s=373.1\n",
      "[e1 b2311/2315] loss=0.7185 avg=1.1354 it/s=372.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7 | loss=1.1347 | val_acc=0.7519 | val_f1=0.7575 | time=104.2s\n",
      "[e2 b1/2315] loss=0.7779 avg=0.7779 it/s=378.6\n",
      "[e2 b2/2315] loss=0.6491 avg=0.7135 it/s=380.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.5072 avg=0.7492 it/s=360.0\n",
      "[e2 b463/2315] loss=0.7730 avg=0.7593 it/s=363.5\n",
      "[e2 b694/2315] loss=0.6168 avg=0.7454 it/s=369.3\n",
      "[e2 b925/2315] loss=0.5957 avg=0.7355 it/s=371.9\n",
      "[e2 b1156/2315] loss=0.4291 avg=0.7249 it/s=373.2\n",
      "[e2 b1387/2315] loss=0.5943 avg=0.7206 it/s=375.2\n",
      "[e2 b1618/2315] loss=0.6351 avg=0.7169 it/s=376.4\n",
      "[e2 b1849/2315] loss=1.0557 avg=0.7107 it/s=377.7\n",
      "[e2 b2080/2315] loss=0.5256 avg=0.7034 it/s=378.9\n",
      "[e2 b2311/2315] loss=0.9365 avg=0.6969 it/s=379.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/7 | loss=0.6968 | val_acc=0.8202 | val_f1=0.8252 | time=102.1s\n",
      "[e3 b1/2315] loss=0.6268 avg=0.6268 it/s=411.6\n",
      "[e3 b2/2315] loss=0.3765 avg=0.5017 it/s=365.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.6070 avg=0.5967 it/s=354.0\n",
      "[e3 b463/2315] loss=0.7024 avg=0.5995 it/s=345.5\n",
      "[e3 b694/2315] loss=0.6969 avg=0.5981 it/s=346.7\n",
      "[e3 b925/2315] loss=0.4670 avg=0.5935 it/s=353.6\n",
      "[e3 b1156/2315] loss=0.3206 avg=0.5909 it/s=358.7\n",
      "[e3 b1387/2315] loss=0.6549 avg=0.5885 it/s=362.3\n",
      "[e3 b1618/2315] loss=0.5566 avg=0.5853 it/s=361.0\n",
      "[e3 b1849/2315] loss=0.4816 avg=0.5809 it/s=359.1\n",
      "[e3 b2080/2315] loss=0.7244 avg=0.5791 it/s=358.3\n",
      "[e3 b2311/2315] loss=0.5066 avg=0.5755 it/s=361.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7 | loss=0.5754 | val_acc=0.8297 | val_f1=0.8346 | time=107.1s\n",
      "[e4 b1/2315] loss=0.4757 avg=0.4757 it/s=387.7\n",
      "[e4 b2/2315] loss=0.5356 avg=0.5057 it/s=354.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.4226 avg=0.5144 it/s=386.7\n",
      "[e4 b463/2315] loss=0.5942 avg=0.5237 it/s=380.8\n",
      "[e4 b694/2315] loss=0.2551 avg=0.5170 it/s=377.4\n",
      "[e4 b925/2315] loss=0.3168 avg=0.5132 it/s=374.3\n",
      "[e4 b1156/2315] loss=0.6630 avg=0.5112 it/s=373.8\n",
      "[e4 b1387/2315] loss=1.0948 avg=0.5060 it/s=373.0\n",
      "[e4 b1618/2315] loss=0.5901 avg=0.5054 it/s=371.4\n",
      "[e4 b1849/2315] loss=0.5337 avg=0.5038 it/s=370.4\n",
      "[e4 b2080/2315] loss=0.6858 avg=0.5045 it/s=371.5\n",
      "[e4 b2311/2315] loss=0.2837 avg=0.5023 it/s=371.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/7 | loss=0.5022 | val_acc=0.8472 | val_f1=0.8517 | time=104.2s\n",
      "[e5 b1/2315] loss=0.5320 avg=0.5320 it/s=285.1\n",
      "[e5 b2/2315] loss=0.5818 avg=0.5569 it/s=328.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.6266 avg=0.4812 it/s=381.4\n",
      "[e5 b463/2315] loss=0.2766 avg=0.4793 it/s=378.3\n",
      "[e5 b694/2315] loss=0.6094 avg=0.4720 it/s=380.5\n",
      "[e5 b925/2315] loss=0.4356 avg=0.4685 it/s=375.6\n",
      "[e5 b1156/2315] loss=0.5469 avg=0.4690 it/s=371.1\n",
      "[e5 b1387/2315] loss=0.4242 avg=0.4656 it/s=369.0\n",
      "[e5 b1618/2315] loss=0.3562 avg=0.4643 it/s=368.7\n",
      "[e5 b1849/2315] loss=0.6112 avg=0.4649 it/s=368.2\n",
      "[e5 b2080/2315] loss=0.2231 avg=0.4643 it/s=364.4\n",
      "[e5 b2311/2315] loss=0.5629 avg=0.4633 it/s=365.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/7 | loss=0.4634 | val_acc=0.8336 | val_f1=0.8372 | time=106.0s\n",
      "[e6 b1/2315] loss=1.2789 avg=1.2789 it/s=385.4\n",
      "[e6 b2/2315] loss=0.2670 avg=0.7729 it/s=437.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.4764 avg=0.4460 it/s=379.2\n",
      "[e6 b463/2315] loss=0.4214 avg=0.4345 it/s=377.9\n",
      "[e6 b694/2315] loss=0.5944 avg=0.4412 it/s=378.6\n",
      "[e6 b925/2315] loss=0.5741 avg=0.4381 it/s=380.2\n",
      "[e6 b1156/2315] loss=0.3822 avg=0.4409 it/s=374.3\n",
      "[e6 b1387/2315] loss=0.7623 avg=0.4413 it/s=369.2\n",
      "[e6 b1618/2315] loss=0.7404 avg=0.4390 it/s=364.4\n",
      "[e6 b1849/2315] loss=0.3665 avg=0.4373 it/s=362.7\n",
      "[e6 b2080/2315] loss=0.3933 avg=0.4354 it/s=363.6\n",
      "[e6 b2311/2315] loss=0.3808 avg=0.4342 it/s=365.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/7 | loss=0.4344 | val_acc=0.8579 | val_f1=0.8615 | time=106.2s\n",
      "[e7 b1/2315] loss=0.2107 avg=0.2107 it/s=338.4\n",
      "[e7 b2/2315] loss=0.4453 avg=0.3280 it/s=281.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.3662 avg=0.4126 it/s=345.2\n",
      "[e7 b463/2315] loss=0.3040 avg=0.4206 it/s=343.0\n",
      "[e7 b694/2315] loss=0.2772 avg=0.4154 it/s=355.1\n",
      "[e7 b925/2315] loss=0.3097 avg=0.4150 it/s=362.8\n",
      "[e7 b1156/2315] loss=0.4270 avg=0.4165 it/s=366.6\n",
      "[e7 b1387/2315] loss=0.5602 avg=0.4154 it/s=364.8\n",
      "[e7 b1618/2315] loss=0.2470 avg=0.4124 it/s=365.3\n",
      "[e7 b1849/2315] loss=0.2069 avg=0.4101 it/s=364.0\n",
      "[e7 b2080/2315] loss=0.4347 avg=0.4097 it/s=365.7\n",
      "[e7 b2311/2315] loss=0.2933 avg=0.4103 it/s=364.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/7 | loss=0.4107 | val_acc=0.8547 | val_f1=0.8581 | time=106.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñá‚ñÜ‚ñÖ‚ñÉ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÖ‚ñÇ‚ñÜ‚ñÜ‚ñÇ‚ñÇ‚ñÇ‚ñá‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñà</td></tr><tr><td>time/epoch_sec</td><td>‚ñÑ‚ñÅ‚ñà‚ñÑ‚ñÜ‚ñá‚ñá</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñà‚ñá‚ñá‚ñá‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÜ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÉ‚ñÖ‚ñà‚ñà‚ñá‚ñà‚ñà‚ñá‚ñÜ‚ñÇ‚ñÜ‚ñá‚ñá‚ñá‚ñá</td></tr><tr><td>train/loss</td><td>‚ñà‚ñà‚ñá‚ñà‚ñá‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÜ‚ñÇ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÑ‚ñÇ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñÖ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñá‚ñÜ‚ñà‚ñá</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.84849</td></tr><tr><td>best_val_mid_f1</td><td>0.84849</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.28177</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>78561029</td></tr><tr><td>step</td><td>16201</td></tr><tr><td>time/epoch_sec</td><td>106.20208</td></tr><tr><td>train/avg_loss_so_far</td><td>0.4103</td></tr><tr><td>train/epoch_loss</td><td>0.41068</td></tr><tr><td>train/items_per_sec</td><td>364.75104</td></tr><tr><td>train/loss</td><td>0.25016</td></tr><tr><td>val/acc</td><td>0.85471</td></tr><tr><td>val/best_epoch</td><td>6</td></tr><tr><td>val/best_f1_so_far</td><td>0.84849</td></tr><tr><td>val/f1</td><td>0.85809</td></tr><tr><td>val/mid_f1</td><td>0.84668</td></tr><tr><td>val/precision</td><td>0.8527</td></tr><tr><td>val/recall</td><td>0.86654</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_19</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e8cqel6t' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/e8cqel6t</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_210850-e8cqel6t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "Best trial: 15. Best value: 0.876329: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [3:33:15<00:00, 639.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 19] f1=0.8615 | unfreeze_k=11 lr=1.54e-05 wd=6.3e-06 suggested_bs=8\n",
      "[I 2025-08-17 21:21:18,523] Trial 19 finished with value: 0.8615373448847018 and parameters: {'num_unfreeze_last_layers': 11, 'lr': 1.5441131471553194e-05, 'weight_decay': 6.254940759615576e-06, 'batch_size': 8}. Best is trial 15 with value: 0.8763286523329038.\n",
      "Best trial: 15 F1: 0.8763286523329038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# ADV DL ‚Äì Part B: Monolingual baseline (RoBERTa) ‚Äì Exercise-4 style\n",
    "# Custom loop + early stopping + W&B + Optuna ONLY; freeze base, unfreeze last k layers\n",
    "# Uses df_train / df_test with columns: OriginalTweet (str), Sentiment (str)\n",
    "# =========================\n",
    "\n",
    "import os, math, random, time, json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "\n",
    "# ---- deps ----\n",
    "# !pip -q install transformers==4.43.3 optuna==3.6.1 wandb==0.17.5 >/dev/null\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    ")\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# -------------------------\n",
    "# Constants (no CFG, Optuna-only workflow)\n",
    "# -------------------------\n",
    "MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "WARMUP_RATIO = 0.06\n",
    "GRAD_CLIP = 1.0\n",
    "USE_AMP = True\n",
    "\n",
    "# ‚ùó New W&B project & run base (to keep things separate)\n",
    "PROJECT = \"adv-dl-p2-deberta-midf1_new_study\"\n",
    "BASE_RUN_NAME = \"microsoft/mdeberta-v3-base_full_ex_4_midf1\"\n",
    "\n",
    "TRIALS = 20\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# ---- GPU perf toggles (Windows-safe) ----\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# -------------------------\n",
    "# Label mapping (5-way sentiment)\n",
    "# -------------------------\n",
    "CANON = {\n",
    "    \"extremely negative\": \"extremely negative\",\n",
    "    \"negative\": \"negative\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"positive\": \"positive\",\n",
    "    \"extremely positive\": \"extremely positive\",\n",
    "}\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return CANON.get(s, s)\n",
    "\n",
    "# -------------------------\n",
    "# Expect df_train, df_test in memory\n",
    "# -------------------------\n",
    "assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns, \"df_train missing required columns\"\n",
    "assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns, \"df_test missing required columns\"\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "dftrain_ = prep_df(df_train)\n",
    "dftest_  = prep_df(df_test)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    dftrain_, test_size=0.1, stratify=dftrain_[\"label\"], random_state=SEED\n",
    ")\n",
    "print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n",
    "\n",
    "\n",
    "\n",
    "IDX_EXT_NEG = LABEL2ID[\"extremely negative\"]\n",
    "IDX_NEG     = LABEL2ID[\"negative\"]\n",
    "IDX_NEU     = LABEL2ID[\"neutral\"]\n",
    "IDX_POS     = LABEL2ID[\"positive\"]\n",
    "IDX_EXT_POS = LABEL2ID[\"extremely positive\"]\n",
    "\n",
    "_num_classes = len(ORDER)\n",
    "\n",
    "# knobs: make mids heavier than extremes\n",
    "MID_WEIGHT = 1.5   # applied to negative/neutral/positive\n",
    "EXT_WEIGHT = 0.8   # applied to extremely negative/positive (less than MID_WEIGHT)\n",
    "\n",
    "weights = np.full(_num_classes, EXT_WEIGHT, dtype=np.float32)\n",
    "weights[[IDX_NEG, IDX_NEU, IDX_POS]] = MID_WEIGHT\n",
    "\n",
    "# normalize so average weight is 1.0 (optional but recommended)\n",
    "weights = weights * (_num_classes / weights.sum())\n",
    "\n",
    "CLASS_WEIGHTS = torch.tensor(weights, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "print(\"[loss] Using tiered class weights; per label:\")\n",
    "print({ID2LABEL[i]: float(CLASS_WEIGHTS[i].item()) for i in range(_num_classes)})\n",
    "\n",
    "# -------------------------\n",
    "# Dataset & Collator\n",
    "# -------------------------\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: transformers.PreTrainedTokenizerBase, max_len: int):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "        enc[\"labels\"] = self.labels[idx]\n",
    "        return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "train_ds = TweetDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_ds   = TweetDataset(val_df, tokenizer, MAX_LEN)\n",
    "test_ds  = TweetDataset(dftest_, tokenizer, MAX_LEN)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "# ---- pad_to_multiple_of=8 for Tensor Cores; Windows: workers=0 is often faster ----\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "# -------------------------\n",
    "# Model & Freeze/Unfreeze strategy\n",
    "# -------------------------\n",
    "def build_model(num_unfreeze_last_layers: int = 4):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    )\n",
    "    base = getattr(model, \"roberta\", None) or getattr(model, \"bert\", None) or getattr(model, \"deberta\", None)\n",
    "    if base is not None:\n",
    "        for p in base.parameters(): p.requires_grad = False\n",
    "        if hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "            k = num_unfreeze_last_layers\n",
    "            if k > 0:\n",
    "                for layer in base.encoder.layer[-k:]:\n",
    "                    for p in layer.parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Eval utilities\n",
    "# -------------------------\n",
    "def get_optimizer_scheduler(model, num_training_steps: int, lr: float, weight_decay: float):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    # try fused AdamW on CUDA (faster step) ‚Äî falls back if unavailable\n",
    "    try:\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay, fused=(DEVICE==\"cuda\"))\n",
    "    except TypeError:\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay)\n",
    "    num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "MIDS = [LABEL2ID[\"negative\"], LABEL2ID[\"neutral\"], LABEL2ID[\"positive\"]]\n",
    "\n",
    "def evaluate(model, loader) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "            # AMP autocast for faster eval math\n",
    "            with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "                          enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "                logits = model(**batch).logits\n",
    "            preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "            labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    # extra: mid-class F1 (negative/neutral/positive)\n",
    "    p_mid, r_mid, f1_mid, _ = precision_recall_fscore_support(\n",
    "        labels, preds, labels=MIDS, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"f1_mid\": f1_mid}\n",
    "\n",
    "def train_one_run(hp: Dict) -> Tuple[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    hp keys: run_name, num_unfreeze_last_layers, lr, weight_decay, epochs, patience, trial_number\n",
    "    \"\"\"\n",
    "    run_name = hp[\"run_name\"]\n",
    "    num_unfreeze = int(hp[\"num_unfreeze_last_layers\"])\n",
    "    lr = float(hp[\"lr\"])\n",
    "    wd = float(hp[\"weight_decay\"])\n",
    "    epochs   = int(hp.get(\"epochs\",   FIXED_EPOCHS))\n",
    "    patience = int(hp.get(\"patience\", FIXED_PATIENCE))\n",
    "    model = build_model(num_unfreeze)\n",
    "    total_steps = int(math.ceil(len(train_loader) * epochs))\n",
    "    optimizer, scheduler = get_optimizer_scheduler(model, total_steps, lr, wd)\n",
    "\n",
    "    scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
    "    best_metric = -1.0\n",
    "    no_improve = 0\n",
    "\n",
    "    # ‚ùó save to a different folder + name to avoid collisions\n",
    "    safe_run_name = run_name.replace(\"/\", \"__\").replace(\"\\\\\", \"__\")\n",
    "    ckpt_dir = \"checkpoints_midf1\"\n",
    "    os.makedirs(ckpt_dir, exist_ok=True)\n",
    "    best_path = os.path.join(ckpt_dir, f\"best_midf1_{safe_run_name}.pt\")\n",
    "\n",
    "    wandb_run = wandb.init(\n",
    "        project=PROJECT,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": epochs,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": wd,\n",
    "            \"warmup_ratio\": WARMUP_RATIO,\n",
    "            \"grad_clip\": GRAD_CLIP,\n",
    "            \"num_unfreeze_last_layers\": num_unfreeze,\n",
    "            \"trial_number\": hp.get(\"trial_number\", -1),\n",
    "            \"suggested_batch_size\": hp.get(\"batch_size\", BATCH_SIZE),\n",
    "        },\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    # nicer W&B charts\n",
    "    wandb.define_metric(\"epoch\")\n",
    "    wandb.define_metric(\"step\")\n",
    "    wandb.define_metric(\"train/*\", step_metric=\"step\")\n",
    "    wandb.define_metric(\"val/*\",   step_metric=\"epoch\")\n",
    "\n",
    "    # print + log trainable params\n",
    "    total_params     = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable params: {trainable_params:,} / {total_params:,} \"\n",
    "          f\"({100.0*trainable_params/total_params:.2f}%) ; unfreeze_last_k={num_unfreeze}\")\n",
    "    wandb.log({\"params/total\": total_params,\n",
    "               \"params/trainable\": trainable_params,\n",
    "               \"params/ratio\": trainable_params/max(1,total_params)}, step=0)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "            labels = batch.pop(\"labels\")  # we compute weighted loss ourselves\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            # use BF16 if supported; else FP16\n",
    "            with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "                          enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "                outputs = model(**batch)\n",
    "                logits = outputs.logits\n",
    "                # weighted CE (+ smoothing if available)\n",
    "                try:\n",
    "                    loss = F.cross_entropy(logits, labels, weight=CLASS_WEIGHTS, label_smoothing=0.05)\n",
    "                except TypeError:\n",
    "                    loss = F.cross_entropy(logits, labels, weight=CLASS_WEIGHTS)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            if GRAD_CLIP is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % 20 == 0:\n",
    "                wandb.log({\"train/loss\": loss.item(), \"step\": step + 1, \"epoch\": epoch + 1})\n",
    "\n",
    "            # periodic console + throughput log (about 10x per epoch)\n",
    "            if step % max(1, len(train_loader)//10) == 0 or step == 1:\n",
    "                avg_loss = running_loss / max(1, (step + 1))\n",
    "                elapsed  = time.time() - t0\n",
    "                items    = (step + 1) * BATCH_SIZE\n",
    "                itps     = items / max(elapsed, 1e-6)\n",
    "                print(f\"[e{epoch+1} b{step+1}/{len(train_loader)}] loss={loss.item():.4f} avg={avg_loss:.4f} it/s={itps:.1f}\")\n",
    "                wandb.log({\"train/avg_loss_so_far\": avg_loss,\n",
    "                           \"train/items_per_sec\": itps,\n",
    "                           \"step\": (epoch * len(train_loader)) + (step + 1),\n",
    "                           \"epoch\": epoch + 1})\n",
    "\n",
    "        # epoch-end validation\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        epoch_loss = running_loss / max(1, len(train_loader))\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        wandb.log({\n",
    "            \"train/epoch_loss\": epoch_loss,\n",
    "            \"val/acc\": val_metrics[\"acc\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "            \"val/mid_f1\": val_metrics[\"f1_mid\"],   # extra log (format unchanged elsewhere)\n",
    "            \"lr\": current_lr,\n",
    "            \"time/epoch_sec\": elapsed,\n",
    "            \"epoch\": epoch + 1,\n",
    "        })\n",
    "\n",
    "        # Early stopping on mid-class F1 (prints stay the same)\n",
    "        target_metric = val_metrics[\"f1_mid\"]\n",
    "        if target_metric > best_metric:\n",
    "            best_metric = target_metric\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            no_improve = 0\n",
    "            wandb_run.summary[\"best_val_f1\"] = best_metric  # kept same key for compatibility\n",
    "            wandb_run.summary[\"best_val_mid_f1\"] = best_metric\n",
    "            wandb_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "            wandb.log({\"val/best_f1_so_far\": best_metric, \"val/best_epoch\": epoch + 1})\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        # console print line unchanged\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"loss={epoch_loss:.4f} | \"\n",
    "              f\"val_acc={val_metrics['acc']:.4f} | val_f1={val_metrics['f1']:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Load best and return path + metrics on val for reference\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    final_val = evaluate(model, val_loader)\n",
    "\n",
    "    # store final val in W&B summary for quick sorting\n",
    "    if wandb.run is not None:\n",
    "        wandb.run.summary[\"final_val_acc\"] = final_val[\"acc\"]\n",
    "        wandb.run.summary[\"final_val_precision\"] = final_val[\"precision\"]\n",
    "        wandb.run.summary[\"final_val_recall\"] = final_val[\"recall\"]\n",
    "        wandb.run.summary[\"final_val_f1\"] = final_val[\"f1\"]\n",
    "        wandb.run.summary[\"final_val_mid_f1\"] = final_val[\"f1_mid\"]\n",
    "\n",
    "    return best_path, final_val\n",
    "\n",
    "# -------------------------\n",
    "# Optuna hyperparameter tuning (ALWAYS ON)\n",
    "# -------------------------\n",
    "\n",
    "# Constants\n",
    "FIXED_EPOCHS = 7\n",
    "FIXED_PATIENCE = 3\n",
    "\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    params = {\n",
    "        \"run_name\": f\"{BASE_RUN_NAME}_optuna_trial_{trial.number}\",\n",
    "        \"num_unfreeze_last_layers\": trial.suggest_int(\"num_unfreeze_last_layers\", 4, 12),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-7, 1e-5, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64]),\n",
    "        \"epochs\": FIXED_EPOCHS,\n",
    "        \"patience\": FIXED_PATIENCE,\n",
    "        \"trial_number\": trial.number,\n",
    "    }\n",
    "    path, val_metrics = train_one_run(params)\n",
    "    # console visibility per trial (unchanged)\n",
    "    print(f\"[Trial {trial.number}] f1={val_metrics['f1']:.4f} | \"\n",
    "          f\"unfreeze_k={params['num_unfreeze_last_layers']} lr={params['lr']:.2e} \"\n",
    "          f\"wd={params['weight_decay']:.1e} suggested_bs={params['batch_size']}\")\n",
    "    # report intermediate value for pruning if enabled (keep macro f1 for study objective)\n",
    "    trial.report(val_metrics[\"f1\"], step=1)\n",
    "    return val_metrics[\"f1\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)\n",
    "print(\"Best trial:\", study.best_trial.number, \"F1:\", study.best_value)\n",
    "best_params = {\"run_name\": f\"{BASE_RUN_NAME}_best_optuna\", **study.best_trial.params}\n",
    "\n",
    "# also persist best params to a different folder/name\n",
    "os.makedirs(\"checkpoints_midf1\", exist_ok=True)\n",
    "with open(os.path.join(\"checkpoints_midf1\", \"best_hparams_optuna_midf1.json\"), \"w\") as f:\n",
    "    json.dump({**best_params, \"epochs\": FIXED_EPOCHS, \"patience\": FIXED_PATIENCE}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e1408-7392-4e6d-8bfc-6c13301996d9",
   "metadata": {},
   "source": [
    "## üìä Results ‚Äì Weighted Loss Experiment (mid-class emphasis)\n",
    "\n",
    "This second run (full training, Ex.4 style + weighted loss) gave **more consistent improvements** compared to the plain loss function:\n",
    "\n",
    "- The **best trial** reached **val/F1 = 0.879** and **mid-class F1 = 0.867**,  \n",
    "  which is stronger than our earlier unweighted runs.  \n",
    "- The **top trials (11, 15, 8, 13)** all sit around **0.86‚Äì0.87 F1**,  \n",
    "  showing that the weighting helped stabilize the optimization and reduce variance.  \n",
    "- As expected, the **best LRs are still in the range 2.5e-5 ‚Äì 7.5e-5** with small weight decay (‚âà1e-6‚Äì1e-5).  \n",
    "- Best unfreeze depth remains **10‚Äì12 layers**, similar to before.  \n",
    "- Almost all successful runs used **batch size = 8**, confirming smaller batches give more stable convergence.  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Key Takeaways\n",
    "- Weighted loss **did boost performance on the mid classes**, which were previously weaker.  \n",
    "- Even though absolute F1 gains are modest, this approach reduced the gap between extreme vs. mid sentiments.  \n",
    "- Bad trials (very high learning rate >1e-3 or shallow unfreezing <8) still collapse (F1 ~0.14),  \n",
    "  but **the good region is now clearer and more reproducible**.  \n",
    "\n",
    "---\n",
    "\n",
    "We will now check for test results. \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4afaf1bf5894bd21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T18:45:50.581359Z",
     "start_time": "2025-08-17T18:45:50.574171Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(os.path.join(\"checkpoints_midf1\", \"best_hparams_optuna_midf1.json\")) as f:\n",
    "    best_hparams = json.load(f)\n",
    "\n",
    "# give a distinct name for the final run\n",
    "best_hparams[\"run_name\"] = f\"{BASE_RUN_NAME}_best_optuna_weighed_classes\"\n",
    "\n",
    "# (optional) bump epochs here; see guidance below\n",
    "best_hparams[\"epochs\"] = 15      # higher cap\n",
    "best_hparams[\"patience\"] = 4     # unchanged\n",
    "# ckpt, val = train_one_run(best_hparams)\n",
    "\n",
    "#\n",
    "# best_ckpt_path, best_val = train_one_run(best_hparams)\n",
    "# print(\"Final best checkpoint:\", best_ckpt_path)\n",
    "# print(\"Final val metrics:\", best_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57751cdde9dbab3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:13:33.449352Z",
     "start_time": "2025-08-17T18:46:01.501988Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:284: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_214603-0f9zzv5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/0f9zzv5d' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_best_optuna_weighed_classes</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/0f9zzv5d' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/0f9zzv5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n",
      "[e1 b1/2315] loss=1.6515 avg=1.6515 it/s=239.0\n",
      "[e1 b2/2315] loss=1.6300 avg=1.6407 it/s=316.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b232/2315] loss=1.4546 avg=1.5346 it/s=328.2\n",
      "[e1 b463/2315] loss=1.2486 avg=1.4866 it/s=346.9\n",
      "[e1 b694/2315] loss=1.2882 avg=1.4268 it/s=350.5\n",
      "[e1 b925/2315] loss=0.9568 avg=1.3558 it/s=352.5\n",
      "[e1 b1156/2315] loss=0.5954 avg=1.2882 it/s=355.5\n",
      "[e1 b1387/2315] loss=1.0275 avg=1.2351 it/s=358.5\n",
      "[e1 b1618/2315] loss=0.8343 avg=1.1911 it/s=362.1\n",
      "[e1 b1849/2315] loss=0.6917 avg=1.1536 it/s=358.7\n",
      "[e1 b2080/2315] loss=0.7476 avg=1.1215 it/s=336.6\n",
      "[e1 b2311/2315] loss=0.9310 avg=1.0926 it/s=340.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 | loss=1.0923 | val_acc=0.6919 | val_f1=0.6834 | time=113.2s\n",
      "[e2 b1/2315] loss=0.7860 avg=0.7860 it/s=281.8\n",
      "[e2 b2/2315] loss=0.6493 avg=0.7176 it/s=319.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e2 b232/2315] loss=0.9844 avg=0.7798 it/s=383.3\n",
      "[e2 b463/2315] loss=0.7437 avg=0.7647 it/s=383.1\n",
      "[e2 b694/2315] loss=0.9229 avg=0.7570 it/s=376.2\n",
      "[e2 b925/2315] loss=0.6513 avg=0.7501 it/s=366.9\n",
      "[e2 b1156/2315] loss=0.6151 avg=0.7455 it/s=360.0\n",
      "[e2 b1387/2315] loss=0.9574 avg=0.7377 it/s=362.3\n",
      "[e2 b1618/2315] loss=0.7503 avg=0.7301 it/s=364.9\n",
      "[e2 b1849/2315] loss=0.4329 avg=0.7224 it/s=366.7\n",
      "[e2 b2080/2315] loss=0.4973 avg=0.7182 it/s=366.8\n",
      "[e2 b2311/2315] loss=0.9460 avg=0.7146 it/s=365.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 | loss=0.7147 | val_acc=0.8141 | val_f1=0.8212 | time=106.2s\n",
      "[e3 b1/2315] loss=0.5274 avg=0.5274 it/s=349.9\n",
      "[e3 b2/2315] loss=0.4017 avg=0.4646 it/s=335.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e3 b232/2315] loss=0.5802 avg=0.5970 it/s=367.2\n",
      "[e3 b463/2315] loss=0.4904 avg=0.6080 it/s=378.9\n",
      "[e3 b694/2315] loss=0.3945 avg=0.6007 it/s=382.9\n",
      "[e3 b925/2315] loss=0.6966 avg=0.6011 it/s=382.2\n",
      "[e3 b1156/2315] loss=0.4373 avg=0.6028 it/s=378.6\n",
      "[e3 b1387/2315] loss=0.7323 avg=0.6024 it/s=377.1\n",
      "[e3 b1618/2315] loss=0.5385 avg=0.6016 it/s=374.5\n",
      "[e3 b1849/2315] loss=0.3216 avg=0.6003 it/s=373.6\n",
      "[e3 b2080/2315] loss=0.3795 avg=0.5995 it/s=373.3\n",
      "[e3 b2311/2315] loss=0.3147 avg=0.5987 it/s=373.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 | loss=0.5985 | val_acc=0.8178 | val_f1=0.8195 | time=103.9s\n",
      "[e4 b1/2315] loss=0.4736 avg=0.4736 it/s=419.0\n",
      "[e4 b2/2315] loss=0.3397 avg=0.4067 it/s=350.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e4 b232/2315] loss=0.7414 avg=0.5443 it/s=376.7\n",
      "[e4 b463/2315] loss=0.5379 avg=0.5450 it/s=379.3\n",
      "[e4 b694/2315] loss=0.3739 avg=0.5389 it/s=380.1\n",
      "[e4 b925/2315] loss=0.3814 avg=0.5348 it/s=380.6\n",
      "[e4 b1156/2315] loss=0.7599 avg=0.5332 it/s=382.0\n",
      "[e4 b1387/2315] loss=0.3558 avg=0.5338 it/s=380.5\n",
      "[e4 b1618/2315] loss=0.4625 avg=0.5325 it/s=377.4\n",
      "[e4 b1849/2315] loss=0.5923 avg=0.5342 it/s=376.8\n",
      "[e4 b2080/2315] loss=0.3626 avg=0.5327 it/s=377.0\n",
      "[e4 b2311/2315] loss=0.5034 avg=0.5338 it/s=378.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 | loss=0.5338 | val_acc=0.8389 | val_f1=0.8412 | time=102.5s\n",
      "[e5 b1/2315] loss=0.6025 avg=0.6025 it/s=352.5\n",
      "[e5 b2/2315] loss=0.4433 avg=0.5229 it/s=346.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e5 b232/2315] loss=0.3617 avg=0.4701 it/s=382.2\n",
      "[e5 b463/2315] loss=0.2705 avg=0.4806 it/s=380.4\n",
      "[e5 b694/2315] loss=0.7763 avg=0.4830 it/s=380.9\n",
      "[e5 b925/2315] loss=0.5015 avg=0.4839 it/s=382.5\n",
      "[e5 b1156/2315] loss=0.3797 avg=0.4873 it/s=383.9\n",
      "[e5 b1387/2315] loss=0.2947 avg=0.4869 it/s=384.6\n",
      "[e5 b1618/2315] loss=0.3788 avg=0.4857 it/s=384.3\n",
      "[e5 b1849/2315] loss=0.7629 avg=0.4868 it/s=379.3\n",
      "[e5 b2080/2315] loss=0.6802 avg=0.4872 it/s=374.5\n",
      "[e5 b2311/2315] loss=0.7892 avg=0.4851 it/s=371.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 | loss=0.4851 | val_acc=0.8435 | val_f1=0.8474 | time=104.2s\n",
      "[e6 b1/2315] loss=0.6863 avg=0.6863 it/s=321.7\n",
      "[e6 b2/2315] loss=0.4430 avg=0.5646 it/s=337.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e6 b232/2315] loss=0.2135 avg=0.4511 it/s=379.8\n",
      "[e6 b463/2315] loss=0.1977 avg=0.4441 it/s=382.8\n",
      "[e6 b694/2315] loss=0.2142 avg=0.4458 it/s=374.4\n",
      "[e6 b925/2315] loss=0.3217 avg=0.4400 it/s=369.0\n",
      "[e6 b1156/2315] loss=0.3781 avg=0.4386 it/s=365.7\n",
      "[e6 b1387/2315] loss=0.7144 avg=0.4440 it/s=365.3\n",
      "[e6 b1618/2315] loss=0.2255 avg=0.4429 it/s=363.8\n",
      "[e6 b1849/2315] loss=0.3836 avg=0.4446 it/s=363.7\n",
      "[e6 b2080/2315] loss=0.2567 avg=0.4447 it/s=363.8\n",
      "[e6 b2311/2315] loss=0.7182 avg=0.4474 it/s=363.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 | loss=0.4475 | val_acc=0.8581 | val_f1=0.8607 | time=106.7s\n",
      "[e7 b1/2315] loss=0.2814 avg=0.2814 it/s=338.3\n",
      "[e7 b2/2315] loss=0.5742 avg=0.4278 it/s=375.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e7 b232/2315] loss=0.2220 avg=0.4152 it/s=375.1\n",
      "[e7 b463/2315] loss=0.3796 avg=0.4062 it/s=367.8\n",
      "[e7 b694/2315] loss=0.3240 avg=0.4065 it/s=369.6\n",
      "[e7 b925/2315] loss=0.5802 avg=0.4121 it/s=369.4\n",
      "[e7 b1156/2315] loss=0.4060 avg=0.4127 it/s=372.6\n",
      "[e7 b1387/2315] loss=0.3994 avg=0.4150 it/s=374.0\n",
      "[e7 b1618/2315] loss=0.3130 avg=0.4143 it/s=375.0\n",
      "[e7 b1849/2315] loss=0.9441 avg=0.4171 it/s=375.6\n",
      "[e7 b2080/2315] loss=0.5803 avg=0.4162 it/s=376.2\n",
      "[e7 b2311/2315] loss=0.3659 avg=0.4152 it/s=374.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 | loss=0.4151 | val_acc=0.8622 | val_f1=0.8646 | time=103.7s\n",
      "[e8 b1/2315] loss=0.3302 avg=0.3302 it/s=409.5\n",
      "[e8 b2/2315] loss=0.3720 avg=0.3511 it/s=393.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e8 b232/2315] loss=0.8476 avg=0.3722 it/s=358.2\n",
      "[e8 b463/2315] loss=0.2176 avg=0.3800 it/s=365.8\n",
      "[e8 b694/2315] loss=0.4325 avg=0.3893 it/s=372.4\n",
      "[e8 b925/2315] loss=0.2408 avg=0.3912 it/s=373.2\n",
      "[e8 b1156/2315] loss=0.3664 avg=0.3933 it/s=374.3\n",
      "[e8 b1387/2315] loss=0.2294 avg=0.3931 it/s=373.3\n",
      "[e8 b1618/2315] loss=0.2881 avg=0.3939 it/s=374.2\n",
      "[e8 b1849/2315] loss=0.3368 avg=0.3920 it/s=375.2\n",
      "[e8 b2080/2315] loss=0.3969 avg=0.3914 it/s=375.3\n",
      "[e8 b2311/2315] loss=0.4150 avg=0.3910 it/s=375.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 | loss=0.3911 | val_acc=0.8644 | val_f1=0.8679 | time=103.1s\n",
      "[e9 b1/2315] loss=0.2956 avg=0.2956 it/s=324.9\n",
      "[e9 b2/2315] loss=0.4088 avg=0.3522 it/s=353.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e9 b232/2315] loss=0.2344 avg=0.3639 it/s=364.0\n",
      "[e9 b463/2315] loss=0.4339 avg=0.3609 it/s=350.6\n",
      "[e9 b694/2315] loss=0.1968 avg=0.3607 it/s=342.4\n",
      "[e9 b925/2315] loss=0.3294 avg=0.3614 it/s=350.4\n",
      "[e9 b1156/2315] loss=0.2863 avg=0.3590 it/s=355.8\n",
      "[e9 b1387/2315] loss=0.2383 avg=0.3606 it/s=358.3\n",
      "[e9 b1618/2315] loss=0.6083 avg=0.3611 it/s=357.6\n",
      "[e9 b1849/2315] loss=0.5238 avg=0.3615 it/s=355.9\n",
      "[e9 b2080/2315] loss=0.2077 avg=0.3638 it/s=353.9\n",
      "[e9 b2311/2315] loss=0.2038 avg=0.3623 it/s=353.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 | loss=0.3622 | val_acc=0.8727 | val_f1=0.8755 | time=109.4s\n",
      "[e10 b1/2315] loss=0.3582 avg=0.3582 it/s=277.5\n",
      "[e10 b2/2315] loss=0.2633 avg=0.3107 it/s=306.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e10 b232/2315] loss=0.2044 avg=0.3298 it/s=339.7\n",
      "[e10 b463/2315] loss=0.2034 avg=0.3402 it/s=343.7\n",
      "[e10 b694/2315] loss=0.1866 avg=0.3380 it/s=348.4\n",
      "[e10 b925/2315] loss=0.2044 avg=0.3402 it/s=349.2\n",
      "[e10 b1156/2315] loss=0.4210 avg=0.3376 it/s=352.8\n",
      "[e10 b1387/2315] loss=0.2004 avg=0.3370 it/s=354.6\n",
      "[e10 b1618/2315] loss=0.4996 avg=0.3386 it/s=355.6\n",
      "[e10 b1849/2315] loss=0.2118 avg=0.3396 it/s=355.4\n",
      "[e10 b2080/2315] loss=0.3073 avg=0.3383 it/s=357.6\n",
      "[e10 b2311/2315] loss=0.4265 avg=0.3379 it/s=359.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 | loss=0.3379 | val_acc=0.8700 | val_f1=0.8735 | time=107.5s\n",
      "[e11 b1/2315] loss=0.3809 avg=0.3809 it/s=378.6\n",
      "[e11 b2/2315] loss=0.4327 avg=0.4068 it/s=429.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e11 b232/2315] loss=0.5474 avg=0.3168 it/s=382.7\n",
      "[e11 b463/2315] loss=0.6194 avg=0.3186 it/s=385.2\n",
      "[e11 b694/2315] loss=0.3039 avg=0.3256 it/s=383.4\n",
      "[e11 b925/2315] loss=0.6494 avg=0.3226 it/s=378.0\n",
      "[e11 b1156/2315] loss=0.2228 avg=0.3210 it/s=375.6\n",
      "[e11 b1387/2315] loss=0.2011 avg=0.3222 it/s=374.9\n",
      "[e11 b1618/2315] loss=0.2174 avg=0.3206 it/s=377.0\n",
      "[e11 b1849/2315] loss=0.5074 avg=0.3200 it/s=377.9\n",
      "[e11 b2080/2315] loss=0.3983 avg=0.3197 it/s=359.5\n",
      "[e11 b2311/2315] loss=0.1989 avg=0.3180 it/s=348.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 | loss=0.3182 | val_acc=0.8690 | val_f1=0.8719 | time=113.9s\n",
      "[e12 b1/2315] loss=0.4724 avg=0.4724 it/s=272.7\n",
      "[e12 b2/2315] loss=0.4184 avg=0.4454 it/s=290.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e12 b232/2315] loss=0.2179 avg=0.3048 it/s=278.6\n",
      "[e12 b463/2315] loss=0.6319 avg=0.3046 it/s=297.5\n",
      "[e12 b694/2315] loss=0.2103 avg=0.2988 it/s=270.3\n",
      "[e12 b925/2315] loss=0.2134 avg=0.2996 it/s=282.0\n",
      "[e12 b1156/2315] loss=0.2221 avg=0.3009 it/s=293.8\n",
      "[e12 b1387/2315] loss=0.2212 avg=0.2997 it/s=304.9\n",
      "[e12 b1618/2315] loss=0.6515 avg=0.3015 it/s=314.1\n",
      "[e12 b1849/2315] loss=0.2259 avg=0.3009 it/s=321.4\n",
      "[e12 b2080/2315] loss=0.2932 avg=0.3004 it/s=325.1\n",
      "[e12 b2311/2315] loss=0.2172 avg=0.2993 it/s=328.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 | loss=0.2992 | val_acc=0.8810 | val_f1=0.8837 | time=117.3s\n",
      "[e13 b1/2315] loss=0.2021 avg=0.2021 it/s=284.8\n",
      "[e13 b2/2315] loss=0.4142 avg=0.3081 it/s=267.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e13 b232/2315] loss=0.5314 avg=0.2878 it/s=363.2\n",
      "[e13 b463/2315] loss=0.2171 avg=0.2871 it/s=363.5\n",
      "[e13 b694/2315] loss=0.2032 avg=0.2857 it/s=361.6\n",
      "[e13 b925/2315] loss=0.1906 avg=0.2851 it/s=348.4\n",
      "[e13 b1156/2315] loss=0.4455 avg=0.2850 it/s=331.6\n",
      "[e13 b1387/2315] loss=0.6391 avg=0.2858 it/s=336.3\n",
      "[e13 b1618/2315] loss=0.2255 avg=0.2845 it/s=340.2\n",
      "[e13 b1849/2315] loss=0.2034 avg=0.2829 it/s=344.5\n",
      "[e13 b2080/2315] loss=0.1958 avg=0.2823 it/s=345.4\n",
      "[e13 b2311/2315] loss=0.2252 avg=0.2828 it/s=348.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 | loss=0.2828 | val_acc=0.8729 | val_f1=0.8762 | time=110.9s\n",
      "[e14 b1/2315] loss=0.3902 avg=0.3902 it/s=370.2\n",
      "[e14 b2/2315] loss=0.2036 avg=0.2969 it/s=396.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e14 b232/2315] loss=0.5252 avg=0.2781 it/s=381.3\n",
      "[e14 b463/2315] loss=0.2478 avg=0.2762 it/s=355.0\n",
      "[e14 b694/2315] loss=0.2752 avg=0.2776 it/s=308.5\n",
      "[e14 b925/2315] loss=0.2102 avg=0.2738 it/s=282.7\n",
      "[e14 b1156/2315] loss=0.4516 avg=0.2728 it/s=295.1\n",
      "[e14 b1387/2315] loss=0.5546 avg=0.2731 it/s=305.4\n",
      "[e14 b1618/2315] loss=0.2114 avg=0.2719 it/s=313.7\n",
      "[e14 b1849/2315] loss=0.3097 avg=0.2705 it/s=321.1\n",
      "[e14 b2080/2315] loss=0.4334 avg=0.2708 it/s=327.2\n",
      "[e14 b2311/2315] loss=0.2371 avg=0.2706 it/s=332.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 | loss=0.2705 | val_acc=0.8751 | val_f1=0.8780 | time=116.0s\n",
      "[e15 b1/2315] loss=0.2192 avg=0.2192 it/s=386.4\n",
      "[e15 b2/2315] loss=0.2025 avg=0.2109 it/s=394.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:339: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e15 b232/2315] loss=0.2093 avg=0.2586 it/s=389.4\n",
      "[e15 b463/2315] loss=0.2093 avg=0.2582 it/s=388.4\n",
      "[e15 b694/2315] loss=0.2091 avg=0.2580 it/s=384.4\n",
      "[e15 b925/2315] loss=0.2029 avg=0.2587 it/s=340.0\n",
      "[e15 b1156/2315] loss=0.2098 avg=0.2601 it/s=321.4\n",
      "[e15 b1387/2315] loss=0.1895 avg=0.2619 it/s=323.9\n",
      "[e15 b1618/2315] loss=0.2427 avg=0.2598 it/s=327.4\n",
      "[e15 b1849/2315] loss=0.5179 avg=0.2611 it/s=333.2\n",
      "[e15 b2080/2315] loss=0.4498 avg=0.2607 it/s=337.4\n",
      "[e15 b2311/2315] loss=0.4474 avg=0.2611 it/s=341.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 | loss=0.2612 | val_acc=0.8763 | val_f1=0.8795 | time=113.2s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>lr</td><td>‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>params/ratio</td><td>‚ñÅ</td></tr><tr><td>params/total</td><td>‚ñÅ</td></tr><tr><td>params/trainable</td><td>‚ñÅ</td></tr><tr><td>step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>time/epoch_sec</td><td>‚ñÜ‚ñÉ‚ñÇ‚ñÅ‚ñÇ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÜ‚ñà‚ñÖ‚ñá‚ñÜ</td></tr><tr><td>train/avg_loss_so_far</td><td>‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch_loss</td><td>‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/items_per_sec</td><td>‚ñÑ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñÖ‚ñà‚ñá‚ñá‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñá‚ñÜ‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñá‚ñÑ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val/acc</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/best_epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>val/best_f1_so_far</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>val/f1</td><td>‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/mid_f1</td><td>‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>val/precision</td><td>‚ñÅ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñá‚ñá‚ñà‚ñá‚ñà‚ñà</td></tr><tr><td>val/recall</td><td>‚ñÅ‚ñÜ‚ñÖ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>best_val_f1</td><td>0.87257</td></tr><tr><td>best_val_mid_f1</td><td>0.87257</td></tr><tr><td>epoch</td><td>15</td></tr><tr><td>lr</td><td>0</td></tr><tr><td>params/ratio</td><td>0.28177</td></tr><tr><td>params/total</td><td>278813189</td></tr><tr><td>params/trainable</td><td>78561029</td></tr><tr><td>step</td><td>34721</td></tr><tr><td>time/epoch_sec</td><td>113.21217</td></tr><tr><td>train/avg_loss_so_far</td><td>0.26112</td></tr><tr><td>train/epoch_loss</td><td>0.26116</td></tr><tr><td>train/items_per_sec</td><td>341.65051</td></tr><tr><td>train/loss</td><td>0.20037</td></tr><tr><td>val/acc</td><td>0.87634</td></tr><tr><td>val/best_epoch</td><td>12</td></tr><tr><td>val/best_f1_so_far</td><td>0.87257</td></tr><tr><td>val/f1</td><td>0.87946</td></tr><tr><td>val/mid_f1</td><td>0.86658</td></tr><tr><td>val/precision</td><td>0.87552</td></tr><tr><td>val/recall</td><td>0.88522</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_best_optuna_weighed_classes</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/0f9zzv5d' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/0f9zzv5d</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_214603-0f9zzv5d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\32348540.py:257: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    }
   ],
   "source": [
    "# Retrain best config to get a clean checkpoint\n",
    "best_ckpt, _ = train_one_run(best_hparams)\n",
    "best_path = best_ckpt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728e4dd8-cdb4-43ec-83ca-cea9b02466ea",
   "metadata": {},
   "source": [
    "## üßæ Final Test Results ‚Äì Weighted Loss Run  \n",
    "\n",
    "**Overall metrics**  \n",
    "`acc = 0.8626 | f1_macro = 0.8658 | precision_macro = 0.8636 | recall_macro = 0.8686`  \n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Comparison with Ex.4 Baseline (no weighted loss)\n",
    "\n",
    "| Label                | Ex.4 F1 | Weighted F1 | Change |\n",
    "|-----------------------|---------|-------------|--------|\n",
    "| extremely negative    | 0.89    | 0.88        | -0.01  |\n",
    "| negative              | 0.87    | 0.85        | -0.02  |\n",
    "| neutral               | 0.86    | 0.86        | ~0.00  |\n",
    "| positive              | 0.84    | 0.85        | +0.01  |\n",
    "| extremely positive    | 0.88    | 0.89        | +0.01  |\n",
    "| **Macro avg**         | **0.87**| **0.87**    | ~0.00  |\n",
    "| **Accuracy**          | **0.87**| **0.86**    | -0.01  |\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Takeaways\n",
    "- **Weighted loss did not drastically change overall macro F1 (~0.87 ‚Üí ~0.87).**  \n",
    "- Gains are seen in **positive and extremely positive**, which improved slightly.  \n",
    "- The **negative class** dropped a bit, suggesting the weighting may have shifted attention slightly away from it.  \n",
    "- Macro balance looks good ‚Äî the mid classes (neutral, positive) now perform closer to extremes.  \n",
    "\n",
    "‚úÖ In short: weighted loss **kept overall performance stable** while slightly **balancing mid-class performance**.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b22bb1d6861b0bbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:14:45.170637Z",
     "start_time": "2025-08-17T19:14:35.886116Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23124\\4076346695.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST | acc=0.8626 | f1_macro=0.8658 | precision_macro=0.8636 | recall_macro=0.8686\n",
      "\n",
      "Per-class report (ids map to labels):\n",
      "{0: 'extremely negative', 1: 'negative', 2: 'neutral', 3: 'positive', 4: 'extremely positive'}\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative       0.86      0.90      0.88       592\n",
      "          negative       0.86      0.84      0.85      1041\n",
      "           neutral       0.84      0.89      0.86       619\n",
      "          positive       0.86      0.83      0.85       947\n",
      "extremely positive       0.90      0.88      0.89       599\n",
      "\n",
      "          accuracy                           0.86      3798\n",
      "         macro avg       0.86      0.87      0.87      3798\n",
      "      weighted avg       0.86      0.86      0.86      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Retrain best config to get a clean checkpoint\n",
    "# best_ckpt, _ = train_one_run(best_params)\n",
    "# best_path = best_ckpt\n",
    "best_params=best_hparams\n",
    "# -------------------------\n",
    "# Final evaluation on TEST (+ W&B logging)\n",
    "# -------------------------\n",
    "model = build_model(best_params[\"num_unfreeze_last_layers\"])\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "        with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "                      enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "            logits = model(**batch).logits\n",
    "        all_preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "print(f\"\\nTEST | acc={acc:.4f} | f1_macro={f1:.4f} | precision_macro={p:.4f} | recall_macro={r:.4f}\\n\")\n",
    "\n",
    "print(\"Per-class report (ids map to labels):\")\n",
    "print(ID2LABEL)\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99612d53-c078-4e19-b865-08ac1ff6952a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ff935dfb2c583a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-17T19:16:15.493386Z",
     "start_time": "2025-08-17T19:16:10.152429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250817_221610-gu0wbzn4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gu0wbzn4' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_test_wighted_classes</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gu0wbzn4' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gu0wbzn4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/acc</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/f1</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/precision</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/recall</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/f1</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/precision</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/recall</td><td>‚ñÅ</td></tr><tr><td>test/f1_macro</td><td>‚ñÅ</td></tr><tr><td>test/negative/f1</td><td>‚ñÅ</td></tr><tr><td>test/negative/precision</td><td>‚ñÅ</td></tr><tr><td>test/negative/recall</td><td>‚ñÅ</td></tr><tr><td>test/neutral/f1</td><td>‚ñÅ</td></tr><tr><td>test/neutral/precision</td><td>‚ñÅ</td></tr><tr><td>test/neutral/recall</td><td>‚ñÅ</td></tr><tr><td>test/positive/f1</td><td>‚ñÅ</td></tr><tr><td>test/positive/precision</td><td>‚ñÅ</td></tr><tr><td>test/positive/recall</td><td>‚ñÅ</td></tr><tr><td>test/precision_macro</td><td>‚ñÅ</td></tr><tr><td>test/recall_macro</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_checkpoint_path</td><td>checkpoints_midf1\\be...</td></tr><tr><td>test/acc</td><td>0.86256</td></tr><tr><td>test/extremely negative/f1</td><td>0.87531</td></tr><tr><td>test/extremely negative/precision</td><td>0.85622</td></tr><tr><td>test/extremely negative/recall</td><td>0.89527</td></tr><tr><td>test/extremely positive/f1</td><td>0.89226</td></tr><tr><td>test/extremely positive/precision</td><td>0.89983</td></tr><tr><td>test/extremely positive/recall</td><td>0.88481</td></tr><tr><td>test/f1_macro</td><td>0.86578</td></tr><tr><td>test/negative/f1</td><td>0.85133</td></tr><tr><td>test/negative/precision</td><td>0.8584</td></tr><tr><td>test/negative/recall</td><td>0.84438</td></tr><tr><td>test/neutral/f1</td><td>0.86164</td></tr><tr><td>test/neutral/precision</td><td>0.8392</td></tr><tr><td>test/neutral/recall</td><td>0.8853</td></tr><tr><td>test/positive/f1</td><td>0.84839</td></tr><tr><td>test/positive/precision</td><td>0.86418</td></tr><tr><td>test/positive/recall</td><td>0.83316</td></tr><tr><td>test/precision_macro</td><td>0.86357</td></tr><tr><td>test/recall_macro</td><td>0.86858</td></tr><tr><td>test_f1_macro</td><td>0.86578</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">microsoft/mdeberta-v3-base_full_ex_4_midf1_test_wighted_classes</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gu0wbzn4' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study/runs/gu0wbzn4</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250817_221610-gu0wbzn4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---- W&B: log test metrics, per-class scores, and confusion matrix ----\n",
    "test_run = wandb.init(project=PROJECT, name=f\"{BASE_RUN_NAME}_test_wighted_classes\", resume=\"allow\", reinit=True)\n",
    "log_payload = {\n",
    "    \"test/acc\": acc,\n",
    "    \"test/precision_macro\": p,\n",
    "    \"test/recall_macro\": r,\n",
    "    \"test/f1_macro\": f1,\n",
    "}\n",
    "for cls_name in ORDER:\n",
    "    if cls_name in report:\n",
    "        log_payload[f\"test/{cls_name}/precision\"] = report[cls_name][\"precision\"]\n",
    "        log_payload[f\"test/{cls_name}/recall\"]    = report[cls_name][\"recall\"]\n",
    "        log_payload[f\"test/{cls_name}/f1\"]        = report[cls_name][\"f1-score\"]\n",
    "\n",
    "wandb.log(log_payload)\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(ORDER))))\n",
    "wandb.log({\n",
    "    \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        y_true=all_labels,\n",
    "        preds=all_preds,\n",
    "        class_names=[ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "    )\n",
    "})\n",
    "test_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "test_run.summary[\"test_f1_macro\"] = f1\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb1f7d-d0dc-44ff-ab10-944202fcdc8b",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Extra Trials ‚Äì Ratio as HP\n",
    "\n",
    "We also tried an extended search where the **mid/extreme weight ratio** (`ratio_mid_ext`) itself was treated as a **hyperparameter**, alongside the usual learning rate, weight decay, batch size, and unfreezing depth.  \n",
    "\n",
    "üîç **Outcome:**  \n",
    "- The results did **not show consistent improvements** beyond our fixed-weight run.  \n",
    "- Best trials stayed around the same macro F1 (~0.86‚Äì0.87), without a clear advantage over the non-weighted baseline.  \n",
    "- Some mid-class F1 gains were offset by drops in other classes, leaving overall performance unchanged.\n",
    "\n",
    "---\n",
    "\n",
    "‚è≥ **Decision**: Since we are limited in time and weighted models did not prove a clear benefit, we will **not include weighted-loss variants in the final ensemble**.  \n",
    "We will continue with the **standard models** that showed stronger and more stable results in Ex.4/Ex.5.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6cc0e6e30490e6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-08-18T06:53:18.219842Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test sizes: 37039/4116/3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "[I 2025-08-18 09:53:20,497] A new study created in memory with name: no-name-c77d1994-8070-4841-86c9-c2ee6d7644f4\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35316\\3429734493.py:244: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250818_095324-7ncaxkeb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study_2/runs/7ncaxkeb' target=\"_blank\">microsoft/mdeberta-v3-base_full_ex_4_midf1_optuna_trial_0</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study_2' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study_2/runs/7ncaxkeb' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/adv-dl-p2-deberta-midf1_new_study_2/runs/7ncaxkeb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 78,561,029 / 278,813,189 (28.18%) ; unfreeze_last_k=11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_35316\\3429734493.py:303: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[e1 b1/1158] loss=1.6729 avg=1.6729 it/s=69.2\n",
      "[e1 b2/1158] loss=1.6312 avg=1.6521 it/s=108.5\n",
      "[e1 b116/1158] loss=1.5415 avg=1.6206 it/s=530.5\n",
      "[e1 b231/1158] loss=1.3327 avg=1.5464 it/s=599.9\n",
      "[e1 b346/1158] loss=1.3846 avg=1.5029 it/s=627.3\n",
      "[e1 b461/1158] loss=1.1854 avg=1.4677 it/s=639.8\n",
      "[e1 b576/1158] loss=1.3628 avg=1.4338 it/s=650.0\n",
      "[e1 b691/1158] loss=1.1391 avg=1.3957 it/s=654.0\n"
     ]
    }
   ],
   "source": [
    "# # =========================\n",
    "# # ADV DL ‚Äì Part B: Monolingual baseline (DeBERTa) ‚Äì Exercise-4 style\n",
    "# # Custom loop + early stopping + W&B + Optuna; freeze base, unfreeze last k layers\n",
    "# # Focus: improve mid-class (negative/neutral/positive) via tiered class-weighted loss\n",
    "# # Uses df_train / df_test with columns: OriginalTweet (str), Sentiment (str)\n",
    "# # =========================\n",
    "\n",
    "# import os, math, random, time, json\n",
    "# from typing import Dict, List, Tuple\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from torch.cuda.amp import autocast, GradScaler\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# # ---- deps ----\n",
    "# # !pip -q install transformers==4.43.3 optuna==3.6.1 wandb==0.17.5 >/dev/null\n",
    "\n",
    "# import transformers\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModelForSequenceClassification,\n",
    "#     DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    "# )\n",
    "\n",
    "# os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "# os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "# import optuna\n",
    "# import wandb\n",
    "\n",
    "# # -------------------------\n",
    "# # Constants (no CFG, Optuna-only workflow)\n",
    "# # -------------------------\n",
    "# MODEL_NAME = \"microsoft/mdeberta-v3-base\"\n",
    "# MAX_LEN = 512\n",
    "# BATCH_SIZE_DEFAULT = 16\n",
    "# WARMUP_RATIO_DEFAULT = 0.06\n",
    "# GRAD_CLIP_DEFAULT = 1.0\n",
    "# USE_AMP = True\n",
    "# FIXED_EPOCHS=8\n",
    "# FIXED_PATIENTS=3\n",
    "# # New W&B project & distinct run base (keeps things separate)\n",
    "# PROJECT = \"adv-dl-p2-deberta-midf1_new_study_2\"\n",
    "# BASE_RUN_NAME = \"microsoft/mdeberta-v3-base_full_ex_4_midf1\"\n",
    "\n",
    "# TRIALS = 30\n",
    "# SEED = 42\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# EARLY_ABORT_F1_E1 = 0.20  # if val macro-F1 after epoch 1 is below this ‚Üí stop the run\n",
    "\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed); np.random.seed(seed)\n",
    "#     torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# set_seed(SEED)\n",
    "\n",
    "# # ---- GPU perf toggles (Windows-safe) ----\n",
    "# torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# torch.backends.cudnn.allow_tf32 = True\n",
    "# try:\n",
    "#     torch.set_float32_matmul_precision(\"high\")\n",
    "# except Exception:\n",
    "#     pass\n",
    "\n",
    "# # -------------------------\n",
    "# # Label mapping (5-way sentiment)\n",
    "# # -------------------------\n",
    "# CANON = {\n",
    "#     \"extremely negative\": \"extremely negative\",\n",
    "#     \"negative\": \"negative\",\n",
    "#     \"neutral\": \"neutral\",\n",
    "#     \"positive\": \"positive\",\n",
    "#     \"extremely positive\": \"extremely positive\",\n",
    "# }\n",
    "# ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "# LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "# ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "# def normalize_label(s: str) -> str:\n",
    "#     s = str(s).strip().lower()\n",
    "#     s = s.replace(\"very negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"very positive\", \"extremely positive\")\n",
    "#     s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "#     return CANON.get(s, s)\n",
    "\n",
    "# # -------------------------\n",
    "# # Expect df_train, df_test in memory\n",
    "# # -------------------------\n",
    "# assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns, \"df_train missing required columns\"\n",
    "# assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns, \"df_test missing required columns\"\n",
    "\n",
    "# def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "#     df = df.copy()\n",
    "#     df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "#     df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "#     df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "#     df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "#     df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "#     return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "# dftrain_ = prep_df(df_train)\n",
    "# dftest_  = prep_df(df_test)\n",
    "\n",
    "# train_df, val_df = train_test_split(\n",
    "#     dftrain_, test_size=0.1, stratify=dftrain_[\"label\"], random_state=SEED\n",
    "# )\n",
    "# print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n",
    "\n",
    "# # -------------------------\n",
    "# # Dataset & Collator\n",
    "# # -------------------------\n",
    "# class TweetDataset(Dataset):\n",
    "#     def __init__(self, df: pd.DataFrame, tokenizer: transformers.PreTrainedTokenizerBase, max_len: int):\n",
    "#         self.texts = df[\"text\"].tolist()\n",
    "#         self.labels = df[\"label\"].tolist()\n",
    "#         self.tok = tokenizer\n",
    "#         self.max_len = max_len\n",
    "#     def __len__(self): return len(self.texts)\n",
    "#     def __getitem__(self, idx):\n",
    "#         enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "#         enc[\"labels\"] = self.labels[idx]\n",
    "#         return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "# train_ds = TweetDataset(train_df, tokenizer, MAX_LEN)\n",
    "# val_ds   = TweetDataset(val_df, tokenizer, MAX_LEN)\n",
    "# test_ds  = TweetDataset(dftest_, tokenizer, MAX_LEN)\n",
    "\n",
    "# # -------------------------\n",
    "# # Model & Freeze/Unfreeze strategy\n",
    "# # -------------------------\n",
    "# def build_model(num_unfreeze_last_layers: int = 4, dropout: float = 0.1):\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         MODEL_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "#     )\n",
    "#     # apply head-level dropout knobs\n",
    "#     model.config.hidden_dropout_prob = float(dropout)\n",
    "#     model.config.attention_probs_dropout_prob = float(dropout)\n",
    "\n",
    "#     base = getattr(model, \"roberta\", None) or getattr(model, \"bert\", None) or getattr(model, \"deberta\", None)\n",
    "#     if base is not None:\n",
    "#         for p in base.parameters(): p.requires_grad = False\n",
    "#         if hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "#             k = int(num_unfreeze_last_layers)\n",
    "#             if k > 0:\n",
    "#                 for layer in base.encoder.layer[-k:]:\n",
    "#                     for p in layer.parameters(): p.requires_grad = True\n",
    "#     for p in model.classifier.parameters(): p.requires_grad = True\n",
    "#     return model.to(DEVICE)\n",
    "\n",
    "# # -------------------------\n",
    "# # Train / Eval utilities\n",
    "# # -------------------------\n",
    "# def get_optimizer_scheduler(model, num_training_steps: int, lr: float, weight_decay: float, warmup_ratio: float):\n",
    "#     no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "#     optimizer_grouped_parameters = [\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
    "#         {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "#     ]\n",
    "#     # fused AdamW on CUDA if available\n",
    "#     try:\n",
    "#         optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay, fused=(DEVICE==\"cuda\"))\n",
    "#     except TypeError:\n",
    "#         optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr, weight_decay=weight_decay)\n",
    "#     num_warmup = int(num_training_steps * warmup_ratio)\n",
    "#     scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps)\n",
    "#     return optimizer, scheduler\n",
    "\n",
    "# # indices and mid set\n",
    "# IDX_EXT_NEG = LABEL2ID[\"extremely negative\"]\n",
    "# IDX_NEG     = LABEL2ID[\"negative\"]\n",
    "# IDX_NEU     = LABEL2ID[\"neutral\"]\n",
    "# IDX_POS     = LABEL2ID[\"positive\"]\n",
    "# IDX_EXT_POS = LABEL2ID[\"extremely positive\"]\n",
    "# MIDS = [IDX_NEG, IDX_NEU, IDX_POS]\n",
    "\n",
    "# def make_tier_weights(ratio_mid_ext: float) -> torch.Tensor:\n",
    "#     \"\"\"\n",
    "#     Assigns the same weight to both extremes, and a higher weight to mid classes.\n",
    "#     Renormalizes so the mean weight is ‚âà 1.0 (keeps loss scale stable).\n",
    "#     \"\"\"\n",
    "#     K = len(ORDER)\n",
    "#     w = np.ones(K, dtype=np.float32)                        # extremes start at 1.0\n",
    "#     w[[IDX_NEG, IDX_NEU, IDX_POS]] = float(ratio_mid_ext)   # mid classes boosted\n",
    "#     w = w * (K / w.sum())                                   # mean ~ 1.0\n",
    "#     return torch.tensor(w, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "# def evaluate(model, loader) -> Dict[str, float]:\n",
    "#     model.eval()\n",
    "#     preds, labels = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in loader:\n",
    "#             batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "#             with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "#                           enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "#                 logits = model(**batch).logits\n",
    "#             preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "#             labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "#     # mid-class F1\n",
    "#     p_mid, r_mid, f1_mid, _ = precision_recall_fscore_support(labels, preds, labels=MIDS, average=\"macro\", zero_division=0)\n",
    "#     return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1, \"f1_mid\": f1_mid}\n",
    "\n",
    "# def make_loaders(batch_size: int):\n",
    "#     collate_fn = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
    "#     train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "#     val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "#     test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "#     return train_loader, val_loader, test_loader\n",
    "\n",
    "# def train_one_run(hp: Dict) -> Tuple[str, Dict[str, float]]:\n",
    "#     \"\"\"\n",
    "#     hp keys: run_name, num_unfreeze_last_layers, lr, weight_decay, epochs, patience, trial_number,\n",
    "#              ratio_mid_ext, label_smoothing, warmup_ratio, grad_clip, dropout, batch_size\n",
    "#     \"\"\"\n",
    "#     run_name        = hp[\"run_name\"]\n",
    "#     num_unfreeze    = int(hp[\"num_unfreeze_last_layers\"])\n",
    "#     lr              = float(hp[\"lr\"])\n",
    "#     wd              = float(hp[\"weight_decay\"])\n",
    "#     epochs          = int(hp.get(\"epochs\",   FIXED_EPOCHS))\n",
    "#     patience        = int(hp.get(\"patience\", FIXED_PATIENCE))\n",
    "#     ratio_mid_ext   = float(hp.get(\"ratio_mid_ext\", 1.6))\n",
    "#     label_smoothing = float(hp.get(\"label_smoothing\", 0.05))\n",
    "#     warmup_ratio    = float(hp.get(\"warmup_ratio\", WARMUP_RATIO_DEFAULT))\n",
    "#     grad_clip       = float(hp.get(\"grad_clip\", GRAD_CLIP_DEFAULT))\n",
    "#     dropout         = float(hp.get(\"dropout\", 0.1))\n",
    "#     bs              = int(hp.get(\"batch_size\", BATCH_SIZE_DEFAULT))\n",
    "\n",
    "#     # model + loaders\n",
    "#     model = build_model(num_unfreeze, dropout=dropout)\n",
    "#     train_loader, val_loader, _ = make_loaders(bs)\n",
    "\n",
    "#     total_steps = int(math.ceil(len(train_loader) * epochs))\n",
    "#     optimizer, scheduler = get_optimizer_scheduler(model, total_steps, lr, wd, warmup_ratio)\n",
    "\n",
    "#     scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
    "#     best_metric = -1.0\n",
    "#     no_improve = 0\n",
    "\n",
    "#     # save to a different folder + name to avoid collisions\n",
    "#     safe_run_name = run_name.replace(\"/\", \"__\").replace(\"\\\\\", \"__\")\n",
    "#     ckpt_dir = \"checkpoints_midf1\"\n",
    "#     os.makedirs(ckpt_dir, exist_ok=True)\n",
    "#     best_path = os.path.join(ckpt_dir, f\"best_midf1_{safe_run_name}.pt\")\n",
    "\n",
    "#     wandb_run = wandb.init(\n",
    "#         project=PROJECT,\n",
    "#         name=run_name,\n",
    "#         config={\n",
    "#             \"model\": MODEL_NAME,\n",
    "#             \"max_len\": MAX_LEN,\n",
    "#             \"batch_size\": bs,\n",
    "#             \"epochs\": epochs,\n",
    "#             \"lr\": lr,\n",
    "#             \"weight_decay\": wd,\n",
    "#             \"warmup_ratio\": warmup_ratio,\n",
    "#             \"grad_clip\": grad_clip,\n",
    "#             \"dropout\": dropout,\n",
    "#             \"ratio_mid_ext\": ratio_mid_ext,\n",
    "#             \"label_smoothing\": label_smoothing,\n",
    "#             \"num_unfreeze_last_layers\": num_unfreeze,\n",
    "#             \"trial_number\": hp.get(\"trial_number\", -1),\n",
    "#         },\n",
    "#         reinit=True,\n",
    "#     )\n",
    "\n",
    "#     # nicer W&B charts\n",
    "#     wandb.define_metric(\"epoch\")\n",
    "#     wandb.define_metric(\"step\")\n",
    "#     wandb.define_metric(\"train/*\", step_metric=\"step\")\n",
    "#     wandb.define_metric(\"val/*\",   step_metric=\"epoch\")\n",
    "\n",
    "#     # print + log trainable params\n",
    "#     total_params     = sum(p.numel() for p in model.parameters())\n",
    "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     print(f\"Trainable params: {trainable_params:,} / {total_params:,} \"\n",
    "#           f\"({100.0*trainable_params/total_params:.2f}%) ; unfreeze_last_k={num_unfreeze}\")\n",
    "#     wandb.log({\"params/total\": total_params,\n",
    "#                \"params/trainable\": trainable_params,\n",
    "#                \"params/ratio\": trainable_params/max(1,total_params)}, step=0)\n",
    "\n",
    "#     # class weights for this run\n",
    "#     class_weights = make_tier_weights(ratio_mid_ext)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         t0 = time.time()\n",
    "#         running_loss = 0.0\n",
    "\n",
    "#         for step, batch in enumerate(train_loader):\n",
    "#             batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "#             labels = batch.pop(\"labels\")  # we compute weighted loss ourselves\n",
    "\n",
    "#             optimizer.zero_grad(set_to_none=True)\n",
    "#             with autocast(dtype=(torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16),\n",
    "#                           enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "#                 outputs = model(**batch)\n",
    "#                 logits = outputs.logits\n",
    "#                 try:\n",
    "#                     loss = F.cross_entropy(logits, labels, weight=class_weights, label_smoothing=label_smoothing)\n",
    "#                 except TypeError:\n",
    "#                     loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
    "\n",
    "#             scaler.scale(loss).backward()\n",
    "#             if grad_clip is not None:\n",
    "#                 scaler.unscale_(optimizer)\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "#             scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             if step % 20 == 0:\n",
    "#                 wandb.log({\"train/loss\": loss.item(), \"step\": step + 1, \"epoch\": epoch + 1})\n",
    "\n",
    "#             # periodic console + throughput log (about 10x per epoch)\n",
    "#             if step % max(1, len(train_loader)//10) == 0 or step == 1:\n",
    "#                 avg_loss = running_loss / max(1, (step + 1))\n",
    "#                 elapsed  = time.time() - t0\n",
    "#                 items    = (step + 1) * bs\n",
    "#                 itps     = items / max(elapsed, 1e-6)\n",
    "#                 print(f\"[e{epoch+1} b{step+1}/{len(train_loader)}] loss={loss.item():.4f} avg={avg_loss:.4f} it/s={itps:.1f}\")\n",
    "#                 wandb.log({\"train/avg_loss_so_far\": avg_loss,\n",
    "#                            \"train/items_per_sec\": itps,\n",
    "#                            \"step\": (epoch * len(train_loader)) + (step + 1),\n",
    "#                            \"epoch\": epoch + 1})\n",
    "\n",
    "#         # epoch-end validation\n",
    "#         val_metrics = evaluate(model, val_loader)\n",
    "#         elapsed = time.time() - t0\n",
    "\n",
    "#         epoch_loss = running_loss / max(1, len(train_loader))\n",
    "#         current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "#         wandb.log({\n",
    "#             \"train/epoch_loss\": epoch_loss,\n",
    "#             \"val/acc\": val_metrics[\"acc\"],\n",
    "#             \"val/precision\": val_metrics[\"precision\"],\n",
    "#             \"val/recall\": val_metrics[\"recall\"],\n",
    "#             \"val/f1\": val_metrics[\"f1\"],\n",
    "#             \"val/mid_f1\": val_metrics[\"f1_mid\"],   # extra log\n",
    "#             \"lr\": current_lr,\n",
    "#             \"time/epoch_sec\": elapsed,\n",
    "#             \"epoch\": epoch + 1,\n",
    "#         })\n",
    "#         # --- Early abort if epoch 1 macro-F1 is too low ---\n",
    "#         if epoch == 0 and val_metrics[\"f1\"] < EARLY_ABORT_F1_E1:\n",
    "#             wandb.log({\"early_stop/epoch1_low_f1\": val_metrics[\"f1\"], \"epoch\": epoch + 1})\n",
    "#             print(f\"[EARLY-EXIT] epoch=1 | val_f1={val_metrics['f1']:.4f} < {EARLY_ABORT_F1_E1:.2f} ‚Üí stopping trial\")\n",
    "#             # ensure a checkpoint exists for downstream code\n",
    "#             if not os.path.exists(best_path):\n",
    "#                 torch.save(model.state_dict(), best_path)\n",
    "#                 wandb_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "#                 wandb_run.summary[\"best_val_f1\"] = val_metrics[\"f1\"]\n",
    "#                 wandb_run.summary[\"best_val_mid_f1\"] = val_metrics[\"f1_mid\"]\n",
    "#             break\n",
    "\n",
    "#         # Early stopping on mid-class F1 (prints stay the same)\n",
    "#         target_metric = val_metrics[\"f1_mid\"]\n",
    "#         if target_metric > best_metric:\n",
    "#             best_metric = target_metric\n",
    "#             torch.save(model.state_dict(), best_path)\n",
    "#             no_improve = 0\n",
    "#             wandb_run.summary[\"best_val_f1\"] = best_metric   # kept same key for compatibility\n",
    "#             wandb_run.summary[\"best_val_mid_f1\"] = best_metric\n",
    "#             wandb_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "#             wandb.log({\"val/best_f1_so_far\": best_metric, \"val/best_epoch\": epoch + 1})\n",
    "#         else:\n",
    "#             no_improve += 1\n",
    "#             if no_improve >= patience:\n",
    "#                 print(f\"Early stopping at epoch {epoch+1}\")\n",
    "#                 break\n",
    "\n",
    "#         # console print line unchanged\n",
    "#         print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "#               f\"loss={epoch_loss:.4f} | \"\n",
    "#               f\"val_acc={val_metrics['acc']:.4f} | val_f1={val_metrics['f1']:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "#     wandb.finish()\n",
    "\n",
    "#     # Load best and return path + metrics on val for reference\n",
    "#     model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "#     # rebuild val loader (for safety if using different bs later)\n",
    "#     _, val_loader, _ = make_loaders(bs)\n",
    "#     final_val = evaluate(model, val_loader)\n",
    "\n",
    "#     # store final val in W&B summary for quick sorting\n",
    "#     if wandb.run is not None:\n",
    "#         wandb.run.summary[\"final_val_acc\"] = final_val[\"acc\"]\n",
    "#         wandb.run.summary[\"final_val_precision\"] = final_val[\"precision\"]\n",
    "#         wandb.run.summary[\"final_val_recall\"] = final_val[\"recall\"]\n",
    "#         wandb.run.summary[\"final_val_f1\"] = final_val[\"f1\"]\n",
    "#         wandb.run.summary[\"final_val_mid_f1\"] = final_val[\"f1_mid\"]\n",
    "\n",
    "#     return best_path, final_val\n",
    "\n",
    "# # -------------------------\n",
    "# # Optuna hyperparameter tuning (ALWAYS ON)\n",
    "# # -------------------------\n",
    "# FIXED_EPOCHS = 8\n",
    "# FIXED_PATIENCE = 3\n",
    "\n",
    "# def objective(trial: optuna.trial.Trial):\n",
    "#     params = {\n",
    "#         \"run_name\": f\"{BASE_RUN_NAME}_optuna_trial_{trial.number}\",\n",
    "#         \"num_unfreeze_last_layers\": trial.suggest_int(\"num_unfreeze_last_layers\", 8, 12),\n",
    "#         \"lr\": trial.suggest_float(\"lr\", 1e-5, 1e-4, log=True),\n",
    "#         \"weight_decay\": trial.suggest_float(\"weight_decay\", 7e-8, 1e-5, log=True),\n",
    "#         \"batch_size\": trial.suggest_categorical(\"batch_size\", [4,8, 16, 32]),\n",
    "#         \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.02, 0.12),\n",
    "#         \"grad_clip\": trial.suggest_float(\"grad_clip\", 0.5, 1.5),\n",
    "#         \"dropout\": trial.suggest_float(\"dropout\", 0.0, 0.2),\n",
    "#         \"ratio_mid_ext\": trial.suggest_float(\"ratio_mid_ext\", 1.2, 2.5),\n",
    "#         \"label_smoothing\": trial.suggest_float(\"label_smoothing\", 0.00, 0.12),\n",
    "#         \"epochs\": FIXED_EPOCHS,\n",
    "#         \"patience\": FIXED_PATIENCE,\n",
    "#         \"trial_number\": trial.number,\n",
    "#     }\n",
    "#     path, val_metrics = train_one_run(params)\n",
    "#     print(f\"[Trial {trial.number}] f1={val_metrics['f1']:.4f} | \"\n",
    "#           f\"unfreeze_k={params['num_unfreeze_last_layers']} lr={params['lr']:.2e} \"\n",
    "#           f\"wd={params['weight_decay']:.1e} suggested_bs={params['batch_size']}\")\n",
    "#     trial.report(val_metrics[\"f1\"], step=1)  # study objective stays macro F1\n",
    "#     return val_metrics[\"f1\"]\n",
    "\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)\n",
    "# print(\"Best trial:\", study.best_trial.number, \"F1:\", study.best_value)\n",
    "# best_params = {\"run_name\": f\"{BASE_RUN_NAME}_best_optuna\", **study.best_trial.params}\n",
    "\n",
    "# # persist best params to a separate folder/name\n",
    "# os.makedirs(\"checkpoints_midf1\", exist_ok=True)\n",
    "# with open(os.path.join(\"checkpoints_midf1\", \"best_hparams_optuna_midf1_new_code.json\"), \"w\") as f:\n",
    "#     json.dump({**best_params, \"epochs\": FIXED_EPOCHS, \"patience\": FIXED_PATIENCE}, f, indent=2)\n",
    "\n",
    "# # (Optional) retrain once on the best params to get a clean checkpoint:\n",
    "# # best_ckpt, _ = train_one_run(best_params)\n",
    "# # print(\"Best checkpoint saved to:\", best_ckpt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl4090)",
   "language": "python",
   "name": "dl4090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
