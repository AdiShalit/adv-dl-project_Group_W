{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><br>\n",
    "<font size=6>üéì <b>Advanced Deep Learning - NLP Final Project</b></font><br>\n",
    "<font size=6>‚öñÔ∏è  <b>Training - microsoft/mdeberta-v3-base EX4</b></font><br>\n",
    "<font size=5>üë• <b>Group W</b></font><br><br>\n",
    "<b>Adi Shalit</b>, ID: <code>206628885</code><br>\n",
    "<b>Gal Gussarsky</b>, ID: <code>206453540</code><br><br>\n",
    "<font size=4>üìò Course ID: <code>05714184</code></font><br>\n",
    "<font size=4>üìÖ Spring 2025</font>\n",
    "<br><br>\n",
    "<hr style=\"width:60%; border:1px solid gray;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìë Table of Contents\n",
    "\n",
    "- [Training](#Training)\n",
    "- [Load best Model & Test](#Load-Best-Model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22993,
     "status": "ok",
     "timestamp": 1755420041662,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "mEdMK2z4m5yH",
    "outputId": "aa11a38a-43d3-4724-ab55-0e8dcf931bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserName  ScreenName   Location     TweetAt  \\\n",
      "0      3799       48751     London  16-03-2020   \n",
      "1      3800       48752         UK  16-03-2020   \n",
      "2      3801       48753  Vagabonds  16-03-2020   \n",
      "3      3802       48754        NaN  16-03-2020   \n",
      "4      3803       48755        NaN  16-03-2020   \n",
      "\n",
      "                                       OriginalTweet           Sentiment  \\\n",
      "0            @MeNyrbie @Phil_Gahan @Chrisitv and and             Neutral   \n",
      "1  advice Talk to your neighbours family to excha...            Positive   \n",
      "2  covid Australia: Woolworths to give elderly, d...            Positive   \n",
      "3  My food stock is not the only one which is emp...            Positive   \n",
      "4  Me, ready to go at supermarket during the covi...  Extremely Negative   \n",
      "\n",
      "  DetectedLang  \n",
      "0           en  \n",
      "1           en  \n",
      "2           en  \n",
      "3           en  \n",
      "4           en  \n",
      "   UserName  ScreenName             Location     TweetAt  \\\n",
      "0         1       44953                  NYC  02-03-2020   \n",
      "1         2       44954          Seattle, WA  02-03-2020   \n",
      "2         3       44955                  NaN  02-03-2020   \n",
      "3         4       44956          Chicagoland  02-03-2020   \n",
      "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
      "\n",
      "                                       OriginalTweet           Sentiment  \\\n",
      "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative   \n",
      "1  When I couldn't find hand sanitizer at Fred Me...            Positive   \n",
      "2  Find out how you can protect yourself and love...  Extremely Positive   \n",
      "3  Panic buying hits NewYork City as anxious shop...            Negative   \n",
      "4  toiletpaper dunnypaper covid coronavirusaustra...             Neutral   \n",
      "\n",
      "  DetectedLang  \n",
      "0           en  \n",
      "1           en  \n",
      "2           en  \n",
      "3           en  \n",
      "4           en  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Paths to your CSV files\n",
    "train_path = \"Corona_NLP_train_cleaned_translated.csv\"\n",
    "test_path  = \"Corona_NLP_test_cleaned_translated.csv\"\n",
    "\n",
    "# Load into DataFrames\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "# Check first rows\n",
    "print(df_train.head())\n",
    "print(df_test.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6>üìä <b>Training ‚Äî RoBERTa-base (Exercise‚Äë4 Style)</b></font><br>\n",
    "\n",
    "**Why RoBERTa?**  \n",
    "RoBERTa (‚ÄúRobustly Optimized BERT‚Äù) is a 12‚Äëlayer Transformer **encoder** (hidden size 768, 12 heads) trained with **dynamic masking**, **longer sequences**, **bigger batches**, and **no NSP**. In practice, it‚Äôs a strong monolingual baseline that usually converges fast and gives reliable sentiment results.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Goal\n",
    "Build a **clean monolingual baseline** on our 5 sentiment labels, using a **custom training loop** so we can control freezing, logging, and early stopping just like Ex.4.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© What we do \n",
    "- üß† **Backbone**: `roberta-base` + a linear **classification head** (5 classes).  \n",
    "- üßä **Freeze ‚Üí Unfreeze**: freeze the base and **unfreeze the last *k* encoder blocks** (k is a hyperparameter).  \n",
    "- ‚öôÔ∏è **Custom loop**: PyTorch + AMP (mixed precision), gradient clipping, linear LR warmup/decay.  \n",
    "- üõë **Early stopping**: on **validation macro‚ÄëF1** to avoid overfitting.  \n",
    "- üîé **Optuna search**: tune **LR**, **weight decay**, **epochs**, **patience**, and **unfreeze depth (k)**.  \n",
    "- üìí **W&B tracking**: log losses, metrics, best checkpoint path, and final test scores.  \n",
    "- üß™ **Data split**: stratified Train/Val from `df_train`, then evaluate once on `df_test`.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What we expect\n",
    "- Best LR in the classic **1e‚Äë6 ‚Üí 5e‚Äë5** band.  \n",
    "- **Deeper unfreezing** (larger *k*) tends to help once the head stabilizes.  \n",
    "- Macro‚ÄëF1 is our north star; we‚Äôll compare this RoBERTa baseline to mDeBERTa later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79WBH74mjdro"
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ADV DL ‚Äì Part B: Monolingual baseline (RoBERTa) ‚Äì Exercise-4 style\n",
    "# Custom loop + early stopping + W&B + Optuna ONLY; freeze base, unfreeze last k layers\n",
    "# Uses df_train / df_test with columns: OriginalTweet (str), Sentiment (str)\n",
    "# =========================\n",
    "\n",
    "import os, math, random, time, json\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# ---- deps ----\n",
    "!pip -q install transformers==4.43.3 optuna==3.6.1 wandb==0.17.5 >/dev/null\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")\n",
    "DRIVE_OUT_DIR = \"adv_dl_models\"\n",
    "os.makedirs(DRIVE_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Constants (no CFG, Optuna-only workflow)\n",
    "# -------------------------\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "WARMUP_RATIO = 0.06\n",
    "GRAD_CLIP = 1.0\n",
    "USE_AMP = True\n",
    "PROJECT = \"adv-dl-p1\"\n",
    "BASE_RUN_NAME = \"roberta-base_ex4_style\"\n",
    "TRIALS = 12\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Label mapping (5-way sentiment)\n",
    "# -------------------------\n",
    "CANON = {\n",
    "    \"extremely negative\": \"extremely negative\",\n",
    "    \"negative\": \"negative\",\n",
    "    \"neutral\": \"neutral\",\n",
    "    \"positive\": \"positive\",\n",
    "    \"extremely positive\": \"extremely positive\",\n",
    "}\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return CANON.get(s, s)\n",
    "\n",
    "# -------------------------\n",
    "# Expect df_train, df_test in memory\n",
    "# -------------------------\n",
    "assert \"OriginalTweet\" in df_train.columns and \"Sentiment\" in df_train.columns, \"df_train missing required columns\"\n",
    "assert \"OriginalTweet\" in df_test.columns and \"Sentiment\" in df_test.columns, \"df_test missing required columns\"\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "dftrain_ = prep_df(df_train)\n",
    "dftest_  = prep_df(df_test)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    dftrain_, test_size=0.1, stratify=dftrain_[\"label\"], random_state=SEED\n",
    ")\n",
    "print(f\"Train/Val/Test sizes: {len(train_df)}/{len(val_df)}/{len(dftest_)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Dataset & Collator\n",
    "# -------------------------\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: transformers.PreTrainedTokenizerBase, max_len: int):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "        enc[\"labels\"] = self.labels[idx]\n",
    "        return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "train_ds = TweetDataset(train_df, tokenizer, MAX_LEN)\n",
    "val_ds   = TweetDataset(val_df, tokenizer, MAX_LEN)\n",
    "test_ds  = TweetDataset(dftest_, tokenizer, MAX_LEN)\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "# -------------------------\n",
    "# Model & Freeze/Unfreeze strategy\n",
    "# -------------------------\n",
    "def build_model(num_unfreeze_last_layers: int = 4):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    )\n",
    "    base = getattr(model, \"roberta\", None) or getattr(model, \"bert\", None)\n",
    "    if base is not None:\n",
    "        for p in base.parameters(): p.requires_grad = False\n",
    "        if hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "            k = num_unfreeze_last_layers\n",
    "            if k > 0:\n",
    "                for layer in base.encoder.layer[-k:]:\n",
    "                    for p in layer.parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Train / Eval utilities\n",
    "# -------------------------\n",
    "def get_optimizer_scheduler(model, num_training_steps: int, lr: float, weight_decay: float):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_grouped_parameters = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "    num_warmup = int(num_training_steps * WARMUP_RATIO)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup, num_training_steps=num_training_steps)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "def evaluate(model, loader) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "            labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    return {\"acc\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "def train_one_run(hp: Dict) -> Tuple[str, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    hp keys: run_name, num_unfreeze_last_layers, lr, weight_decay, epochs, patience, trial_number\n",
    "    \"\"\"\n",
    "    run_name = hp[\"run_name\"]\n",
    "    num_unfreeze = int(hp[\"num_unfreeze_last_layers\"])\n",
    "    lr = float(hp[\"lr\"])\n",
    "    wd = float(hp[\"weight_decay\"])\n",
    "    epochs = int(hp[\"epochs\"])\n",
    "    patience = int(hp[\"patience\"])\n",
    "\n",
    "    model = build_model(num_unfreeze)\n",
    "    total_steps = int(math.ceil(len(train_loader) * epochs))\n",
    "    optimizer, scheduler = get_optimizer_scheduler(model, total_steps, lr, wd)\n",
    "\n",
    "    scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
    "    best_metric = -1.0\n",
    "    best_path = os.path.join(DRIVE_OUT_DIR, f\"best_{run_name}.pt\")\n",
    "    no_improve = 0\n",
    "\n",
    "    wandb_run = wandb.init(\n",
    "        project=PROJECT,\n",
    "        name=run_name,\n",
    "        config={\n",
    "            \"model\": MODEL_NAME,\n",
    "            \"max_len\": MAX_LEN,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": epochs,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": wd,\n",
    "            \"warmup_ratio\": WARMUP_RATIO,\n",
    "            \"grad_clip\": GRAD_CLIP,\n",
    "            \"num_unfreeze_last_layers\": num_unfreeze,\n",
    "        },\n",
    "        reinit=True,\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        t0 = time.time()\n",
    "        running_loss = 0.0\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "                outputs = model(**batch)\n",
    "                loss = outputs.loss\n",
    "            scaler.scale(loss).backward()\n",
    "            if GRAD_CLIP is not None:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "            scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % 20 == 0:\n",
    "                wandb.log({\"train/loss\": loss.item(), \"step\": step + 1, \"epoch\": epoch + 1})\n",
    "\n",
    "        # epoch-end validation\n",
    "        val_metrics = evaluate(model, val_loader)\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        epoch_loss = running_loss / max(1, len(train_loader))\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        wandb.log({\n",
    "            \"train/epoch_loss\": epoch_loss,\n",
    "            \"val/acc\": val_metrics[\"acc\"],\n",
    "            \"val/precision\": val_metrics[\"precision\"],\n",
    "            \"val/recall\": val_metrics[\"recall\"],\n",
    "            \"val/f1\": val_metrics[\"f1\"],\n",
    "            \"lr\": current_lr,\n",
    "            \"time/epoch_sec\": elapsed,\n",
    "            \"epoch\": epoch + 1,\n",
    "        })\n",
    "\n",
    "        # Early stopping on val f1\n",
    "        if val_metrics[\"f1\"] > best_metric:\n",
    "            best_metric = val_metrics[\"f1\"]\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            no_improve = 0\n",
    "            wandb_run.summary[\"best_val_f1\"] = best_metric\n",
    "            wandb_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "              f\"loss={epoch_loss:.4f} | \"\n",
    "              f\"val_acc={val_metrics['acc']:.4f} | val_f1={val_metrics['f1']:.4f} | time={elapsed:.1f}s\")\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # Load best and return path + metrics on val for reference\n",
    "    model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "    final_val = evaluate(model, val_loader)\n",
    "    return best_path, final_val\n",
    "\n",
    "# -------------------------\n",
    "# Optuna hyperparameter tuning (ALWAYS ON)\n",
    "# -------------------------\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    params = {\n",
    "        \"run_name\": f\"{BASE_RUN_NAME}_optuna_trial_{trial.number}\",\n",
    "        \"num_unfreeze_last_layers\": trial.suggest_int(\"num_unfreeze_last_layers\", 1, 6),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-6, 5e-5, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-1, log=True),\n",
    "        \"epochs\": trial.suggest_int(\"epochs\", 4, 12),\n",
    "        \"patience\": trial.suggest_int(\"patience\", 1, 4),\n",
    "        \"trial_number\": trial.number,\n",
    "    }\n",
    "    path, val_metrics = train_one_run(params)\n",
    "    # report intermediate value for pruning if enabled\n",
    "    trial.report(val_metrics[\"f1\"], step=1)\n",
    "    return val_metrics[\"f1\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)\n",
    "print(\"Best trial:\", study.best_trial.number, \"F1:\", study.best_value)\n",
    "best_params = {\"run_name\": f\"{BASE_RUN_NAME}_best_optuna\", **study.best_trial.params}\n",
    "\n",
    "# Retrain best config to get a clean checkpoint\n",
    "best_ckpt, _ = train_one_run(best_params)\n",
    "best_path = best_ckpt\n",
    "\n",
    "# -------------------------\n",
    "# Final evaluation on TEST (+ W&B logging)\n",
    "# -------------------------\n",
    "model = build_model(best_params[\"num_unfreeze_last_layers\"])\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        logits = model(**batch).logits\n",
    "        all_preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "print(f\"\\nTEST | acc={acc:.4f} | f1_macro={f1:.4f} | precision_macro={p:.4f} | recall_macro={r:.4f}\\n\")\n",
    "\n",
    "print(\"Per-class report (ids map to labels):\")\n",
    "print(ID2LABEL)\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# # ---- W&B: log test metrics, per-class scores, and confusion matrix ----\n",
    "# test_run = wandb.init(project=PROJECT, name=f\"{BASE_RUN_NAME}_test\", resume=\"allow\", reinit=True)\n",
    "# log_payload = {\n",
    "#     \"test/acc\": acc,\n",
    "#     \"test/precision_macro\": p,\n",
    "#     \"test/recall_macro\": r,\n",
    "#     \"test/f1_macro\": f1,\n",
    "# }\n",
    "# for cls_name in ORDER:\n",
    "#     if cls_name in report:\n",
    "#         log_payload[f\"test/{cls_name}/precision\"] = report[cls_name][\"precision\"]\n",
    "#         log_payload[f\"test/{cls_name}/recall\"]    = report[cls_name][\"recall\"]\n",
    "#         log_payload[f\"test/{cls_name}/f1\"]        = report[cls_name][\"f1-score\"]\n",
    "\n",
    "# wandb.log(log_payload)\n",
    "\n",
    "# cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(ORDER))))\n",
    "# wandb.log({\n",
    "#     \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "#         y_true=all_labels,\n",
    "#         preds=all_preds,\n",
    "#         class_names=[ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "#     )\n",
    "# })\n",
    "# test_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "# test_run.summary[\"test_f1_macro\"] = f1\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXIOGDM0YJkr"
   },
   "source": [
    "# üîß Refined Training Study ‚Äì RoBERTa (EX4 Style)\n",
    "\n",
    "From the previous runs we saw that **freezing more layers improved performance** (easy to observe from the W&B hyperparameter graphs üìà).  \n",
    "\n",
    "In this refined study we make two important adjustments:  \n",
    "- ‚è≥ **Fixed Epochs & Patience**: to keep results consistent and reduce randomness from early stopping.  \n",
    "- üìâ **Expanded Learning Rate Range**: Optuna now searches up to **7e-5**, giving a wider window to explore.  \n",
    "\n",
    "Additional details:  \n",
    "- üßä **Frozen Layers**: number of unfreezed layers remains in the [4‚Äì6] range.  \n",
    "- ‚öñÔ∏è **Weight Decay**: still optimized in [1e-6, 1e-1].  \n",
    "- üì¶ **Batch Size**: multiple options considered [4, 8, 16, 32, 64].  \n",
    "\n",
    "This setup should allow us to better capture the trade-off between **learning rate**, **frozen layers**, and **batch size**, while keeping training length controlled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wFchgrnngHJy"
   },
   "outputs": [],
   "source": [
    "\n",
    "PROJECT = \"adv-dl-p2\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Optuna hyperparameter tuning (ALWAYS ON)\n",
    "# -------------------------\n",
    "\n",
    "# Constants\n",
    "FIXED_EPOCHS = 12\n",
    "FIXED_PATIENCE = 4\n",
    "\n",
    "\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    params = {\n",
    "        \"run_name\": f\"{BASE_RUN_NAME}_optuna_trial_{trial.number}\",\n",
    "        \"num_unfreeze_last_layers\": trial.suggest_int(\"num_unfreeze_last_layers\", 4, 6),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-6, 7e-5, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-6, 1e-1, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32, 64]),\n",
    "        \"epochs\": FIXED_EPOCHS,\n",
    "        \"patience\": FIXED_PATIENCE,\n",
    "        \"trial_number\": trial.number,\n",
    "    }\n",
    "    path, val_metrics = train_one_run(params)\n",
    "    # report intermediate value for pruning if enabled\n",
    "    trial.report(val_metrics[\"f1\"], step=1)\n",
    "    return val_metrics[\"f1\"]\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=TRIALS, show_progress_bar=True)\n",
    "print(\"Best trial:\", study.best_trial.number, \"F1:\", study.best_value)\n",
    "best_params = {\"run_name\": f\"{BASE_RUN_NAME}_best_optuna\", **study.best_trial.params}\n",
    "\n",
    "# Retrain best config to get a clean checkpoint\n",
    "best_ckpt, _ = train_one_run(best_params)\n",
    "best_path = best_ckpt\n",
    "\n",
    "# -------------------------\n",
    "# Final evaluation on TEST (+ W&B logging)\n",
    "# -------------------------\n",
    "model = build_model(best_params[\"num_unfreeze_last_layers\"])\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        logits = model(**batch).logits\n",
    "        all_preds.extend(torch.argmax(logits, dim=-1).detach().cpu().tolist())\n",
    "        all_labels.extend(batch[\"labels\"].detach().cpu().tolist())\n",
    "\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "p, r, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "print(f\"\\nTEST | acc={acc:.4f} | f1_macro={f1:.4f} | precision_macro={p:.4f} | recall_macro={r:.4f}\\n\")\n",
    "\n",
    "print(\"Per-class report (ids map to labels):\")\n",
    "print(ID2LABEL)\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# # ---- W&B: log test metrics, per-class scores, and confusion matrix ----\n",
    "# test_run = wandb.init(project=PROJECT, name=f\"{BASE_RUN_NAME}_test\", resume=\"allow\", reinit=True)\n",
    "# log_payload = {\n",
    "#     \"test/acc\": acc,\n",
    "#     \"test/precision_macro\": p,\n",
    "#     \"test/recall_macro\": r,\n",
    "#     \"test/f1_macro\": f1,\n",
    "# }\n",
    "# for cls_name in ORDER:\n",
    "#     if cls_name in report:\n",
    "#         log_payload[f\"test/{cls_name}/precision\"] = report[cls_name][\"precision\"]\n",
    "#         log_payload[f\"test/{cls_name}/recall\"]    = report[cls_name][\"recall\"]\n",
    "#         log_payload[f\"test/{cls_name}/f1\"]        = report[cls_name][\"f1-score\"]\n",
    "\n",
    "# wandb.log(log_payload)\n",
    "\n",
    "# cm = confusion_matrix(all_labels, all_preds, labels=list(range(len(ORDER))))\n",
    "# wandb.log({\n",
    "#     \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "#         y_true=all_labels,\n",
    "#         preds=all_preds,\n",
    "#         class_names=[ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "#     )\n",
    "# })\n",
    "# test_run.summary[\"best_checkpoint_path\"] = best_path\n",
    "# test_run.summary[\"test_f1_macro\"] = f1\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxhZEa_-bpzu"
   },
   "source": [
    "# üìä Training Results ‚Äì Refined Study (RoBERTa EX4 Style)\n",
    "\n",
    "‚úÖ The results indicate that **small batch sizes** and a **higher number of unfrozen layers** lead to better performance.  \n",
    "It is possible that unfreezing even more layers could further improve results, but we decided to **stick with 6 layers** to balance the **performance‚Äìcomputation trade-off**.  \n",
    "\n",
    "---\n",
    "\n",
    "### üèÜ Best Trial Hyperparameters\n",
    "```json\n",
    "{\n",
    "  \"num_unfreeze_last_layers\": 6,\n",
    "  \"lr\": 4.2813e-5,\n",
    "  \"weight_decay\": 6.2665e-6,\n",
    "  \"batch_size\": 4,\n",
    "  \"epochs\": 12,\n",
    "  \"patience\": 4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üèÅ Final Training ‚Äì RoBERTa-base (Best Optuna HP)\n",
    "\n",
    "After completing our hyperparameter search, we now perform a **one-shot full training** run using the **best trial parameters**.  \n",
    "All results are logged to **Weights & Biases (W&B)** for tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Final Hyperparameters\n",
    "- **Model**: `roberta-base`  \n",
    "- **Max length**: 512  \n",
    "- **Batch size**: 4  \n",
    "- **Learning rate**: 4.28e-5  \n",
    "- **Weight decay**: 6.27e-6  \n",
    "- **Unfrozen layers**: last 6  \n",
    "- **Epochs**: 12 (with early stopping, patience = 4)  \n",
    "- **Optimizer**: AdamW + linear warmup (6% steps)  \n",
    "- **Mixed Precision**: ‚úÖ (AMP)  \n",
    "- **Grad Clipping**: 1.0  \n",
    "\n",
    "---\n",
    "\n",
    "## üìä Training Setup\n",
    "- üîí Freeze most RoBERTa layers, unfreeze **last 6** for fine-tuning.  \n",
    "- üè∑ 5-way sentiment classification head.  \n",
    "- ‚è≥ Early stopping on validation F1 (patience = 4).  \n",
    "- üìà W&B logs include **loss curves, F1/accuracy per epoch, and confusion matrix**.  \n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Final Test Evaluation\n",
    "At the end of training, we load the **best checkpoint** and evaluate on the test set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "363141a7095c42f0ab1146b7733bca9f",
      "6bb81d47abfb44d9b1cd82e24ac6ea73",
      "bb25fa4befbb428ca972a4c5c33cd671",
      "709c561436624f91b4acf75c8920c58a",
      "05edc0eb3ecd4d449ddb600647ff47ec",
      "41268f513e7d41ad93db45c38f81533f",
      "a78dbd87c8ae42bba1ef0da00273080b",
      "647ee239889d47beb87b8b31264568e4",
      "313ba92cfeb94480837d73393b326b43",
      "f709533f9b7641dfaf73f63ff6c80e7f",
      "6babdae5c57c4a7ba9f7ec912a2f568f",
      "78a70153092e47728d4affb788c74119",
      "ff80485bbe3040beaf7a651e8e68a780",
      "3aeb3346681f40b4a1f4ce8b00943054",
      "74a33ec0722c4f0d97914b771259a07d",
      "1c313a418b324ff09ce9e469929673ba",
      "217cabf489e84e85916004c71d8d1e35",
      "65bb4463bad3482abccaa7b2bf700713",
      "ca88eef9d32b4310a03038850ba52c33",
      "b082a4b5882a40289aca95326b0ff38a",
      "494dbd6d70424741a276d29dd0544d4d",
      "36fb010cb96d4ccd9d2725d862da9fcc",
      "09a6016375074951b7856a4f08d6547f",
      "1cdae25579f3453ab68ae228ac973c6a",
      "87675490f67a4da5b88aeff2085ca8a2",
      "d2f403a4529e4665b5aa7a526dd9fd74",
      "87c2257ccc1740d7ab6345ecf44d8e68",
      "2ea1e0f2042d4093b544d338545a4cdb",
      "90abbcf547cb4619866fe38b0e4081a8",
      "8ba0715e6a3c4615a84f50d2e6e643c6",
      "f7a9bb98db994c32840f6fefe8e29a28",
      "9d2e8534a09d4d9e82db35e48da67afc",
      "97bffe63fd0749d6944802fa0376ae96",
      "c449e800bcca4022b8ef533ed819c8e3",
      "30e3cb72ae974b4aa7b8bb8394496ca2",
      "61d12c7e44df4b9dadbff87e3b211f6d",
      "9f2c89e1526048248a27c2bbef0f49fd",
      "8181e155f1054a7b9dcd3f8f1a996ea1",
      "536cd8b50bf8481dbd4ed61c15c62da7",
      "39a38537dadb4d588d894084ad404e89",
      "1651a7ab14ca49108bcf0833c7789f82",
      "9efa883a47ab4b6dba438e3b61bc3b64",
      "2c364055efba4f07b85504205d51215e",
      "a7e8bb424c7845b5a339d3b5463907c4",
      "6a14d43268884bb4a43c68dfc790e027",
      "fc0a6590119542c4adf425514f993957",
      "bc86d9c1c671473282f5e6cc799a844d",
      "4739868b580d498cb1b668cf7f32316a",
      "67f7c6a839b746229b82dee0257ec4db",
      "0d66f3d69dff49b6a0043673a8cdb562",
      "6bb7a81447c0447fa4ff1700f1d84ab2",
      "4ad0a9c02cd6410f8e7d947bcd1f7d86",
      "1bb73de03fa6497498a557283677108c",
      "fc7bae0a78074fa0ae06e1165142b8dc",
      "ff053753110244da96c1a5b46301616f",
      "26f038a0655b4398b42e0063e5a4d1e5",
      "072c7e0ec3ac4169b7e0f8e9a7d9b44a",
      "edc327dc3f674e76a5b633c75572abf8",
      "f071a276190946f5867989f083f7cf15",
      "e1719e93d28e4d75b419c4d37cae8021",
      "048d1f185d4c400e841f518bafdef807",
      "bd342092f1fa4930ac1e00832e8226e4",
      "e2c3ac1fb5fc426581b80cbef7b18526",
      "277b95a31f1649e885941cc5ea1b3a39",
      "d6ba3986c2fa4147b96b02aaf8fe91f0",
      "a8400ca460224ad5af700763c561f558"
     ]
    },
    "executionInfo": {
     "elapsed": 7486529,
     "status": "ok",
     "timestamp": 1755164132614,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "cEEli3PmewoJ",
    "outputId": "20e55875-edbd-4e3f-c55a-7f66f8025726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "363141a7095c42f0ab1146b7733bca9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a70153092e47728d4affb788c74119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a6016375074951b7856a4f08d6547f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c449e800bcca4022b8ef533ed819c8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a14d43268884bb4a43c68dfc790e027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f038a0655b4398b42e0063e5a4d1e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipython-input-2654536824.py:162: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
      "wandb: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgal2361\u001b[0m (\u001b[33mgal2361-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/content/wandb/run-20250814_073305-fs67x2jm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3/runs/fs67x2jm' target=\"_blank\">roberta_base_best_manual</a></strong> to <a href='https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3' target=\"_blank\">https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3/runs/fs67x2jm' target=\"_blank\">https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3/runs/fs67x2jm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
      "/tmp/ipython-input-2654536824.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST METRICS:\n",
      "Accuracy: 0.7725\n",
      "F1 Macro: 0.7794\n",
      "Precision: 0.7786\n",
      "Recall: 0.7805\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñá‚ñá‚ñà</td></tr><tr><td>test/acc</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/f1</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/precision</td><td>‚ñÅ</td></tr><tr><td>test/extremely negative/recall</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/f1</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/precision</td><td>‚ñÅ</td></tr><tr><td>test/extremely positive/recall</td><td>‚ñÅ</td></tr><tr><td>test/f1_macro</td><td>‚ñÅ</td></tr><tr><td>test/negative/f1</td><td>‚ñÅ</td></tr><tr><td>test/negative/precision</td><td>‚ñÅ</td></tr><tr><td>test/negative/recall</td><td>‚ñÅ</td></tr><tr><td>test/neutral/f1</td><td>‚ñÅ</td></tr><tr><td>test/neutral/precision</td><td>‚ñÅ</td></tr><tr><td>test/neutral/recall</td><td>‚ñÅ</td></tr><tr><td>test/positive/f1</td><td>‚ñÅ</td></tr><tr><td>test/positive/precision</td><td>‚ñÅ</td></tr><tr><td>test/positive/recall</td><td>‚ñÅ</td></tr><tr><td>test/precision_macro</td><td>‚ñÅ</td></tr><tr><td>test/recall_macro</td><td>‚ñÅ</td></tr><tr><td>train/acc</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/f1</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÅ‚ñÅ</td></tr><tr><td>train/precision</td><td>‚ñÅ‚ñÉ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/recall</td><td>‚ñÅ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>12</td></tr><tr><td>test/acc</td><td>0.77251</td></tr><tr><td>test/extremely negative/f1</td><td>0.80336</td></tr><tr><td>test/extremely negative/precision</td><td>0.79933</td></tr><tr><td>test/extremely negative/recall</td><td>0.80743</td></tr><tr><td>test/extremely positive/f1</td><td>0.81657</td></tr><tr><td>test/extremely positive/precision</td><td>0.82705</td></tr><tr><td>test/extremely positive/recall</td><td>0.80634</td></tr><tr><td>test/f1_macro</td><td>0.77942</td></tr><tr><td>test/negative/f1</td><td>0.74709</td></tr><tr><td>test/negative/precision</td><td>0.75367</td></tr><tr><td>test/negative/recall</td><td>0.74063</td></tr><tr><td>test/neutral/f1</td><td>0.78549</td></tr><tr><td>test/neutral/precision</td><td>0.76733</td></tr><tr><td>test/neutral/recall</td><td>0.80452</td></tr><tr><td>test/positive/f1</td><td>0.74458</td></tr><tr><td>test/positive/precision</td><td>0.74576</td></tr><tr><td>test/positive/recall</td><td>0.7434</td></tr><tr><td>test/precision_macro</td><td>0.77863</td></tr><tr><td>test/recall_macro</td><td>0.78047</td></tr><tr><td>train/acc</td><td>0.98428</td></tr><tr><td>train/f1</td><td>0.98442</td></tr><tr><td>train/loss</td><td>0.17386</td></tr><tr><td>train/precision</td><td>0.98411</td></tr><tr><td>train/recall</td><td>0.98487</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta_base_best_manual</strong> at: <a href='https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3/runs/fs67x2jm' target=\"_blank\">https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3/runs/fs67x2jm</a><br> View project at: <a href='https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3' target=\"_blank\">https://wandb.ai/gal2361-tel-aviv-university/adv-dl-p3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250814_073305-fs67x2jm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# ADV DL ‚Äì One-shot RoBERTa training using best Optuna params\n",
    "# Logs everything to W&B\n",
    "# =========================\n",
    "\n",
    "import os, math, time, random\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "DRIVE_OUT_DIR = \"adv_dl_models_final\"\n",
    "os.makedirs(DRIVE_OUT_DIR, exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Constants / HPs\n",
    "# -------------------------\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "PROJECT = \"adv-dl-p3\"\n",
    "RUN_NAME = \"roberta_base_best_manual\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "LR = 4.2813e-5\n",
    "WEIGHT_DECAY = 6.2665e-6\n",
    "NUM_UNFREEZE = 6\n",
    "EPOCHS = 12\n",
    "PATIENCE = 4\n",
    "GRAD_CLIP = 1.0\n",
    "WARMUP_RATIO = 0.06\n",
    "USE_AMP = True\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Labels\n",
    "# -------------------------\n",
    "ORDER = [\"extremely negative\", \"negative\", \"neutral\", \"positive\", \"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "CANON = {k: k for k in ORDER}\n",
    "\n",
    "\n",
    "train_df = prep_df(df_train)\n",
    "test_df  = prep_df(df_test)\n",
    "\n",
    "# -------------------------\n",
    "# Dataset & Tokenization\n",
    "# -------------------------\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "        enc[\"labels\"] = self.labels[idx]\n",
    "        return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_ds = TweetDataset(train_df, tokenizer, MAX_LEN)\n",
    "test_ds  = TweetDataset(test_df, tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "\n",
    "# -------------------------\n",
    "# Model Setup (Freeze + Unfreeze last k)\n",
    "# -------------------------\n",
    "def build_model(num_unfreeze):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    )\n",
    "    base = getattr(model, \"roberta\", None) or getattr(model, \"bert\", None)\n",
    "    if base:\n",
    "        for p in base.parameters(): p.requires_grad = False\n",
    "        if hasattr(base, \"encoder\") and hasattr(base.encoder, \"layer\"):\n",
    "            for layer in base.encoder.layer[-num_unfreeze:]:\n",
    "                for p in layer.parameters(): p.requires_grad = True\n",
    "    for p in model.classifier.parameters(): p.requires_grad = True\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer + Scheduler\n",
    "# -------------------------\n",
    "def get_optimizer_scheduler(model, total_steps, lr, weight_decay):\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    grouped = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(grouped, lr=lr)\n",
    "    warmup_steps = int(WARMUP_RATIO * total_steps)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation\n",
    "# -------------------------\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            preds.extend(logits.argmax(dim=-1).cpu().tolist())\n",
    "            labels.extend(batch[\"labels\"].cpu().tolist())\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\", zero_division=0)\n",
    "    return acc, p, r, f1, labels, preds\n",
    "\n",
    "# -------------------------\n",
    "# Train + Save Best\n",
    "# -------------------------\n",
    "model = build_model(NUM_UNFREEZE)\n",
    "steps_per_epoch = math.ceil(len(train_loader))\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "optimizer, scheduler = get_optimizer_scheduler(model, total_steps, LR, WEIGHT_DECAY)\n",
    "scaler = GradScaler(enabled=(DEVICE == \"cuda\" and USE_AMP))\n",
    "\n",
    "best_f1, no_improve = -1, 0\n",
    "best_path = os.path.join(DRIVE_OUT_DIR, f\"{RUN_NAME}.pt\")\n",
    "\n",
    "wandb.init(project=PROJECT, name=RUN_NAME, reinit=True, config={\n",
    "    \"lr\": LR,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"num_unfreeze\": NUM_UNFREEZE\n",
    "})\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    losses = []\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast(enabled=(DEVICE == \"cuda\" and USE_AMP)):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "        scaler.scale(loss).backward()\n",
    "        if GRAD_CLIP:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
    "        scaler.step(optimizer); scaler.update(); scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    avg_loss = np.mean(losses)\n",
    "    acc, p, r, f1, _, _ = evaluate(model, train_loader)\n",
    "    wandb.log({\n",
    "        \"train/loss\": avg_loss,\n",
    "        \"train/acc\": acc,\n",
    "        \"train/precision\": p,\n",
    "        \"train/recall\": r,\n",
    "        \"train/f1\": f1,\n",
    "        \"epoch\": epoch + 1\n",
    "    })\n",
    "\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        no_improve = 0\n",
    "        torch.save(model.state_dict(), best_path)\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# -------------------------\n",
    "# Final Test Evaluation + W&B Logging\n",
    "# -------------------------\n",
    "model.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "acc, p, r, f1, all_labels, all_preds = evaluate(model, test_loader)\n",
    "\n",
    "print(f\"\\nTEST METRICS:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"F1 Macro: {f1:.4f}\")\n",
    "print(f\"Precision: {p:.4f}\")\n",
    "print(f\"Recall: {r:.4f}\")\n",
    "\n",
    "report = classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=[ID2LABEL[i] for i in range(len(ORDER))],\n",
    "    zero_division=0, output_dict=True\n",
    ")\n",
    "\n",
    "wandb.log({\n",
    "    \"test/acc\": acc,\n",
    "    \"test/precision_macro\": p,\n",
    "    \"test/recall_macro\": r,\n",
    "    \"test/f1_macro\": f1,\n",
    "    \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "        y_true=all_labels,\n",
    "        preds=all_preds,\n",
    "        class_names=[ID2LABEL[i] for i in range(len(ID2LABEL))]\n",
    "    )\n",
    "})\n",
    "for cls in ORDER:\n",
    "    if cls in report:\n",
    "        wandb.log({\n",
    "            f\"test/{cls}/precision\": report[cls][\"precision\"],\n",
    "            f\"test/{cls}/recall\": report[cls][\"recall\"],\n",
    "            f\"test/{cls}/f1\": report[cls][\"f1-score\"]\n",
    "        })\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrweeTPtnQI0"
   },
   "source": [
    "Creating a detailed classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31195,
     "status": "ok",
     "timestamp": 1755175624646,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "bwz08csFnOUu",
    "outputId": "46853b54-97e1-417d-f9d6-02dddfc920f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\adishalit1\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative       0.80      0.81      0.80       592\n",
      "          negative       0.75      0.74      0.75      1041\n",
      "           neutral       0.77      0.80      0.79       619\n",
      "          positive       0.75      0.74      0.74       947\n",
      "extremely positive       0.83      0.81      0.82       599\n",
      "\n",
      "          accuracy                           0.77      3798\n",
      "         macro avg       0.78      0.78      0.78      3798\n",
      "      weighted avg       0.77      0.77      0.77      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "# from google.colab import drive\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -----------------------------\n",
    "# Mount Google Drive\n",
    "# -----------------------------\n",
    "# drive.mount(\"/content/drive\")\n",
    "# -------------------------\n",
    "# Constants\n",
    "# -------------------------\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "MODEL_PATH = \"adv_dl_models_final/roberta_base_best_manual.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "ORDER = [\"extremely negative\", \"negative\", \"neutral\", \"positive\", \"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in enumerate(ORDER)}\n",
    "\n",
    "# -------------------------\n",
    "# Load Tokenizer + Model\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(ORDER))\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# Data Preparation\n",
    "# -------------------------\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return s\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "# Load your test dataframe\n",
    "df_test = pd.read_csv(\"Corona_NLP_test_cleaned_translated.csv\")\n",
    "test_df = prep_df(df_test)\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts = df[\"text\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tokenizer(self.texts[idx], truncation=True, max_length=self.max_len, padding=False)\n",
    "        enc[\"labels\"] = self.labels[idx]\n",
    "        return {k: torch.tensor(v) for k, v in enc.items()}\n",
    "\n",
    "test_ds = TweetDataset(test_df, tokenizer, MAX_LEN)\n",
    "collate_fn = DataCollatorWithPadding(tokenizer)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# -------------------------\n",
    "# Evaluation\n",
    "# -------------------------\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# -------------------------\n",
    "# Report\n",
    "# -------------------------\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=ORDER,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Final Test Results ‚Äì RoBERTa Run\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ TEST METRICS\n",
    "- **Accuracy**: 0.7725  \n",
    "- **F1 Macro**: 0.7794  \n",
    "- **Precision Macro**: 0.7786  \n",
    "- **Recall Macro**: 0.7805  \n",
    "\n",
    "---\n",
    "\n",
    "## üìë Per-class Report\n",
    "| Sentiment            | Precision | Recall | F1   | Support |\n",
    "|-----------------------|-----------|--------|------|---------|\n",
    "| Extremely Negative    | 0.80      | 0.81   | 0.80 | 592     |\n",
    "| Negative              | 0.75      | 0.74   | 0.75 | 1041    |\n",
    "| Neutral               | 0.77      | 0.80   | 0.79 | 619     |\n",
    "| Positive              | 0.75      | 0.74   | 0.74 | 947     |\n",
    "| Extremely Positive    | 0.83      | 0.81   | 0.82 | 599     |\n",
    "\n",
    "**Accuracy**: 0.77 | **Macro Avg**: 0.78 | **Weighted Avg**: 0.77  \n",
    "\n",
    "---\n",
    "\n",
    "## üìà Run Summary\n",
    "- **Epochs**: 12  \n",
    "- **Train Acc**: 0.9843  \n",
    "- **Train F1**: 0.9844  \n",
    "- **Train Loss**: 0.1739  \n",
    "\n",
    "**Train vs Test Gap**: Strong training performance (~98%) but lower test generalization (~77%), indicating potential **overfitting**.  \n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ Key Observations\n",
    "- Mid-sentiment classes (`negative`, `neutral`, `positive`) underperform compared to extremes.  \n",
    "- **Extremely Negative / Extremely Positive** achieve **>0.80 F1**, showing the model learns polarized sentiments better.  \n",
    "- Further regularization or balanced class weighting might help close the gap.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R8TUwW6nH5a"
   },
   "source": [
    "Taking best model from optuna to see if it is any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30430,
     "status": "ok",
     "timestamp": 1755171740408,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "AMGCUEJFXGig",
    "outputId": "e8d1bfb9-7fdb-4c80-d396-2fd812b59fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "Extremely Negative       0.81      0.75      0.78       592\n",
      "          Negative       0.70      0.76      0.73      1041\n",
      "           Neutral       0.85      0.78      0.81       619\n",
      "          Positive       0.72      0.74      0.73       947\n",
      "Extremely Positive       0.81      0.80      0.80       599\n",
      "\n",
      "          accuracy                           0.76      3798\n",
      "         macro avg       0.78      0.76      0.77      3798\n",
      "      weighted avg       0.77      0.76      0.76      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "# from torch.utils.data import DataLoader\n",
    "# import pandas as pd\n",
    "# from google.colab import drive\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# # -----------------------------\n",
    "# # PATHS & CONSTANTS\n",
    "# # -----------------------------\n",
    "# MODEL_PATH = \"adv_dl_models/best_roberta-base_ex4_style_2_optuna_trial_4.pt\"\n",
    "# MODEL_NAME = \"roberta-base\"\n",
    "# BATCH_SIZE = 4\n",
    "# NUM_LABELS = 5\n",
    "\n",
    "# # -----------------------------\n",
    "# # Load Tokenizer and Model\n",
    "# # -----------------------------\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "# model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)\n",
    "# model.load_state_dict(torch.load(MODEL_PATH, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "# model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Load Test Data\n",
    "# # -----------------------------\n",
    "# df_test = pd.read_csv(\"Corona_NLP_test_cleaned_translated.csv\")\n",
    "\n",
    "# label_map = {\n",
    "#     \"Extremely Negative\": 0,\n",
    "#     \"Negative\": 1,\n",
    "#     \"Neutral\": 2,\n",
    "#     \"Positive\": 3,\n",
    "#     \"Extremely Positive\": 4,\n",
    "# }\n",
    "\n",
    "# texts = df_test[\"OriginalTweet\"].tolist()\n",
    "# labels = df_test[\"Sentiment\"].map(label_map).tolist()\n",
    "\n",
    "# # -----------------------------\n",
    "# # Tokenize\n",
    "# # -----------------------------\n",
    "# encodings = tokenizer(texts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "# input_ids = encodings[\"input_ids\"]\n",
    "# attention_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "# # -----------------------------\n",
    "# # Dataset & Dataloader\n",
    "# # -----------------------------\n",
    "# class TestDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, input_ids, attention_mask, labels):\n",
    "#         self.input_ids = input_ids\n",
    "#         self.attention_mask = attention_mask\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             \"input_ids\": self.input_ids[idx],\n",
    "#             \"attention_mask\": self.attention_mask[idx],\n",
    "#             \"labels\": torch.tensor(self.labels[idx]),\n",
    "#         }\n",
    "\n",
    "# dataset = TestDataset(input_ids, attention_mask, labels)\n",
    "# loader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# # -----------------------------\n",
    "# # Evaluation Loop\n",
    "# # -----------------------------\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# with torch.no_grad():\n",
    "#     for batch in loader:\n",
    "#         input_ids = batch[\"input_ids\"].to(device)\n",
    "#         attention_mask = batch[\"attention_mask\"].to(device)\n",
    "#         labels = batch[\"labels\"].to(device)\n",
    "\n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "#         all_preds.extend(preds.cpu().numpy())\n",
    "#         all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# # -----------------------------\n",
    "# # Classification Report\n",
    "# # -----------------------------\n",
    "# print(classification_report(all_labels, all_preds, target_names=[\n",
    "#     \"Extremely Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Extremely Positive\"\n",
    "# ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCRZqm8JX0xb"
   },
   "source": [
    "# üìä Validation vs Test ‚Äì Best RoBERTa Trials\n",
    "\n",
    "We compare the **best trial from Optuna (validation performance)** against the **final best model evaluated on the test set**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Test (Best Trial)\n",
    "| Class                | Precision | Recall | F1-score | Support |\n",
    "|-----------------------|-----------|--------|----------|---------|\n",
    "| Extremely Negative    | 0.80      | 0.81   | 0.80     | 592     |\n",
    "| Negative              | 0.75      | 0.74   | 0.75     | 1041    |\n",
    "| Neutral               | 0.77      | 0.80   | 0.79     | 619     |\n",
    "| Positive              | 0.75      | 0.74   | 0.74     | 947     |\n",
    "| Extremely Positive    | 0.83      | 0.81   | 0.82     | 599     |\n",
    "\n",
    "**Overall:**  \n",
    "- Accuracy = **0.77**  \n",
    "- Macro F1 = **0.78**  \n",
    "- Weighted F1 = **0.77**\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Optuna Best (Validation)\n",
    "| Class                | Precision | Recall | F1-score | Support |\n",
    "|-----------------------|-----------|--------|----------|---------|\n",
    "| Extremely Negative    | 0.81      | 0.75   | 0.78     | 592     |\n",
    "| Negative              | 0.70      | 0.76   | 0.73     | 1041    |\n",
    "| Neutral               | 0.85      | 0.78   | 0.81     | 619     |\n",
    "| Positive              | 0.72      | 0.74   | 0.73     | 947     |\n",
    "| Extremely Positive    | 0.81      | 0.80   | 0.80     | 599     |\n",
    "\n",
    "**Overall:**  \n",
    "- Accuracy = **0.76**  \n",
    "- Macro F1 = **0.77**  \n",
    "- Weighted F1 = **0.76**\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Observation\n",
    "- Both models are **very close** in macro F1 (0.77‚Äì0.78).  \n",
    "- Test model is slightly **more balanced**, performing better on *Negative* and *Positive*.  \n",
    "- Validation (Optuna best) showed stronger *Neutral* class performance.  \n",
    "- Overall, both are in line with **RoBERTa-base general performance (~0.77‚Äì0.78 F1)**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOrG0PGHYzHK5GDU4YhYca2",
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl4090)",
   "language": "python",
   "name": "dl4090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "048d1f185d4c400e841f518bafdef807": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05edc0eb3ecd4d449ddb600647ff47ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "072c7e0ec3ac4169b7e0f8e9a7d9b44a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_048d1f185d4c400e841f518bafdef807",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_bd342092f1fa4930ac1e00832e8226e4",
      "value": "model.safetensors:‚Äá100%"
     }
    },
    "09a6016375074951b7856a4f08d6547f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1cdae25579f3453ab68ae228ac973c6a",
       "IPY_MODEL_87675490f67a4da5b88aeff2085ca8a2",
       "IPY_MODEL_d2f403a4529e4665b5aa7a526dd9fd74"
      ],
      "layout": "IPY_MODEL_87c2257ccc1740d7ab6345ecf44d8e68"
     }
    },
    "0d66f3d69dff49b6a0043673a8cdb562": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1651a7ab14ca49108bcf0833c7789f82": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bb73de03fa6497498a557283677108c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1c313a418b324ff09ce9e469929673ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cdae25579f3453ab68ae228ac973c6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ea1e0f2042d4093b544d338545a4cdb",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_90abbcf547cb4619866fe38b0e4081a8",
      "value": "vocab.json:‚Äá100%"
     }
    },
    "217cabf489e84e85916004c71d8d1e35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26f038a0655b4398b42e0063e5a4d1e5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_072c7e0ec3ac4169b7e0f8e9a7d9b44a",
       "IPY_MODEL_edc327dc3f674e76a5b633c75572abf8",
       "IPY_MODEL_f071a276190946f5867989f083f7cf15"
      ],
      "layout": "IPY_MODEL_e1719e93d28e4d75b419c4d37cae8021"
     }
    },
    "277b95a31f1649e885941cc5ea1b3a39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c364055efba4f07b85504205d51215e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ea1e0f2042d4093b544d338545a4cdb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30e3cb72ae974b4aa7b8bb8394496ca2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_536cd8b50bf8481dbd4ed61c15c62da7",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_39a38537dadb4d588d894084ad404e89",
      "value": "merges.txt:‚Äá100%"
     }
    },
    "313ba92cfeb94480837d73393b326b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "363141a7095c42f0ab1146b7733bca9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6bb81d47abfb44d9b1cd82e24ac6ea73",
       "IPY_MODEL_bb25fa4befbb428ca972a4c5c33cd671",
       "IPY_MODEL_709c561436624f91b4acf75c8920c58a"
      ],
      "layout": "IPY_MODEL_05edc0eb3ecd4d449ddb600647ff47ec"
     }
    },
    "36fb010cb96d4ccd9d2725d862da9fcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "39a38537dadb4d588d894084ad404e89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3aeb3346681f40b4a1f4ce8b00943054": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca88eef9d32b4310a03038850ba52c33",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b082a4b5882a40289aca95326b0ff38a",
      "value": 481
     }
    },
    "41268f513e7d41ad93db45c38f81533f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4739868b580d498cb1b668cf7f32316a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc7bae0a78074fa0ae06e1165142b8dc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_ff053753110244da96c1a5b46301616f",
      "value": "‚Äá1.36M/1.36M‚Äá[00:00&lt;00:00,‚Äá4.25MB/s]"
     }
    },
    "494dbd6d70424741a276d29dd0544d4d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ad0a9c02cd6410f8e7d947bcd1f7d86": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "536cd8b50bf8481dbd4ed61c15c62da7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61d12c7e44df4b9dadbff87e3b211f6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1651a7ab14ca49108bcf0833c7789f82",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9efa883a47ab4b6dba438e3b61bc3b64",
      "value": 456318
     }
    },
    "647ee239889d47beb87b8b31264568e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65bb4463bad3482abccaa7b2bf700713": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "67f7c6a839b746229b82dee0257ec4db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6a14d43268884bb4a43c68dfc790e027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fc0a6590119542c4adf425514f993957",
       "IPY_MODEL_bc86d9c1c671473282f5e6cc799a844d",
       "IPY_MODEL_4739868b580d498cb1b668cf7f32316a"
      ],
      "layout": "IPY_MODEL_67f7c6a839b746229b82dee0257ec4db"
     }
    },
    "6babdae5c57c4a7ba9f7ec912a2f568f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6bb7a81447c0447fa4ff1700f1d84ab2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6bb81d47abfb44d9b1cd82e24ac6ea73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41268f513e7d41ad93db45c38f81533f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a78dbd87c8ae42bba1ef0da00273080b",
      "value": "tokenizer_config.json:‚Äá100%"
     }
    },
    "709c561436624f91b4acf75c8920c58a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f709533f9b7641dfaf73f63ff6c80e7f",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6babdae5c57c4a7ba9f7ec912a2f568f",
      "value": "‚Äá25.0/25.0‚Äá[00:00&lt;00:00,‚Äá2.98kB/s]"
     }
    },
    "74a33ec0722c4f0d97914b771259a07d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_494dbd6d70424741a276d29dd0544d4d",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_36fb010cb96d4ccd9d2725d862da9fcc",
      "value": "‚Äá481/481‚Äá[00:00&lt;00:00,‚Äá58.3kB/s]"
     }
    },
    "78a70153092e47728d4affb788c74119": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ff80485bbe3040beaf7a651e8e68a780",
       "IPY_MODEL_3aeb3346681f40b4a1f4ce8b00943054",
       "IPY_MODEL_74a33ec0722c4f0d97914b771259a07d"
      ],
      "layout": "IPY_MODEL_1c313a418b324ff09ce9e469929673ba"
     }
    },
    "8181e155f1054a7b9dcd3f8f1a996ea1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "87675490f67a4da5b88aeff2085ca8a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8ba0715e6a3c4615a84f50d2e6e643c6",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f7a9bb98db994c32840f6fefe8e29a28",
      "value": 898823
     }
    },
    "87c2257ccc1740d7ab6345ecf44d8e68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8ba0715e6a3c4615a84f50d2e6e643c6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90abbcf547cb4619866fe38b0e4081a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "97bffe63fd0749d6944802fa0376ae96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9d2e8534a09d4d9e82db35e48da67afc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9efa883a47ab4b6dba438e3b61bc3b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9f2c89e1526048248a27c2bbef0f49fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c364055efba4f07b85504205d51215e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a7e8bb424c7845b5a339d3b5463907c4",
      "value": "‚Äá456k/456k‚Äá[00:00&lt;00:00,‚Äá1.06MB/s]"
     }
    },
    "a78dbd87c8ae42bba1ef0da00273080b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7e8bb424c7845b5a339d3b5463907c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a8400ca460224ad5af700763c561f558": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b082a4b5882a40289aca95326b0ff38a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bb25fa4befbb428ca972a4c5c33cd671": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_647ee239889d47beb87b8b31264568e4",
      "max": 25,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_313ba92cfeb94480837d73393b326b43",
      "value": 25
     }
    },
    "bc86d9c1c671473282f5e6cc799a844d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ad0a9c02cd6410f8e7d947bcd1f7d86",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1bb73de03fa6497498a557283677108c",
      "value": 1355863
     }
    },
    "bd342092f1fa4930ac1e00832e8226e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c449e800bcca4022b8ef533ed819c8e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_30e3cb72ae974b4aa7b8bb8394496ca2",
       "IPY_MODEL_61d12c7e44df4b9dadbff87e3b211f6d",
       "IPY_MODEL_9f2c89e1526048248a27c2bbef0f49fd"
      ],
      "layout": "IPY_MODEL_8181e155f1054a7b9dcd3f8f1a996ea1"
     }
    },
    "ca88eef9d32b4310a03038850ba52c33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2f403a4529e4665b5aa7a526dd9fd74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9d2e8534a09d4d9e82db35e48da67afc",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_97bffe63fd0749d6944802fa0376ae96",
      "value": "‚Äá899k/899k‚Äá[00:00&lt;00:00,‚Äá1.37MB/s]"
     }
    },
    "d6ba3986c2fa4147b96b02aaf8fe91f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e1719e93d28e4d75b419c4d37cae8021": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2c3ac1fb5fc426581b80cbef7b18526": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edc327dc3f674e76a5b633c75572abf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e2c3ac1fb5fc426581b80cbef7b18526",
      "max": 498818054,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_277b95a31f1649e885941cc5ea1b3a39",
      "value": 498818054
     }
    },
    "f071a276190946f5867989f083f7cf15": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6ba3986c2fa4147b96b02aaf8fe91f0",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a8400ca460224ad5af700763c561f558",
      "value": "‚Äá499M/499M‚Äá[00:02&lt;00:00,‚Äá375MB/s]"
     }
    },
    "f709533f9b7641dfaf73f63ff6c80e7f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f7a9bb98db994c32840f6fefe8e29a28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc0a6590119542c4adf425514f993957": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d66f3d69dff49b6a0043673a8cdb562",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_6bb7a81447c0447fa4ff1700f1d84ab2",
      "value": "tokenizer.json:‚Äá100%"
     }
    },
    "fc7bae0a78074fa0ae06e1165142b8dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff053753110244da96c1a5b46301616f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ff80485bbe3040beaf7a651e8e68a780": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_217cabf489e84e85916004c71d8d1e35",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_65bb4463bad3482abccaa7b2bf700713",
      "value": "config.json:‚Äá100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
