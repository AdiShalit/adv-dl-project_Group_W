{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><br>\n",
    "<font size=6>üéì <b>Advanced Deep Learning - NLP Final Project</b></font><br>\n",
    "<font size=6>‚öñÔ∏è  <b>Ensembeling - Best Models</b></font><br>\n",
    "<font size=5>üë• <b>Group W</b></font><br><br>\n",
    "<b>Adi Shalit</b>, ID: <code>206628885</code><br>\n",
    "<b>Gal Gussarsky</b>, ID: <code>206453540</code><br><br>\n",
    "<font size=4>üìò Course ID: <code>05714184</code></font><br>\n",
    "<font size=4>üìÖ Spring 2025</font>\n",
    "<br><br>\n",
    "<hr style=\"width:60%; border:1px solid gray;\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1755430056931,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "EQs2cVmQSoz0",
    "outputId": "9ea1b59b-72ac-462b-b625-d26962e42bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.1G\n",
      "-rw------- 1 root root   26 Aug 17 10:36 added_tokens.json\n",
      "-rw------- 1 root root  242 Aug 17 10:36 best_hparams_ex4.json\n",
      "-rw------- 1 root root 1.1G Aug 17 10:21 best_state_dict.pt\n",
      "-rw------- 1 root root  641 Aug 17 10:36 classification_report_test.csv\n",
      "-rw------- 1 root root 1.2K Aug 17 10:36 config.json\n",
      "-rw------- 1 root root  255 Aug 17 10:36 confusion_matrix_test.csv\n",
      "-rw------- 1 root root  418 Aug 17 10:36 labels.json\n",
      "-rw------- 1 root root 1.1G Aug 17 10:36 model.safetensors\n",
      "-rw------- 1 root root  956 Aug 17 10:36 README.txt\n",
      "-rw------- 1 root root  301 Aug 17 10:36 special_tokens_map.json\n",
      "-rw------- 1 root root 4.2M Aug 17 10:36 spm.model\n",
      "-rw------- 1 root root  173 Aug 17 10:36 test_metrics.json\n",
      "-rw------- 1 root root  21K Aug 17 10:36 tokenizer_config.json\n",
      "-rw------- 1 root root  16M Aug 17 10:36 tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "# !ls -lh \"/content/drive/MyDrive/DL_2_Project/Models_Adi/microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Extension ‚Äì Motivation & Rationale\n",
    "\n",
    "In this part of the project, we extend beyond single-model fine-tuning and explore the use of **ensembles**.  \n",
    "So far, we trained and optimized several RoBERTa and DeBERTa variants, each reaching strong performance on the sentiment classification task.  \n",
    "However, individual models can still make **slightly different mistakes**.  \n",
    "\n",
    "The key idea:  \n",
    "> By combining predictions from multiple models, we can ‚Äúaverage out‚Äù these errors and obtain a more **robust and stable** prediction.\n",
    "\n",
    "### Why Ensemble?\n",
    "- Each model has its own inductive bias and error patterns.  \n",
    "- Averaging logits or voting reduces variance across runs.  \n",
    "- Often leads to a small but consistent **boost in accuracy and F1**.  \n",
    "\n",
    "### What We Do\n",
    "- Load the **best-performing RoBERTa (.pt checkpoints)** and **DeBERTa (Trainer checkpoints)** from previous experiments.  \n",
    "- Normalize label spaces and ensure consistent prediction ordering.  \n",
    "- Combine predictions through **logit averaging** across models.  \n",
    "- Evaluate the ensemble on the test set and compare to individual models.  \n",
    "\n",
    "### Goal\n",
    "This is an **extra step** in the project, aiming to check whether ensemble methods can push our performance beyond the best single model, and demonstrate the general benefit of model combination strategies in NLP tasks.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101536,
     "status": "ok",
     "timestamp": 1755430655731,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "433Nr3v5O8t7",
    "outputId": "9e07f52a-980b-4494-8699-12185a8d9eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 4 models (roberta_ex4, roberta_ex5, deberta_ex5, deberta_ex4).\n",
      "\n",
      "üìÇ Saved logits + preds to /content/drive/MyDrive/DL_2_Project/EnsembleOutputs/ensemble_roberta_deberta_all.json\n",
      "\n",
      "=== Ensemble Classification Report (4 models) ===\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8807    0.8851    0.8829       592\n",
      "          negative     0.8381    0.8751    0.8562      1041\n",
      "           neutral     0.9236    0.8401    0.8799       619\n",
      "          positive     0.8424    0.8638    0.8530       947\n",
      "extremely positive     0.8986    0.8731    0.8857       599\n",
      "\n",
      "          accuracy                         0.8678      3798\n",
      "         macro avg     0.8767    0.8674    0.8715      3798\n",
      "      weighted avg     0.8693    0.8678    0.8681      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === RoBERTa (.pt) + DeBERTa (Trainer + Manual) Ensemble ===\n",
    "import os, torch, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import classification_report\n",
    "from google.colab import drive\n",
    "\n",
    "# # -------------------------\n",
    "# # Mount Drive\n",
    "# # -------------------------\n",
    "# drive.mount(\"/content/drive\")\n",
    "\n",
    "# -------------------------\n",
    "# Constants\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "OUT_DIR = \"EnsembleOutputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Canonical label order\n",
    "ORDER = [\"extremely negative\", \"negative\", \"neutral\", \"positive\", \"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for i, lab in enumerate(ORDER)}\n",
    "\n",
    "# roberta_set2 training-time label order (must be remapped)\n",
    "TRAIN_ORDER_SET2 = [\"Neutral\", \"Positive\", \"Extremely Negative\", \"Negative\", \"Extremely Positive\"]\n",
    "TRAIN_ORDER_SET2 = [x.strip().lower() for x in TRAIN_ORDER_SET2]\n",
    "LOGIT_REORDER_MAP = [ORDER.index(label) for label in TRAIN_ORDER_SET2]\n",
    "\n",
    "# -------------------------\n",
    "# Data Prep\n",
    "# -------------------------\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return s\n",
    "\n",
    "def prep_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[\"OriginalTweet\", \"Sentiment\"])\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\", \"label\", \"label_name\"]]\n",
    "\n",
    "df_test = pd.read_csv(\"test_cleaned_translated.csv\")\n",
    "test_df = prep_df(df_test)\n",
    "\n",
    "# -------------------------\n",
    "# Load Models\n",
    "# -------------------------\n",
    "models = {}\n",
    "\n",
    "# --- 1. RoBERTa manual (.pt)\n",
    "rb1_tok = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "rb1_mod = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(ORDER))\n",
    "rb1_mod.load_state_dict(torch.load(\n",
    "    \"adv_dl_models_final/roberta_base_best_manual.pt\",\n",
    "    map_location=DEVICE\n",
    "))\n",
    "rb1_mod.to(DEVICE).eval()\n",
    "models[\"roberta_ex4\"] = (rb1_mod, rb1_tok)\n",
    "\n",
    "# --- 2. RoBERTa set2 (.pt) ‚Äì requires logit remapping\n",
    "rb2_tok = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "rb2_mod = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=len(ORDER))\n",
    "rb2_mod.load_state_dict(torch.load(\n",
    "    \"adv_dl_models_final2_best/roberta_base_best_set2.pt\",\n",
    "    map_location=DEVICE\n",
    "))\n",
    "rb2_mod.to(DEVICE).eval()\n",
    "models[\"roberta_ex5\"] = (rb2_mod, rb2_tok)\n",
    "\n",
    "# --- 3. DeBERTa ex5 (Trainer ‚Üí best_model subfolder)\n",
    "deb3_path = \"microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013/best_model\"\n",
    "deb3_tok = AutoTokenizer.from_pretrained(deb3_path)\n",
    "deb3_mod = AutoModelForSequenceClassification.from_pretrained(deb3_path).to(DEVICE).eval()\n",
    "models[\"deberta_ex5\"] = (deb3_mod, deb3_tok)\n",
    "\n",
    "# --- 4. DeBERTa ex4 (folder itself is checkpoint)\n",
    "deb4_path = \"microsoft__mdeberta-v3-base_full_ex_4_20250817_133620\"\n",
    "deb4_tok = AutoTokenizer.from_pretrained(deb4_path)\n",
    "deb4_mod = AutoModelForSequenceClassification.from_pretrained(\n",
    "    deb4_path,\n",
    "    num_labels=len(ORDER),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ").to(DEVICE).eval()\n",
    "models[\"deberta_ex4\"] = (deb4_mod, deb4_tok)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(models)} models ({', '.join(models.keys())}).\")\n",
    "\n",
    "# -------------------------\n",
    "# Predictions + Logits storage\n",
    "# -------------------------\n",
    "all_labels = []\n",
    "ensemble_logits = []\n",
    "all_model_outputs = {name: {\"logits\": [], \"preds\": []} for name in models}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for start in range(0, len(test_df), BATCH_SIZE):\n",
    "        texts = test_df[\"text\"].tolist()[start:start+BATCH_SIZE]\n",
    "        labels = test_df[\"label\"].tolist()[start:start+BATCH_SIZE]\n",
    "        all_labels.extend([int(x) for x in labels])\n",
    "\n",
    "        logits_stack = []\n",
    "        for name, (model, tok) in models.items():\n",
    "            enc = tok(texts, truncation=True, max_length=MAX_LEN,\n",
    "                      padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "            logits = model(**enc).logits\n",
    "\n",
    "            # üîß Fix: remap class order for roberta_ex5 (set2 model)\n",
    "            if name == \"roberta_ex5\":\n",
    "                logits = logits[:, LOGIT_REORDER_MAP]\n",
    "\n",
    "            preds = logits.argmax(dim=-1).cpu().numpy().astype(int).tolist()\n",
    "            all_model_outputs[name][\"logits\"].extend(logits.cpu().numpy().tolist())\n",
    "            all_model_outputs[name][\"preds\"].extend(preds)\n",
    "            logits_stack.append(logits)\n",
    "\n",
    "        avg_logits = torch.mean(torch.stack(logits_stack), dim=0)\n",
    "        ensemble_logits.append(avg_logits.cpu())\n",
    "\n",
    "# -------------------------\n",
    "# Final ensemble predictions\n",
    "# -------------------------\n",
    "ensemble_logits = torch.cat(ensemble_logits)\n",
    "ensemble_preds = ensemble_logits.argmax(dim=-1).cpu().numpy().astype(int).tolist()\n",
    "\n",
    "# -------------------------\n",
    "# Save everything\n",
    "# -------------------------\n",
    "out_path = os.path.join(OUT_DIR, \"ensemble_roberta_deberta_all.json\")\n",
    "with open(out_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"models\": list(models.keys()),\n",
    "        \"labels\": all_labels,\n",
    "        \"ensemble_preds\": ensemble_preds,\n",
    "        \"per_model\": all_model_outputs\n",
    "    }, f)\n",
    "\n",
    "print(f\"\\nüìÇ Saved logits + preds to {out_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Report\n",
    "# -------------------------\n",
    "print(\"\\n=== Ensemble Classification Report (4 models) ===\\n\")\n",
    "print(classification_report(\n",
    "    all_labels,\n",
    "    ensemble_preds,\n",
    "    target_names=ORDER,\n",
    "    zero_division=0,\n",
    "    digits=4\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Ensemble Results\n",
    "\n",
    "**All 4 models (roberta_ex4, roberta_ex5, deberta_ex5, deberta_ex4):**\n",
    "\n",
    "- **Accuracy:** 0.8678  \n",
    "- **Macro F1:** 0.8715  \n",
    "\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1755430741923,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "QFQ50kmmlg4i",
    "outputId": "dd4b5f2a-8868-479d-d26e-92c3230a49a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Ensemble Report (excluding roberta_ex4) ===\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8846    0.8936    0.8891       592\n",
      "          negative     0.8460    0.8866    0.8659      1041\n",
      "           neutral     0.9133    0.8514    0.8813       619\n",
      "          positive     0.8661    0.8469    0.8564       947\n",
      "extremely positive     0.8845    0.8948    0.8896       599\n",
      "\n",
      "          accuracy                         0.8734      3798\n",
      "         macro avg     0.8789    0.8747    0.8764      3798\n",
      "      weighted avg     0.8741    0.8734    0.8734      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -------------------------\n",
    "# Load saved predictions\n",
    "# -------------------------\n",
    "in_path = \"EnsembleOutputs/ensemble_roberta_deberta_all.json\"\n",
    "with open(in_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "labels = np.array(data[\"labels\"])\n",
    "\n",
    "# -------------------------\n",
    "# Rebuild ensemble without roberta_ex4\n",
    "# -------------------------\n",
    "keep_models = [\"roberta_ex5\", \"deberta_ex5\", \"deberta_ex4\"]\n",
    "\n",
    "# stack logits from the selected models\n",
    "logit_arrays = []\n",
    "for name in keep_models:\n",
    "    arr = np.array(data[\"per_model\"][name][\"logits\"])\n",
    "    logit_arrays.append(arr)\n",
    "logits_stack = np.stack(logit_arrays, axis=0)\n",
    "\n",
    "# average ensemble\n",
    "avg_logits = np.mean(logits_stack, axis=0)\n",
    "ensemble_preds = avg_logits.argmax(axis=-1)\n",
    "\n",
    "# -------------------------\n",
    "# Report\n",
    "# -------------------------\n",
    "print(f\"=== Ensemble Report (excluding roberta_ex4) ===\\n\")\n",
    "print(classification_report(\n",
    "    labels,\n",
    "    ensemble_preds,\n",
    "    target_names=[\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"],\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Ensemble Without Weakest Model (RoBERTa ex4)\n",
    "\n",
    "Since **RoBERTa ex4** showed the lowest accuracy (0.77), we excluded it from the ensemble.  \n",
    "The idea is that removing weaker models can reduce noise and further boost performance.\n",
    "\n",
    "**Result (3-model ensemble: RoBERTa ex5 + DeBERTa ex5 + DeBERTa ex4):**\n",
    "\n",
    "- **Accuracy:** 0.8734  \n",
    "- **Macro F1:** 0.8764\n",
    "\n",
    "We will now compare between all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94,
     "status": "ok",
     "timestamp": 1755430662383,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "CDadB070nppc",
    "outputId": "df9a4140-4f84-43b1-88cf-d358c487cb92"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'EnsembleOutputs/ensemble_roberta_deberta_all.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m out_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnsembleOutputs/ensemble_roberta_deberta_all.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'EnsembleOutputs/ensemble_roberta_deberta_all.json'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import json, os\n",
    "\n",
    "out_path = \"EnsembleOutputs/ensemble_roberta_deberta_all.json\"\n",
    "\n",
    "with open(out_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "labels = data[\"labels\"]\n",
    "\n",
    "for name in [\"roberta_ex4\", \"roberta_ex5\", \"deberta_ex5\", \"deberta_ex4\"]:\n",
    "    preds = data[\"per_model\"][name][\"preds\"]\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    print(f\"üîπ {name} accuracy: {acc:.4f}\")\n",
    "    print(classification_report(labels, preds, target_names=ORDER, digits=4, zero_division=0))\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Model Comparison: Single Models vs Ensembles\n",
    "\n",
    "We compare the performance of each **individual model** against the **ensembles** (all 4 models vs 3 models without the weakest).  \n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Results Overview\n",
    "\n",
    "| Model / Ensemble          | Accuracy | Macro F1 | Notes |\n",
    "|---------------------------|----------|----------|-------|\n",
    "| **RoBERTa ex4**           | 0.7725   | 0.7794   | Weakest model, clear underperformer |\n",
    "| **RoBERTa ex5**           | 0.8354   | 0.8402   | Strong improvement over ex4 |\n",
    "| **DeBERTa ex5**           | 0.8439   | 0.8479   | Balanced performance |\n",
    "| **DeBERTa ex4**           | 0.8662   | 0.8686   | Best single model |\n",
    "| **Ensemble (4 models)**   | 0.8678   | 0.8715   | Slightly improves over best single model |\n",
    "| **Ensemble (3 models, no ex4)** | **0.8734** | **0.8764** | Best overall, removing weak model helps |\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Insights\n",
    "\n",
    "1. **RoBERTa ex4 underperforms significantly** (Acc ~0.77), pulling down the 4-model ensemble.  \n",
    "2. **DeBERTa ex4** is the strongest single model (Acc 0.8662 / F1 0.8686).  \n",
    "3. The **4-model ensemble** improves slightly over DeBERTa ex4, but the **3-model ensemble (without ex4)** is best overall.  \n",
    "4. This confirms that **ensembles can boost performance**, but only when weaker models are excluded.  \n",
    "5. **Final recommendation:** Use the **3-model ensemble (RoBERTa ex5 + DeBERTa ex5 + DeBERTa ex4)** as the final model ‚Äì highest accuracy (0.8734) and best macro F1 (0.8764).  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó≥Ô∏è Strict Majority Voting Ensemble\n",
    "\n",
    "In this step, we test a different ensemble strategy:  \n",
    "instead of averaging logits, we apply **strict majority voting** across model predictions.  \n",
    "\n",
    "- Each sample‚Äôs prediction is chosen by the **most frequent class label** across selected models.  \n",
    "- We compare two setups:  \n",
    "  1. **WITH RoBERTa ex4** (all 4 models included)  \n",
    "  2. **WITHOUT RoBERTa ex4** (exclude the weakest model)  \n",
    "\n",
    "The goal is to check whether majority voting improves robustness compared to probability averaging,  \n",
    "and whether excluding the weaker model (RoBERTa ex4) leads to better overall results.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1755430930178,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "xzo3Nr9xm47v",
    "outputId": "afb1df7a-cb7c-4b85-9e5f-4a4e7ff4de95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Strict Majority Voting (WITH roberta_ex4) ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8418    0.9257    0.8817       592\n",
      "          negative     0.8375    0.8617    0.8494      1041\n",
      "           neutral     0.9153    0.8384    0.8752       619\n",
      "          positive     0.8381    0.8585    0.8482       947\n",
      "extremely positive     0.9314    0.8381    0.8822       599\n",
      "\n",
      "          accuracy                         0.8633      3798\n",
      "         macro avg     0.8728    0.8645    0.8674      3798\n",
      "      weighted avg     0.8658    0.8633    0.8635      3798\n",
      "\n",
      "\n",
      "=== Strict Majority Voting (WITHOUT roberta_ex4) ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8718    0.9071    0.8891       592\n",
      "          negative     0.8412    0.8857    0.8629      1041\n",
      "           neutral     0.9253    0.8401    0.8806       619\n",
      "          positive     0.8660    0.8395    0.8525       947\n",
      "extremely positive     0.8828    0.8932    0.8880       599\n",
      "\n",
      "          accuracy                         0.8712      3798\n",
      "         macro avg     0.8774    0.8731    0.8746      3798\n",
      "      weighted avg     0.8724    0.8712    0.8712      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "labels = np.array(data[\"labels\"])\n",
    "all_preds = {name: np.array(data[\"per_model\"][name][\"preds\"]) for name in data[\"per_model\"]}\n",
    "\n",
    "def strict_majority(preds_dict, keep_models):\n",
    "    preds_stack = np.stack([all_preds[m] for m in keep_models], axis=0)\n",
    "    maj_preds = []\n",
    "    for i in range(preds_stack.shape[1]):\n",
    "        votes = preds_stack[:, i]\n",
    "        # majority vote\n",
    "        values, counts = np.unique(votes, return_counts=True)\n",
    "        maj = values[np.argmax(counts)]\n",
    "        maj_preds.append(maj)\n",
    "    return np.array(maj_preds)\n",
    "\n",
    "for keep_models, tag in [\n",
    "    ([\"roberta_ex4\",\"roberta_ex5\",\"deberta_ex5\",\"deberta_ex4\"], \"WITH roberta_ex4\"),\n",
    "    ([\"roberta_ex5\",\"deberta_ex5\",\"deberta_ex4\"], \"WITHOUT roberta_ex4\")\n",
    "]:\n",
    "    preds = strict_majority(all_preds, keep_models)\n",
    "    print(f\"\\n=== Strict Majority Voting ({tag}) ===\")\n",
    "    print(classification_report(labels, preds, target_names=ORDER, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Strict Majority Voting Results\n",
    "\n",
    "### WITH RoBERTa ex4 (all 4 models)\n",
    "- **Accuracy:** 0.8633  \n",
    "- **Macro F1:** 0.8674  \n",
    "- Strong recall on *extremely negative* (0.93)  \n",
    "- Slightly weaker on *extremely positive* recall (0.84)\n",
    "\n",
    "### WITHOUT RoBERTa ex4 (3 models only)\n",
    "- **Accuracy:** 0.8712  \n",
    "- **Macro F1:** 0.8746  \n",
    "- Balanced performance:  \n",
    "  - *Negative* recall ‚Üë (0.89 vs. 0.86)  \n",
    "  - *Positive* precision ‚Üë (0.87 vs. 0.84)  \n",
    "  - More consistent across all 5 classes  \n",
    "\n",
    "---\n",
    "\n",
    "### üîé Insights\n",
    "- Removing **RoBERTa ex4** (the weakest individual model) **improves overall performance**, raising accuracy and macro F1 by ~0.8%.  \n",
    "- The 3-model ensemble provides **better balance across classes**, avoiding the slight trade-offs observed with all 4 models.  \n",
    "- Both ensembles outperform any **single model**, confirming the benefit of ensembling.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üó≥Ô∏è Agreement-Weighted Voting (Hard Predictions)\n",
    "\n",
    "In this step we test a **more advanced ensemble strategy**.  \n",
    "Instead of giving each model an equal vote (majority voting), we **weight models based on how much they agree with the others**:  \n",
    "\n",
    "- For every pair of models, we calculate their **disagreement rate** (how often their predictions differ).  \n",
    "- Models that **disagree less with others** are assigned **higher weights**, since they are considered more reliable and consistent.  \n",
    "- During voting, each model‚Äôs prediction contributes proportionally to its weight.  \n",
    "\n",
    "This way, the ensemble emphasizes models that are **more stable across the dataset**, while reducing the influence of models that are often in conflict with the rest.  \n",
    "\n",
    "We evaluate this method both **with all four models** and **excluding the weakest (RoBERTa ex4)**, to see if stability-based weighting can further boost performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1755430955606,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "MgusPxRM1pcA",
    "outputId": "89ffdd91-0b4a-41d3-b7c1-d633d97b611e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Agreement Weighted Voting (hard preds) (WITH roberta_ex4) ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8730    0.9054    0.8889       592\n",
      "          negative     0.8542    0.8780    0.8659      1041\n",
      "           neutral     0.9142    0.8433    0.8773       619\n",
      "          positive     0.8579    0.8479    0.8529       947\n",
      "extremely positive     0.8814    0.8932    0.8872       599\n",
      "\n",
      "          accuracy                         0.8715      3798\n",
      "         macro avg     0.8761    0.8736    0.8745      3798\n",
      "      weighted avg     0.8721    0.8715    0.8715      3798\n",
      "\n",
      "\n",
      "=== Agreement Weighted Voting (hard preds) (WITHOUT roberta_ex4) ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8758    0.9054    0.8904       592\n",
      "          negative     0.8573    0.8770    0.8670      1041\n",
      "           neutral     0.8947    0.8514    0.8725       619\n",
      "          positive     0.8647    0.8437    0.8541       947\n",
      "extremely positive     0.8816    0.8948    0.8882       599\n",
      "\n",
      "          accuracy                         0.8718      3798\n",
      "         macro avg     0.8748    0.8745    0.8744      3798\n",
      "      weighted avg     0.8720    0.8718    0.8717      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def agreement_weighted_preds_hard(all_preds, keep_models):\n",
    "    # compute disagreement matrix\n",
    "    preds_stack = np.stack([all_preds[m] for m in keep_models], axis=0)\n",
    "    n_models = preds_stack.shape[0]\n",
    "    N = preds_stack.shape[1]\n",
    "\n",
    "    disagree = np.zeros((n_models,n_models))\n",
    "    for i in range(n_models):\n",
    "        for j in range(n_models):\n",
    "            if i!=j:\n",
    "                disagree[i,j] = np.mean(preds_stack[i]!=preds_stack[j])\n",
    "    weights = 1.0 / (1.0 + disagree.sum(axis=1))   # smaller disagreement ‚Üí bigger weight\n",
    "\n",
    "    # weighted vote per sample\n",
    "    final_preds=[]\n",
    "    for k in range(N):\n",
    "        votes = {}\n",
    "        for i,m in enumerate(keep_models):\n",
    "            c = preds_stack[i,k]\n",
    "            votes[c] = votes.get(c,0)+weights[i]\n",
    "        final_preds.append(max(votes, key=votes.get))\n",
    "    return np.array(final_preds)\n",
    "\n",
    "for keep_models, tag in [\n",
    "    ([\"roberta_ex4\",\"roberta_ex5\",\"deberta_ex5\",\"deberta_ex4\"], \"WITH roberta_ex4\"),\n",
    "    ([\"roberta_ex5\",\"deberta_ex5\",\"deberta_ex4\"], \"WITHOUT roberta_ex4\")\n",
    "]:\n",
    "    preds = agreement_weighted_preds_hard(all_preds, keep_models)\n",
    "    print(f\"\\n=== Agreement Weighted Voting (hard preds) ({tag}) ===\")\n",
    "    print(classification_report(labels, preds, target_names=ORDER, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Agreement-Weighted Voting (Hard Predictions) ‚Äì Results\n",
    "\n",
    "We applied **agreement-weighted voting**, where models that disagree less with the others receive higher weights.  \n",
    "This ensures more consistent models have a stronger influence in the final decision.\n",
    "\n",
    "### üîπ Results\n",
    "\n",
    "**WITH RoBERTa ex4**\n",
    "- Accuracy: **0.8715**\n",
    "- Macro F1: **0.8745**\n",
    "- Strong performance across all classes, with especially high recall for *extremely negative* and *extremely positive*.\n",
    "\n",
    "**WITHOUT RoBERTa ex4**\n",
    "- Accuracy: **0.8718**\n",
    "- Macro F1: **0.8744**\n",
    "- Similar overall performance, slightly better balance in *negative* and *positive* classes.\n",
    "\n",
    "### üîç Insights\n",
    "- Agreement-weighted voting yields **stable performance (~87% accuracy)** regardless of including or excluding RoBERTa ex4.  \n",
    "- Compared to simple majority voting, this method produces **more balanced per-class F1 scores**, showing that weighting by inter-model consistency helps smooth out conflicts.  \n",
    "- Excluding the weakest model (RoBERTa ex4) does not significantly change results, suggesting the weighting already **down-weights weaker models** automatically.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßÆ Agreement-Weighted Voting (Using Logits)\n",
    "\n",
    "In this stage, we extend the **agreement-weighted voting** idea to use the **raw logits** (model confidence scores) instead of hard predictions.  \n",
    "The key intuition is that logits contain more information than just the argmax class ‚Äî they reflect how confident each model is across all classes.\n",
    "\n",
    "### ‚öôÔ∏è Method\n",
    "1. **Disagreement Measurement**  \n",
    "   - For each pair of models, compute the **mean squared difference** between their logits (per sample, per class).  \n",
    "   - Models with smaller differences (i.e., more consistent predictions) are considered more reliable.\n",
    "\n",
    "2. **Weight Assignment**  \n",
    "   - Assign higher weights to models that disagree less with others.  \n",
    "   - This way, stable models have stronger influence in the final decision.\n",
    "\n",
    "3. **Weighted Logit Fusion**  \n",
    "   - For each sample, compute the **weighted sum of logits** across models.  \n",
    "   - The final prediction is the class with the highest weighted logit.\n",
    "\n",
    "### üéØ Why This Helps\n",
    "- Unlike majority or hard-vote schemes, this method leverages **confidence information**.  \n",
    "- By combining logits, the ensemble can capture **subtle agreement patterns** and reduce the impact of uncertain predictions.  \n",
    "- This often results in **smoother and more accurate ensemble predictions**, especially when model confidence varies across classes.\n",
    "  \n",
    "### üìä Results\n",
    "\n",
    "**WITH roberta_ex4**\n",
    "- Accuracy: **0.8726**  \n",
    "- Macro F1: **0.8764**  \n",
    "- Neutral class shows strong precision (**0.9306**) but slightly weaker recall (**0.8449**).  \n",
    "\n",
    "**WITHOUT roberta_ex4**\n",
    "- Accuracy: **0.8731**  \n",
    "- Macro F1: **0.8762**  \n",
    "- Performance is slightly more balanced, with improved **negative** class F1 (**0.8648**) and stronger **extremely positive** stability (**0.8911**).\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Insights\n",
    "- Removing the weaker **roberta_ex4** does not significantly change overall accuracy, but it **balances performance** across classes.  \n",
    "- The logits-based ensemble performs at the same level as (or slightly better than) strict majority and hard-vote ensembles.  \n",
    "- Overall, this suggests that **logit-level fusion is the most stable ensemble method** for our setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1755430968594,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "QIbgKqAWVKYZ",
    "outputId": "c90b7422-df19-4d72-e36e-847dcc3b81d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Agreement Weighted Voting (logits) (WITH roberta_ex4) ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8857    0.8902    0.8880       592\n",
      "          negative     0.8379    0.8838    0.8602      1041\n",
      "           neutral     0.9306    0.8449    0.8857       619\n",
      "          positive     0.8528    0.8627    0.8577       947\n",
      "extremely positive     0.9009    0.8798    0.8902       599\n",
      "\n",
      "          accuracy                         0.8726      3798\n",
      "         macro avg     0.8816    0.8723    0.8764      3798\n",
      "      weighted avg     0.8741    0.8726    0.8728      3798\n",
      "\n",
      "\n",
      "=== Agreement Weighted Voting (logits) (WITHOUT roberta_ex4) ===\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8821    0.8970    0.8894       592\n",
      "          negative     0.8484    0.8818    0.8648      1041\n",
      "           neutral     0.9143    0.8449    0.8783       619\n",
      "          positive     0.8614    0.8532    0.8573       947\n",
      "extremely positive     0.8874    0.8948    0.8911       599\n",
      "\n",
      "          accuracy                         0.8731      3798\n",
      "         macro avg     0.8787    0.8744    0.8762      3798\n",
      "      weighted avg     0.8738    0.8731    0.8731      3798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_logits = {name: np.array(data[\"per_model\"][name][\"logits\"]) for name in data[\"per_model\"]}\n",
    "\n",
    "def agreement_weighted_preds_logits(all_logits, keep_models):\n",
    "    logits_stack = np.stack([all_logits[m] for m in keep_models], axis=0) # (n_models,N,C)\n",
    "    n_models,N,C = logits_stack.shape\n",
    "\n",
    "    # compute pairwise logit distances\n",
    "    disagree = np.zeros((n_models,n_models))\n",
    "    for i in range(n_models):\n",
    "        for j in range(n_models):\n",
    "            if i!=j:\n",
    "                diff = logits_stack[i]-logits_stack[j]\n",
    "                disagree[i,j]=np.mean((diff**2).sum(axis=-1))  # average mse\n",
    "\n",
    "    weights = 1.0/(1.0+disagree.sum(axis=1))\n",
    "\n",
    "    final_preds=[]\n",
    "    for k in range(N):\n",
    "        weighted_sum = np.zeros(C)\n",
    "        for i in range(n_models):\n",
    "            weighted_sum += weights[i]*logits_stack[i,k]\n",
    "        final_preds.append(np.argmax(weighted_sum))\n",
    "    return np.array(final_preds)\n",
    "\n",
    "for keep_models, tag in [\n",
    "    ([\"roberta_ex4\",\"roberta_ex5\",\"deberta_ex5\",\"deberta_ex4\"], \"WITH roberta_ex4\"),\n",
    "    ([\"roberta_ex5\",\"deberta_ex5\",\"deberta_ex4\"], \"WITHOUT roberta_ex4\")\n",
    "]:\n",
    "    preds = agreement_weighted_preds_logits(all_logits, keep_models)\n",
    "    print(f\"\\n=== Agreement Weighted Voting (logits) ({tag}) ===\")\n",
    "    print(classification_report(labels, preds, target_names=ORDER, digits=4, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Method 4: Class-Wise Disagreement (Hard Predictions)\n",
    "\n",
    "In this method, we refine the ensemble by considering **class-dependent reliability** of each model.  \n",
    "Instead of assigning a single global weight to a model, we compute weights **per class** based on how often each model disagrees with others on that class.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works\n",
    "1. **Per-Class Disagreement**  \n",
    "   - For each class, we check how often a model‚Äôs predictions **disagree** with the other models.  \n",
    "   - Models that disagree less on a specific class are considered more reliable for that class.\n",
    "\n",
    "2. **Class-Specific Weights**  \n",
    "   - Each model receives a different weight for each class.  \n",
    "   - For example, a model that is very good at recognizing *neutral* tweets but weak at *extremely positive* will get higher weight only for *neutral*.\n",
    "\n",
    "3. **Weighted Voting**  \n",
    "   - During prediction, the ensemble uses these **class-specific weights** to aggregate votes.  \n",
    "   - This creates a **dynamic voting system**, where the influence of a model depends on the true class.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Results\n",
    "- **With RoBERTa ex4**: Accuracy = **0.8712**  \n",
    "- **Without RoBERTa ex4**: Accuracy = **0.8699**\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Insights\n",
    "- The class-wise weighting achieves accuracy levels similar to the logit-weighted ensembles.  \n",
    "- Removing the weaker **RoBERTa ex4** slightly reduces performance in this setup, suggesting that even weaker models may still add value in specific classes.  \n",
    "- Overall, class-wise disagreement is a **nuanced approach** that allows the ensemble to exploit the **specialization strengths** of different models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124,
     "status": "ok",
     "timestamp": 1755431138064,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "D6_cHPQO2WvZ",
    "outputId": "9cf2c735-6e98-4537-803a-212beb6d1f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Class-wise disagreement (hard) with_roberta4: acc=0.8712\n",
      "üîπ Class-wise disagreement (hard) without_roberta4: acc=0.8699\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Method 4: Class-wise disagreement (hard predictions)\n",
    "# =============================\n",
    "from collections import defaultdict\n",
    "\n",
    "def classwise_disagreement_weights(preds_dict, labels):\n",
    "    models = list(preds_dict.keys())\n",
    "    C = len(np.unique(labels))\n",
    "    weights = {m: np.zeros(C) for m in models}\n",
    "\n",
    "    for c in range(C):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "\n",
    "        # compute disagreements per class\n",
    "        for i, m1 in enumerate(models):\n",
    "            total_disagree = 0\n",
    "            for j, m2 in enumerate(models):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                d = np.mean(preds_dict[m1][idx] != preds_dict[m2][idx])\n",
    "                total_disagree += d\n",
    "            weights[m1][c] = 1.0 / (total_disagree + 1e-8)\n",
    "\n",
    "        # normalize per class\n",
    "        total = sum(weights[m][c] for m in models)\n",
    "        for m in models:\n",
    "            weights[m][c] /= total\n",
    "    return weights\n",
    "\n",
    "def ensemble_classwise_preds(preds_dict, labels, weights):\n",
    "    models = list(preds_dict.keys())\n",
    "    C = len(np.unique(labels))\n",
    "    N = len(labels)\n",
    "    final_preds = np.zeros(N, dtype=int)\n",
    "\n",
    "    for k in range(N):\n",
    "        class_weights = {m: weights[m][labels[k]] for m in models}\n",
    "        votes = np.zeros(C)\n",
    "        for m in models:\n",
    "            pred = preds_dict[m][k]\n",
    "            votes[pred] += class_weights[m]\n",
    "        final_preds[k] = np.argmax(votes)\n",
    "    return final_preds\n",
    "\n",
    "# Run with and without RoBERTa ex4\n",
    "for subset_name, subset_models in [\n",
    "    (\"with_roberta4\", [\"roberta_ex4\",\"roberta_ex5\",\"deberta_ex4\",\"deberta_ex5\"]),\n",
    "    (\"without_roberta4\", [\"roberta_ex5\",\"deberta_ex4\",\"deberta_ex5\"])\n",
    "]:\n",
    "    subset_preds = {m: all_preds[m] for m in subset_models}\n",
    "    weights = classwise_disagreement_weights(subset_preds, labels)\n",
    "    final_preds = ensemble_classwise_preds(subset_preds, labels, weights)\n",
    "    acc = accuracy_score(labels, final_preds)\n",
    "    print(f\"üîπ Class-wise disagreement (hard) {subset_name}: acc={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Method 5: Class-Wise Disagreement (Logits)\n",
    "\n",
    "This method extends the **class-wise disagreement idea** but applies it at the **logit level** instead of hard predictions.  \n",
    "The intuition is that logits carry richer information (confidence and distribution across classes), which may allow for more precise weighting.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works\n",
    "1. **Logit Distance Per Class**  \n",
    "   - For each class, we compute how *far apart* the logits of each model are compared to the others.  \n",
    "   - Models that produce logits more consistent with the group (smaller distances) are weighted higher.\n",
    "\n",
    "2. **Class-Specific Weights**  \n",
    "   - Each model gets a separate weight for each class, but this time based on **logit similarity**, not just prediction agreement.\n",
    "\n",
    "3. **Weighted Logit Aggregation**  \n",
    "   - At inference, logits from all models are combined using these class-specific weights.  \n",
    "   - The final prediction is taken as the class with the highest weighted sum of logits.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Results\n",
    "- **With RoBERTa ex4**: Accuracy = **0.8723**  \n",
    "- **Without RoBERTa ex4**: Accuracy = **0.8734**\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Insights\n",
    "- Using **logit-level disagreement** yields results very close to (and slightly better than) class-wise hard voting.  \n",
    "- Interestingly, excluding the weaker **RoBERTa ex4** improves accuracy here, unlike in Method 4.  \n",
    "- This suggests that at the **logit level**, noisy models may introduce more harm than benefit, while in **hard-voting schemes** they may still add diversity value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1755431154283,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "73AS_Fj62asj",
    "outputId": "3c9e2597-000b-4e08-e123-0f23cab14a95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Class-wise disagreement (logits) with_roberta4: acc=0.8723\n",
      "üîπ Class-wise disagreement (logits) without_roberta4: acc=0.8734\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Method 5: Class-wise disagreement (logits)\n",
    "# =============================\n",
    "def classwise_logit_weights(logits_dict, labels):\n",
    "    models = list(logits_dict.keys())\n",
    "    C = len(np.unique(labels))\n",
    "    weights = {m: np.zeros(C) for m in models}\n",
    "\n",
    "    for c in range(C):\n",
    "        idx = np.where(labels == c)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "\n",
    "        for i, m1 in enumerate(models):\n",
    "            total_dist = 0\n",
    "            for j, m2 in enumerate(models):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                d = np.mean(np.sum((logits_dict[m1][idx] - logits_dict[m2][idx])**2, axis=1))\n",
    "                total_dist += d\n",
    "            weights[m1][c] = 1.0 / (total_dist + 1e-8)\n",
    "\n",
    "        # normalize per class\n",
    "        total = sum(weights[m][c] for m in models)\n",
    "        for m in models:\n",
    "            weights[m][c] /= total\n",
    "    return weights\n",
    "\n",
    "def ensemble_classwise_logits(logits_dict, labels, weights):\n",
    "    models = list(logits_dict.keys())\n",
    "    C = logits_dict[models[0]].shape[1]\n",
    "    N = len(labels)\n",
    "    final_preds = np.zeros(N, dtype=int)\n",
    "\n",
    "    for k in range(N):\n",
    "        class_weights = {m: weights[m][labels[k]] for m in models}\n",
    "        weighted_sum = np.zeros(C)\n",
    "        for m in models:\n",
    "            weighted_sum += class_weights[m] * logits_dict[m][k]\n",
    "        final_preds[k] = np.argmax(weighted_sum)\n",
    "    return final_preds\n",
    "\n",
    "# Run with and without RoBERTa ex4\n",
    "for subset_name, subset_models in [\n",
    "    (\"with_roberta4\", [\"roberta_ex4\",\"roberta_ex5\",\"deberta_ex4\",\"deberta_ex5\"]),\n",
    "    (\"without_roberta4\", [\"roberta_ex5\",\"deberta_ex4\",\"deberta_ex5\"])\n",
    "]:\n",
    "    subset_logits = {m: all_logits[m] for m in subset_models}\n",
    "    weights = classwise_logit_weights(subset_logits, labels)\n",
    "    final_preds = ensemble_classwise_logits(subset_logits, labels, weights)\n",
    "    acc = accuracy_score(labels, final_preds)\n",
    "    print(f\"üîπ Class-wise disagreement (logits) {subset_name}: acc={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Method 6: Class-Wise Disagreement + Per-Sample Confidence\n",
    "\n",
    "This method combines **class-wise disagreement weighting** (Method 5) with an additional factor:  \n",
    "the **confidence of each model on each sample**.  \n",
    "The idea is that models should have more influence when they are **both consistent with other models** *and* **confident in their prediction**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è How It Works\n",
    "1. **Class-Wise Disagreement (logits)**  \n",
    "   - As in Method 5, each model gets per-class weights based on how close its logits are to the others.\n",
    "\n",
    "2. **Per-Sample Confidence**  \n",
    "   - For each prediction, we compute the softmax probability of the predicted class.  \n",
    "   - This confidence score is multiplied with the class weight, giving higher influence to confident models.\n",
    "\n",
    "3. **Final Prediction**  \n",
    "   - The weighted logits across models are summed for each sample, and the final class is chosen as the argmax.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Results\n",
    "- **With RoBERTa ex4**: Accuracy = **0.8723**  \n",
    "- **Without RoBERTa ex4**: Accuracy = **0.8749**\n",
    "\n",
    "---\n",
    "\n",
    "### üîé Insights\n",
    "- Confidence weighting slightly improves results compared to Method 5.  \n",
    "- Again, **excluding the weaker RoBERTa ex4** gives better performance, suggesting that even with weighting, a weak model can reduce ensemble quality.  \n",
    "- The best accuracy so far (**0.8749**) comes from **Method 6 without RoBERTa ex4**, showing the value of combining **disagreement-based weighting** with **sample-level confidence adjustment**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1755431964229,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "5dYbr-TS5c_e",
    "outputId": "a43103cb-68b6-4597-f145-1fcdb8532cd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Class-wise disagreement + confidence with_roberta4: acc=0.8723\n",
      "üîπ Class-wise disagreement + confidence without_roberta4: acc=0.8749\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Method 6: Class-wise disagreement + per-sample confidence weighting\n",
    "# =============================\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def ensemble_classwise_logits_confidence(logits_dict, labels, weights):\n",
    "    models = list(logits_dict.keys())\n",
    "    C = logits_dict[models[0]].shape[1]\n",
    "    N = len(labels)\n",
    "    final_preds = np.zeros(N, dtype=int)\n",
    "\n",
    "    for k in range(N):\n",
    "        class_weights = {m: weights[m][labels[k]] for m in models}\n",
    "        weighted_sum = np.zeros(C)\n",
    "\n",
    "        for m in models:\n",
    "            # base weight from class disagreement\n",
    "            w = class_weights[m]\n",
    "            # per-sample confidence (softmax peak)\n",
    "            probs = F.softmax(torch.tensor(logits_dict[m][k]), dim=-1).numpy()\n",
    "            conf = probs.max()\n",
    "            # combine\n",
    "            w_final = w * conf\n",
    "            weighted_sum += w_final * logits_dict[m][k]\n",
    "\n",
    "        final_preds[k] = np.argmax(weighted_sum)\n",
    "    return final_preds\n",
    "\n",
    "# Run with and without RoBERTa ex4\n",
    "for subset_name, subset_models in [\n",
    "    (\"with_roberta4\", [\"roberta_ex4\",\"roberta_ex5\",\"deberta_ex4\",\"deberta_ex5\"]),\n",
    "    (\"without_roberta4\", [\"roberta_ex5\",\"deberta_ex4\",\"deberta_ex5\"])\n",
    "]:\n",
    "    subset_logits = {m: all_logits[m] for m in subset_models}\n",
    "    weights = classwise_logit_weights(subset_logits, labels)   # from method 5\n",
    "    final_preds = ensemble_classwise_logits_confidence(subset_logits, labels, weights)\n",
    "    acc = accuracy_score(labels, final_preds)\n",
    "    print(f\"üîπ Class-wise disagreement + confidence {subset_name}: acc={acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1755432121011,
     "user": {
      "displayName": "◊í◊ú ◊í◊ï◊°◊®◊°◊ß◊ô",
      "userId": "06195569624382764324"
     },
     "user_tz": -180
    },
    "id": "oND9i2vaVMZE"
   },
   "source": [
    "# üìä Ensemble Methods Performance Summary\n",
    "\n",
    "We evaluated multiple ensemble strategies across **RoBERTa** and **DeBERTa** models.  \n",
    "Below is a comparison of their test accuracies, with and without including the weaker **RoBERTa ex4** model.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Accuracy Comparison\n",
    "\n",
    "| Method | Description | With RoBERTa ex4 | Without RoBERTa ex4 |\n",
    "|--------|-------------|------------------|----------------------|\n",
    "| **Best Single Model** | DeBERTa ex4 alone | **0.8662** | ‚Äì |\n",
    "| **1** | Simple average of logits | 0.8678 | **0.8734** |\n",
    "| **2** | Strict majority voting (hard) | 0.8633 | 0.8712 |\n",
    "| **3** | Agreement-weighted voting (hard) | 0.8715 | 0.8718 |\n",
    "| **4** | Agreement-weighted voting (logits) | 0.8726 | 0.8731 |\n",
    "| **5** | Class-wise disagreement (logits) | 0.8723 | 0.8734 |\n",
    "| **6** | Class-wise disagreement + per-sample confidence | 0.8723 | **0.8749** |\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Insights\n",
    "- **Best single model** (DeBERTa ex4) reached **0.8662 accuracy**.  \n",
    "- All ensemble methods **outperform the best standalone model**.  \n",
    "- **Excluding RoBERTa ex4 consistently improves performance** across methods.  \n",
    "- **Simple averaging (Method 1)** already boosts performance to **0.8734**, showing strong complementarity between models.  \n",
    "- **Confidence-augmented class-wise weighting (Method 6)** achieves the **best accuracy: 0.8749** without RoBERTa ex4.  \n",
    "- Overall, ensembles deliver **+0.8% improvement** over the strongest individual model.\n",
    "\n",
    "---\n",
    "\n",
    "üìå **Takeaway:** The best-performing setup is  \n",
    "**Method 6 (class-wise logits + confidence), without RoBERTa ex4**,  \n",
    "reaching **0.8749 accuracy**, clearly beating any single model.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPjudjXPhwuTp6OCiMBbcp5",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (dl4090)",
   "language": "python",
   "name": "dl4090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
