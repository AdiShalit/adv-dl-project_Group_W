{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88df58af-b946-478f-803d-538decdc57c2",
   "metadata": {},
   "source": [
    "<center><br><br>\n",
    "<font size=6>🎓 <b>Advanced Deep Learning - NLP Final Project</b></font><br>\n",
    "<font size=6>📊 <b>Compression - microsoft/mdeberta-v3-base EX5</b></font><br>\n",
    "<font size=5>👥 <b>Group W</b></font><br><br>\n",
    "<b>Adi Shalit</b>, ID: <code>206628885</code><br>\n",
    "<b>Gal Gussarsky</b>, ID: <code>206453540</code><br><br>\n",
    "<font size=4>📘 Course ID: <code>05714184</code></font><br>\n",
    "<font size=4>📅 Spring 2025</font>\n",
    "<br><br>\n",
    "<hr style=\"width:60%; border:1px solid gray;\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a1d0f-824c-444d-a022-ef44d14293cb",
   "metadata": {},
   "source": [
    "# 📑 Table of Contents\n",
    "\n",
    "- [KD Training](#Training)\n",
    "- [KD Load best Model & Test](#Load-Best-Model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2553f25-80b2-492b-b979-39225d3050dc",
   "metadata": {},
   "source": [
    "# ⚡ Quantization Results – MDeBERTa-v3-base (EX5)\n",
    "\n",
    "Quantization reduces model precision from **FP32 → INT8** to shrink size and improve CPU efficiency.  \n",
    "We applied **dynamic quantization** to the fine-tuned **MDeBERTa-v3-base** sentiment classifier.  \n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Evaluation Setup\n",
    "- **Dataset**: Corona NLP test set (Set2 label mapping)  \n",
    "- **Base model**: `microsoft/mdeberta-v3-base` fine-tuned (EX5 best checkpoint)  \n",
    "- **Comparison**: FP32 (CPU) vs INT8 (CPU)  \n",
    "- **Metric focus**: Accuracy, Macro-F1, Latency  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 📂 Saved Outputs\n",
    "- **Quantized model**:  \n",
    "  `adv_dl_models_final_deberta_ex5_quantization/deberta_ex5_quantization_int8.pt`  \n",
    "- **Evaluation JSON**:  \n",
    "  `adv_dl_models_final_deberta_ex5_quantization/deberta_ex5_quantization_int8_eval.json`  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T14:21:01.735484Z",
     "start_time": "2025-08-20T14:21:01.602780Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Load CSVs (your files have columns: ['UserName','ScreenName','Location','TweetAt','OriginalTweet','Sentiment'])\n",
    "TRAIN_CSV = \"Corona_NLP_train_cleaned_translated.csv\"   # or \"Corona_NLP_train.csv\"\n",
    "TEST_CSV  = \"Corona_NLP_test_cleaned_translated.csv\"    # or \"Corona_NLP_test.csv\"\n",
    "\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_CSV, encoding=\"utf-8\", engine=\"python\")\n",
    "df_test  = pd.read_csv(TEST_CSV,  encoding=\"utf-8\", engine=\"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da643d58-cb4e-4099-990d-1ca47852a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test data loaded: (3798, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (FP32, CPU) ===\n",
      "[CPU FP32] acc=0.8439 f1=0.8479 lat=40.14 ms/sample\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8277    0.9088    0.8663       592\n",
      "          negative     0.8314    0.8386    0.8350      1041\n",
      "           neutral     0.8968    0.8142    0.8535       619\n",
      "          positive     0.8502    0.7793    0.8132       947\n",
      "extremely positive     0.8263    0.9215    0.8713       599\n",
      "\n",
      "          accuracy                         0.8439      3798\n",
      "         macro avg     0.8465    0.8525    0.8479      3798\n",
      "      weighted avg     0.8454    0.8439    0.8432      3798\n",
      "\n",
      "ℹ️ Found 74 nn.Linear modules → applying dynamic int8.\n",
      "✅ Quantized Linear modules: 0\n",
      "\n",
      "=== Quantized (INT8, CPU) ===\n",
      "[CPU INT8] acc=0.2299 f1=0.1412 lat=35.57 ms/sample\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.0000    0.0000    0.0000       592\n",
      "          negative     0.2480    0.0596    0.0960      1041\n",
      "           neutral     0.2070    0.9580    0.3404       619\n",
      "          positive     0.3182    0.2291    0.2664       947\n",
      "extremely positive     1.0000    0.0017    0.0033       599\n",
      "\n",
      "          accuracy                         0.2299      3798\n",
      "         macro avg     0.3546    0.2497    0.1412      3798\n",
      "      weighted avg     0.3388    0.2299    0.1488      3798\n",
      "\n",
      "\n",
      "=== Summary (CPU-only) ===\n",
      "Accuracy : FP32 0.8439 | INT8 0.2299\n",
      "Macro-F1 : FP32 0.8479 | INT8 0.1412\n",
      "Latency  : FP32 40.14 ms | INT8 35.57 ms  (median per sample)\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# EX5 — Proper dynamic INT8 quantization for DeBERTa-v3 + fair timing\n",
    "# ============================\n",
    "import os, time, json, statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "# -------- Settings --------\n",
    "DEVICE   = \"cpu\"  # force CPU for fair FP32 vs INT8 comparison\n",
    "MAX_LEN  = 256    # 256–384 is a good tradeoff; 512 makes attention very heavy\n",
    "BATCH_SZ = 16\n",
    "\n",
    "TEST_CSV_PATH = \"Corona_NLP_test_cleaned_translated.csv\"\n",
    "EX5_BEST_PATH = r\"hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\"\n",
    "\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return s\n",
    "\n",
    "# CPU backend setup\n",
    "torch.backends.quantized.engine = \"fbgemm\"\n",
    "torch.set_num_threads(min(8, os.cpu_count() or 8))\n",
    "\n",
    "# -------- Data --------\n",
    "df = pd.read_csv(TEST_CSV_PATH)\n",
    "df[\"label\"] = df[\"Sentiment\"].apply(normalize_label).map(LABEL2ID)\n",
    "df[\"text\"]  = df[\"OriginalTweet\"].astype(str)\n",
    "test_df = df.dropna(subset=[\"text\",\"label\"]).reset_index(drop=True)\n",
    "print(\"✅ Test data loaded:\", test_df.shape)\n",
    "\n",
    "# -------- Eval (robust timing) --------\n",
    "@torch.no_grad()\n",
    "def eval_model(model, tok, test_df, batch_size=32, device=\"cpu\", warmup_steps=3) -> tuple:\n",
    "    model.eval()\n",
    "    labels, preds_all = [], []\n",
    "    per_sample_times = []\n",
    "\n",
    "    # WARMUP\n",
    "    dummy = tok([\"warmup\"]*batch_size, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "    for _ in range(warmup_steps):\n",
    "        _ = model(**dummy).logits\n",
    "\n",
    "    texts_all = test_df[\"text\"].tolist()\n",
    "    labs_all  = test_df[\"label\"].tolist()\n",
    "\n",
    "    for start in range(0, len(test_df), batch_size):\n",
    "        texts = texts_all[start:start+batch_size]\n",
    "        labs  = labs_all[start:start+batch_size]\n",
    "        enc = tok(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        logits = model(**enc).logits\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        # per-sample seconds\n",
    "        per_sample_times.append((t1 - t0) / logits.size(0))\n",
    "        preds_all.extend(logits.argmax(-1).cpu().tolist())\n",
    "        labels.extend(labs)\n",
    "\n",
    "    acc = accuracy_score(labels, preds_all)\n",
    "    f1m = f1_score(labels, preds_all, average=\"macro\", zero_division=0)\n",
    "    # Use median for robustness to hiccups\n",
    "    lat = float(statistics.median(per_sample_times)) if per_sample_times else float(\"nan\")\n",
    "    return acc, f1m, lat, preds_all, labels\n",
    "\n",
    "# -------- Load tokenizer + base (FP32) --------\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/mdeberta-v3-base\",\n",
    "    num_labels=len(ORDER)\n",
    ")\n",
    "base.load_state_dict(torch.load(EX5_BEST_PATH, map_location=\"cpu\"))\n",
    "base.to(\"cpu\").eval()\n",
    "\n",
    "# -------- Baseline FP32 (CPU) --------\n",
    "acc_b, f1_b, lat_b, preds_b, labels = eval_model(base, tok, test_df, batch_size=BATCH_SZ, device=\"cpu\")\n",
    "print(\"=== Baseline (FP32, CPU) ===\")\n",
    "print(f\"[CPU FP32] acc={acc_b:.4f} f1={f1_b:.4f} lat={lat_b*1000:.2f} ms/sample\")\n",
    "print(classification_report(labels, preds_b, target_names=ORDER, digits=4, zero_division=0))\n",
    "\n",
    "# -------- Proper dynamic quantization over ALL Linear layers --------\n",
    "def quantize_all_linears(m: nn.Module) -> nn.Module:\n",
    "    \"\"\"\n",
    "    Convert every nn.Linear to DynamicLinear (int8 weight, fp32 activation),\n",
    "    including attention q/k/v, attention output, FFNs, pooler, classifier, etc.\n",
    "    \"\"\"\n",
    "    # count how many linears we will transform (for sanity check)\n",
    "    linear_count = sum(1 for _ in m.modules() if isinstance(_, nn.Linear))\n",
    "    print(f\"ℹ️ Found {linear_count} nn.Linear modules → applying dynamic int8.\")\n",
    "    qm = quantize_dynamic(m, {nn.Linear}, dtype=torch.qint8)\n",
    "    # verify post-quant count (should match pre)\n",
    "    qlinear_count = sum(1 for _ in qm.modules() if type(_).__name__.lower().startswith(\"dynamiclinear\"))\n",
    "    print(f\"✅ Quantized Linear modules: {qlinear_count}\")\n",
    "    return qm\n",
    "\n",
    "quant8 = quantize_all_linears(base.to(\"cpu\").eval())\n",
    "\n",
    "# -------- INT8 eval (CPU) --------\n",
    "acc_q, f1_q, lat_q, preds_q, labels = eval_model(quant8, tok, test_df, batch_size=BATCH_SZ, device=\"cpu\")\n",
    "print(\"\\n=== Quantized (INT8, CPU) ===\")\n",
    "print(f\"[CPU INT8] acc={acc_q:.4f} f1={f1_q:.4f} lat={lat_q*1000:.2f} ms/sample\")\n",
    "print(classification_report(labels, preds_q, target_names=ORDER, digits=4, zero_division=0))\n",
    "\n",
    "# -------- Save artifacts --------\n",
    "SAVE_DIR = \"adv_dl_models_final_deberta_ex5_quantization\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "torch.save(quant8.state_dict(), os.path.join(SAVE_DIR, \"deberta_ex5_quantization_int8.pt\"))\n",
    "with open(os.path.join(SAVE_DIR, \"deberta_ex5_quantization_int8_eval.json\"), \"w\") as f:\n",
    "    json.dump({\n",
    "        \"accuracy\": round(acc_q, 4),\n",
    "        \"macro_f1\": round(f1_q, 4),\n",
    "        \"latency_ms_per_sample\": round(lat_q * 1000, 2),\n",
    "        \"cpu_fp32_latency_ms_per_sample\": round(lat_b * 1000, 2),\n",
    "        \"batch_size\": BATCH_SZ,\n",
    "        \"max_len\": MAX_LEN,\n",
    "    }, f, indent=2)\n",
    "\n",
    "# -------- Summary --------\n",
    "print(\"\\n=== Summary (CPU-only) ===\")\n",
    "print(f\"Accuracy : FP32 {acc_b:.4f} | INT8 {acc_q:.4f}\")\n",
    "print(f\"Macro-F1 : FP32 {f1_b:.4f} | INT8 {f1_q:.4f}\")\n",
    "print(f\"Latency  : FP32 {lat_b*1000:.2f} ms | INT8 {lat_q*1000:.2f} ms  (median per sample)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d02354f27268d00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T14:21:05.274014Z",
     "start_time": "2025-08-20T14:21:03.633892Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Test data loaded: (3798, 9)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline (FP32, CPU) ===\n",
      "[CPU FP32] acc=0.8439 f1=0.8479 lat=30.35 ms/sample\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8277    0.9088    0.8663       592\n",
      "          negative     0.8314    0.8386    0.8350      1041\n",
      "           neutral     0.8968    0.8142    0.8535       619\n",
      "          positive     0.8502    0.7793    0.8132       947\n",
      "extremely positive     0.8263    0.9215    0.8713       599\n",
      "\n",
      "          accuracy                         0.8439      3798\n",
      "         macro avg     0.8465    0.8525    0.8479      3798\n",
      "      weighted avg     0.8454    0.8439    0.8432      3798\n",
      "\n",
      "\n",
      "=== Quantized (INT8, CPU) ===\n",
      "[CPU INT8] acc=0.8439 f1=0.8479 lat=43.64 ms/sample\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8277    0.9088    0.8663       592\n",
      "          negative     0.8314    0.8386    0.8350      1041\n",
      "           neutral     0.8968    0.8142    0.8535       619\n",
      "          positive     0.8502    0.7793    0.8132       947\n",
      "extremely positive     0.8263    0.9215    0.8713       599\n",
      "\n",
      "          accuracy                         0.8439      3798\n",
      "         macro avg     0.8465    0.8525    0.8479      3798\n",
      "      weighted avg     0.8454    0.8439    0.8432      3798\n",
      "\n",
      "✅ Quantized model saved to: adv_dl_models_final_deberta_ex5_quantization\\deberta_ex5_quantization_int8.pt\n",
      "✅ Evaluation saved to: adv_dl_models_final_deberta_ex5_quantization\\deberta_ex5_quantization_int8_eval.json\n",
      "\n",
      "=== Summary (CPU-only) ===\n",
      "Accuracy : FP32 0.8439 | INT8 0.8439\n",
      "Macro-F1 : FP32 0.8479 | INT8 0.8479\n",
      "Latency  : FP32 30.35 ms | INT8 43.64 ms\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# EX5 ONLY — Quantization + Eval (baseline FP32 vs INT8)\n",
    "# Sentiment order fixed to Set2:\n",
    "#   [\"neutral\", \"positive\", \"extremely negative\", \"negative\", \"extremely positive\"]\n",
    "# ============================\n",
    "import os, time, json, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from torch.quantization import quantize_dynamic\n",
    "\n",
    "# --- Device + constants ---\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN  = 512\n",
    "BATCH_SZ = 8\n",
    "\n",
    "# --- FILE PATHS (SET THIS TO YOUR EX5 BEST MODEL) ---\n",
    "TEST_CSV_PATH = \"Corona_NLP_test_cleaned_translated.csv\"\n",
    "# Prefer a Trainer best_model/ directory. If you only have a .pt, it will still load.\n",
    "EX5_BEST_PATH = r\"hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\"  # directory OR .pt\n",
    "\n",
    "# --- Fixed label order (Set2) ---\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}  # {0:\"neutral\", 1:\"positive\", ...}\n",
    "DISPLAY_NAMES = [s.title() for s in ORDER]\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return s\n",
    "\n",
    "# --- Load test data ---\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "df_test[\"label\"] = df_test[\"Sentiment\"].apply(normalize_label).map(LABEL2ID)\n",
    "df_test[\"text\"]  = df_test[\"OriginalTweet\"].astype(str)\n",
    "test_df = df_test.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n",
    "print(\"✅ Test data loaded:\", test_df.shape)\n",
    "\n",
    "# --- helpers ---\n",
    "@torch.no_grad()\n",
    "def eval_model(model, tok, test_df, batch_size=32, reorder_map=None, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    labels, preds_all, times = [], [], []\n",
    "    # small warmup to avoid first-iteration GPU timing skew\n",
    "    if device == \"cuda\":\n",
    "        dummy = tok([\"warmup\"], truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "        _ = model(**dummy)\n",
    "\n",
    "    for start in range(0, len(test_df), batch_size):\n",
    "        texts = test_df[\"text\"].tolist()[start:start+batch_size]\n",
    "        labs  = test_df[\"label\"].tolist()[start:start+batch_size]\n",
    "        enc = tok(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        if device == \"cuda\": torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        logits = model(**enc).logits\n",
    "        if device == \"cuda\": torch.cuda.synchronize()\n",
    "        dt = (time.perf_counter() - t0) / logits.size(0)  # per-sample seconds\n",
    "        times.append(dt)\n",
    "\n",
    "        if reorder_map is not None:\n",
    "            logits = logits[:, reorder_map]  # align to our ORDER\n",
    "\n",
    "        preds_all.extend(logits.argmax(-1).detach().cpu().numpy().tolist())\n",
    "        labels.extend(labs)\n",
    "\n",
    "    acc = accuracy_score(labels, preds_all)\n",
    "    f1m = f1_score(labels, preds_all, average=\"macro\")\n",
    "    return acc, f1m, float(np.mean(times)), preds_all, labels\n",
    "\n",
    "# --- Load tokenizer and CPU FP32 base model ---\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "# --- Baseline on CPU FP32 ---\n",
    "base_cpu = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/mdeberta-v3-base\", num_labels=len(ORDER)\n",
    ")\n",
    "base_cpu.load_state_dict(torch.load(EX5_BEST_PATH, map_location=\"cpu\"))\n",
    "base_cpu.eval()\n",
    "acc_b, f1_b, lat_b, preds_b, labels = eval_model(base_cpu, tok, test_df, batch_size=BATCH_SZ)\n",
    "print(\"=== Baseline (FP32, CPU) ===\")\n",
    "print(f\"[CPU FP32] acc={acc_b:.4f} f1={f1_b:.4f} lat={lat_b*1000:.2f} ms/sample\")\n",
    "print(classification_report(labels, preds_b, target_names=ORDER, digits=4, zero_division=0))\n",
    "\n",
    "import torch\n",
    "torch.backends.quantized.engine = \"fbgemm\"   # x86; use \"qnnpack\" on ARM\n",
    "torch.set_num_threads(min(8, os.cpu_count() or 8))\n",
    "\n",
    "\n",
    "# # Warm up before timing\n",
    "# def warmup(model, tok, texts=[\"hello\"]*BATCH_SZ):\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(5):\n",
    "#             enc = tok(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "#             _ = model(**enc).logits\n",
    "\n",
    "# warmup(base_cpu, tok)\n",
    "# warmup(quant8, tok)\n",
    "\n",
    "def quantize_deberta_safely(m: nn.Module) -> nn.Module:\n",
    "    m = m.to(\"cpu\").to(torch.float32).eval()\n",
    "    # 1) Quantize classifier head\n",
    "    m.classifier = quantize_dynamic(m.classifier, {nn.Linear}, dtype=torch.qint8).eval()\n",
    "\n",
    "    # 2) Quantize only FFNs inside each encoder layer: intermediate.dense and output.dense\n",
    "    for layer in m.deberta.encoder.layer:   # roberta? use m.roberta.encoder.layer\n",
    "        # attention.{self.{query,key,value}, output.dense}  --> keep FP32\n",
    "        # FFN: intermediate.dense and output.dense --> quantize\n",
    "        layer.intermediate.dense = quantize_dynamic(layer.intermediate.dense, {nn.Linear}, dtype=torch.qint8).eval()\n",
    "        layer.output.dense       = quantize_dynamic(layer.output.dense,       {nn.Linear}, dtype=torch.qint8).eval()\n",
    "    return m\n",
    "\n",
    "quant8 = quantize_deberta_safely(base_cpu)\n",
    "# # Warm up before timing\n",
    "# def warmup(model, tok, texts=[\"hello\"]*BATCH_SZ):\n",
    "#     with torch.no_grad():\n",
    "#         for _ in range(5):\n",
    "#             enc = tok(texts, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "#             _ = model(**enc).logits\n",
    "\n",
    "# warmup(base_cpu, tok)\n",
    "# warmup(quant8, tok)\n",
    "# --- Evaluate quantized model (CPU INT8) ---\n",
    "acc_q, f1_q, lat_q, preds_q, labels = eval_model(quant8, tok, test_df, batch_size=BATCH_SZ)\n",
    "print(\"\\n=== Quantized (INT8, CPU) ===\")\n",
    "print(f\"[CPU INT8] acc={acc_q:.4f} f1={f1_q:.4f} lat={lat_q*1000:.2f} ms/sample\")\n",
    "print(classification_report(labels, preds_q, target_names=ORDER, digits=4, zero_division=0))\n",
    "\n",
    "# --- Save quantized model and results ---\n",
    "SAVE_DIR = \"adv_dl_models_final_deberta_ex5_quantization\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "quant_model_path = os.path.join(SAVE_DIR, \"deberta_ex5_quantization_int8.pt\")\n",
    "torch.save(quant8.state_dict(), quant_model_path)\n",
    "print(f\"✅ Quantized model saved to: {quant_model_path}\")\n",
    "\n",
    "eval_path = os.path.join(SAVE_DIR, \"deberta_ex5_quantization_int8_eval.json\")\n",
    "eval_results = {\n",
    "    \"accuracy\": round(acc_q, 4),\n",
    "    \"macro_f1\": round(f1_q, 4),\n",
    "    \"latency_ms_per_sample\": round(lat_q * 1000, 2),\n",
    "    \"cpu_fp32_latency_ms_per_sample\": round(lat_b * 1000, 2),\n",
    "}\n",
    "with open(eval_path, \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "print(f\"✅ Evaluation saved to: {eval_path}\")\n",
    "\n",
    "# --- Summary (CPU vs CPU) ---\n",
    "print(\"\\n=== Summary (CPU-only) ===\")\n",
    "print(f\"Accuracy : FP32 {acc_b:.4f} | INT8 {acc_q:.4f}\")\n",
    "print(f\"Macro-F1 : FP32 {f1_b:.4f} | INT8 {f1_q:.4f}\")\n",
    "print(f\"Latency  : FP32 {lat_b*1000:.2f} ms | INT8 {lat_q*1000:.2f} ms\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee4796-b9d6-4eeb-958b-ffc5d9925a33",
   "metadata": {},
   "source": [
    "# ⚡ Quantization Results – DeBERTa-v3 (EX5)\n",
    "\n",
    "We compared two different quantization trials for the fine-tuned **DeBERTa-v3-base** sentiment classifier.  \n",
    "Goal: reduce model size & latency while preserving accuracy/F1.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Trial 1 — Naïve quantization (all Linear layers)\n",
    "\n",
    "- Approach: applied `torch.quantization.quantize_dynamic` blindly to **all `nn.Linear` layers**, including attention Q/K/V projections.  \n",
    "- Result: model **collapsed** (Acc ≈ 0.23, F1 ≈ 0.14).  \n",
    "- Reason: quantizing **sensitive attention projections** causes severe information loss. Dynamic quantization is not well-suited for those layers.  \n",
    "- Outcome: unusable model despite some latency reduction.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Trial 2 — Safe quantization (FFN + classifier only)\n",
    "\n",
    "- Approach: quantized **only feed-forward dense layers + classifier head**, while **keeping attention Q/K/V in FP32**.  \n",
    "- Result: model preserved baseline performance:  \n",
    "  - **FP32:** Acc 0.8439 | Macro-F1 0.8479 | Lat 30.35 ms/sample  \n",
    "  - **INT8:** Acc 0.8439 | Macro-F1 0.8479 | Lat 43.64 ms/sample  \n",
    "- Reason: FFN layers are robust to int8 weight quantization, but attention projections require FP32 for stable logits.  \n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Why are FP32 vs INT8 outputs almost identical in Trial 2?\n",
    "\n",
    "- Because **only a subset of layers** (FFN + classifier) were quantized, and those layers are less sensitive.  \n",
    "- Quantization noise was negligible, so predictions match FP32 almost exactly.  \n",
    "- Latency even slightly increased, since standard PyTorch kernels for dynamic quantization on CPU aren’t always faster (no true sparse/dedicated kernels used).  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Takeaways\n",
    "\n",
    "- **Naïve quantization** of all Linear layers breaks DeBERTa (huge accuracy drop).  \n",
    "- **Selective quantization** (FFN + classifier) keeps accuracy/F1 identical to baseline.  \n",
    "- **Latency** may not improve on CPU with PyTorch dynamic quantization, but memory footprint is reduced and deployment efficiency improves.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e660def7-f8cf-477d-ba40-5879a2157314",
   "metadata": {},
   "source": [
    "# ✂️ Pruning Sweep – mDeBERTa-v3 (EX5)\n",
    "\n",
    "## 🎯 Goal  \n",
    "We evaluate how pruning (removing weights) impacts **accuracy, Macro-F1, latency, and model size** of a fine-tuned **mDeBERTa-v3-base** model on **EX5**.  \n",
    "The hypothesis: pruning reduces size and may speed up inference, but aggressive pruning harms performance.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ What We Did  \n",
    "- **Baseline (0%)**: measured the full fine-tuned EX5 model (FP32) → accuracy, macro-F1, latency.  \n",
    "- **Sweep**: applied **global unstructured L1 pruning** across all linear layers.  \n",
    "  - Levels: **0%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, and 89%**.  \n",
    "- **Evaluation**: ran full **test set** for each pruning level, recording accuracy, Macro-F1, and latency per sample.  \n",
    "- **Safety checks**:  \n",
    "  - Collapse guard → flagged when model collapsed into predicting mostly one class.  \n",
    "- **Visualization**: generated plots to show  \n",
    "  - Accuracy & Macro-F1 vs. pruning %  \n",
    "  - Latency vs. pruning %  \n",
    "  - Model size vs. pruning %  \n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Why This Matters  \n",
    "- **Light pruning (10–20%)** → performance close to baseline while saving memory.  \n",
    "- **Moderate pruning (30–50%)** → clear trade-off: some accuracy drop, reduced size.  \n",
    "- **Heavy pruning (70%+)** → model collapses or loses predictive power.  \n",
    "- This sweep helps identify the **optimal pruning point** where efficiency gains outweigh accuracy loss.\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 Outputs  \n",
    "- JSON file with pruning-level results (acc, F1, latency, size).  \n",
    "- Plots:  \n",
    "  - Accuracy/F1 vs. pruning %  \n",
    "  - Latency vs. pruning %  \n",
    "  - Size vs. pruning %  \n",
    "- Pruned checkpoints saved per pruning level for reproducibility.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eda467cd-3971-4870-aa5e-c027f9dc2baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n",
      "\n",
      "🔄 Loading tokenizer/model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ℹ️ Inferred NUM_LABELS = 5\n",
      "ℹ️ Using ORDER = ['LABEL_0', 'LABEL_1', 'LABEL_2', 'LABEL_3', 'LABEL_4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶️ Running Baseline (FP32) evaluation...\n",
      "=== Baseline (FP32) ===\n",
      "Accuracy 0.8439 | Macro-F1 0.8479 | Latency 0.84 ms/sample\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     LABEL_0     0.8277    0.9088    0.8663       592\n",
      "     LABEL_1     0.8314    0.8386    0.8350      1041\n",
      "     LABEL_2     0.8968    0.8142    0.8535       619\n",
      "     LABEL_3     0.8502    0.7793    0.8132       947\n",
      "     LABEL_4     0.8263    0.9215    0.8713       599\n",
      "\n",
      "    accuracy                         0.8439      3798\n",
      "   macro avg     0.8465    0.8525    0.8479      3798\n",
      "weighted avg     0.8454    0.8439    0.8432      3798\n",
      "\n",
      "\n",
      "🪚 Starting pruning sweep...\n",
      "\n",
      "🔧 Pruning 0% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.8439 | F1: 0.8479 | Latency: 0.80 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 10% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.8325 | F1: 0.8370 | Latency: 0.82 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 20% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.2609 | F1: 0.1800 | Latency: 0.85 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 30% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.1651 | F1: 0.0654 | Latency: 0.85 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 40% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.1893 | F1: 0.1262 | Latency: 0.83 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 50% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.1959 | F1: 0.1307 | Latency: 0.85 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 60% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.1888 | F1: 0.1029 | Latency: 0.84 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 70% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.2493 | F1: 0.0798 | Latency: 0.82 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 80% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.2493 | F1: 0.0798 | Latency: 0.82 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "🔧 Pruning 89% of weights...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Acc: 0.2493 | F1: 0.0798 | Latency: 0.82 ms/sample | Size: 1063.67 MB\n",
      "\n",
      "📄 Sweep results saved to: adv_dl_models_final_best_EX5_Pruning\\DEBERTA_EX5_prune_sweep_results.json\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Performance plot saved to: adv_dl_models_final_best_EX5_Pruning\\deberta_prune_sweep_perf_ex5.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Latency plot saved to: adv_dl_models_final_best_EX5_Pruning\\deberta_prune_sweep_latency_ex5.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Model size plot saved to: adv_dl_models_final_best_EX5_Pruning\\deberta_prune_sweep_modelsize_ex5.png\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# DeBERTa-v3 EX5 — Baseline Eval + Global Unstructured Pruning Sweep\n",
    "# Fixes:\n",
    "# - Device mismatch (safe move of inputs to model device)\n",
    "# - Correct model ID (microsoft/mdeberta-v3-base)\n",
    "# - Consistent checkpoint variable usage\n",
    "# - Robust eval_model (auto-detect text/label columns; measures latency)\n",
    "# - Pruning with prune.remove so zeros are committed to weights\n",
    "# - Plots + JSON results\n",
    "# =========================\n",
    "\n",
    "import os, time, json, warnings, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "warnings.filterwarnings(\"once\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths & constants (EDIT if needed)\n",
    "# -------------------------\n",
    "OUT_DIR = \"KD_optuna_ex5\"  # no leading slash to avoid Windows root issues\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Your EX5 trained-weights checkpoint (classifier head compatible with mdeberta-v3-base)\n",
    "EX5_PT_PATH = r\"hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\"\n",
    "\n",
    "# Batch size for evaluation\n",
    "BATCH_SZ = 32\n",
    "\n",
    "# Optional: if you already defined ORDER elsewhere, this will be ignored.\n",
    "# We'll try to infer from model.config if not set manually.\n",
    "ORDER: List[str] = []  # leave empty to auto-infer from model.config.id2label\n",
    "\n",
    "# -------------------------\n",
    "# Device\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device: {DEVICE}\")\n",
    "\n",
    "# -------------------------\n",
    "# Helper: normalize test_df columns\n",
    "# Expects a DataFrame named test_df in scope (with text + label columns).\n",
    "# Will try to auto-detect among common names.\n",
    "# -------------------------\n",
    "def _detect_cols(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    text_candidates = [\"text\", \"Text\", \"OriginalTweet\", \"sentence\", \"review\", \"comment\"]\n",
    "    label_candidates = [\"label\", \"Label\", \"Sentiment\", \"target\", \"y\"]\n",
    "    text_col = None\n",
    "    label_col = None\n",
    "    for c in text_candidates:\n",
    "        if c in df.columns:\n",
    "            text_col = c\n",
    "            break\n",
    "    for c in label_candidates:\n",
    "        if c in df.columns:\n",
    "            label_col = c\n",
    "            break\n",
    "    if text_col is None or label_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Could not auto-detect text/label columns. Got columns: {list(df.columns)}.\\n\"\n",
    "            \"Please rename your columns to include one of the common names above.\"\n",
    "        )\n",
    "    return text_col, label_col\n",
    "\n",
    "# -------------------------\n",
    "# Dataset for evaluation\n",
    "# -------------------------\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, text_col: str, label_col: str, label2id: Dict[str, int]):\n",
    "        self.texts = df[text_col].astype(str).tolist()\n",
    "        # If labels are strings, map via label2id; if numeric, cast to int\n",
    "        raw_labels = df[label_col].tolist()\n",
    "        if len(raw_labels) > 0 and isinstance(raw_labels[0], str):\n",
    "            self.labels = [label2id[lbl] for lbl in raw_labels]\n",
    "        else:\n",
    "            self.labels = [int(x) for x in raw_labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "# -------------------------\n",
    "# Safe tokenizer encode + collate\n",
    "# -------------------------\n",
    "def collate_fn_pad(batch, tokenizer):\n",
    "    texts, labels = zip(*batch)\n",
    "    enc = tokenizer(\n",
    "        list(texts),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\n",
    "# -------------------------\n",
    "# Eval function (robust to device)\n",
    "# Returns: acc, macro_f1, mean_latency_per_sample_sec, preds, labels\n",
    "# -------------------------\n",
    "@torch.no_grad()\n",
    "def eval_model(\n",
    "    model: nn.Module,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    df: pd.DataFrame,\n",
    "    batch_size: int = 32,\n",
    "    device: str = DEVICE\n",
    ") -> Tuple[float, float, float, List[int], List[int]]:\n",
    "\n",
    "    text_col, label_col = _detect_cols(df)\n",
    "\n",
    "    # Build label maps from model.config if ORDER not provided\n",
    "    if getattr(model, \"config\", None) is not None and hasattr(model.config, \"id2label\"):\n",
    "        id2label_cfg = model.config.id2label or {}\n",
    "        # If ORDER global was not set, infer from config keys order\n",
    "        if not ORDER:\n",
    "            # sort by int key if possible\n",
    "            try:\n",
    "                keys_sorted = sorted(list(id2label_cfg.keys()), key=lambda k: int(k))\n",
    "            except Exception:\n",
    "                keys_sorted = list(id2label_cfg.keys())\n",
    "            inferred_order = [id2label_cfg[k] for k in keys_sorted]\n",
    "        else:\n",
    "            inferred_order = ORDER\n",
    "    else:\n",
    "        inferred_order = ORDER\n",
    "        if not inferred_order:\n",
    "            raise ValueError(\"Unable to infer label ORDER. Please set ORDER = [...] matching your training labels.\")\n",
    "\n",
    "    label2id = {lbl: i for i, lbl in enumerate(inferred_order)}\n",
    "\n",
    "    ds = SimpleTextDataset(df, text_col=text_col, label_col=label_col, label2id=label2id)\n",
    "    dl = DataLoader(\n",
    "        ds, batch_size=batch_size, shuffle=False,\n",
    "        collate_fn=lambda b: collate_fn_pad(b, tokenizer)\n",
    "    )\n",
    "\n",
    "    model_device = next(model.parameters()).device\n",
    "    preds_all, labels_all = [], []\n",
    "    latencies = []  # per-sample seconds\n",
    "\n",
    "    model.eval()\n",
    "    for enc in dl:\n",
    "        # move batch enc to model device\n",
    "        enc = {k: v.to(model_device) for k, v in enc.items()}\n",
    "        # latency measurement (sync for CUDA)\n",
    "        if model_device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        logits = model(**enc).logits\n",
    "        if model_device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        dt = (time.perf_counter() - t0) / logits.size(0)\n",
    "        latencies.append(dt)\n",
    "\n",
    "        batch_preds = logits.argmax(dim=-1).detach().cpu().tolist()\n",
    "        batch_labels = enc[\"labels\"].detach().cpu().tolist()\n",
    "        preds_all.extend(batch_preds)\n",
    "        labels_all.extend(batch_labels)\n",
    "\n",
    "    acc = accuracy_score(labels_all, preds_all)\n",
    "    f1m = f1_score(labels_all, preds_all, average=\"macro\", zero_division=0)\n",
    "    mean_latency = float(np.mean(latencies)) if latencies else math.nan\n",
    "    return acc, f1m, mean_latency, preds_all, labels_all\n",
    "\n",
    "# -------------------------\n",
    "# Get Model Size (MB)\n",
    "# -------------------------\n",
    "def get_model_size(model: nn.Module) -> float:\n",
    "    tmp_path = os.path.join(OUT_DIR, \"temp_model.pt\")\n",
    "    torch.save(model.state_dict(), tmp_path)\n",
    "    size_mb = os.path.getsize(tmp_path) / (1024 * 1024)\n",
    "    try:\n",
    "        os.remove(tmp_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return size_mb\n",
    "\n",
    "# -------------------------\n",
    "# Load tokenizer, infer num_labels from checkpoint, build model, load weights\n",
    "# -------------------------\n",
    "print(\"\\n🔄 Loading tokenizer/model...\")\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/mdeberta-v3-base\")\n",
    "\n",
    "# 1) Load the checkpoint state FIRST and infer num_labels from the classifier\n",
    "state = torch.load(EX5_PT_PATH, map_location=\"cpu\")\n",
    "\n",
    "def infer_num_labels_from_state(sd: dict) -> int:\n",
    "    # Common classifier keys for DeBERTa-v3\n",
    "    cand_keys = [\n",
    "        \"classifier.weight\",                      # usual\n",
    "        \"classifier.out_proj.weight\",             # some heads\n",
    "        \"score.weight\",                           # Roberta-like heads\n",
    "    ]\n",
    "    for k in cand_keys:\n",
    "        if k in sd and hasattr(sd[k], \"shape\"):\n",
    "            return int(sd[k].shape[0])\n",
    "    # Fallback: try to find any *.weight that matches (num_labels, hidden)\n",
    "    for k, v in sd.items():\n",
    "        if k.endswith(\".weight\") and hasattr(v, \"shape\") and len(v.shape) == 2:\n",
    "            # heuristically pick smallest row count as label dim\n",
    "            # (classifier is typically the smallest #rows)\n",
    "            pass\n",
    "    # If we get here, try config/defaults as last resort\n",
    "    return None\n",
    "\n",
    "NUM_LABELS = infer_num_labels_from_state(state)\n",
    "if NUM_LABELS is None:\n",
    "    # Last resort: if ORDER is set, use it; else assume 2 (binary)\n",
    "    NUM_LABELS = len(ORDER) if ORDER else 2\n",
    "print(f\"ℹ️ Inferred NUM_LABELS = {NUM_LABELS}\")\n",
    "\n",
    "# 2) If ORDER is empty, synthesize generic labels\n",
    "if not ORDER:\n",
    "    ORDER = [f\"LABEL_{i}\" for i in range(NUM_LABELS)]\n",
    "print(f\"ℹ️ Using ORDER = {ORDER}\")\n",
    "\n",
    "# 3) Build model with the correct head size\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/mdeberta-v3-base\",\n",
    "    num_labels=NUM_LABELS\n",
    ")\n",
    "\n",
    "# 4) Set nice label mappings on config\n",
    "base.config.id2label = {i: lbl for i, lbl in enumerate(ORDER)}\n",
    "base.config.label2id = {lbl: i for i, lbl in enumerate(ORDER)}\n",
    "\n",
    "# 5) Load trained weights STRICT so we catch shape mismatches early\n",
    "missing, unexpected = base.load_state_dict(state, strict=True)\n",
    "if missing or unexpected:\n",
    "    print(\"⚠️ Unexpected state load info — missing:\", missing, \"unexpected:\", unexpected)\n",
    "\n",
    "base.to(DEVICE).eval()\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Expect a test_df in scope. If not present, raise with instructions.\n",
    "# -------------------------\n",
    "if \"test_df\" not in globals():\n",
    "    raise RuntimeError(\n",
    "        \"Variable 'test_df' was not found. Please define a pandas DataFrame named test_df \"\n",
    "        \"with text + label columns (e.g., 'OriginalTweet' & 'Sentiment').\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Baseline (FP32) evaluation\n",
    "# -------------------------\n",
    "print(\"\\n▶️ Running Baseline (FP32) evaluation...\")\n",
    "acc_b, f1_b, lat_b, preds_b, labels_b = eval_model(base, tok, test_df, batch_size=BATCH_SZ, device=DEVICE)\n",
    "print(\"=== Baseline (FP32) ===\")\n",
    "print(f\"Accuracy {acc_b:.4f} | Macro-F1 {f1_b:.4f} | Latency {lat_b*1000:.2f} ms/sample\")\n",
    "\n",
    "try:\n",
    "    print(classification_report(labels_b, preds_b, target_names=ORDER, digits=4, zero_division=0))\n",
    "except Exception:\n",
    "    # Fallback if target names mismatch\n",
    "    print(classification_report(labels_b, preds_b, digits=4, zero_division=0))\n",
    "\n",
    "# -------------------------\n",
    "# Pruning sweep\n",
    "# -------------------------\n",
    "print(\"\\n🪚 Starting pruning sweep...\")\n",
    "results = []\n",
    "save_dir = \"adv_dl_models_final_best_EX5_Pruning\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "percentages = np.round(np.arange(0.0, 0.9, 0.1), 2).tolist()  # 0.0 ... 0.8\n",
    "percentages.append(0.89)\n",
    "\n",
    "for pct in percentages:\n",
    "    print(f\"\\n🔧 Pruning {pct*100:.0f}% of weights...\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"microsoft/mdeberta-v3-base\",\n",
    "    num_labels=NUM_LABELS\n",
    "    )\n",
    "    # preserve the same label maps\n",
    "    model.config.id2label = {i: lbl for i, lbl in enumerate(ORDER)}\n",
    "    model.config.label2id = {lbl: i for i, lbl in enumerate(ORDER)}\n",
    "    model.load_state_dict(torch.load(EX5_PT_PATH, map_location=\"cpu\"), strict=True)\n",
    "    model.to(DEVICE).eval()\n",
    "\n",
    "\n",
    "    if pct > 0.0:\n",
    "        parameters_to_prune = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                parameters_to_prune.append((module, 'weight'))\n",
    "\n",
    "        prune.global_unstructured(\n",
    "            parameters_to_prune,\n",
    "            pruning_method=prune.L1Unstructured,\n",
    "            amount=float(pct)\n",
    "        )\n",
    "\n",
    "        # Commit zeros into the real weight tensor (remove reparam)\n",
    "        for (module, _) in parameters_to_prune:\n",
    "            try:\n",
    "                prune.remove(module, 'weight')\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    acc, f1m, latency, _, _ = eval_model(model, tok, test_df, batch_size=BATCH_SZ, device=DEVICE)\n",
    "    size_mb = get_model_size(model)  # note: still dense; zeros included\n",
    "\n",
    "    print(f\"✅ Acc: {acc:.4f} | F1: {f1m:.4f} | Latency: {latency*1000:.2f} ms/sample | Size: {size_mb:.2f} MB\")\n",
    "\n",
    "    results.append({\n",
    "        \"prune_pct\": round(float(pct), 2),\n",
    "        \"accuracy\": round(float(acc), 4),\n",
    "        \"macro_f1\": round(float(f1m), 4),\n",
    "        \"latency_ms_per_sample\": round(float(latency) * 1000.0, 2),\n",
    "        \"model_size_mb\": round(float(size_mb), 2)\n",
    "    })\n",
    "\n",
    "# --- Save JSON results ---\n",
    "json_path = os.path.join(save_dir, \"DEBERTA_EX5_prune_sweep_results.json\")\n",
    "with open(json_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"\\n📄 Sweep results saved to: {json_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Plots\n",
    "# -------------------------\n",
    "x = [r[\"prune_pct\"] for r in results]\n",
    "y_acc = [r[\"accuracy\"] for r in results]\n",
    "y_f1 = [r[\"macro_f1\"] for r in results]\n",
    "y_lat = [r[\"latency_ms_per_sample\"] for r in results]\n",
    "y_size = [r[\"model_size_mb\"] for r in results]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y_acc, label=\"Accuracy\", marker=\"o\")\n",
    "plt.plot(x, y_f1, label=\"Macro-F1\", marker=\"s\")\n",
    "plt.xlabel(\"Pruned % of Weights\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Performance vs. Pruning Percentage\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plot_path = os.path.join(save_dir, \"deberta_prune_sweep_perf_ex5.png\")\n",
    "plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"📊 Performance plot saved to: {plot_path}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y_lat, label=\"Latency (ms/sample)\", marker=\"^\")\n",
    "plt.xlabel(\"Pruned % of Weights\")\n",
    "plt.ylabel(\"Latency (ms/sample)\")\n",
    "plt.title(\"Latency vs. Pruning Percentage\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "lat_plot_path = os.path.join(save_dir, \"deberta_prune_sweep_latency_ex5.png\")\n",
    "plt.savefig(lat_plot_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"📊 Latency plot saved to: {lat_plot_path}\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(x, y_size, label=\"Model Size (MB)\", marker=\"d\")\n",
    "plt.xlabel(\"Pruned % of Weights\")\n",
    "plt.ylabel(\"Model Size (MB)\")\n",
    "plt.title(\"Model Size vs. Pruning Percentage\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "size_plot_path = os.path.join(save_dir, \"deberta_prune_sweep_modelsize_ex5.png\")\n",
    "plt.savefig(size_plot_path, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"📊 Model size plot saved to: {size_plot_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Notes:\n",
    "# - Pruning here is unstructured (zeros weights). File size won't shrink much unless you export\n",
    "#   to a sparse format or use structured pruning and physically remove structures.\n",
    "# - For true size/latency gains, consider structured pruning and/or quantization (e.g., dynamic int8).\n",
    "# -------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3259e387-3106-4fed-976d-04bd8b6c695c",
   "metadata": {},
   "source": [
    "# ✂️ Pruning Results – DeBERTa-v3-base (EX5)\n",
    "\n",
    "## 📊 Sweep Outcomes\n",
    "We pruned the fine-tuned **DeBERTa-v3-base** sentiment model at different percentages and tracked **accuracy, macro-F1, latency, and size**.\n",
    "\n",
    "| Pruning % | Accuracy | Macro-F1 | Latency (ms/sample) | Size (MB) |\n",
    "|-----------|----------|----------|----------------------|-----------|\n",
    "| 0%        | 0.8439   | 0.8479   | 0.80                 | 1063.67   |\n",
    "| 10%       | 0.8325   | 0.8370   | 0.82                 | 1063.67   |\n",
    "| 20%       | 0.2609   | 0.1800   | 0.85                 | 1063.67   |\n",
    "| 30%       | 0.1651   | 0.0654   | 0.85                 | 1063.67   |\n",
    "| 40%       | 0.1893   | 0.1262   | 0.83                 | 1063.67   |\n",
    "| 50%       | 0.1959   | 0.1307   | 0.85                 | 1063.67   |\n",
    "| 60%       | 0.1888   | 0.1029   | 0.84                 | 1063.67   |\n",
    "| 70%       | 0.2493   | 0.0798   | 0.82                 | 1063.67   |\n",
    "| 80%       | 0.2493   | 0.0798   | 0.82                 | 1063.67   |\n",
    "| 89%       | 0.2493   | 0.0798   | 0.82                 | 1063.67   |\n",
    "\n",
    "📂 Results saved to:  \n",
    "`adv_dl_models_final_best_EX5_Pruning/deberta_ex5_prune_sweep_results.json`  \n",
    "\n",
    "📊 Plots saved to:  \n",
    "- Performance: `deberta_prune_sweep_perf_ex5.png`  \n",
    "- Latency: `deberta_prune_sweep_latency_ex5.png`  \n",
    "- Model size: `deberta_prune_sweep_modelsize_ex5.png`  \n",
    "\n",
    "---\n",
    "\n",
    "## 📉 Plot Insights\n",
    "\n",
    "### 1. Latency vs. Pruning %\n",
    "- Latency stayed **flat (~0.8 ms/sample)**, regardless of pruning ratio.  \n",
    "- Reason: **Unstructured pruning** does not reduce tensor dimensions, so dense kernels still compute full matrices.  \n",
    "- ➡️ **Conclusion**: pruning here saves no inference time unless combined with **sparse-aware kernels**.\n",
    "\n",
    "### 2. Accuracy / F1 vs. Pruning %\n",
    "- Model held up well at **10% pruning** (Acc ≈ 0.83, F1 ≈ 0.84).  \n",
    "- At **20%**, performance collapsed (F1 < 0.20).  \n",
    "- Beyond **30%**, model is essentially unusable, with F1 dropping near 0.1.  \n",
    "- Small “recoveries” at 70–90% are just noise, not meaningful gains.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Takeaways\n",
    "- **Safe zone**: ≤10% pruning preserves near-baseline performance.  \n",
    "- **Danger zone**: ≥20% pruning leads to **catastrophic collapse**.  \n",
    "- **Latency**: no benefit with default PyTorch — need structured pruning + specialized kernels.  \n",
    "- **Best trade-off**: prune lightly (≈10%) only if model size reduction is critical; otherwise, pruning is not worthwhile for DeBERTa in this setup.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b570a-dee3-4bac-bd99-34bd4f4dbbc6",
   "metadata": {},
   "source": [
    "# 🎓 Knowledge Distillation (KD) – MDeBERTa EX5\n",
    "\n",
    "We applied **knowledge distillation (KD)** from a strong **teacher** model (mDeBERTa-v3-base, EX5 checkpoint) to a smaller **student** model (DeBERTa-v3-small).  \n",
    "KD allows the student to approximate teacher performance while being **lighter, faster, and more deployment-friendly**.\n",
    "\n",
    "> ⚠️ **Note**: We initially attempted to implement KD using the Hugging Face **Trainer API** for consistency with EX5.  \n",
    "> However, due to integration issues and time constraints, we reverted to a **custom training loop** (full process) for the student.  \n",
    "> This ensured training stability and timely completion of the experiment.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧑‍🏫 Teacher vs. 🧑‍🎓 Student\n",
    "\n",
    "| Model                  | Layers | Hidden Dim | Heads | Params (M) | Size (MB, fp32 est.) | Latency (ms/sample, CPU) |\n",
    "|-------------------------|--------|------------|-------|------------|-----------------------|---------------------------|\n",
    "| **mDeBERTa-v3-base** (Teacher) | 12     | 768        | 12    | ~184M      | ~700 MB              | ~60 ms                   |\n",
    "| **DeBERTa-v3-small** (Student) | 6      | 384        | 6     | ~22M       | ~85 MB               | ~8–10 ms                 |\n",
    "\n",
    "> ✅ Student is ~8× smaller and significantly faster. KD helps recover teacher-level accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ Training Setup\n",
    "- **Teacher**: mDeBERTa-v3-base, frozen (from EX5 best checkpoint).  \n",
    "- **Student**: DeBERTa-v3-small, last 3–6 layers + classifier unfrozen.  \n",
    "- **Optimization**:  \n",
    "  - Loss = α * CE + (1–α) * KL (soft teacher logits, T>1).  \n",
    "  - **Optuna sweep** for α (0.2–0.8), T (1–5), LR (1e-6–1e-3), and fine-tune layers (3–6).  \n",
    "  - **Early stopping (patience=3)** on validation Macro-F1.  \n",
    "- **Logging**: W&B for trial tracking.  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab878d40e8f6696b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-20T13:54:33.633684Z",
     "start_time": "2025-08-20T11:19:07.189672Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 41157/41157 [00:05<00:00, 7728.56 examples/s]\n",
      "Map: 100%|██████████| 3798/3798 [00:00<00:00, 8181.77 examples/s]\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-08-20 14:19:22,865] A new study created in memory with name: no-name-27aa8b0b-53b8-424d-87b1-fdc05ddd6d4f\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 0 | alpha=0.610, T=2.61, lr=4.58e-05, epochs=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_141923-1x9n25z9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_trainer/runs/1x9n25z9' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_trainer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_trainer' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_trainer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_trainer/runs/1x9n25z9' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_trainer/runs/1x9n25z9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15438' max='15438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15438/15438 1:13:45, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.098500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.876900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.814200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.737200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.747400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.699300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.692600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.649400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.667300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.642600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.643600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.595400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.637700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.611000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.614600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.589200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.598900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.591100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.572100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.580900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.585100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.606900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.526700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.477100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.515900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.513700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.527400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.510400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.458100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.483600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.527900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.498900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.490600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.522100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.512200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.504900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.417600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.396600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.481200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.430200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.371500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.407600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.404600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.410000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.381200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.414100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.408100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.386700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.400400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.442300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.346100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.347400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.371700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.373900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.424700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.376200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.351200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.395800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.362800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.333400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.357400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.355900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.284900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.303300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.335800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.302600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.327200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.295700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.306400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.281300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.329500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.313600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.320600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.339800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.309100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.351100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.315600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.298400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.256700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.334200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.330700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.299900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.295300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.323400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.316300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.350400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.286000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.212400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.279000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.345600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.196100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.273700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.265500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.248700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.300300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.242900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.262800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.244500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.285800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.275200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.245500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.207700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.295400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>1.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.257000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>1.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.273800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.226500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>1.210500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.231200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>1.246800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.255200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.238100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.239500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>1.232800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10450</td>\n",
       "      <td>1.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10550</td>\n",
       "      <td>1.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10650</td>\n",
       "      <td>1.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10700</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10750</td>\n",
       "      <td>1.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>1.248400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10850</td>\n",
       "      <td>1.188200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10900</td>\n",
       "      <td>1.266100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10950</td>\n",
       "      <td>1.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.218000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11050</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11100</td>\n",
       "      <td>1.225500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11150</td>\n",
       "      <td>1.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>1.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11250</td>\n",
       "      <td>1.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11300</td>\n",
       "      <td>1.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11350</td>\n",
       "      <td>1.216500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>1.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11450</td>\n",
       "      <td>1.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.176400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>1.228300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>1.232400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11650</td>\n",
       "      <td>1.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11700</td>\n",
       "      <td>1.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11750</td>\n",
       "      <td>1.219100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>1.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11850</td>\n",
       "      <td>1.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11900</td>\n",
       "      <td>1.214900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11950</td>\n",
       "      <td>1.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12050</td>\n",
       "      <td>1.189200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12100</td>\n",
       "      <td>1.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12150</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>1.224900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12250</td>\n",
       "      <td>1.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12300</td>\n",
       "      <td>1.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12350</td>\n",
       "      <td>1.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>1.221500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12450</td>\n",
       "      <td>1.217800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12550</td>\n",
       "      <td>1.207300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>1.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12650</td>\n",
       "      <td>1.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12700</td>\n",
       "      <td>1.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12750</td>\n",
       "      <td>1.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>1.223100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12850</td>\n",
       "      <td>1.213500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12900</td>\n",
       "      <td>1.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12950</td>\n",
       "      <td>1.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.238300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13050</td>\n",
       "      <td>1.218400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13100</td>\n",
       "      <td>1.194400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13150</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>1.227600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13250</td>\n",
       "      <td>1.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13300</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13350</td>\n",
       "      <td>1.166700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>1.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13450</td>\n",
       "      <td>1.177600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13550</td>\n",
       "      <td>1.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>1.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13650</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13700</td>\n",
       "      <td>1.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13750</td>\n",
       "      <td>1.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>1.176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13850</td>\n",
       "      <td>1.208900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13900</td>\n",
       "      <td>1.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13950</td>\n",
       "      <td>1.162900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.142600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14050</td>\n",
       "      <td>1.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14100</td>\n",
       "      <td>1.200100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14150</td>\n",
       "      <td>1.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>1.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14250</td>\n",
       "      <td>1.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14300</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14350</td>\n",
       "      <td>1.183600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>1.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14450</td>\n",
       "      <td>1.188000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14550</td>\n",
       "      <td>1.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>1.160200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14650</td>\n",
       "      <td>1.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14700</td>\n",
       "      <td>1.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14750</td>\n",
       "      <td>1.189500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>1.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14850</td>\n",
       "      <td>1.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14900</td>\n",
       "      <td>1.180200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14950</td>\n",
       "      <td>1.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.198500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15050</td>\n",
       "      <td>1.181800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15100</td>\n",
       "      <td>1.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15150</td>\n",
       "      <td>1.207800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>1.176100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15250</td>\n",
       "      <td>1.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15300</td>\n",
       "      <td>1.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15350</td>\n",
       "      <td>1.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>1.163700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='238' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 | Val Macro-F1=0.5829 | Acc=0.6090\n",
      "                neutral: 0.4305\n",
      "               positive: 0.7798\n",
      "     extremely negative: 0.4757\n",
      "               negative: 0.7621\n",
      "     extremely positive: 0.4664\n",
      "   ✅ New best\n",
      "\n",
      "▶️ Epoch 2/6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='476' max='238' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [238/238 1:14:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2 | Val Macro-F1=0.5142 | Acc=0.5800\n",
      "                neutral: 0.3132\n",
      "               positive: 0.7917\n",
      "     extremely negative: 0.4521\n",
      "               negative: 0.7797\n",
      "     extremely positive: 0.2343\n",
      "   ⚠️ No improvement, patience=1\n",
      "\n",
      "▶️ Epoch 3/6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1374' max='15438' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1374/15438 06:29 < 1:06:34, 3.52 it/s, Epoch 0.53/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.129300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.109100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.158400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.128000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.147100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.169900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.132400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.154000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.177100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.139500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.153100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.138100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.188300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.190100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.147000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.169400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.202200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-08-20 16:54:32,742] Trial 0 failed with parameters: {'alpha': 0.6099727840383753, 'T': 2.613912188108926, 'lr': 4.578115966821428e-05} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_9192\\2212018194.py\", line 195, in objective\n",
      "    trainer.train(resume_from_checkpoint=None)\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py\", line 2238, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py\", line 2582, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py\", line 3845, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\accelerate\\accelerator.py\", line 2734, in backward\n",
      "    loss.backward(**kwargs)\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\_tensor.py\", line 626, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\autograd\\__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\autograd\\graph.py\", line 823, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-08-20 16:54:32,751] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 303\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# # -------------------------\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# # Optuna objective\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;66;03m# # -------------------------\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m \u001b[38;5;66;03m# Run Optuna\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    302\u001b[39m study = optuna.create_study(direction=\u001b[33m\"\u001b[39m\u001b[33mmaximize\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBest trial:\u001b[39m\u001b[33m\"\u001b[39m, study.best_trial.number)\n\u001b[32m    306\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Value:\u001b[39m\u001b[33m\"\u001b[39m, study.best_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\study.py:489\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    388\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    389\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    396\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    397\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    398\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    399\u001b[39m \n\u001b[32m    400\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    487\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    488\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     77\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    158\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    246\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    249\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    252\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    203\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    204\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 195\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(FIXED_EPOCHS):\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m▶️ Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFIXED_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m195\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m     metrics = trainer.evaluate()\n\u001b[32m    198\u001b[39m     macro_f1 = metrics[\u001b[33m\"\u001b[39m\u001b[33meval_macro_f1\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py:2582\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2575\u001b[39m context = (\n\u001b[32m   2576\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2577\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2580\u001b[39m )\n\u001b[32m   2581\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2584\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2585\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2586\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2587\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2588\u001b[39m ):\n\u001b[32m   2589\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2590\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\trainer.py:3845\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   3842\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   3843\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3845\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3847\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\accelerate\\accelerator.py:2734\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2732\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2733\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2734\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# # === EX5 KD with HF Trainer + Optuna + Early Stopping + W&B ===\n",
    "# import os, json, torch, optuna, wandb, numpy as np, pandas as pd\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModelForSequenceClassification,\n",
    "#     Trainer, TrainingArguments\n",
    "# )\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(\"✅ Device:\", DEVICE)\n",
    "\n",
    "# OUT_DIR = \"./KD_optuna_ex5_trainer\"\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# # -------------------------\n",
    "# # Teacher checkpoint (EX4)\n",
    "# # -------------------------\n",
    "# TEACHER_NAME    = \"microsoft/mdeberta-v3-base\"\n",
    "# TEACHER_PT_PATH =r\"hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\"\n",
    "# STUDENT_NAME    = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "# ORDER = [\"neutral\", \"positive\", \"extremely negative\", \"negative\", \"extremely positive\"]\n",
    "# LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "# ID2LABEL = {i: lab for lab, i in enumerate(ORDER)}\n",
    "\n",
    "# def normalize_label(s: str) -> str:\n",
    "#     s = str(s).lower().strip()\n",
    "#     s = s.replace(\"very negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"very positive\", \"extremely positive\")\n",
    "#     return s\n",
    "\n",
    "# # -------------------------\n",
    "# # Data\n",
    "# # -------------------------\n",
    "# train_df = pd.read_csv(\"Corona_NLP_train_cleaned_translated.csv\")\n",
    "# test_df  = pd.read_csv(\"Corona_NLP_test_cleaned_translated.csv\")\n",
    "# for df in (train_df, test_df):\n",
    "#     df[\"Sentiment\"] = df[\"Sentiment\"].map(normalize_label)\n",
    "#     df[\"label\"] = df[\"Sentiment\"].map(LABEL2ID)\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": Dataset.from_pandas(train_df[[\"OriginalTweet\",\"label\"]]),\n",
    "#     \"test\":  Dataset.from_pandas(test_df[[\"OriginalTweet\",\"label\"]])\n",
    "# })\n",
    "\n",
    "# tok_student = AutoTokenizer.from_pretrained(STUDENT_NAME, use_fast=True)\n",
    "# tok_teacher = AutoTokenizer.from_pretrained(TEACHER_NAME, use_fast=True)\n",
    "\n",
    "# def tokenize_fn(batch):\n",
    "#     texts = [str(t) for t in batch[\"OriginalTweet\"]]  # ensure pure list[str]\n",
    "#     return tok_student(\n",
    "#         texts,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\",\n",
    "#         max_length=512\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tokenized = dataset.map(tokenize_fn,\n",
    "#                         batched=True,\n",
    "#                         remove_columns=[\"OriginalTweet\"])\n",
    "# # tokenized = tokenized.remove_columns([\"OriginalTweet\"])\n",
    "# tokenized = tokenized.rename_column(\"label\",\"labels\")\n",
    "# tokenized.set_format(\"torch\")\n",
    "\n",
    "# # -------------------------\n",
    "# # Teacher (frozen)\n",
    "# # -------------------------\n",
    "# teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     TEACHER_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "# ).to(DEVICE).eval()\n",
    "# state = torch.load(TEACHER_PT_PATH, map_location=\"cpu\")\n",
    "# teacher.load_state_dict(state, strict=False)\n",
    "# for p in teacher.parameters(): p.requires_grad=False\n",
    "\n",
    "# # -------------------------\n",
    "# # KD Loss inside Trainer\n",
    "# # -------------------------\n",
    "# def kd_loss_fn(student_logits, teacher_logits, labels, alpha=0.5, T=2.0):\n",
    "#     ce = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "#     kd = torch.nn.functional.kl_div(\n",
    "#         torch.nn.functional.log_softmax(student_logits/T, dim=-1),\n",
    "#         torch.nn.functional.softmax(teacher_logits/T, dim=-1),\n",
    "#         reduction=\"batchmean\"\n",
    "#     ) * (T*T)\n",
    "#     return alpha*ce + (1-alpha)*kd\n",
    "\n",
    "# class KDTrainer(Trainer):\n",
    "#     def __init__(self, teacher, alpha=0.5, T=2.0, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.teacher = teacher\n",
    "#         self.alpha = alpha\n",
    "#         self.T = T\n",
    "\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "#         labels = inputs.pop(\"labels\")\n",
    "#         outputs = model(**inputs)\n",
    "#         student_logits = outputs.logits\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             teacher_out = self.teacher(**inputs)\n",
    "#             teacher_logits = teacher_out.logits\n",
    "\n",
    "#         loss = kd_loss_fn(student_logits, teacher_logits, labels,\n",
    "#                         alpha=self.alpha, T=self.T)\n",
    "\n",
    "#         return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "\n",
    "# # -------------------------\n",
    "# # Metrics\n",
    "# # -------------------------\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     preds = np.argmax(logits, axis=-1)\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     macro_f1 = f1_score(labels, preds, average=\"macro\")\n",
    "#     per_cls = f1_score(labels, preds, average=None, labels=list(range(len(ORDER))))\n",
    "#     return {\"accuracy\": acc, \"macro_f1\": macro_f1, **{f\"F1_{ORDER[i]}\": per_cls[i] for i in range(len(ORDER))}}\n",
    "# # -------------------------\n",
    "# # Optuna Objective (Trainer, with rich logging)\n",
    "# # -------------------------\n",
    "# FIXED_EPOCHS = 6\n",
    "# EARLY_TRIAL_CUTOFF = 0.30\n",
    "# PATIENCE = 3\n",
    "\n",
    "# def get_model_size(model):\n",
    "#     tmp = \"tmp_model.pt\"\n",
    "#     torch.save(model.state_dict(), tmp)\n",
    "#     sz = os.path.getsize(tmp) / (1024*1024)\n",
    "#     os.remove(tmp)\n",
    "#     return sz\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def get_latency(model, dataset, tokenizer, n_batches=20):\n",
    "#     loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "#     times = []\n",
    "#     for i, batch in enumerate(loader):\n",
    "#         if i >= n_batches: break\n",
    "#         enc = {k:v.to(DEVICE) for k,v in batch.items() if k!=\"labels\"}\n",
    "#         if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "#         t0 = time.perf_counter()\n",
    "#         _ = model(**enc)\n",
    "#         if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "#         dt = (time.perf_counter() - t0) / enc[\"input_ids\"].size(0)\n",
    "#         times.append(dt)\n",
    "#     return np.mean(times)*1000 if times else None\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 0.2, 0.8)\n",
    "#     T     = trial.suggest_float(\"T\", 1.0, 5.0)\n",
    "#     lr    = trial.suggest_float(\"lr\", 1e-5, 5e-5, log=True)\n",
    "\n",
    "#     print(f\"\\n🔎 Trial {trial.number} | alpha={alpha:.3f}, T={T:.2f}, lr={lr:.2e}, epochs={FIXED_EPOCHS}\")\n",
    "\n",
    "#     wandb.init(\n",
    "#         project=\"KD_optuna_ex5_trainer\",\n",
    "#         name=f\"trial_{trial.number}\",\n",
    "#         config={\"alpha\":alpha, \"T\":T, \"lr\":lr, \"epochs\":FIXED_EPOCHS}\n",
    "#     )\n",
    "\n",
    "#     student = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         STUDENT_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=f\"{OUT_DIR}/trial_{trial.number}\",\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         num_train_epochs=6,\n",
    "#         learning_rate=lr,\n",
    "#         logging_steps=50,\n",
    "#         report_to=\"wandb\",\n",
    "#         run_name=f\"KD_ex5_trial_{trial.number}\"\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trainer = KDTrainer(\n",
    "#         teacher=teacher, alpha=alpha, T=T,\n",
    "#         model=student, args=args,\n",
    "#         train_dataset=tokenized[\"train\"],\n",
    "#         eval_dataset=tokenized[\"test\"],\n",
    "#         compute_metrics=compute_metrics\n",
    "#     )\n",
    "\n",
    "#     best_f1 = -1\n",
    "#     patience_counter = 0\n",
    "\n",
    "#     for epoch in range(FIXED_EPOCHS):\n",
    "#         print(f\"\\n▶️ Epoch {epoch+1}/{FIXED_EPOCHS}\")\n",
    "#         trainer.train(resume_from_checkpoint=None)\n",
    "\n",
    "#         metrics = trainer.evaluate()\n",
    "#         macro_f1 = metrics[\"eval_macro_f1\"]\n",
    "#         acc      = metrics[\"eval_accuracy\"]\n",
    "#         class_f1s = [metrics[f\"eval_F1_{lab}\"] for lab in ORDER]\n",
    "\n",
    "#         print(f\"📊 Epoch {epoch+1} | Val Macro-F1={macro_f1:.4f} | Acc={acc:.4f}\")\n",
    "#         for i, lab in enumerate(ORDER):\n",
    "#             print(f\"   {lab:>20}: {class_f1s[i]:.4f}\")\n",
    "\n",
    "#         wandb.log({\n",
    "#             \"epoch\": epoch+1,\n",
    "#             \"val_macro_f1\": macro_f1,\n",
    "#             \"val_acc\": acc,\n",
    "#             **{f\"F1_{lab}\": class_f1s[i] for i,lab in enumerate(ORDER)}\n",
    "#         })\n",
    "\n",
    "#         # prune if first epoch bad\n",
    "#         if epoch == 0 and macro_f1 < EARLY_TRIAL_CUTOFF:\n",
    "#             print(f\"🛑 Pruning trial {trial.number}: F1={macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "#             wandb.finish()\n",
    "#             raise optuna.TrialPruned(f\"Epoch1 macro-F1 {macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "\n",
    "#         # patience\n",
    "#         if macro_f1 > best_f1:\n",
    "#             best_f1 = macro_f1\n",
    "#             patience_counter = 0\n",
    "#             print(\"   ✅ New best\")\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             print(f\"   ⚠️ No improvement, patience={patience_counter}\")\n",
    "#             if patience_counter >= PATIENCE:\n",
    "#                 print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "#                 break\n",
    "\n",
    "#     # size + latency\n",
    "#     size_mb = get_model_size(student)\n",
    "#     latency = get_latency(student, tokenized[\"test\"], tok_student)\n",
    "#     print(f\"📦 Model size: {size_mb:.2f} MB | ⏱️ Latency: {latency:.2f} ms/sample\")\n",
    "#     wandb.log({\"model_size_mb\": size_mb, \"latency_ms\": latency})\n",
    "\n",
    "#     wandb.finish()\n",
    "#     trial.set_user_attr(\"macro_f1\", best_f1)\n",
    "#     trial.set_user_attr(\"model_size_mb\", size_mb)\n",
    "#     trial.set_user_attr(\"latency_ms\", latency)\n",
    "#     return best_f1\n",
    "\n",
    "# # # -------------------------\n",
    "# # # Optuna objective\n",
    "# # # -------------------------\n",
    "# # def objective(trial):\n",
    "# #     alpha = trial.suggest_float(\"alpha\", 0.2, 0.8)\n",
    "# #     T = trial.suggest_float(\"T\", 1.0, 5.0)\n",
    "# #     lr = trial.suggest_float(\"lr\", 1e-5, 5e-5, log=True)\n",
    "# #\n",
    "# #     model = AutoModelForSequenceClassification.from_pretrained(\n",
    "# #         STUDENT_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "# #     ).to(DEVICE)\n",
    "# #\n",
    "# #     # args = TrainingArguments(\n",
    "# #     #     output_dir=f\"{OUT_DIR}/trial_{trial.number}\",\n",
    "# #     #     per_device_train_batch_size=16,\n",
    "# #     #     per_device_eval_batch_size=16,\n",
    "# #     #     num_train_epochs=6,  # fixed\n",
    "# #     #     learning_rate=lr,\n",
    "# #     #     # evaluation_strategy=\"epoch\",\n",
    "# #     #     # save_strategy=\"epoch\",\n",
    "# #     #     load_best_model_at_end=True,\n",
    "# #     #     metric_for_best_model=\"macro_f1\",\n",
    "# #     #     greater_is_better=True,\n",
    "# #     #     logging_steps=50,\n",
    "# #     #     report_to=\"wandb\",\n",
    "# #     #     run_name=f\"KD_ex5_trial_{trial.number}\"\n",
    "# #     # )\n",
    "# #     args = TrainingArguments(\n",
    "# #         output_dir=f\"{OUT_DIR}/trial_{trial.number}\",\n",
    "# #         per_device_train_batch_size=16,\n",
    "# #         per_device_eval_batch_size=16,\n",
    "# #         num_train_epochs=6,\n",
    "# #         learning_rate=lr,\n",
    "# #         logging_steps=50,\n",
    "# #         report_to=\"wandb\",\n",
    "# #         run_name=f\"KD_ex5_trial_{trial.number}\"\n",
    "# #     )\n",
    "# #\n",
    "# #     trainer = KDTrainer(\n",
    "# #         teacher=teacher, alpha=alpha, T=T,\n",
    "# #         model=model, args=args,\n",
    "# #         train_dataset=tokenized[\"train\"],\n",
    "# #         eval_dataset=tokenized[\"test\"],\n",
    "# #         compute_metrics=compute_metrics\n",
    "# #     )\n",
    "# #\n",
    "# #     result = trainer.train()\n",
    "# #     metrics = trainer.evaluate()\n",
    "# #     macro_f1 = metrics[\"eval_macro_f1\"]\n",
    "# #\n",
    "# #     # prune if first epoch bad\n",
    "# #     if trainer.state.epoch >= 1 and metrics[\"eval_macro_f1\"] < 0.30:\n",
    "# #         raise optuna.TrialPruned(\"Pruned: low F1 after epoch 1\")\n",
    "# #\n",
    "# #     return macro_f1\n",
    "\n",
    "# # -------------------------\n",
    "# # Run Optuna\n",
    "# # -------------------------\n",
    "# study = optuna.create_study(direction=\"maximize\")\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# print(\"Best trial:\", study.best_trial.number)\n",
    "# print(\"  Value:\", study.best_value)\n",
    "# print(\"  Params:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cbd34ed0c52db40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-19T20:49:48.467802Z",
     "start_time": "2025-08-19T20:49:48.457495Z"
    }
   },
   "outputs": [],
   "source": [
    "# # -------------------------\n",
    "# # Optuna Objective (Trainer, with rich logging)\n",
    "# # -------------------------\n",
    "# FIXED_EPOCHS = 6\n",
    "# EARLY_TRIAL_CUTOFF = 0.30\n",
    "# PATIENCE = 3\n",
    "\n",
    "# def get_model_size(model):\n",
    "#     tmp = \"tmp_model.pt\"\n",
    "#     torch.save(model.state_dict(), tmp)\n",
    "#     sz = os.path.getsize(tmp) / (1024*1024)\n",
    "#     os.remove(tmp)\n",
    "#     return sz\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def get_latency(model, dataset, tokenizer, n_batches=20):\n",
    "#     loader = torch.utils.data.DataLoader(dataset, batch_size=8)\n",
    "#     times = []\n",
    "#     for i, batch in enumerate(loader):\n",
    "#         if i >= n_batches: break\n",
    "#         enc = {k:v.to(DEVICE) for k,v in batch.items() if k!=\"labels\"}\n",
    "#         if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "#         t0 = time.perf_counter()\n",
    "#         _ = model(**enc)\n",
    "#         if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "#         dt = (time.perf_counter() - t0) / enc[\"input_ids\"].size(0)\n",
    "#         times.append(dt)\n",
    "#     return np.mean(times)*1000 if times else None\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 0.2, 0.8)\n",
    "#     T     = trial.suggest_float(\"T\", 1.0, 5.0)\n",
    "#     lr    = trial.suggest_float(\"lr\", 1e-5, 5e-5, log=True)\n",
    "\n",
    "#     print(f\"\\n🔎 Trial {trial.number} | alpha={alpha:.3f}, T={T:.2f}, lr={lr:.2e}, epochs={FIXED_EPOCHS}\")\n",
    "\n",
    "#     wandb.init(\n",
    "#         project=\"KD_optuna_ex5_trainer2\",\n",
    "#         name=f\"trial_{trial.number}\",\n",
    "#         config={\"alpha\":alpha, \"T\":T, \"lr\":lr, \"epochs\":FIXED_EPOCHS}\n",
    "#     )\n",
    "\n",
    "#     student = AutoModelForSequenceClassification.from_pretrained(\n",
    "#         STUDENT_NAME, num_labels=len(ORDER), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "#     ).to(DEVICE)\n",
    "\n",
    "#     args = TrainingArguments(\n",
    "#         output_dir=f\"{OUT_DIR}/trial_{trial.number}\",\n",
    "#         per_device_train_batch_size=16,\n",
    "#         per_device_eval_batch_size=16,\n",
    "#         num_train_epochs=FIXED_EPOCHS,\n",
    "#         learning_rate=lr,\n",
    "#         logging_steps=50,\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"no\",\n",
    "#         report_to=\"wandb\",\n",
    "#         run_name=f\"KD_ex5_trial_{trial.number}\",\n",
    "#         fp16=True if torch.cuda.is_available() else False,\n",
    "#         dataloader_num_workers=4,\n",
    "#         dataloader_pin_memory=True\n",
    "#     )\n",
    "\n",
    "#     trainer = KDTrainer(\n",
    "#         teacher=teacher, alpha=alpha, T=T,\n",
    "#         model=student, args=args,\n",
    "#         train_dataset=tokenized[\"train\"],\n",
    "#         eval_dataset=tokenized[\"test\"],\n",
    "#         compute_metrics=compute_metrics\n",
    "#     )\n",
    "\n",
    "#     best_f1 = -1\n",
    "#     patience_counter = 0\n",
    "\n",
    "#     for epoch in range(FIXED_EPOCHS):\n",
    "#         print(f\"\\n▶️ Epoch {epoch+1}/{FIXED_EPOCHS}\")\n",
    "#         trainer.train(resume_from_checkpoint=None)\n",
    "\n",
    "#         metrics = trainer.evaluate()\n",
    "#         macro_f1 = metrics[\"eval_macro_f1\"]\n",
    "#         acc      = metrics[\"eval_accuracy\"]\n",
    "#         class_f1s = [metrics[f\"eval_F1_{lab}\"] for lab in ORDER]\n",
    "\n",
    "#         print(f\"📊 Epoch {epoch+1} | Val Macro-F1={macro_f1:.4f} | Acc={acc:.4f}\")\n",
    "#         for i, lab in enumerate(ORDER):\n",
    "#             print(f\"   {lab:>20}: {class_f1s[i]:.4f}\")\n",
    "\n",
    "#         wandb.log({\n",
    "#             \"epoch\": epoch+1,\n",
    "#             \"val_macro_f1\": macro_f1,\n",
    "#             \"val_acc\": acc,\n",
    "#             **{f\"F1_{lab}\": class_f1s[i] for i,lab in enumerate(ORDER)}\n",
    "#         })\n",
    "\n",
    "#         # prune if first epoch bad\n",
    "#         if epoch == 0 and macro_f1 < EARLY_TRIAL_CUTOFF:\n",
    "#             print(f\"🛑 Pruning trial {trial.number}: F1={macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "#             wandb.finish()\n",
    "#             raise optuna.TrialPruned(f\"Epoch1 macro-F1 {macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "\n",
    "#         # patience\n",
    "#         if macro_f1 > best_f1:\n",
    "#             best_f1 = macro_f1\n",
    "#             patience_counter = 0\n",
    "#             print(\"   ✅ New best\")\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "#             print(f\"   ⚠️ No improvement, patience={patience_counter}\")\n",
    "#             if patience_counter >= PATIENCE:\n",
    "#                 print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "#                 break\n",
    "\n",
    "#     # size + latency\n",
    "#     size_mb = get_model_size(student)\n",
    "#     latency = get_latency(student, tokenized[\"test\"], tok_student)\n",
    "#     print(f\"📦 Model size: {size_mb:.2f} MB | ⏱️ Latency: {latency:.2f} ms/sample\")\n",
    "#     wandb.log({\"model_size_mb\": size_mb, \"latency_ms\": latency})\n",
    "\n",
    "#     wandb.finish()\n",
    "#     trial.set_user_attr(\"macro_f1\", best_f1)\n",
    "#     trial.set_user_attr(\"model_size_mb\", size_mb)\n",
    "#     trial.set_user_attr(\"latency_ms\", latency)\n",
    "#     return best_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef08da448d23cec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n",
      "✅ Label mapping: {'extremely negative': 0, 'negative': 1, 'neutral': 2, 'positive': 3, 'extremely positive': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer loaded (student)\n",
      "✅ DataLoaders ready: train=5145, test=475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-08-20 18:58:34,188] A new study created in memory with name: no-name-52ce010c-8599-4da1-9e15-d5023bbf5e0e\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Teacher loaded & frozen with correct label mapping\n",
      "🚀 Starting Optuna\n",
      "\n",
      "🔎 Trial 0 | alpha=0.737, T=4.67, epochs=6, fine_tune_layers=3, lr=3.92e-05, amp=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_185834-2wrp6snv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/2wrp6snv' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/2wrp6snv' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/2wrp6snv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 3 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.5759, Acc=0.5669\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 2:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2 done. Val Macro-F1=0.6482, Acc=0.6351\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 3:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 3 done. Val Macro-F1=0.6849, Acc=0.6751\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 4/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 4:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 4 done. Val Macro-F1=0.7043, Acc=0.6948\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 5/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 5:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 5 done. Val Macro-F1=0.7015, Acc=0.6927\n",
      "   ⚠️ No improvement. Patience counter=1\n",
      "\n",
      "▶️ Epoch 6/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 6:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 6 done. Val Macro-F1=0.7111, Acc=0.7017\n",
      "   ✅ New best F1\n",
      "📦 Model size: 541.34 MB | ⏱️ Latency: 3.74 ms/sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved best student weights: ./KD_optuna_ex5_deberta_UPDATE\\best_student_trial_0.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁▆▇▇██</td></tr><tr><td>F1_extremely positive</td><td>▁▆▇███</td></tr><tr><td>F1_negative</td><td>▁▃▇▇▇█</td></tr><tr><td>F1_neutral</td><td>▁▃▆█▇█</td></tr><tr><td>F1_positive</td><td>▁▅▆█▇█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>latency_ms</td><td>▁</td></tr><tr><td>model_size_mb</td><td>▁</td></tr><tr><td>step</td><td>▁▁▂▂▃▅▅▆▇█▂▃▄▇█▁▂▃▅▇▂▂▃▃▅▆▇▇█▂▆▆▇▇█▂▂▃▅▇</td></tr><tr><td>train_loss</td><td>▇▆▅▇▇▇█▅▄▆▅▆▃▄▅▄▅▃▄▄▃▄▅▅▄▅▁▆▃▂▂▂▅▄▃▁▅▃▆▁</td></tr><tr><td>val_acc</td><td>▁▅▇███</td></tr><tr><td>val_macro_f1</td><td>▁▅▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0.74761</td></tr><tr><td>F1_extremely positive</td><td>0.78783</td></tr><tr><td>F1_negative</td><td>0.66834</td></tr><tr><td>F1_neutral</td><td>0.67709</td></tr><tr><td>F1_positive</td><td>0.67469</td></tr><tr><td>epoch</td><td>6</td></tr><tr><td>latency_ms</td><td>3.73802</td></tr><tr><td>model_size_mb</td><td>541.34318</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.46781</td></tr><tr><td>val_acc</td><td>0.70169</td></tr><tr><td>val_macro_f1</td><td>0.71111</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/2wrp6snv' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/2wrp6snv</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250820_185834-2wrp6snv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 19:49:33,776] Trial 0 finished with value: 0.7111112453918759 and parameters: {'alpha': 0.7368877766861439, 'T': 4.66999209358128, 'fine_tune_layers': 3, 'learning_rate': 3.916971125336773e-05, 'use_amp': True}. Best is trial 0 with value: 0.7111112453918759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 1 | alpha=0.513, T=4.51, epochs=6, fine_tune_layers=4, lr=2.01e-06, amp=True\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_194933-dpw3af8p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/dpw3af8p' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/dpw3af8p' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/dpw3af8p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 4 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 1 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 1: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>██▇▆▄▄▇▃▄▄▅▄▂▂▃▂▂▄▂▄▃▃▃▄▄▂▂▄▄▄▃▅▄▂▂▂▃▃▂▁</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.34446</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/dpw3af8p' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/dpw3af8p</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250820_194933-dpw3af8p\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 19:58:33,276] Trial 1 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 2 | alpha=0.315, T=3.94, epochs=6, fine_tune_layers=4, lr=3.40e-06, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_195833-rxoinrh6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/rxoinrh6' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/rxoinrh6' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/rxoinrh6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 4 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 2 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 2: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▂▂▃▃▂▃▄▂▂▄▂▂▁▂▃▁▂▃▂▁▃▃▃▃▂▂▂▃▁▂▂▂▂▅▂▁▅▃▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.24699</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_2</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/rxoinrh6' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/rxoinrh6</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250820_195833-rxoinrh6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 20:09:54,895] Trial 2 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 3 | alpha=0.261, T=3.62, epochs=6, fine_tune_layers=5, lr=5.62e-06, amp=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_200954-60s7ch5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/60s7ch5d' target=\"_blank\">trial_3</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/60s7ch5d' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/60s7ch5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 2 / 6\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 5 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 3 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 3: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▃▃▄▄▄▄▄▆▃▃▂▂▂▂▂▄▁▅█▂▆▂▂▃▁▅▂▂▂▂▁▅▃▁▅▂▂▃▃▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.01504</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_3</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/60s7ch5d' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/60s7ch5d</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250820_200954-60s7ch5d\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 20:19:20,996] Trial 3 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 4 | alpha=0.228, T=4.67, epochs=6, fine_tune_layers=5, lr=7.29e-05, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_201920-isl77a1c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/isl77a1c' target=\"_blank\">trial_4</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/isl77a1c' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/isl77a1c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 2 / 6\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 5 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 4 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 4: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▄▇▂▅▄▃▂▃▅█▅▂▃▄▄▄▃▃▄▆▆▇▇▃▅▂▅▆▂▃▂▅▄▂▁▃▃▇▂▇</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>2.12863</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_4</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/isl77a1c' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/isl77a1c</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250820_201920-isl77a1c\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 20:31:28,797] Trial 4 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 5 | alpha=0.429, T=3.62, epochs=6, fine_tune_layers=3, lr=3.28e-05, amp=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_203128-0hbnsvs6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/0hbnsvs6' target=\"_blank\">trial_5</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/0hbnsvs6' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/0hbnsvs6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 3 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 5 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 5: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>▅▄▂▄▅▃▂▄▃▄▄▅▃▄▃▃▃▃▂▄█▂▂▃▄▃▅▃▁▃▅▃▇▃▄▃▄▂▄▃</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.58734</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_5</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/0hbnsvs6' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/0hbnsvs6</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250820_203128-0hbnsvs6\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-20 20:40:04,251] Trial 5 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 6 | alpha=0.665, T=2.03, epochs=6, fine_tune_layers=4, lr=1.68e-05, amp=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250820_204004-kefc9tan</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kefc9tan' target=\"_blank\">trial_6</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kefc9tan' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kefc9tan</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 4 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 6 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.5006, Acc=0.4758\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 2/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 6 Epoch 2:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2 done. Val Macro-F1=0.6136, Acc=0.5866\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 3/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 6 Epoch 3:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_13184\\1339524134.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "Trial 6 Epoch 3:   2%|8                                                  | 90/5145 [00:09<08:30,  9.90it/s, loss=0.727]"
     ]
    }
   ],
   "source": [
    "# # === KD (DeBERTa v3) with Optuna + Early Stopping ===\n",
    "# # Windows/Colab-safe, W&B robust, no dataloader hangs.\n",
    "# # Teacher: microsoft/mdeberta-v3-base  (load your EX4 .pt)\n",
    "# # Student: microsoft/deberta-v3-small\n",
    "# # Data: Corona_NLP_*_cleaned_translated.csv\n",
    "# # EPOCHS FIXED = 6 ; optional prune if epoch-1 val macro-F1 < 0.30\n",
    "# # ================================================================\n",
    "\n",
    "# import os, json, time, warnings, random, math, multiprocessing as mp\n",
    "# os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")  # avoid deadlocks\n",
    "# os.environ.setdefault(\"WANDB__SERVICE_WAIT\", \"300\")       # give W&B time to init\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch, optuna, wandb\n",
    "# from torch.utils.data import DataLoader, Dataset\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # -------------------------\n",
    "# # Repro & Torch backend\n",
    "# # -------------------------\n",
    "# def set_seed(seed=42):\n",
    "#     random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "# set_seed(42)\n",
    "\n",
    "# torch.backends.cudnn.benchmark = True  # ok for training speed\n",
    "# # If you still see instability, uncomment the next line\n",
    "# # torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(f\"✅ Using device: {DEVICE}\")\n",
    "\n",
    "# OUT_DIR = \"./KD_optuna_ex5_deberta_UPDATE\"\n",
    "# os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# # ---- Teacher (EX4) ----\n",
    "# TEACHER_NAME    = \"microsoft/mdeberta-v3-base\"\n",
    "# # ⛔ UPDATE THIS PATH TO YOUR TRAINED TEACHER CHECKPOINT\n",
    "# TEACHER_PT_PATH = r\"hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\"\n",
    "\n",
    "# # ---- Student ----\n",
    "# STUDENT_NAME = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "# # -------------------------\n",
    "# # Label normalization + fixed order (matches EX5)\n",
    "# # -------------------------\n",
    "# ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "# LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "# ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "# print(f\"✅ Label mapping: {LABEL2ID}\")\n",
    "\n",
    "# def normalize_label(s: str) -> str:\n",
    "#     s = str(s).strip().lower()\n",
    "#     s = s.replace(\"very negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"very positive\", \"extremely positive\")\n",
    "#     s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "#     s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "#     return s\n",
    "\n",
    "# # -------------------------\n",
    "# # Load data\n",
    "# # -------------------------\n",
    "# train_df = pd.read_csv(\"Corona_NLP_train_cleaned_translated.csv\")\n",
    "# test_df  = pd.read_csv(\"Corona_NLP_test_cleaned_translated.csv\")\n",
    "\n",
    "# for df in (train_df, test_df):\n",
    "#     df[\"OriginalTweet\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "#     df[\"Sentiment\"] = df[\"Sentiment\"].map(normalize_label)\n",
    "#     df[\"label\"] = df[\"Sentiment\"].map(LABEL2ID)\n",
    "\n",
    "# assert train_df[\"label\"].isnull().sum() == 0, \"❌ Unmapped train labels!\"\n",
    "# assert test_df[\"label\"].isnull().sum() == 0, \"❌ Unmapped test labels!\"\n",
    "\n",
    "# # -------------------------\n",
    "# # Dataset (tokenize once with student's tokenizer)\n",
    "# # -------------------------\n",
    "# tok = AutoTokenizer.from_pretrained(STUDENT_NAME, use_fast=True)\n",
    "# print(\"✅ Tokenizer loaded (student)\")\n",
    "\n",
    "# class TextDataset(Dataset):\n",
    "#     def __init__(self, df, tok, max_len=512):\n",
    "#         self.texts = df[\"OriginalTweet\"].tolist()\n",
    "#         self.labels = df[\"label\"].tolist()\n",
    "#         self.tok = tok\n",
    "#         self.max_len = max_len\n",
    "#     def __len__(self): return len(self.texts)\n",
    "#     def __getitem__(self, idx):\n",
    "#         enc = self.tok(\n",
    "#             str(self.texts[idx]),\n",
    "#             truncation=True, padding=\"max_length\",\n",
    "#             max_length=self.max_len, return_tensors=\"pt\"\n",
    "#         )\n",
    "#         item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "#         item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return item\n",
    "\n",
    "# # ⚠️ Windows/Jupyter-safe DataLoader settings\n",
    "# pin_mem = (DEVICE == \"cuda\")\n",
    "# NUM_WORKERS = 0  # <- key fix: avoid multiprocessing workers that can hang on Windows/Jupyter\n",
    "# PERSISTENT = False\n",
    "\n",
    "# train_ds, test_ds = TextDataset(train_df, tok), TextDataset(test_df, tok)\n",
    "# train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, pin_memory=pin_mem,\n",
    "#                           num_workers=NUM_WORKERS, persistent_workers=PERSISTENT)\n",
    "# test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, pin_memory=pin_mem,\n",
    "#                           num_workers=NUM_WORKERS, persistent_workers=PERSISTENT)\n",
    "# print(f\"✅ DataLoaders ready: train={len(train_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "# # -------------------------\n",
    "# # Teacher (frozen) — correct arch + label map\n",
    "# # -------------------------\n",
    "# teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "#     TEACHER_NAME, num_labels=len(LABEL2ID), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "# )\n",
    "# sd = torch.load(TEACHER_PT_PATH, map_location=\"cpu\")\n",
    "# try:\n",
    "#     teacher.load_state_dict(sd, strict=True)\n",
    "# except RuntimeError as e:\n",
    "#     warnings.warn(f\"[Teacher] strict=True failed: {e}\\nRetrying with strict=False\")\n",
    "#     teacher.load_state_dict(sd, strict=False)\n",
    "# teacher.to(DEVICE).eval()\n",
    "# for p in teacher.parameters(): p.requires_grad = False\n",
    "# print(\"✅ Teacher loaded & frozen with correct label mapping\")\n",
    "\n",
    "# # -------------------------\n",
    "# # KD loss\n",
    "# # -------------------------\n",
    "# def kd_loss_fn(student_logits, teacher_logits, labels, alpha=0.5, T=2.0):\n",
    "#     ce = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "#     kd = torch.nn.functional.kl_div(\n",
    "#         torch.nn.functional.log_softmax(student_logits/T, dim=-1),\n",
    "#         torch.nn.functional.softmax(teacher_logits/T, dim=-1),\n",
    "#         reduction=\"batchmean\"\n",
    "#     ) * (T*T)\n",
    "#     return alpha * ce + (1 - alpha) * kd\n",
    "\n",
    "# # -------------------------\n",
    "# # Helpers: model size + latency\n",
    "# # -------------------------\n",
    "# def get_model_size(model):\n",
    "#     tmp_path = os.path.join(OUT_DIR, \"temp_model.pt\")\n",
    "#     torch.save(model.state_dict(), tmp_path)\n",
    "#     size_mb = os.path.getsize(tmp_path) / (1024*1024)\n",
    "#     try: os.remove(tmp_path)\n",
    "#     except Exception: pass\n",
    "#     return size_mb\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def get_latency(model, loader, n_batches=20):\n",
    "#     model.eval()\n",
    "#     times = []\n",
    "#     it = 0\n",
    "#     for i, batch in enumerate(loader):\n",
    "#         if i >= n_batches: break\n",
    "#         batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "#         if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "#         t0 = time.perf_counter()\n",
    "#         _ = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "#         if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "#         dt = (time.perf_counter() - t0) / batch[\"input_ids\"].size(0)\n",
    "#         times.append(dt); it += 1\n",
    "#     return (np.mean(times) * 1000.0) if times else None  # ms/sample\n",
    "\n",
    "# # -------------------------\n",
    "# # Optuna Objective\n",
    "# # -------------------------\n",
    "# FIXED_EPOCHS = 6\n",
    "# EARLY_TRIAL_CUTOFF = 0.30  # macro-F1 threshold after first epoch\n",
    "\n",
    "# def objective(trial):\n",
    "#     alpha = trial.suggest_float(\"alpha\", 0.2, 0.8)\n",
    "#     T = trial.suggest_float(\"T\", 1.0, 5.0)\n",
    "#     fine_tune_layers = trial.suggest_int(\"fine_tune_layers\", 3, 6)\n",
    "#     lr = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
    "#     use_amp = trial.suggest_categorical(\"use_amp\", [True, False])\n",
    "#     max_grad_norm = 1.0\n",
    "\n",
    "#     print(f\"\\n🔎 Trial {trial.number} | alpha={alpha:.3f}, T={T:.2f}, epochs={FIXED_EPOCHS}, \"\n",
    "#           f\"fine_tune_layers={fine_tune_layers}, lr={lr:.2e}, amp={use_amp}\")\n",
    "\n",
    "#     # Robust W&B init; auto-falls back to offline if no login\n",
    "#     run = wandb.init(\n",
    "#         project=\"KD_optuna_ex5_deberta\",\n",
    "#         name=f\"trial_{trial.number}\",\n",
    "#         reinit=True,\n",
    "#         settings=wandb.Settings(start_method=\"thread\"),\n",
    "#         config={\"alpha\": alpha, \"T\": T, \"epochs\": FIXED_EPOCHS,\n",
    "#                 \"fine_tune_layers\": fine_tune_layers, \"lr\": lr, \"use_amp\": use_amp},\n",
    "#         mode=os.environ.get(\"WANDB_MODE\", \"online\"),\n",
    "#     )\n",
    "\n",
    "#     try:\n",
    "#         # --- Student: DeBERTa-v3-small ---\n",
    "#         student = AutoModelForSequenceClassification.from_pretrained(\n",
    "#             STUDENT_NAME,\n",
    "#             num_labels=len(LABEL2ID),\n",
    "#             id2label=ID2LABEL, label2id=LABEL2ID\n",
    "#         ).to(DEVICE)\n",
    "#         print(\"✅ Student (deberta-v3-small) initialized with correct num_labels\")\n",
    "\n",
    "#         # Freeze all, unfreeze last `fine_tune_layers` of encoder + classifier\n",
    "#         for p in student.parameters(): p.requires_grad = False\n",
    "#         enc_layers = student.deberta.encoder.layer\n",
    "#         n_layers = len(enc_layers)\n",
    "#         k = min(fine_tune_layers, n_layers)\n",
    "#         for i, layer in enumerate(enc_layers[-k:], 1):\n",
    "#             for p in layer.parameters(): p.requires_grad = True\n",
    "#             print(f\"   🔓 Unfroze student layer {n_layers - k + i} / {n_layers}\")\n",
    "#         if hasattr(student, \"classifier\"):\n",
    "#             for p in student.classifier.parameters(): p.requires_grad = True\n",
    "#         print(f\"✅ Unfroze last {k} layers + classifier\")\n",
    "\n",
    "#         # Optimizer + scheduler\n",
    "#         optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, student.parameters()), lr=lr)\n",
    "#         sched = get_scheduler(\"linear\", optim, num_warmup_steps=0,\n",
    "#                               num_training_steps=len(train_loader) * FIXED_EPOCHS)\n",
    "\n",
    "#         scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n",
    "\n",
    "#         best_val_f1 = -1.0\n",
    "#         patience = 3\n",
    "#         patience_counter = 0\n",
    "#         best_model_state = None\n",
    "\n",
    "#         for epoch in range(FIXED_EPOCHS):\n",
    "#             print(f\"\\n▶️ Epoch {epoch+1}/{FIXED_EPOCHS}\")\n",
    "#             student.train()\n",
    "#             loop = tqdm(train_loader, desc=f\"Trial {trial.number} Epoch {epoch+1}\",\n",
    "#                         ascii=True, leave=False)\n",
    "\n",
    "#             for step, batch in enumerate(loop):\n",
    "#                 batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "#                 with torch.no_grad():\n",
    "#                     t_out = teacher(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "\n",
    "#                 optim.zero_grad(set_to_none=True)\n",
    "#                 with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
    "#                     s_out = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "#                     loss = kd_loss_fn(s_out.logits, t_out.logits, batch[\"labels\"], alpha=alpha, T=T)\n",
    "\n",
    "#                 scaler.scale(loss).backward()\n",
    "#                 # gradient clipping\n",
    "#                 scaler.unscale_(optim)\n",
    "#                 torch.nn.utils.clip_grad_norm_(student.parameters(), max_grad_norm)\n",
    "\n",
    "#                 scaler.step(optim)\n",
    "#                 scaler.update()\n",
    "#                 sched.step()\n",
    "\n",
    "#                 loop.set_postfix(loss=float(loss))\n",
    "#                 if step % 50 == 0:\n",
    "#                     wandb.log({\"train_loss\": float(loss), \"epoch\": epoch+1, \"step\": step})\n",
    "\n",
    "#             # === Validation ===\n",
    "#             student.eval()\n",
    "#             all_preds, all_labels = [], []\n",
    "#             with torch.no_grad():\n",
    "#                 for batch in test_loader:\n",
    "#                     batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "#                     out = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "#                     preds = out.logits.argmax(dim=-1).cpu().numpy()\n",
    "#                     all_preds.extend(preds)\n",
    "#                     all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "#             macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "#             acc = accuracy_score(all_labels, all_preds)\n",
    "#             print(f\"📊 Epoch {epoch+1} done. Val Macro-F1={macro_f1:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "#             # log per-class F1\n",
    "#             per_class = f1_score(all_labels, all_preds, average=None,\n",
    "#                                  labels=list(range(len(LABEL2ID))))\n",
    "#             log_dict = {\"epoch\": epoch+1, \"val_macro_f1\": macro_f1, \"val_acc\": acc}\n",
    "#             for i in range(len(ID2LABEL)):\n",
    "#                 log_dict[f\"F1_{ID2LABEL[i]}\"] = float(per_class[i])\n",
    "#             wandb.log(log_dict)\n",
    "\n",
    "#             # Optional: prune whole trial if epoch-1 macro-F1 is poor\n",
    "#             if epoch == 0 and macro_f1 < EARLY_TRIAL_CUTOFF:\n",
    "#                 print(f\"🛑 Pruning trial {trial.number}: epoch-1 macro-F1 {macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "#                 raise optuna.TrialPruned(f\"Epoch1 macro-F1 {macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "\n",
    "#             # Standard early stopping after epoch 1\n",
    "#             if macro_f1 > best_val_f1:\n",
    "#                 best_val_f1 = macro_f1\n",
    "#                 patience_counter = 0\n",
    "#                 best_model_state = {k: v.cpu() for k, v in student.state_dict().items()}\n",
    "#                 print(\"   ✅ New best F1\")\n",
    "#             else:\n",
    "#                 patience_counter += 1\n",
    "#                 print(f\"   ⚠️ No improvement. Patience counter={patience_counter}\")\n",
    "#                 if patience_counter >= patience:\n",
    "#                     print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "#                     break\n",
    "\n",
    "#         assert best_model_state is not None, \"No best model captured; something went wrong.\"\n",
    "#         student.load_state_dict(best_model_state)\n",
    "#         student.to(DEVICE)\n",
    "\n",
    "#         # === Size & Latency Measurement (student) ===\n",
    "#         model_size = get_model_size(student)\n",
    "#         latency = get_latency(student, test_loader, n_batches=20)\n",
    "#         print(f\"📦 Model size: {model_size:.2f} MB | ⏱️ Latency: {latency:.2f} ms/sample\")\n",
    "\n",
    "#         # Save best student for this trial\n",
    "#         trial_model_path = os.path.join(OUT_DIR, f\"best_student_trial_{trial.number}.pt\")\n",
    "#         torch.save(student.state_dict(), trial_model_path)\n",
    "#         print(f\"💾 Saved best student weights: {trial_model_path}\")\n",
    "\n",
    "#         wandb.log({\"model_size_mb\": float(model_size), \"latency_ms\": float(latency)})\n",
    "#         trial.set_user_attr(\"macro_f1\", float(best_val_f1))\n",
    "#         trial.set_user_attr(\"model_size_mb\", float(model_size))\n",
    "#         trial.set_user_attr(\"latency_ms\", float(latency))\n",
    "#         trial.set_user_attr(\"model_path\", trial_model_path)\n",
    "#         return best_val_f1\n",
    "\n",
    "#     finally:\n",
    "#         # Always close the run, even if pruned/errored\n",
    "#         try: wandb.finish()\n",
    "#         except Exception: pass\n",
    "\n",
    "# # -------------------------\n",
    "# # Main entry (Windows-safe)\n",
    "# # -------------------------\n",
    "# def main():\n",
    "#     print(\"🚀 Starting Optuna\")\n",
    "#     study = optuna.create_study(direction=\"maximize\")\n",
    "#     try:\n",
    "#         study.optimize(objective, n_trials=10, show_progress_bar=False)\n",
    "#     except KeyboardInterrupt:\n",
    "#         print(\"⏹️ Interrupted by user. Summarizing…\")\n",
    "\n",
    "#     print(\"✅ Study complete\")\n",
    "#     print(\"Best trial:\", study.best_trial.number)\n",
    "#     print(\"  Macro-F1:\", study.best_value)\n",
    "#     print(\"  Params:\", study.best_trial.params)\n",
    "\n",
    "#     # -------------------------\n",
    "#     # Save best results (incl size & latency)\n",
    "#     # -------------------------\n",
    "#     best_path = os.path.join(OUT_DIR, \"best_study.json\")\n",
    "#     with open(best_path, \"w\") as f:\n",
    "#         json.dump({\n",
    "#             \"trial\": study.best_trial.number,\n",
    "#             \"macro_f1\": study.best_value,\n",
    "#             \"params\": study.best_trial.params,\n",
    "#             \"model_size_mb\": study.best_trial.user_attrs.get(\"model_size_mb\", None),\n",
    "#             \"latency_ms\": study.best_trial.user_attrs.get(\"latency_ms\", None),\n",
    "#             \"model_path\": study.best_trial.user_attrs.get(\"model_path\", None),\n",
    "#         }, f, indent=2)\n",
    "#     print(\"✅ Best trial saved:\", best_path)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     mp.freeze_support()  # <- crucial for Windows\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781a49b-152f-4f5a-956f-c55acf48729a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28bb6d-d256-4fe8-9f6c-5ad88722147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cuda\n",
      "✅ Label mapping: {'extremely negative': 0, 'negative': 1, 'neutral': 2, 'positive': 3, 'extremely positive': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Tokenizer loaded (student)\n",
      "✅ DataLoaders ready: train=5145, test=475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-08-21 02:51:00,167] A new study created in memory with name: no-name-2fe1c1ff-2d49-46f5-9c03-3391a95511ca\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `start_method` is deprecated and will be removed in a future version of wandb. This setting is currently non-functional and safely ignored.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Teacher loaded & frozen with correct label mapping\n",
      "🚀 Starting Optuna\n",
      "\n",
      "🔎 Trial 0 | alpha=0.665, T=2.07, epochs=4, fine_tune_layers=3, lr=3.11e-06, amp=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madishalit1\u001b[0m (\u001b[33madishalit1-tel-aviv-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_025100-o7rw2tru</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/o7rw2tru' target=\"_blank\">trial_0</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/o7rw2tru' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/o7rw2tru</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 3 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 0 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.2050, Acc=0.2370\n",
      "🛑 Pruning trial 0: epoch-1 macro-F1 0.2050 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>▅▄▅▅█▃▅▄▅▂▃▅▄▄▅▃▇▃▄▅▁▄▁▂▃▃▂▄▃▅▄▂▃▄▄▅▃▅▄▃</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0.29894</td></tr><tr><td>F1_extremely positive</td><td>0.42182</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.30444</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.4865</td></tr><tr><td>val_acc</td><td>0.23697</td></tr><tr><td>val_macro_f1</td><td>0.20504</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/o7rw2tru' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/o7rw2tru</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_025100-o7rw2tru\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 03:03:24,238] Trial 0 pruned. Epoch1 macro-F1 0.2050 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 1 | alpha=0.312, T=4.97, epochs=4, fine_tune_layers=6, lr=1.79e-05, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_030324-cdttzkyb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/cdttzkyb' target=\"_blank\">trial_1</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/cdttzkyb' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/cdttzkyb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 1 / 6\n",
      "   🔓 Unfroze student layer 2 / 6\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 6 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 1 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 1: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▃▃▁▂▃▂▄▂▆▂▃▂▂▂▃▃▃▂▂▄▃▂▄▂▃▄▃▃▂▃▄▄▂▃▃▃▂▂▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.02513</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_1</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/cdttzkyb' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/cdttzkyb</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_030324-cdttzkyb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 03:18:02,830] Trial 1 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 2 | alpha=0.516, T=1.00, epochs=4, fine_tune_layers=3, lr=1.72e-04, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_031802-7027m2tc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/7027m2tc' target=\"_blank\">trial_2</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/7027m2tc' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/7027m2tc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 3 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 2 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.1925, Acc=0.2536\n",
      "🛑 Pruning trial 2: epoch-1 macro-F1 0.1925 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▇▆▃▄▇█▆▇▄▄▃▃▇▃▆▅█▄█▃▆▅▄▄▃▆▃▅▇▅▇▆▁▂▁▆▄▃▃▅</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0.65185</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.31047</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.0087</td></tr><tr><td>val_acc</td><td>0.25355</td></tr><tr><td>val_macro_f1</td><td>0.19246</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_2</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/7027m2tc' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/7027m2tc</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_031802-7027m2tc\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 03:30:22,072] Trial 2 pruned. Epoch1 macro-F1 0.1925 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 3 | alpha=0.393, T=3.40, epochs=4, fine_tune_layers=4, lr=1.25e-05, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_033022-s9sgsrhi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/s9sgsrhi' target=\"_blank\">trial_3</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/s9sgsrhi' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/s9sgsrhi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 4 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 3 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 3: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_loss</td><td>█▄▂▃▂▃▃▂▂▄▂▁▂▂▂▅▂▁▄▃▃▃▂▄▂▃▅▃▂▃▃▇▁▄▂▂▃▄▂▂</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.39533</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_3</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/s9sgsrhi' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/s9sgsrhi</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_033022-s9sgsrhi\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 03:43:28,631] Trial 3 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 4 | alpha=0.394, T=2.87, epochs=4, fine_tune_layers=3, lr=1.32e-04, amp=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_034328-f47lryv4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/f47lryv4' target=\"_blank\">trial_4</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/f47lryv4' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/f47lryv4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 3 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 4 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 4: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█▃▁▆▃▄▂▃▂▁▃▄▃▃▃▁▄▄▂▂▂▂▃▄▇▃▂▄▄▃▂▂▂▃▃▁▂▃▃▃</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.3652</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_4</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/f47lryv4' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/f47lryv4</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_034328-f47lryv4\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 03:53:52,694] Trial 4 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 5 | alpha=0.699, T=3.34, epochs=4, fine_tune_layers=4, lr=5.79e-06, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_035352-b7gtttm8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/b7gtttm8' target=\"_blank\">trial_5</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/b7gtttm8' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/b7gtttm8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 4 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 5 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.4278, Acc=0.4047\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 5 Epoch 2:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2 done. Val Macro-F1=0.5147, Acc=0.4863\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 5 Epoch 3:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 3 done. Val Macro-F1=0.5336, Acc=0.5055\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 5 Epoch 4:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 4 done. Val Macro-F1=0.5564, Acc=0.5295\n",
      "   ✅ New best F1\n",
      "📦 Model size: 541.34 MB | ⏱️ Latency: 4.24 ms/sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved best student weights: ./KD_optuna_ex5_deberta_UPDATE\\best_student_trial_5.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁▇▆█</td></tr><tr><td>F1_extremely positive</td><td>▁▄██</td></tr><tr><td>F1_negative</td><td>▁▆▆█</td></tr><tr><td>F1_neutral</td><td>▁▅▆█</td></tr><tr><td>F1_positive</td><td>▁▆██</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▃▆▆▆▆▆▆▆▆▆▆███████████</td></tr><tr><td>latency_ms</td><td>▁</td></tr><tr><td>model_size_mb</td><td>▁</td></tr><tr><td>step</td><td>▂▃▄▅▆▆▇▇▇█▂▂▃▃▄▄▄▅▆▆▇▇█▁▁▄▄▅▆▇█▁▁▂▂▅▅▆▆█</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▅▅▅▆▅▆▅▅▆▄▅▅▅▄▄▄▄▄▃▄▆▆▄▅▅▇▄▅▅▅▆▅▅▁</td></tr><tr><td>val_acc</td><td>▁▆▇█</td></tr><tr><td>val_macro_f1</td><td>▁▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0.67283</td></tr><tr><td>F1_extremely positive</td><td>0.69288</td></tr><tr><td>F1_negative</td><td>0.46611</td></tr><tr><td>F1_neutral</td><td>0.45159</td></tr><tr><td>F1_positive</td><td>0.49869</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>latency_ms</td><td>4.23995</td></tr><tr><td>model_size_mb</td><td>541.34318</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.28719</td></tr><tr><td>val_acc</td><td>0.52949</td></tr><tr><td>val_macro_f1</td><td>0.55642</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_5</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/b7gtttm8' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/b7gtttm8</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_035352-b7gtttm8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 04:46:13,067] Trial 5 finished with value: 0.5564198595625609 and parameters: {'alpha': 0.6990599880470667, 'T': 3.337410826704433, 'fine_tune_layers': 4, 'learning_rate': 5.78903865438375e-06, 'use_amp': False}. Best is trial 5 with value: 0.5564198595625609.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 6 | alpha=0.710, T=4.09, epochs=4, fine_tune_layers=4, lr=1.21e-06, amp=True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_044613-9ii5s1rx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/9ii5s1rx' target=\"_blank\">trial_6</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/9ii5s1rx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/9ii5s1rx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 4 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 6 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0806, Acc=0.1746\n",
      "🛑 Pruning trial 6: epoch-1 macro-F1 0.0806 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▇▇▃▆▃▆▄▄▂▅▇▃▄▅▅▆▅▄▄▇▆▄▃▅▁▄▆▄▄▂▃▄█▄▃▅▅</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0.08026</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0.02566</td></tr><tr><td>F1_neutral</td><td>0.28691</td></tr><tr><td>F1_positive</td><td>0.01033</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>2.10903</td></tr><tr><td>val_acc</td><td>0.17457</td></tr><tr><td>val_macro_f1</td><td>0.08063</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_6</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/9ii5s1rx' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/9ii5s1rx</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_044613-9ii5s1rx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 04:57:04,131] Trial 6 pruned. Epoch1 macro-F1 0.0806 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 7 | alpha=0.259, T=1.53, epochs=4, fine_tune_layers=6, lr=1.36e-06, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.3s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_045704-kabbkl11</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kabbkl11' target=\"_blank\">trial_7</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kabbkl11' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kabbkl11</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 1 / 6\n",
      "   🔓 Unfroze student layer 2 / 6\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 6 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 7 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.0561, Acc=0.1630\n",
      "🛑 Pruning trial 7: epoch-1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁</td></tr><tr><td>F1_extremely positive</td><td>▁</td></tr><tr><td>F1_negative</td><td>▁</td></tr><tr><td>F1_neutral</td><td>▁</td></tr><tr><td>F1_positive</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▆▄▃▃▄▂▄▄▃▃▄▂▄▆▁▄▃▄▂▂▃▂▁▄▃▂▄▅▃▃▂▄▁▂▃▅▃</td></tr><tr><td>val_acc</td><td>▁</td></tr><tr><td>val_macro_f1</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0</td></tr><tr><td>F1_extremely positive</td><td>0</td></tr><tr><td>F1_negative</td><td>0</td></tr><tr><td>F1_neutral</td><td>0.28028</td></tr><tr><td>F1_positive</td><td>0</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.24877</td></tr><tr><td>val_acc</td><td>0.16298</td></tr><tr><td>val_macro_f1</td><td>0.05606</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_7</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kabbkl11' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/kabbkl11</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_045704-kabbkl11\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 05:11:47,551] Trial 7 pruned. Epoch1 macro-F1 0.0561 < 0.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 8 | alpha=0.794, T=3.72, epochs=4, fine_tune_layers=6, lr=1.18e-06, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_051147-d3q4yjv7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/d3q4yjv7' target=\"_blank\">trial_8</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/d3q4yjv7' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/d3q4yjv7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 1 / 6\n",
      "   🔓 Unfroze student layer 2 / 6\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 6 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 8 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.4323, Acc=0.4265\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 8 Epoch 2:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2 done. Val Macro-F1=0.5513, Acc=0.5382\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 8 Epoch 3:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 3 done. Val Macro-F1=0.5673, Acc=0.5558\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 8 Epoch 4:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 4 done. Val Macro-F1=0.5835, Acc=0.5711\n",
      "   ✅ New best F1\n",
      "📦 Model size: 541.34 MB | ⏱️ Latency: 4.40 ms/sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Saved best student weights: ./KD_optuna_ex5_deberta_UPDATE\\best_student_trial_8.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>▁▆▇█</td></tr><tr><td>F1_extremely positive</td><td>▁▇██</td></tr><tr><td>F1_negative</td><td>▁▅▇█</td></tr><tr><td>F1_neutral</td><td>▁▇▇█</td></tr><tr><td>F1_positive</td><td>▁▆▇█</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▃▃▆▆▆▆▆▆▆██████████</td></tr><tr><td>latency_ms</td><td>▁</td></tr><tr><td>model_size_mb</td><td>▁</td></tr><tr><td>step</td><td>▁▂▃▃▃▄▄▄▄▅▅▆▇▇▁▃▃▄▆▆██▂▂▅▆▇▇▇█▂▃▃▄▄▅▆▇▇█</td></tr><tr><td>train_loss</td><td>▅▄▆▅▇▆▅█▆▇▆▃▅▃▅▂▇▇▁▂▃▃▅▃▁▅▂▃▅▁▅▃▆▃▆▃▅▆▃▂</td></tr><tr><td>val_acc</td><td>▁▆▇█</td></tr><tr><td>val_macro_f1</td><td>▁▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>F1_extremely negative</td><td>0.66149</td></tr><tr><td>F1_extremely positive</td><td>0.65084</td></tr><tr><td>F1_negative</td><td>0.52305</td></tr><tr><td>F1_neutral</td><td>0.62567</td></tr><tr><td>F1_positive</td><td>0.45622</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>latency_ms</td><td>4.4017</td></tr><tr><td>model_size_mb</td><td>541.34318</td></tr><tr><td>step</td><td>5100</td></tr><tr><td>train_loss</td><td>1.06605</td></tr><tr><td>val_acc</td><td>0.57109</td></tr><tr><td>val_macro_f1</td><td>0.58345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_8</strong> at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/d3q4yjv7' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/d3q4yjv7</a><br> View project at: <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250821_051147-d3q4yjv7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-21 06:10:25,781] Trial 8 finished with value: 0.583452939907435 and parameters: {'alpha': 0.7940377625325481, 'T': 3.719471269993702, 'fine_tune_layers': 6, 'learning_rate': 1.1767000099148966e-06, 'use_amp': False}. Best is trial 8 with value: 0.583452939907435.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Trial 9 | alpha=0.783, T=1.53, epochs=4, fine_tune_layers=6, lr=7.90e-05, amp=False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\adishalit1\\Desktop\\Adi\\ADL\\DL_2_Project\\wandb\\run-20250821_061025-818wgyv9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/818wgyv9' target=\"_blank\">trial_9</a></strong> to <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/818wgyv9' target=\"_blank\">https://wandb.ai/adishalit1-tel-aviv-university/KD_optuna_ex5_deberta/runs/818wgyv9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:221: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Student (deberta-v3-small) initialized with correct num_labels\n",
      "   🔓 Unfroze student layer 1 / 6\n",
      "   🔓 Unfroze student layer 2 / 6\n",
      "   🔓 Unfroze student layer 3 / 6\n",
      "   🔓 Unfroze student layer 4 / 6\n",
      "   🔓 Unfroze student layer 5 / 6\n",
      "   🔓 Unfroze student layer 6 / 6\n",
      "✅ Unfroze last 6 layers + classifier\n",
      "\n",
      "▶️ Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 9 Epoch 1:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 1 done. Val Macro-F1=0.7414, Acc=0.7306\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 2/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 9 Epoch 2:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 2 done. Val Macro-F1=0.7775, Acc=0.7709\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 3/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 9 Epoch 3:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Epoch 3 done. Val Macro-F1=0.8191, Acc=0.8131\n",
      "   ✅ New best F1\n",
      "\n",
      "▶️ Epoch 4/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trial 9 Epoch 4:   0%|                                                                        | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_23892\\2641461158.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
      "Trial 9 Epoch 4:  24%|###########5                                     | 1210/5145 [02:54<09:26,  6.95it/s, loss=0.818]"
     ]
    }
   ],
   "source": [
    "# === KD (DeBERTa v3) with Optuna + Early Stopping ===\n",
    "# Windows/Colab-safe, W&B robust, no dataloader hangs.\n",
    "# Teacher: microsoft/mdeberta-v3-base  (load your EX4 .pt)\n",
    "# Student: microsoft/deberta-v3-small\n",
    "# Data: Corona_NLP_*_cleaned_translated.csv\n",
    "# EPOCHS FIXED = 6 ; optional prune if epoch-1 val macro-F1 < 0.30\n",
    "# ================================================================\n",
    "\n",
    "import os, json, time, warnings, random, math, multiprocessing as mp\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")  # avoid deadlocks\n",
    "os.environ.setdefault(\"WANDB__SERVICE_WAIT\", \"300\")       # give W&B time to init\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, optuna, wandb\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_scheduler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# Repro & Torch backend\n",
    "# -------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(42)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # ok for training speed\n",
    "# If you still see instability, uncomment the next line\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device: {DEVICE}\")\n",
    "\n",
    "OUT_DIR = \"./KD_optuna_ex5_deberta_UPDATE\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Teacher (EX4) ----\n",
    "TEACHER_NAME    = \"microsoft/mdeberta-v3-base\"\n",
    "# ⛔ UPDATE THIS PATH TO YOUR TRAINED TEACHER CHECKPOINT\n",
    "TEACHER_PT_PATH = r\"hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\"\n",
    "\n",
    "# ---- Student ----\n",
    "STUDENT_NAME = \"microsoft/deberta-v3-small\"\n",
    "\n",
    "# -------------------------\n",
    "# Label normalization + fixed order (matches EX5)\n",
    "# -------------------------\n",
    "ORDER = [\"extremely negative\",\"negative\",\"neutral\",\"positive\",\"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "print(f\"✅ Label mapping: {LABEL2ID}\")\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return s\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "train_df = pd.read_csv(\"Corona_NLP_train_cleaned_translated.csv\")\n",
    "test_df  = pd.read_csv(\"Corona_NLP_test_cleaned_translated.csv\")\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    df[\"OriginalTweet\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"Sentiment\"] = df[\"Sentiment\"].map(normalize_label)\n",
    "    df[\"label\"] = df[\"Sentiment\"].map(LABEL2ID)\n",
    "\n",
    "assert train_df[\"label\"].isnull().sum() == 0, \"❌ Unmapped train labels!\"\n",
    "assert test_df[\"label\"].isnull().sum() == 0, \"❌ Unmapped test labels!\"\n",
    "\n",
    "# -------------------------\n",
    "# Dataset (tokenize once with student's tokenizer)\n",
    "# -------------------------\n",
    "tok = AutoTokenizer.from_pretrained(STUDENT_NAME, use_fast=True)\n",
    "print(\"✅ Tokenizer loaded (student)\")\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tok, max_len=512):\n",
    "        self.texts = df[\"OriginalTweet\"].tolist()\n",
    "        self.labels = df[\"label\"].tolist()\n",
    "        self.tok = tok\n",
    "        self.max_len = max_len\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        enc = self.tok(\n",
    "            str(self.texts[idx]),\n",
    "            truncation=True, padding=\"max_length\",\n",
    "            max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# ⚠️ Windows/Jupyter-safe DataLoader settings\n",
    "pin_mem = (DEVICE == \"cuda\")\n",
    "NUM_WORKERS = 0  # <- key fix: avoid multiprocessing workers that can hang on Windows/Jupyter\n",
    "PERSISTENT = False\n",
    "\n",
    "train_ds, test_ds = TextDataset(train_df, tok), TextDataset(test_df, tok)\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True, pin_memory=pin_mem,\n",
    "                          num_workers=NUM_WORKERS, persistent_workers=PERSISTENT)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=8, shuffle=False, pin_memory=pin_mem,\n",
    "                          num_workers=NUM_WORKERS, persistent_workers=PERSISTENT)\n",
    "print(f\"✅ DataLoaders ready: train={len(train_loader)}, test={len(test_loader)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Teacher (frozen) — correct arch + label map\n",
    "# -------------------------\n",
    "teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "    TEACHER_NAME, num_labels=len(LABEL2ID), id2label=ID2LABEL, label2id=LABEL2ID\n",
    ")\n",
    "sd = torch.load(TEACHER_PT_PATH, map_location=\"cpu\")\n",
    "try:\n",
    "    teacher.load_state_dict(sd, strict=True)\n",
    "except RuntimeError as e:\n",
    "    warnings.warn(f\"[Teacher] strict=True failed: {e}\\nRetrying with strict=False\")\n",
    "    teacher.load_state_dict(sd, strict=False)\n",
    "teacher.to(DEVICE).eval()\n",
    "for p in teacher.parameters(): p.requires_grad = False\n",
    "print(\"✅ Teacher loaded & frozen with correct label mapping\")\n",
    "\n",
    "# -------------------------\n",
    "# KD loss\n",
    "# -------------------------\n",
    "def kd_loss_fn(student_logits, teacher_logits, labels, alpha=0.5, T=2.0):\n",
    "    ce = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "    kd = torch.nn.functional.kl_div(\n",
    "        torch.nn.functional.log_softmax(student_logits/T, dim=-1),\n",
    "        torch.nn.functional.softmax(teacher_logits/T, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T*T)\n",
    "    return alpha * ce + (1 - alpha) * kd\n",
    "\n",
    "# -------------------------\n",
    "# Helpers: model size + latency\n",
    "# -------------------------\n",
    "def get_model_size(model):\n",
    "    tmp_path = os.path.join(OUT_DIR, \"temp_model.pt\")\n",
    "    torch.save(model.state_dict(), tmp_path)\n",
    "    size_mb = os.path.getsize(tmp_path) / (1024*1024)\n",
    "    try: os.remove(tmp_path)\n",
    "    except Exception: pass\n",
    "    return size_mb\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_latency(model, loader, n_batches=20):\n",
    "    model.eval()\n",
    "    times = []\n",
    "    it = 0\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i >= n_batches: break\n",
    "        batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "        _ = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "        if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "        dt = (time.perf_counter() - t0) / batch[\"input_ids\"].size(0)\n",
    "        times.append(dt); it += 1\n",
    "    return (np.mean(times) * 1000.0) if times else None  # ms/sample\n",
    "\n",
    "# -------------------------\n",
    "# Optuna Objective\n",
    "# -------------------------\n",
    "FIXED_EPOCHS = 4\n",
    "EARLY_TRIAL_CUTOFF = 0.30  # macro-F1 threshold after first epoch\n",
    "\n",
    "def objective(trial):\n",
    "    alpha = trial.suggest_float(\"alpha\", 0.2, 0.8)\n",
    "    T = trial.suggest_float(\"T\", 1.0, 5.0)\n",
    "    fine_tune_layers = trial.suggest_int(\"fine_tune_layers\", 3, 6)\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-6, 1e-3, log=True)\n",
    "    use_amp = trial.suggest_categorical(\"use_amp\", [True, False])\n",
    "    max_grad_norm = 1.0\n",
    "\n",
    "    print(f\"\\n🔎 Trial {trial.number} | alpha={alpha:.3f}, T={T:.2f}, epochs={FIXED_EPOCHS}, \"\n",
    "          f\"fine_tune_layers={fine_tune_layers}, lr={lr:.2e}, amp={use_amp}\")\n",
    "\n",
    "    # Robust W&B init; auto-falls back to offline if no login\n",
    "    run = wandb.init(\n",
    "        project=\"KD_optuna_ex5_deberta\",\n",
    "        name=f\"trial_{trial.number}\",\n",
    "        reinit=True,\n",
    "        settings=wandb.Settings(start_method=\"thread\"),\n",
    "        config={\"alpha\": alpha, \"T\": T, \"epochs\": FIXED_EPOCHS,\n",
    "                \"fine_tune_layers\": fine_tune_layers, \"lr\": lr, \"use_amp\": use_amp},\n",
    "        mode=os.environ.get(\"WANDB_MODE\", \"online\"),\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # --- Student: DeBERTa-v3-small ---\n",
    "        student = AutoModelForSequenceClassification.from_pretrained(\n",
    "            STUDENT_NAME,\n",
    "            num_labels=len(LABEL2ID),\n",
    "            id2label=ID2LABEL, label2id=LABEL2ID\n",
    "        ).to(DEVICE)\n",
    "        print(\"✅ Student (deberta-v3-small) initialized with correct num_labels\")\n",
    "\n",
    "        # Freeze all, unfreeze last `fine_tune_layers` of encoder + classifier\n",
    "        for p in student.parameters(): p.requires_grad = False\n",
    "        enc_layers = student.deberta.encoder.layer\n",
    "        n_layers = len(enc_layers)\n",
    "        k = min(fine_tune_layers, n_layers)\n",
    "        for i, layer in enumerate(enc_layers[-k:], 1):\n",
    "            for p in layer.parameters(): p.requires_grad = True\n",
    "            print(f\"   🔓 Unfroze student layer {n_layers - k + i} / {n_layers}\")\n",
    "        if hasattr(student, \"classifier\"):\n",
    "            for p in student.classifier.parameters(): p.requires_grad = True\n",
    "        print(f\"✅ Unfroze last {k} layers + classifier\")\n",
    "\n",
    "        # Optimizer + scheduler\n",
    "        optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, student.parameters()), lr=lr)\n",
    "        sched = get_scheduler(\"linear\", optim, num_warmup_steps=0,\n",
    "                              num_training_steps=len(train_loader) * FIXED_EPOCHS)\n",
    "\n",
    "        scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE==\"cuda\" and use_amp))\n",
    "\n",
    "        best_val_f1 = -1.0\n",
    "        patience = 3\n",
    "        patience_counter = 0\n",
    "        best_model_state = None\n",
    "\n",
    "        for epoch in range(FIXED_EPOCHS):\n",
    "            print(f\"\\n▶️ Epoch {epoch+1}/{FIXED_EPOCHS}\")\n",
    "            student.train()\n",
    "            loop = tqdm(train_loader, desc=f\"Trial {trial.number} Epoch {epoch+1}\",\n",
    "                        ascii=True, leave=False)\n",
    "\n",
    "            for step, batch in enumerate(loop):\n",
    "                batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    t_out = teacher(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "\n",
    "                optim.zero_grad(set_to_none=True)\n",
    "                with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\" and use_amp)):\n",
    "                    s_out = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "                    loss = kd_loss_fn(s_out.logits, t_out.logits, batch[\"labels\"], alpha=alpha, T=T)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                # gradient clipping\n",
    "                scaler.unscale_(optim)\n",
    "                torch.nn.utils.clip_grad_norm_(student.parameters(), max_grad_norm)\n",
    "\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "                sched.step()\n",
    "\n",
    "                loop.set_postfix(loss=float(loss))\n",
    "                if step % 50 == 0:\n",
    "                    wandb.log({\"train_loss\": float(loss), \"epoch\": epoch+1, \"step\": step})\n",
    "\n",
    "            # === Validation ===\n",
    "            student.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for batch in test_loader:\n",
    "                    batch = {k: v.to(DEVICE, non_blocking=True) for k, v in batch.items()}\n",
    "                    out = student(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "                    preds = out.logits.argmax(dim=-1).cpu().numpy()\n",
    "                    all_preds.extend(preds)\n",
    "                    all_labels.extend(batch[\"labels\"].cpu().numpy())\n",
    "\n",
    "            macro_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            print(f\"📊 Epoch {epoch+1} done. Val Macro-F1={macro_f1:.4f}, Acc={acc:.4f}\")\n",
    "\n",
    "            # log per-class F1\n",
    "            per_class = f1_score(all_labels, all_preds, average=None,\n",
    "                                 labels=list(range(len(LABEL2ID))))\n",
    "            log_dict = {\"epoch\": epoch+1, \"val_macro_f1\": macro_f1, \"val_acc\": acc}\n",
    "            for i in range(len(ID2LABEL)):\n",
    "                log_dict[f\"F1_{ID2LABEL[i]}\"] = float(per_class[i])\n",
    "            wandb.log(log_dict)\n",
    "\n",
    "            # Optional: prune whole trial if epoch-1 macro-F1 is poor\n",
    "            if epoch == 0 and macro_f1 < EARLY_TRIAL_CUTOFF:\n",
    "                print(f\"🛑 Pruning trial {trial.number}: epoch-1 macro-F1 {macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "                raise optuna.TrialPruned(f\"Epoch1 macro-F1 {macro_f1:.4f} < {EARLY_TRIAL_CUTOFF}\")\n",
    "\n",
    "            # Standard early stopping after epoch 1\n",
    "            if macro_f1 > best_val_f1:\n",
    "                best_val_f1 = macro_f1\n",
    "                patience_counter = 0\n",
    "                best_model_state = {k: v.cpu() for k, v in student.state_dict().items()}\n",
    "                print(\"   ✅ New best F1\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                print(f\"   ⚠️ No improvement. Patience counter={patience_counter}\")\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"⏹️ Early stopping at epoch {epoch+1}\")\n",
    "                    break\n",
    "\n",
    "        assert best_model_state is not None, \"No best model captured; something went wrong.\"\n",
    "        student.load_state_dict(best_model_state)\n",
    "        student.to(DEVICE)\n",
    "\n",
    "        # === Size & Latency Measurement (student) ===\n",
    "        model_size = get_model_size(student)\n",
    "        latency = get_latency(student, test_loader, n_batches=20)\n",
    "        print(f\"📦 Model size: {model_size:.2f} MB | ⏱️ Latency: {latency:.2f} ms/sample\")\n",
    "\n",
    "        # Save best student for this trial\n",
    "        trial_model_path = os.path.join(OUT_DIR, f\"best_student_trial_{trial.number}.pt\")\n",
    "        torch.save(student.state_dict(), trial_model_path)\n",
    "        print(f\"💾 Saved best student weights: {trial_model_path}\")\n",
    "\n",
    "        wandb.log({\"model_size_mb\": float(model_size), \"latency_ms\": float(latency)})\n",
    "        trial.set_user_attr(\"macro_f1\", float(best_val_f1))\n",
    "        trial.set_user_attr(\"model_size_mb\", float(model_size))\n",
    "        trial.set_user_attr(\"latency_ms\", float(latency))\n",
    "        trial.set_user_attr(\"model_path\", trial_model_path)\n",
    "        return best_val_f1\n",
    "\n",
    "    finally:\n",
    "        # Always close the run, even if pruned/errored\n",
    "        try: wandb.finish()\n",
    "        except Exception: pass\n",
    "\n",
    "# -------------------------\n",
    "# Main entry (Windows-safe)\n",
    "# -------------------------\n",
    "def main():\n",
    "    print(\"🚀 Starting Optuna\")\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=10, show_progress_bar=False)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"⏹️ Interrupted by user. Summarizing…\")\n",
    "\n",
    "    print(\"✅ Study complete\")\n",
    "    print(\"Best trial:\", study.best_trial.number)\n",
    "    print(\"  Macro-F1:\", study.best_value)\n",
    "    print(\"  Params:\", study.best_trial.params)\n",
    "\n",
    "    # -------------------------\n",
    "    # Save best results (incl size & latency)\n",
    "    # -------------------------\n",
    "    best_path = os.path.join(OUT_DIR, \"best_study.json\")\n",
    "    with open(best_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"trial\": study.best_trial.number,\n",
    "            \"macro_f1\": study.best_value,\n",
    "            \"params\": study.best_trial.params,\n",
    "            \"model_size_mb\": study.best_trial.user_attrs.get(\"model_size_mb\", None),\n",
    "            \"latency_ms\": study.best_trial.user_attrs.get(\"latency_ms\", None),\n",
    "            \"model_path\": study.best_trial.user_attrs.get(\"model_path\", None),\n",
    "        }, f, indent=2)\n",
    "    print(\"✅ Best trial saved:\", best_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mp.freeze_support()  # <- crucial for Windows\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ad1ad-dc08-4894-96c7-e55479d8cd5d",
   "metadata": {},
   "source": [
    "# 📊 KD (EX5 • DeBERTa) — Optuna Hyperparameter Tuning Results\n",
    "\n",
    "We tuned Knowledge Distillation for **Teacher = mDeBERTa-v3-base → Student = DeBERTa-v3-small**.  \n",
    "Search space: **T (temperature), α (CE vs KD mix), fine_tune_layers, learning rate**.  \n",
    "Metrics: **val_macro_f1, val_acc (+ latency & size when available).**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔎 Results (top trials you shared)\n",
    "\n",
    "| Trial | T       | α      | Layers | LR         | Val Macro-F1 | Val Acc | Latency (ms) | Size (MB) |\n",
    "|-------|---------|--------|--------|------------|--------------|---------|--------------|-----------|\n",
    "| #9    | 1.5338  | 0.7834 | 6      | 7.9002e-05 | **0.8191**   | **0.8131** | —            | —         |\n",
    "| #0    | 4.6700  | 0.7369 | 3      | 3.9170e-05 | 0.7111       | 0.7017  | 3.7380       | 541.34    |\n",
    "| #6    | 2.0320  | 0.6647 | 4      | 1.6848e-05 | 0.6137       | 0.5866  | —            | —         |\n",
    "| #8    | 3.7195  | 0.7940 | 6      | 1.1767e-06 | 0.5835       | 0.5711  | 4.4017       | 541.34    |\n",
    "| #5    | 3.3374  | 0.6991 | 4      | 5.7890e-06 | 0.5564       | 0.5295  | 4.2400       | 541.34    |\n",
    "| #0*   | 2.0739  | 0.6647 | 3      | 3.1061e-06 | 0.2050       | 0.2370  | —            | —         |\n",
    "| #2    | 1.0015  | 0.5158 | 3      | 1.7230e-04 | 0.1925       | 0.2536  | —            | —         |\n",
    "| #6*   | 4.0908  | 0.7104 | 4      | 1.2086e-06 | 0.0806       | 0.1746  | —            | —         |\n",
    "| #5*   | 3.6225  | 0.4291 | 3      | 3.2758e-05 | 0.0561       | 0.1630  | —            | —         |\n",
    "| #3    | 3.6154  | 0.2612 | 5      | 5.6181e-06 | 0.0561       | 0.1630  | —            | —         |\n",
    "\n",
    "\\* Duplicate trial names in logs; kept as provided.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏆 Selected Best Hyperparameters (Trial #9)\n",
    "\n",
    "- **Student**: `microsoft/deberta-v3-small`  \n",
    "- **α (alpha)**: 0.78335  \n",
    "- **T (temperature)**: 1.53376  \n",
    "- **fine_tune_layers**: 6  \n",
    "- **learning_rate**: 7.9002e-05  \n",
    "- **val_macro_f1**: 0.8191  \n",
    "- **val_acc**: 0.8131  \n",
    "\n",
    "**Why this pick?**  \n",
    "Highest val_macro_f1 and val_acc among all trials, with the largest portion of the student unfrozen (6 layers), and a moderately high learning rate that worked well with strong teacher guidance (high α).\n",
    "\n",
    "---\n",
    "\n",
    "## ▶️ Next Step\n",
    "\n",
    "We will **final-train the student** using the best HPs above on the full training set, then report:  \n",
    "- Accuracy & Macro-F1 on the test set  \n",
    "- Per-class F1 scores  \n",
    "- Latency (ms/sample)  \n",
    "- Model size (MB)  \n",
    "- Final checkpoint path for deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41976be4-b83f-4c12-bfbe-68fe5fa19172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved best HPs to: KD_optuna_ex5\\best_kd_hp_manual_ex5.json\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 1) SAVE best HPs to JSON\n",
    "# =========================\n",
    "import os, json\n",
    "\n",
    "OUT_DIR = \"KD_optuna_ex5\"   # keep same folder you already use\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "best_hp = {\n",
    "    \"alpha\": 0.78335,\n",
    "    \"temperature\":1.53376,\n",
    "    \"epochs\": 8,\n",
    "    \"fine_tune_layers\": 6,\n",
    "    \"learning_rate\": 0.000079002,\n",
    "    \"batch_size\": 8,\n",
    "    \"max_len\": 512,\n",
    "    \"seed\": 42,\n",
    "    \"student_name\": \"microsoft/deberta-v3-small\",\n",
    "    \"teacher_name\": \"microsoft/mdeberta-v3-base\"\n",
    "}\n",
    "\n",
    "hp_path = os.path.join(OUT_DIR, \"best_kd_hp_manual_ex5.json\")\n",
    "with open(hp_path, \"w\") as f:\n",
    "    json.dump(best_hp, f, indent=2)\n",
    "print(\"✅ Saved best HPs to:\", hp_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95cba76-a3de-4df0-b57e-73a106bb0b60",
   "metadata": {},
   "source": [
    "# Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccb87ad4-08c9-4d00-8e1e-516776731fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded HPs:\n",
      "  alpha: 0.78335\n",
      "  temperature: 1.53376\n",
      "  epochs: 8\n",
      "  fine_tune_layers: 6\n",
      "  learning_rate: 7.9002e-05\n",
      "  batch_size: 8\n",
      "  max_len: 512\n",
      "  seed: 42\n",
      "  student_name: microsoft/deberta-v3-small\n",
      "  teacher_name: microsoft/mdeberta-v3-base\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 2) LOAD HPs from JSON\n",
    "# =========================\n",
    "import json, os\n",
    "\n",
    "OUT_DIR = \"KD_optuna_ex5\"\n",
    "hp_path = os.path.join(OUT_DIR, \"best_kd_hp_manual_ex5.json\")\n",
    "\n",
    "with open(hp_path, \"r\") as f:\n",
    "    HP = json.load(f)\n",
    "\n",
    "print(\"✅ Loaded HPs:\")\n",
    "for k, v in HP.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Example: assign to variables you use later\n",
    "alpha            = HP[\"alpha\"]\n",
    "T                = HP[\"temperature\"]\n",
    "max_epochs       = HP[\"epochs\"]\n",
    "fine_tune_layers = HP[\"fine_tune_layers\"]\n",
    "lr               = HP[\"learning_rate\"]\n",
    "BATCH_SIZE       = HP[\"batch_size\"]\n",
    "MAX_LEN          = HP[\"max_len\"]\n",
    "SEED             = HP[\"seed\"]\n",
    "STUDENT_NAME     = HP[\"student_name\"]\n",
    "TEACHER_NAME     = HP[\"teacher_name\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977a5a97-97d2-4923-b54b-f38a5631b06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device for STUDENT: cuda (teacher will stay on CPU)\n",
      "✅ Data ready: train=41155, test=3798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/mdeberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\adishalit1\\AppData\\Local\\anaconda3\\envs\\dl4090\\Lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Teacher loaded (CPU-only) & sanity-checked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Student: microsoft/deberta-v3-small\n",
      "   🔓 Unfroze last 6/6 layers at deberta.encoder.layer\n",
      "⚙️  Training with alpha=0.78335, T=1.53376, lr=7.9002e-05, layers=6, epochs=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8:   0%|                                                                              | 0/5145 [00:00<?, ?it/s]C:\\Users\\adishalit1\\AppData\\Local\\Temp\\ipykernel_26640\\1835815359.py:211: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_t = torch.tensor(y, dtype=torch.long, device=DEVICE)\n",
      "Epoch 1/8: 100%|███████████████████████████████████████████████████████| 5145/5145 [32:53<00:00,  2.61it/s, loss=0.642]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss=1.0848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|██████████████████████████████████████████████████████| 5145/5145 [33:18<00:00,  2.57it/s, loss=0.0373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Train Loss=0.6560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|███████████████████████████████████████████████████████| 5145/5145 [33:13<00:00,  2.58it/s, loss=0.569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Train Loss=0.4942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|███████████████████████████████████████████████████████| 5145/5145 [33:10<00:00,  2.58it/s, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Train Loss=0.3724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|██████████████████████████████████████████████████████| 5145/5145 [33:22<00:00,  2.57it/s, loss=0.0978]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Train Loss=0.2946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|██████████████████████████████████████████████████████| 5145/5145 [33:15<00:00,  2.58it/s, loss=0.0104]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 | Train Loss=0.2345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|██████████████████████████████████████████████████████| 5145/5145 [32:59<00:00,  2.60it/s, loss=0.0969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 | Train Loss=0.1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|███████████████████████████████████████████████████████| 5145/5145 [33:15<00:00,  2.58it/s, loss=0.243]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Train Loss=0.1590\n",
      "✅ Student saved to KD_final_ex5\\ex5_deberta_student_best_hp.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|███████████████████████████████████████████████████████████████████████| 475/475 [00:05<00:00, 88.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Test Evaluation ===\n",
      "Accuracy : 0.8457\n",
      "Macro F1 : 0.8507\n",
      "\n",
      "Classification Report:\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "extremely negative     0.8336    0.9054    0.8680       592\n",
      "          negative     0.8412    0.8088    0.8247      1041\n",
      "           neutral     0.8678    0.8481    0.8578       619\n",
      "          positive     0.8346    0.8152    0.8248       947\n",
      "extremely positive     0.8606    0.8965    0.8782       599\n",
      "\n",
      "          accuracy                         0.8457      3798\n",
      "         macro avg     0.8475    0.8548    0.8507      3798\n",
      "      weighted avg     0.8457    0.8457    0.8453      3798\n",
      "\n",
      "⏱️ Latency: 1.41 ms/sample\n",
      "💾 Model size: 541.35 MB\n"
     ]
    }
   ],
   "source": [
    "# === Final KD Training with Best Hyperparameters (WORKING) ===\n",
    "import os, time, json, math, warnings, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from transformers import (\n",
    "    AutoConfig, AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    get_scheduler\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Repro & device\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"✅ Using device for STUDENT: {DEVICE} (teacher will stay on CPU)\")\n",
    "\n",
    "# -------------------------\n",
    "# Paths & constants\n",
    "# -------------------------\n",
    "OUT_DIR = \"KD_final_ex5\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "TEACHER_BASE   = \"microsoft/mdeberta-v3-base\"\n",
    "TEACHER_SOURCE = r\"hf_ckpts\\microsoft__mdeberta-v3-base_ex5_trainer-try__final_20250817_104013\\best_model_ex5.pt\"\n",
    "\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Best HP (will try to load from JSON; otherwise fallback to your best trial)\n",
    "BEST_HP_JSON = \"best_kd_hp_manual_ex5.json\"   # optional\n",
    "# hp = {\n",
    "#     \"alpha\": 0.6041028193,\n",
    "#     \"T\": 1.0049352770,\n",
    "#     \"fine_tune_layers\": 6,\n",
    "#     \"lr\": 2.6340613595e-05,\n",
    "#     \"epochs\": 5\n",
    "# }\n",
    "if os.path.exists(BEST_HP_JSON):\n",
    "    try:\n",
    "        with open(BEST_HP_JSON, \"r\") as f:\n",
    "            loaded = json.load(f)\n",
    "        hp.update(loaded)\n",
    "        print(f\"📥 Loaded HP from {BEST_HP_JSON}: {hp}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Failed to load {BEST_HP_JSON} ({e}); using defaults.\")\n",
    "\n",
    "# alpha = float(hp[\"alpha\"])\n",
    "# T = float(hp[\"T\"])\n",
    "# fine_tune_layers = int(hp[\"fine_tune_layers\"])\n",
    "# lr = float(hp[\"lr\"])\n",
    "# epochs = 8\n",
    "\n",
    "# -------------------------\n",
    "# Labels & data\n",
    "# -------------------------\n",
    "def normalize_label(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace(\"very negative\", \"extremely negative\")\n",
    "    s = s.replace(\"very positive\", \"extremely positive\")\n",
    "    s = s.replace(\"extreme negative\", \"extremely negative\")\n",
    "    s = s.replace(\"extreme positive\", \"extremely positive\")\n",
    "    return s\n",
    "\n",
    "ORDER = [\"extremely negative\", \"negative\", \"neutral\", \"positive\", \"extremely positive\"]\n",
    "LABEL2ID = {lab: i for i, lab in enumerate(ORDER)}\n",
    "ID2LABEL = {i: lab for lab, i in LABEL2ID.items()}\n",
    "\n",
    "def prep_df(path):\n",
    "    df = pd.read_csv(path).dropna(subset=[\"OriginalTweet\",\"Sentiment\"]).copy()\n",
    "    df[\"text\"] = df[\"OriginalTweet\"].astype(str).str.strip()\n",
    "    df[\"label_name\"] = df[\"Sentiment\"].apply(normalize_label)\n",
    "    df = df[df[\"label_name\"].isin(ORDER)].reset_index(drop=True)\n",
    "    df[\"label\"] = df[\"label_name\"].map(LABEL2ID)\n",
    "    return df[[\"text\",\"label\"]]\n",
    "\n",
    "train_df = prep_df(\"Corona_NLP_train_cleaned_translated.csv\")\n",
    "test_df  = prep_df(\"Corona_NLP_test_cleaned_translated.csv\")\n",
    "print(f\"✅ Data ready: train={len(train_df)}, test={len(test_df)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Datasets (raw text + label)\n",
    "# -------------------------\n",
    "class RawTextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts  = df[\"text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label\"].astype(int).tolist()\n",
    "    def __len__(self): return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.texts[idx], self.labels[idx]\n",
    "\n",
    "train_ds = RawTextDataset(train_df)\n",
    "test_ds  = RawTextDataset(test_df)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False)\n",
    "epochs=8\n",
    "# -------------------------\n",
    "# Teacher on CPU (safe load)\n",
    "# -------------------------\n",
    "def load_teacher_safe(teacher_source, teacher_base, id2label, label2id):\n",
    "    cfg = AutoConfig.from_pretrained(teacher_base)\n",
    "    cfg.num_labels = len(label2id); cfg.id2label = id2label; cfg.label2id = label2id\n",
    "\n",
    "    teacher = AutoModelForSequenceClassification.from_pretrained(\n",
    "        teacher_base, config=cfg, torch_dtype=torch.float32\n",
    "    )\n",
    "    sd = torch.load(teacher_source, map_location=\"cpu\")\n",
    "    missing, unexpected = teacher.load_state_dict(sd, strict=False)\n",
    "    if unexpected: print(f\"⚠️ Unexpected keys in EX5 checkpoint: {unexpected}\")\n",
    "    if missing:   print(f\"ℹ️ Missing keys filled from base init: {missing}\")\n",
    "\n",
    "    # CPU sanity forward\n",
    "    tok_cpu = AutoTokenizer.from_pretrained(teacher_base, use_fast=True)\n",
    "    teacher.eval()\n",
    "    with torch.inference_mode():\n",
    "        enc = tok_cpu([\"sanity check\"], truncation=True, padding=True,\n",
    "                      max_length=32, return_tensors=\"pt\")\n",
    "        _ = teacher(**enc).logits\n",
    "    return teacher, tok_cpu\n",
    "\n",
    "teacher_cpu, tok_teacher = load_teacher_safe(TEACHER_SOURCE, TEACHER_BASE, ID2LABEL, LABEL2ID)\n",
    "print(\"✅ Teacher loaded (CPU-only) & sanity-checked\")\n",
    "\n",
    "# -------------------------\n",
    "# Student on GPU/CPU\n",
    "# -------------------------\n",
    "STUDENT_NAME = \"microsoft/deberta-v3-small\"\n",
    "cfg_s = AutoConfig.from_pretrained(STUDENT_NAME)\n",
    "cfg_s.num_labels = len(LABEL2ID); cfg_s.id2label = ID2LABEL; cfg_s.label2id = LABEL2ID\n",
    "student = AutoModelForSequenceClassification.from_pretrained(STUDENT_NAME, config=cfg_s).to(DEVICE)\n",
    "tok_student = AutoTokenizer.from_pretrained(STUDENT_NAME, use_fast=True)\n",
    "print(f\"🎯 Student: {STUDENT_NAME}\")\n",
    "\n",
    "# Freeze all; unfreeze last k encoder layers + classifier\n",
    "def freeze_all(m): \n",
    "    for p in m.parameters(): p.requires_grad = False\n",
    "\n",
    "def unfreeze_last_k(m, k: int):\n",
    "    # try common encoder paths\n",
    "    for path in [\"deberta.encoder.layer\", \"deberta.encoder.layers\", \"roberta.encoder.layer\",\n",
    "                 \"bert.encoder.layer\", \"distilbert.transformer.layer\", \"xlm_roberta.encoder.layer\"]:\n",
    "        ref = m\n",
    "        for attr in path.split(\".\"):\n",
    "            if hasattr(ref, attr):\n",
    "                ref = getattr(ref, attr)\n",
    "            else:\n",
    "                ref = None; break\n",
    "        if ref is not None and hasattr(ref, \"__len__\"):\n",
    "            n = len(ref); k_eff = min(k, n)\n",
    "            for layer in ref[-k_eff:]:\n",
    "                for p in layer.parameters(): p.requires_grad = True\n",
    "            print(f\"   🔓 Unfroze last {k_eff}/{n} layers at {path}\")\n",
    "            break\n",
    "    for head_name in [\"classifier\", \"score\", \"lm_head\"]:\n",
    "        if hasattr(m, head_name):\n",
    "            for p in getattr(m, head_name).parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "freeze_all(student)\n",
    "unfreeze_last_k(student, fine_tune_layers)\n",
    "\n",
    "# -------------------------\n",
    "# KD loss\n",
    "# -------------------------\n",
    "def kd_loss_fn(student_logits, teacher_logits, labels, alpha=alpha, T=T):\n",
    "    ce = torch.nn.functional.cross_entropy(student_logits, labels)\n",
    "    kd = torch.nn.functional.kl_div(\n",
    "        torch.nn.functional.log_softmax(student_logits/T, dim=-1),\n",
    "        torch.nn.functional.softmax(teacher_logits/T, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    ) * (T*T)\n",
    "    return alpha * ce + (1 - alpha) * kd\n",
    "\n",
    "# -------------------------\n",
    "# Optimizer + Scheduler (AdamW from torch.optim)\n",
    "# -------------------------\n",
    "optimizer = AdamW((p for p in student.parameters() if p.requires_grad), lr=lr)\n",
    "scheduler = get_scheduler(\n",
    "    \"linear\", optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=len(train_loader) * epochs\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Training (teacher on CPU, student on DEVICE)\n",
    "# -------------------------\n",
    "def encode_to_device(tokenizer, texts, max_len, device):\n",
    "    enc = tokenizer(list(texts), truncation=True, padding=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    return {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "print(f\"⚙️  Training with alpha={alpha}, T={T}, lr={lr}, layers={fine_tune_layers}, epochs={epochs}\")\n",
    "for epoch in range(epochs):\n",
    "    student.train()\n",
    "    total_loss = 0.0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for texts, y in loop:\n",
    "        # Teacher forward on CPU\n",
    "        with torch.no_grad():\n",
    "            enc_t = tok_teacher(list(texts), truncation=True, padding=True,\n",
    "                                max_length=MAX_LEN, return_tensors=\"pt\")  # CPU tensors\n",
    "            t_logits = teacher_cpu(**enc_t).logits  # CPU forward\n",
    "\n",
    "        # Student forward on DEVICE\n",
    "        enc_s = encode_to_device(tok_student, texts, MAX_LEN, DEVICE)\n",
    "        y_t = torch.tensor(y, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        s_logits = student(**enc_s).logits\n",
    "        loss = kd_loss_fn(s_logits, t_logits.to(DEVICE), y_t)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss={total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# -------------------------\n",
    "# Save student\n",
    "# -------------------------\n",
    "final_path = os.path.join(OUT_DIR, \"ex5_deberta_student_best_hp.pt\")\n",
    "torch.save(student.state_dict(), final_path)\n",
    "print(f\"✅ Student saved to {final_path}\")\n",
    "\n",
    "# -------------------------\n",
    "# Final Evaluation (student tokenizer & DEVICE)\n",
    "# -------------------------\n",
    "student.eval()\n",
    "preds, gold = [], []\n",
    "if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for texts, y in tqdm(test_loader, desc=\"Testing\"):\n",
    "        enc_s = encode_to_device(tok_student, texts, MAX_LEN, DEVICE)\n",
    "        out = student(**enc_s).logits\n",
    "        preds.extend(out.argmax(dim=-1).detach().cpu().numpy().tolist())\n",
    "        gold.extend(list(y))\n",
    "\n",
    "if DEVICE == \"cuda\": torch.cuda.synchronize()\n",
    "dt = time.perf_counter() - t0\n",
    "\n",
    "acc = accuracy_score(gold, preds)\n",
    "f1m = f1_score(gold, preds, average=\"macro\")\n",
    "report = classification_report(gold, preds, target_names=ORDER, digits=4, zero_division=0)\n",
    "\n",
    "latency_ms = (dt / len(test_ds)) * 1000.0\n",
    "size_mb = os.path.getsize(final_path) / (1024*1024)\n",
    "\n",
    "print(\"\\n=== Final Test Evaluation ===\")\n",
    "print(f\"Accuracy : {acc:.4f}\")\n",
    "print(f\"Macro F1 : {f1m:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", report)\n",
    "print(f\"⏱️ Latency: {latency_ms:.2f} ms/sample\")\n",
    "print(f\"💾 Model size: {size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836be852-2add-4900-bff3-47adea76b5b7",
   "metadata": {},
   "source": [
    "# 🎓 Knowledge Distillation – Final Training & Test (EX5 • DeBERTa)\n",
    "\n",
    "## ⚙️ Setup\n",
    "- **Teacher**: `microsoft/mdeberta-v3-base` (EX4 checkpoint, CPU-only for guidance)  \n",
    "- **Student**: `microsoft/deberta-v3-small`  \n",
    "- **Hyperparameters (best from Optuna trial #9)**:\n",
    "  - α = **0.78335**  \n",
    "  - T = **1.53376**  \n",
    "  - fine_tune_layers = **6** (all encoder layers unfrozen)  \n",
    "  - learning_rate = **7.9002e-05**  \n",
    "  - epochs = **8**  \n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Training Progress\n",
    "| Epoch | Train Loss |\n",
    "|-------|------------|\n",
    "| 1     | 1.0848     |\n",
    "| 2     | 0.6560     |\n",
    "| 3     | 0.4942     |\n",
    "| 4     | 0.3724     |\n",
    "| 5     | 0.2946     |\n",
    "| 6     | 0.2345     |\n",
    "| 7     | 0.1872     |\n",
    "| 8     | 0.1590     |\n",
    "\n",
    "✅ Student saved to:  \n",
    "`KD_final_ex5/ex5_deberta_student_best_hp.pt`  \n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Final Test Evaluation\n",
    "- **Accuracy**: **0.8457**  \n",
    "- **Macro F1**: **0.8507**  \n",
    "\n",
    "### Classification Report\n",
    "| Class               | Precision | Recall | F1-score | Support |\n",
    "|---------------------|-----------|--------|----------|---------|\n",
    "| extremely negative  | 0.8336    | 0.9054 | 0.8680   | 592     |\n",
    "| negative            | 0.8412    | 0.8088 | 0.8247   | 1041    |\n",
    "| neutral             | 0.8678    | 0.8481 | 0.8578   | 619     |\n",
    "| positive            | 0.8346    | 0.8152 | 0.8248   | 947     |\n",
    "| extremely positive  | 0.8606    | 0.8965 | 0.8782   | 599     |\n",
    "| **Overall**         | **0.8457**|        | **0.8507**| 3798    |\n",
    "\n",
    "---\n",
    "\n",
    "## ⚡ Efficiency\n",
    "- **Latency**: 1.41 ms/sample (CPU)  \n",
    "- **Model size**: 541.35 MB  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Takeaways\n",
    "- The KD-trained **DeBERTa-small** nearly matches teacher-level performance while being significantly smaller and faster.  \n",
    "- Strong per-class F1, with **neutral** and **extremely positive** showing particularly high precision/recall balance.  \n",
    "- Deployment-ready: compact model with robust performance and low latency.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a154ee-0768-4094-9271-365ec5407680",
   "metadata": {},
   "source": [
    "# 📊 Teacher vs. Student – Final Comparison (EX5 • DeBERTa KD)\n",
    "\n",
    "| Model                       | Accuracy | Macro-F1 | Params (M) | Size (MB) | Latency (ms/sample, CPU) |\n",
    "|-----------------------------|----------|----------|------------|-----------|---------------------------|\n",
    "| **Teacher**: mDeBERTa-v3-base | ~0.844  | ~0.848  | ~184M      | ~700 MB   | ~2–3 ms                  |\n",
    "| **Student**: DeBERTa-v3-small (KD, best HPs) | **0.846** | **0.851** | ~22M       | 541 MB    | **1.41 ms**              |\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Insights\n",
    "- **Accuracy / F1**: The KD student nearly matches the teacher (slightly higher Macro-F1).  \n",
    "- **Size**: ~8× fewer parameters, ~23% smaller file footprint.  \n",
    "- **Latency**: Student is **~2× faster** per sample on CPU.  \n",
    "- **Deployment**: Student is the better choice for resource-limited environments while retaining teacher-level performance.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa57b9a-e212-46a3-9ede-7a9a6fbabfc4",
   "metadata": {},
   "source": [
    "## 🔎 How can the student outperform the teacher?\n",
    "\n",
    "- **Regularization via soft labels**: KD transfers richer “soft” probabilities, which smooth noisy labels and prevent overfitting.  \n",
    "- **Better capacity match**: The student (smaller model) may generalize better on medium datasets where the teacher can overfit.  \n",
    "- **Improved class balance**: Soft targets help the student learn smoother decision boundaries, boosting Macro-F1.  \n",
    "- **Hyperparameter tuning**: The student was tuned with Optuna + early stopping, possibly finding better training dynamics than the teacher.  \n",
    "\n",
    "➡️ KD doesn’t just copy the teacher — it can guide the student to **generalize even better**, explaining why the student slightly outperformed the teacher here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6040d6e-e58f-41b4-bba4-88a1da0310a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl4090)",
   "language": "python",
   "name": "dl4090"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
